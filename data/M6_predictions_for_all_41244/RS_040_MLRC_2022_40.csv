text,target_M6_predict,target_predict_M6_label
"Some results are missing because that data is unavailable, for example, the model structure of victim C is ViT-B so that only MAE algorithm could be used(See Fig.",0,negative
"We use three self-supervised learning algorithms including SimCLR, MoCo v2 and MAE to pre-train the encoder and use ResNet-18 [35], ResNet-34, ResNet-50, VGG-16 [36], DenseNet-121 [37], ViT-B [38], ViT-L and ViT-H networks as model structures of pre-trained encoder.",2,positive
"AS a typical representation learning paradigm, selfsupervised learning(SSL) [1]–[3] has been popularly used to pre-train encoders.",1,neutral
"We have selected three representative algorithms for each of the two lines in this work, SimCLR [1], MoCo v2 [2] for the former, and MAE [3] for the latter.",2,positive
"This is because EAs are challenging to generate, especially as models become larger [9, 72, 28] and datasets more elaborate [41, 66, 44].",1,neutral
"(He et al., 2021; Caron et al., 2021), and vision-language models (Radford et al.",1,neutral
"BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2018) in NLP, and MAE (He et al., 2021) and DINO (Caron et al., 2021) in computer vision train transformer networks via self-supervision on large amounts of unlabeled data.",2,positive
", 2018) in NLP, and MAE (He et al., 2021) and DINO (Caron et al.",2,positive
Implementation details: We experiment with ViT-B and ViT-L architectures with supervised and self-supervised (MAE) initialization.,2,positive
"Point-MAE [17] and Point-M2AE [32] directly utilize Masked Autoencoders (MAE) [11], achieving superior representation capabilities.",2,positive
This paper draws inspiration from [50] that uses a similar technique for SelfSupervised Image Pre-training (SSIP).,2,positive
"MAE (He et al., 2022) employs an encoder-decoder framework to perform image reconstruction tasks.",2,positive
"On the other hand, the Masked Image Modeling paradigm (Bao et al., 2021; Xie et al., 2022; He et al., 2022; Gao et al., 2022), which is inspired by Masked Language Modeling in the field of Natural Language Processing, involves randomly masking a portion of an input image and learning to reconstruct the missing pixels based on the visible part.",1,neutral
"As a typical MIM method, Masked AutoEncoder (MAE) (He et al., 2022) represents a significant milestone for meaningful visual representation learning.",1,neutral
"In this paper, we adopt a novel perspective to explain what contributes to “a rich hidden representation inside the MAE” (He et al., 2022), focusing on analyzing its decoder’s behaviors.",2,positive
"Masked Autoencoders (MAE) (He et al., 2022) is a straightforward yet efficacious self-supervised method for pretraining Vision Transformers (ViT)(Dosovitskiy et al.",1,neutral
"To verify this, we computed the attention distance of MAE (He et al., 2022), DINO (Caron et al.",1,neutral
"Tian et al. extended their experimental context to both SimCLR and MAE, demonstrating that their newly proposed StableRep [2] pipeline (Figure 9), which utilizes Stable Diffusion to generate synthetic images before data augmentation, leads to a per-IEEE formance boost with learned representations compared to training on real images alone.",2,positive
"Self-supervised learning algorithms can be broadly categorized into two families: (1) contrastive learning that contrasts the positive pairs with negative pairs of the same image in embedding space, with SimCLR [208] being the classic work; (2) masked image modeling that leverages unmasked patches to predict masked patches, with MAE [209] being the representative.",1,neutral
"2022, 2021) with ImageNet22K, MAE (He et al. 2022b) and BEiT (Bao et al.",2,positive
"Later, various designs considering the pyramid of vision architectures are proposed with great success, e.g. Swin Transformers (Liu et al. 2022, 2021) with ImageNet22K, MAE (He et al. 2022b) and BEiT (Bao et al. 2022) with large unlabelled datasets.",2,positive
The absolute di ff erence between a prediction and the actual value is calculated for each sample in a dataset known as the L1 loss or Mean Absolute Error (MAE).,1,neutral
L1 or mean absolute error (MAE) has been calculated to train ReconResNet.,2,positive
", 2017), Masked autoencoders (MAE)(He et al., 2022) etc.",2,positive
"Di ff erent types of variational auto-encoder(Makhzani et al., 2015) methods can be used, such as Factorised Variational Auto-encoder (FactorVAE) (Kim and Mnih, 2018), Vector Quantised Variational Auto-encoder (VQ-VAE) (Van Den Oord et al., 2017), Masked autoencoders (MAE)(He et al., 2022) etc.",1,neutral
"Similarly, a very recent foundation model VC-1 [17] explores the scaling up of MAE for motor control and achieves consistently strong results across a wide range of benchmarks.",2,positive
"MVP [30] pre-trains vision transformers [4] with Masked AutoEncoder (MAE) [9] on internet-scale data, achieving strong results on dexterous manipulation tasks.",2,positive
"In contrast to previous works that also learn representations from human videos [17, 18, 30], there are two major benefits of our framework: i) H-InDex explicitly learn human dexterity by forcing the model to predict the 3D hand pose instead of predicting or discriminating pixels unsupervisedly using masked auto-encoding [9] or time contrastive learning [25]; ii) H-InDex directly adopts the off-the-shelf visual model that is designed to capture human hands rather than training large models on large-scale datasets for specific robotic tasks.",2,positive
"However, MAE has a significantly low ID classification accuracy with linear probing (He et al., 2022; Xie et al., 2022) because it reconstructs raw pixels, which means that the representation of the last stage contains more low-level information and is not suitable for classification without fine-tuning.",2,positive
"For ViT, Masked AutoEncoder (MAE) (He et al., 2022) is well known for self-supervised learning.",1,neutral
"Therefore, we did not use MAE and its variants (He et al., 2022; Xie et al., 2022; Huang et al., 2022) for comparisons.",2,positive
"masking on the image pixels [4,23] or on the discretized tokens after image quantization [6,35].",1,neutral
"Despite the lack of image-specific inductive bias (translation equivariance, locality), vision transformers can achieve state-of-the-art performance when combined with a large amount of training data [20,67,103].",1,neutral
"[103] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Other classic approaches [63], [64] based on Masked Autoencoders have also been demonstrated to learn robust image representations, but they mainly explore the Transformer architecture.",1,neutral
"learning [45], [46], [47], [48] in high-dimension space to replace labor-intensive annotation, which are collected under the umbrella of self-supervised learning.",1,neutral
"Inspired by the latest art MAE [45], we consider auto-encoder-based models are likely to contain unlimited potential.",2,positive
"CIFAR-10 CIFAR-100 ImageNet-10 SVM K-M SVM K-M SVM K-M Supervised 11.5 95.1 95.1 75.9 73.6 96.4 96.3 Random 11.5 42.9 22.0 18.3 8.9 48.2 28.3 DINO (Caron et al., 2021) 11.5 89.7 63.9 65.6 36.7 87.8 68.0 NNCLR (Dwibedi et al., 2021) 11.5 91.7 69.3 69.7 40.4 91.4 66.8 SimCLR (Chen et al., 2020) 11.5 90.6 75.3 65.6 41.3 89.0 65.7 BYOL (Grill et al., 2020) 11.5 93.1 75.0 70.6 42.8 90.4 67.3 SWAV (Caron et al., 2020) 11.5 89.1 64.5 65.0 35.2 90.0 61.9 MAE (He et al., 2022) 20.4 82.3 37.0 57.1 17.9 88.4 45.8 DDPM (Ho et al., 2020) 41",0,negative
", 2020) and MAE (He et al., 2022), our model demonstrates superior efficiency by utilizing fewer parameters (11.",2,positive
", 2018; 2019), and many recent works (Dosovitskiy et al., 2020a; He et al., 2022) show it is also a promising architecture on many computer vision tasks like image classification (Touvron et al.",2,positive
"On a different note, the masked autoencoder (MAE) [22] introduces a different way to tackle self-supervised learning.",1,neutral
"Notably, several MIM methods, including iBOT [64], SimMIM [57], and MAE [22], have demonstrated promising results in this domain.",1,neutral
"Interestingly, we find these quantities can be powerful tools in understanding and improving existing selfsupervised methods regardless of whether they are contrastive, feature decorrelation-based, or masking-based [22].",1,neutral
We shall briefly introduce MAE [22] as an example.,1,neutral
"Both contrastive methods [17, 20, 61, 53, 25, 48, 39] and generative models [52, 19, 49] have achieved great success in unsupervised representation learning.",1,neutral
", 2021)) to various downstream tasks (He et al., 2019; Radford et al., 2021; He et al., 2022; Brown et al., 2020).",2,positive
"We employ commonly utilized augmentations, such as resizing, crop, rotation, color jitter, translation, and horizontal flip, in conjunction with the masked autoencoder [11] to alter the feature space.",1,neutral
"Moreover, previous studies have shown that language tokens are typically less redundant and have a higher information density in their representation [13, 37].",1,neutral
20 words per sentence) and high information density [13].,1,neutral
"Inspired by MaskAE [13] approaches, we demonstrate that we can maintain a comparable fine-4 tuning model performance by retaining only a small group of vision tokens.",2,positive
"Inspired by MaskAE [13] approaches, we demonstrate that we can maintain a comparable fine-",2,positive
"Existing methods can generate 2D human via deep generative models [16, 43, 34].",1,neutral
"It is an extension of masked image modeling (He et al., 2022) processing pairs of images (x,x′), which correspond to two different views of the same scene with important overlap.",1,neutral
"Recent pre-trained visual encoders, like DINO (Caron et al., 2021) and masked autoencoders (MAE) (He et al., 2022), have been used in (Yadav et al., 2022) and (Yadav et al., 2023), respectively.",2,positive
", 2021) and masked autoencoders (MAE) (He et al., 2022), have been used in (Yadav et al.",2,positive
"Keeping the original ViT-base structure also allows us to make use of image-based pre-training either contrastive [53, 56] or self-supervised [6, 28].",2,positive
"(3) Finally, the ViT benefits from large-scale pre-trained image models on vision-language tasks [53] or self-supervised objectives [28].",1,neutral
"Vision models pretrained with self-supervised learning (SSL) often outperform supervised models on downstream tasks [24, 5, 10].",1,neutral
"Alternatively, masked autoencoders [24] are trained by reconstructing masked patches from unmasked patches of an image (Fig.",1,neutral
A 25-million parameter decoder [24] is used for patch reconstructions.,1,neutral
"In this work, we train masked autoencoders (MAEs) [24] on progressively larger HCS image sets and show that these models are scalable learners of cellular morphology, outperforming previous SOTA methods at inferring known biological relationships in whole-genome HCS screens.",2,positive
"MAEs have been successfully applied to images [24], audio [28], video [20] and multimodal audio-video datasets [27].",1,neutral
"As MAE is a self-supervised method, we use all three datasets as well as Cityscapes [58] as external data for the pre-training process.",2,positive
"As illustrated in Table IX, it becomes evident that MAE-based pre-training consistently outperforms other methods across all three datasets.",1,neutral
"Additionally, we incorporate Masked Autoencoders (MAE) [28], a self-supervised pre-training algorithm, to foster enhanced fine-tuning of the ViT [19] on facade-centric data.",2,positive
These findings provide robust evidence of the effectiveness of MAE-based pre-training for enhancing the performance of models in facade segmentation tasks ViT Structure The configuration of the ViT model plays a pivotal role in influencing the performance of our RTFP.,2,positive
"Notably, the incorporation of the pre-training method, MAE, amplifies the prowess of ViT even further.",2,positive
MAE is implemented based on its official training pipeline.,2,positive
We thus adopt an MAE [28] pre-training to initialize ViT methods.,2,positive
"Notably, we introduced MAE pre-training specifically tailored for facade segmentation, marking the first application of this approach in the context of facade-related tasks.",2,positive
We thus adopt an MAE He et al. (2022) pre-training to initialize ViT methods.,2,positive
"Unlike the traditional Masked AutoEncoders[31] that can only learn the data structure by reconstructing from raw sensor data, our FMAE can recover cross-modality latent features inside the original multimodal model.",2,positive
The masked autoencoder (MAE) is a cutting-edge method for self-supervised computer vision tasks [31].,1,neutral
The MAE [31] model masks randomly selected patches of the input image before it is fed into the encoder transformer.,2,positive
"By learning the representations of masked atoms via global interpolation of visible atoms, Masked Autoencoders can capture the topology information that what atoms are close to or connected with each other.",1,neutral
"As a ground-breaking self-supervised paradigm for pre-training, the Masked Autoencoders [48] achieves state-of-the-art on various downstream tasks in the field of computer vision and natural language processing.",1,neutral
One particular approach is MAE [33] which first masks large portion of the input (e.,1,neutral
We initialize the encoder weights with the self-supervised ImageNet pre-training [33].,2,positive
"More notably, our approach improves over MAE [33] which is also pre-trained with ImageNet and ScanNet (+2.",2,positive
MAE [33] RGB ViT-B ImageNet+ScanNet RGB 64.,1,neutral
"Pre-training Vision Transformers [24] (ViTs) using Masked Autoencoders (MAE) followed by fine-tuning gives rise to the state-of-the-art results for various computer vision tasks such as image classification [9, 33, 66], video activity recognition [26, 60, 62], semantic segmentation [27,36,45] and 3D scene understanding [14,52,70].",1,neutral
MAE [33] RGB ViT-B ImageNet+ScanNet RGB 48.,1,neutral
"Following [33], we first normalize the output patches as well as target patches, and then used compute the MSE loss between the ground truth and the predicted pixels.",2,positive
"Inspired by the success in NLP domain such as bidirectional encoder (BERT) [21] and Generative Pre-Training (GPT) [54], several masked image prediction methods for pre-training vision transformers have been proposed with various reconstruction targets such as pixels [4, 15, 24, 25, 33, 66], features [6, 63], discrete tokens via dVAE [9, 74].",1,neutral
"Inspired by masked autoencoders (MAE) [8], a promising generative SSL approach in the computer vision domain, we propose Spatial-Temporal Masked AutoEncoders (STMAE), a versatile framework that is able to elevate the capability of existing spatial-temporal models in MTS forecasting.",2,positive
"Masked autoencoders (MAE) [3, 8, 5] are a type of generative SSL method used for robust feature representation learning.",1,neutral
"proposed MAE [8], an asymmetric vision Transformer-based autoencoder to learn robust image representations by reconstructing masked images.",1,neutral
"Drawing inspiration from MAE works in other domains [3, 8], STMAE employs a two-stage training scheme comprising pretraining and fine-tuning.",2,positive
"Self-supervised learning (SSL) uses auxiliary or pretext tasks with training objectives similar to those used for supervised learning while obtaining supervision signals directly from unlabeled data by making neural networks predict withheld or altered parts or properties of the inputs [13, 18, 35].",1,neutral
"Improvements in self-supervised representation learning [13, 18, 23, 31, 33, 35, 52, 75] have driven significant improvements in anomaly detection [38, 61].",1,neutral
"Numerous architectural improvements and training techniques have been proposed to address the challenges associated with achieving robustness against various domain variations [12, 21, 45].",2,positive
"These approaches include pretraining techniques [21, 45, 65], finetuning methods [34, 62], and",1,neutral
"Following [2, 10], we use a shared, learnable mask token as the initial embedding of each masked value.",1,neutral
"In this paper, we present REMASKER, a novel method that extends the masked autoencoding (MAE) framework [2, 10] to imputing missing values of tabular data.",2,positive
"Unlike conventional MAE, due to the naturally missing values in tabular data, relying on re-masked values provides limited supervisory signals.",1,neutral
"By extending the siamese form of MAE [15], we show that R E M ASKER encourages learning missingness-invariant representations of input data, which requires a holistic understanding of the data even in the presence of missing values.",1,neutral
"In this paper, we present R E M ASKER , a novel method that extends the masked autoencoding (MAE) framework [2, 10] to imputing missing values of tabular data.",2,positive
"However, unlike conventional MAE, as the data in the imputation task is inherently incomplete ( i.e. , naturally masked), we employ a “re-masking” approach that explicitly accounts for this incompleteness in applying masking and reconstruction.",1,neutral
This finding is different from the vision domain in which computing the loss on unmasked image patches reduces accuracy [10].,1,neutral
"Particularly, the seminal MAE [10] represents the state of the art in self-supervised pre-training on the ImageNet-1K benchmark.",2,positive
"Similar to [10], we use an asymmetric design with a deep encoder and a shallow decoder (e.",2,positive
"The R E M ASKER imputer extends the MAE framework [3, 1, 10] that reconstructs masked components based on observed components.",2,positive
Conventional MAE focuses on representation learning and uses the decoder only in the training phase.,2,positive
"The REMASKER imputer extends the MAE framework [3, 1, 10] that reconstructs masked components based on observed components.",2,positive
"This may be explained by that with higher feature correlation (or more information redundancy), a larger masking ratio often leads to more effective representation learning, which corroborates the existing studies on MAE [10].",1,neutral
"Combining self-supervised learning [7, 9, 17, 18] with VLMs has been previously explored by different openvocabulary segmentation methods [37, 40, 43].",1,neutral
"Inspired by [21,37,43,22,23], the pixel correspondence can be learned in a self-learning manner, and previous work [11,27,36] also verified that a high proportion of pixels in an image (e.",1,neutral
"Regular ViT assumes complete knowledge of the image, while many derivative works mask or remove a portion of input tokens [9, 16].",1,neutral
"Some of them were already considered, like ViT resiliency to missing data [9] has already been conducted [16, 18, 14, 24], but their cumulative effect on performance is unknown.",0,negative
The vanilla ViT-Base [15] model pre-trained with MAE [15] is adopted as the backbone for our encoder.,2,positive
"ondly, it offers convenience for feature extraction in deep networks [8, 15, 17] due to its regular orientation, allowing for easy subdivision of a rectangular window into a matrix of pooled cells.",1,neutral
"For instance, MAE [17] as a seminal work proposes an autoencoder architecture where the transformerbased encoder turns the unmasked image patches into feature representations, which are further decoded back to the original image; while SimMIM [37] encodes the entire image, including the masked patches, and predicts the missing region with a lightweight one-layer head.",1,neutral
"the influential work from masked autoencoder (MAE) [17] and the related ones such as SimMIM [37], BEiT [1], and iBOT [41]).",1,neutral
"As described in lines 101-113 in our main manuscript, and we would like to clarify again here: most existing studies of adopting masking operations (together with self-reconstruction objective) to realize self-supervised learning are based on the transformer backbone thanks to the to-kenized input (where the masking is simply to block out some tokens), and the prior works (e.g. SemMAE [22], MST [23], BEiT [1], iBOT [41], MAE [17], and Sim-MIM [37]) are designed for transformers as well.",2,positive
"SemMAE [22], MST [23], BEiT [1], iBOT [41], MAE [17], and SimMIM [37]) are designed for transformers as well.",1,neutral
"convolutional neural networks, has difficulty incorporating the random masking operation (on image patches), because the resultant edges between masksed and unmasked regions could cause problems for learning convolution kernels, and the nature of performing convolutions on regular grids also hinder it from adopting positional embeddings or masked tokens as the typical transformer models [17].",1,neutral
"Moreover, even there exists some transformer-based prior works adopting the saliency operations as well, the ways of their applying saliency masking are also different from ours: For instance, SemMAE [22] requires a two-stage training process to determine where to apply the mask, while our approach achieves the same goal with a single feature extractor and end-to-end training; MST [23] also aims to avoid masking important objects, while our method of explicitly distributing masked patches across foreground and background empirically leads to better performance.",2,positive
"Nevertheless, the direct adaptation of such techniques (especially masking and self-reconstruction) to image data [28] only contributes to slight improvement (at least not as significant as what happens in the field of natural language processing), in which such a predicament was later relieved with the help of vision transformers [8] (e.g. the influential work from masked autoencoder (MAE) [17] and the related ones such as Sim-MIM [37], BEiT [1], and iBOT [41]).",1,neutral
"For instance, MAE [17] as a seminal work proposes an autoencoder architecture where the transformer-based encoder turns the unmasked image patches into feature representations, which are further decoded back to the original image; while SimMIM [37] encodes the entire image, including the masked patches, and predicts the missing region with a lightweight one-layer head.",1,neutral
"To pretrain the ViT, we adopt the MAE training scheme [He et al., 2022].",2,positive
", 2022) include video data in the robot learning pipeline by performing self-supervised visual representation learning on video data (He et al., 2021), followed by downstream policy learning via behavioral cloning using the learned representations.",2,positive
"Moreover, noise injection techniques have also been extended to image learning, where they have been empirically confirmed to be effective in enhancing model performance (21-22).",1,neutral
"Recently, MAE [47] has been introduced for learning informative visual representations through the utilization of local complementary information.",1,neutral
"MAE [15]), it is really a challenge to assemble such large annotated medical image datasets due to the extensive and burdensome annotation effort and the requirement of expertise.",2,positive
"Although well pre-trained models can dramatically improve the performance of downstream segmentation task (e.g. MAE [15]), it is really a challenge to assemble such large annotated medical image datasets due to the extensive and burdensome annotation effort and the requirement of expertise.",1,neutral
"Our method is compared with the State-Of-The-Art (SOTA) class-agnostic SVR method MAGE [21] that uses spatial-masking MIM, and with the SOTA class-dependent SVR method AdaCode [23] that uses a dense weight map.",2,positive
"Our M-AdaCode can also be seen as a method of Masked Image Modeling (MIM) [14,21].",1,neutral
Early meth-ods like MAE [14] and CMAE [15] favor the performance of the representations on downstream tasks instead of the quality of the reconstructed images.,1,neutral
"One example of this baseline SVR-based compression method is MAGE [21], which uses MIM to learn a general SOTA visual codebook for general image reconstruction with very low bitrates.",1,neutral
MIM has been shown effective in learning HQ visual representations via self-supervised learning.,1,neutral
"The recent MAGE [21] learns a generic VQGAN representation by a single token-based MIM framework with variable masking ratios, which improves unconditioned image generation performance.",2,positive
"From another perspective, our M-AdaCode can be seen as an MIM method.",2,positive
"For example, MIM is combined with product quantization of VQ-VAE in [10] to achieve extreme compression rates.",1,neutral
Early methods like MAE [14] and CMAE [15] favor the performance of the representations on downstream tasks instead of the quality of the reconstructed images.,1,neutral
ViT’s datahungry nature [9] often leads to forget pre-trained knowledge,1,neutral
"data-hungry nature [9] poses challenges when training with limited Deepfake data, such as FaceForensics++ [10].",2,positive
", 2022), and knowledge data extraction for Large Language Models (LLMs) (Brown et al., 2020; Chung et al., 2022; He et al., 2022).",2,positive
"In order to match dimension with audio and text embeddings, we pass the output from SATMAE encoder to a ReLU activation followed by a 512-dimension linear layer.",2,positive
SATMAE [7] was pre-trained on large-scale (over 700K) satellite imagery of the world.,0,negative
"For overhead imagery, we adopt the same data augmentation as SATMAE [7].",2,positive
"In the recent years, masked auto-encoders (MAE) [18] based models trained on satellite imagery have demonstrated to be a good starting checkpoints to be finetuned for various downstream tasks [7, 34].",1,neutral
"In our work, we start with the pre-trained weights of Vision Transformer (ViT) [11] encoder of SATMAE [7] as the overhead-image encoder for GeoCLAP.",2,positive
"For encoding overhead image, we use the pre-trained vit_base_patch16 encoder of SATMAE [7].",2,positive
"IN autonomous driving, large-scale semantic instance annotations of real-world scenes are foundational for bootstrapping perception models [17], [26], [65].",1,neutral
Masked autoencoders (MAE) [25] represent another type of self-supervised learners that learn to reconstruct areas in an image that have been masked.,1,neutral
"After pretraining, MAEs can be fine-tuned for various downstream tasks.",1,neutral
"Prior research demonstrates that the pretrain-then-finetune paradigm is an effective and scalable method of building multitasking algorithmic systems for speech [1, 2], audio [3, 4], and vision [5, 6].",1,neutral
"[32] introduced a feature-masking task to encourage feature interactions, which inspired us to use methods in CV [17] and NLP [18] to stimulate feature interactions in the learned scene representation.",1,neutral
"In our work, we adopt a method similar to MAE [17], where the mask tokens are not passed to the encoder.",2,positive
"Concurrently to our work, [30], [31] used MAE to pre-train the encoder.",2,positive
We adapt MAE [17] into a self-supervised pre-training framework for trajectory forecasting.,2,positive
the existence of a masked object to be reconstructed [17].,1,neutral
"Further, in self-supervised pretraining, we substitute the supervised pre-training task with self-supervised masked reconstruction tasks [17] tailored to trajectory forecasting, i.",1,neutral
MAEST [14] proposes an autoencoder architecture inspired by MAE [16] for hyperspectral data.,2,positive
"Recently, Masked Image Modeling (MIM) [16], [17] has been introduced as a simple yet powerful self-supervised pre-training framework for Vision Transformers (ViT) without",1,neutral
"Since meaningful augmentations in spectra of HSIs are challenging and also large negative samples cannot be generated due to data limitation, MIM approaches are more suitable for hyperspectral data compared to contrastive learning approaches.",1,neutral
"Inspired by Masked Image Modeling (MIM), we design a new pre-training strategy for both spectral and spatial transformers.",2,positive
"Masked Image Modeling (MIM) approaches captures meaningful representations by predicting the masked-out regions of images and have shown promising results for ViTs [16], [17].",1,neutral
"Recently, Masked Image Modeling (MIM) [16], [17] has been introduced as a simple yet powerful self-supervised pre-training framework for Vision Transformers (ViT) without the use of labels.",1,neutral
"To address this problem, different techniques like self-supervised learning [14], [16], [21] and domain adaptation [22]–[24] have been explored in recent literature.",1,neutral
"However, the MIM pre-training strategies often have been limited to a single modality (RGB).",1,neutral
"- The teacher model can be pre-trained using advanced generative methods, such as MAE [22], CAE [5], BEiT [1], as a result, the student model can learn the visual representation from a heterogeneous teacher via sparse convolution and hierarchical structure.",1,neutral
"In contrast, (a) Transformer-based generative methods (like MAE [22], BEiT [1]) have no such side effect due to the ability to process variable-length input (shown in blue).",1,neutral
"However, they have difficulty in learning the knowledge from single-scale masked modeling possessed by heterogeneous MIM teacher models (such as CAE [5], MAE [22]) and in processing masked images (discussed in Section 3.",1,neutral
"The MAE [22] paper studies masking ratio and visible patches, while CAE [5] focuses on learning capability.",2,positive
We also apply MAE [22] and BEiT [1] as teacher networks.,2,positive
"We first pre-train a MIM model, which could be MAE [22] or BEiT [1] as the teacher model.",2,positive
[17] proposed Masked Autoencoders (SSL-MAE) for ViT backbone models,2,positive
"2 (a), leveraging the capabilities of the transformer network, especially the ViT [18] that has been pre-trained by mask-image-modeling(MIM) [19], [20].",2,positive
"fusion within the backbone network, enjoying the powerful mask-image-modeling (MIM) pretraining method [19], [20], [36].",1,neutral
"For setting a solid baseline, we enhanced OSTrack by substituting its MAE [19] pretrained weights with those of CAE [20], the outcome of which is enumerated in Tab.",2,positive
"While some methods rely only on one modality such as images [6, 23] or text [51], there is also increasing interest in multi-modal representations [50, 30, 56, 32, 37, 55] which require multi-modal aligned pairs.",1,neutral
"Representation learning [50, 31, 10, 23, 68, 6, 65] aims to obtain general representations that improve performance on downstream tasks such as retrieval [38, 49, 65, 31], classification [10, 68, 6], segmentation [6], question-answering [31, 65] and captioning [31, 65].",1,neutral
"Masked Autoencoder for Images (MAE) [7] adopts the idea of masking tokens in BERT for NLP, transforming an image into multiple tokens and masking some of them, allowing the model to predict the masked tokens.",1,neutral
"[7], shows that the concept of masked tokens can also be applied to vision representation learning and it turns out to be effective and high accuracy.",1,neutral
"The Masked Autoencoder, proposed by [2], uses a masking encoder to reconstruct missing pixels in images during the pretraining phase, resulting in better performance",2,positive
"Our approach is an extension of the masked autoencoder [1], [2] for time-sequential trajectory data and aims to",2,positive
"The technique of random masking has demonstrated its effectiveness in various fields, such as natural language processing (NLP) and computer vision (CV), as evidenced by models like BERT [1] and Masked Autoencoders [2] in conjunction with Vision Transformers (ViT [3]).",1,neutral
"Secondly, this paper designs a frequency domain masked image modeling (FD-MIM) that adapts to high-frequency and low-frequency information of RS images, which improves the pretraining effect of lightweight foundation models by combining self-supervised learning [32, 33].",1,neutral
"Masked image modeling (MIM) [32, 33] can combine the intrinsic data relationship to guide the model to understand complex RS images better.",1,neutral
"Recently, generative learning has also been developed in the computer vision area, like MAE [20] and SimMIM [21] introduced the ViT architecture and proposed a MIMbased training method.",2,positive
"MIM Many works have proved that reconstructing the masked patches is a meaningful self-supervised task which drives the network to learn the latent representation of the image, such as MAE [20] and SimMIM [21].",1,neutral
"[20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Given the color augmented image xc, the loss Lc of this view can be calculated as same as the main loss:
Lc = H(Pt(x), Ps(xc)), (3)
MIM Many works have proved that reconstructing the masked patches is a meaningful self-supervised task which drives the network to learn the latent representation of the image, such as MAE [20] and SimMIM [21].",1,neutral
"In computer vision, multiple works [8], [10], [23] have proven the effectiveness of ViT as the encoder to extract deep features from large-scale image/video datasets.",1,neutral
"Following the standard MAE-based methods [23] [10], an MSE loss is employed to train the model.",1,neutral
"Therefore, the autoencoding-based strategy [25] is proposed, including the ImageMAE [10].",2,positive
"In addition, CAV-MAE focuses on contrastive learning for feature alignment, which highly relies on data quality and data augmentation [10].",2,positive
Previous work based on mask autoencoders such as ImageMAE [10] and VideoMAE [23] has proven the effectiveness of learning representations in images and videos through reconstruction strategy in the visual modality.,1,neutral
"Recently, the ViT-based singlemodal Masked Autoencoders (MAE) [10], [13], [23] have",1,neutral
This considers the information redundancy in the visual modality mentioned in MAE-based models [10].,1,neutral
"Moreover, this model focuses on the contrastive learning approach to align the distribution of multi-modality features, which highly relies on data quality and data augmentation [10].",1,neutral
"The models based on MAE [10], [13], [23] have shown a strong ability to learn representations efficiently in a single modality (visual/audio).",1,neutral
"Demandconditioned features are concatenated with a bounding box and logits and input into a Transformer Encoder, then passed into a Transformer Decoder with Demand BERT features and global visual features (encoded by a pre-trained Vision Transformer [57, 58]).",2,positive
"• Demand-Driven Navigation Dataset 8.1
– Generation Process 8.1.1 – Statistical Features 8.1.2
• Demand-Driven Navigation Method 8.2
– Textual Attribute Feature Learning 8.2.1
* Language-grounding Mappings 8.2.1 * t-SNE of Demand-conditioned Attribute Feature 8.2.1
– Policy Learning 8.2.2
* Pipeline in Transformer 8.2.2 * Trajectory Collection 8.2.2 * Demand-based Visual Grounding Model 8.2.2
– Model Pre-training and Fine-tuning(8.2.3
* Image Encoder: Vision Transformer 8.2.3 * DETR 8.2.3
• Experiments 8.3
– Metrics 8.3.1 – Baselines 8.3.2
* VTN-object, VTN-demand, VTN-GPT 8.3.2 * ZSON-object, ZSON-demand, ZSON-GPT 8.3.2 * GPT-3+Prompt, MiniGPT-3 8.3.2
• Ethics 8.4",2,positive
"Image Encoder: Vision Transformer We use the mae_vit_large_patch16 version of Vision Transformer [57, 58] as the Image Encoder to obtain global visual features.",2,positive
"image representation learning methods, the constrative learning method SimCLR [7] and the masked autoencoders [8], as baselines.",1,neutral
"Recent image representation learning methods, such as contrastive learning [7] and masked autoencoders [8], have garnered considerable interest.",1,neutral
We adopt a commonly used linear probing protocol [8] for evaluation.,1,neutral
"The pre-training comes in the form of initializing the inverse dynamics model with weights from VC-1 [33], a vision-transformer (ViT-B) [10] trained on ego-centric images with masked-autoencoding objective [17].",2,positive
"We use the Masked Autoencoder (MAE) architecture [18], which enables efficient pretraining.",2,positive
All our models use only 2 layers in the decoder as in [18].,2,positive
"Masked autoencoders (MAE) were proposed in [18] as a self-supervised model for computer vision and an alternative to Vision Transformers (ViT), and have been recently used Fig.",1,neutral
"We use the assymetric encoder-decoder architecture proposed in [18], with a lightweight decoder and a larger encoder.",2,positive
"We choose YOLO over a masked autoencoder model [15] since the latter focuses more on image reconstruction, which is not aligned with our pressure map data in signal format.",2,positive
"Afterward, a large number of ViT variants and related techniques have been proposed, such as [2, 9, 13, 19, 22, 31, 32, 35].",1,neutral
"Moreover, multi-modal training [59] and self-supervised learning [28] also accelerate the broad use of ViT.",1,neutral
"To mimic the high-quality features by the fusion features, utilizing L1 and L2 to measure the feature distance is acceptable [13, 44].",0,negative
We initially selected the MAE trained solely on the image modality as our baseline model.,2,positive
We then incorporated CLIP into MAE (denote as +CLIP) to achieve granular alignment.,2,positive
MAE [4] employs a random masking technique on image patches within the input data.,1,neutral
"By engaging in the reconstruction process, MAE is able to learn image features that can be subsequently utilized for down-stream tasks.",2,positive
"They carefully design self-supervised pretext tasks, such as using masked autoencoders (MAE) to obtain robust image representations [4], and then integrate them with the text branch to enhance the performance of down-stream fine-tuning tasks [6, 7].",1,neutral
"Since MAE lacks the ability to interact with reports, the grounding capabilities is limited.",0,negative
"Here, we adopt MAE proposed by [4] as our primary image representation extractor.",2,positive
"Drawing inspiration from MAE, [6] employ a masked mechanism in both the reports branch and the image branch of their models (MRM).",2,positive
MAE achieved AUC of 81.3% on the RSNA dataset with 1% annotations.,0,negative
"Emphasis will be placed on widely adopted techniques such as MAE and CLIP, along with their associated derivative methods.",2,positive
"By deploying simple task-based heads with well-learned feature representations obtained from the foundation model, such methods can achieve good performance in specific tasks without requiring that much of manual annotations compare to custom deep-learning training process [4].",2,positive
"For both the pre-training and fine-tuning of the image classification task, we warmed up the network by linearly increasing the learning rate to the set value, and then decreased the learning rate using the cosine decay schedule, following the approach reported in MAE.",2,positive
"The architecture typically contains an transformer encoder that processes nonmasked patches, follows by a decoder, usually a lightweight transformer, that reconstructs the original patches (He et al., 2022).",2,positive
The utilization of Masked Autoencoders [23] brought forth the integration of self-supervised learning in the field of computer vision.,1,neutral
", 2010) and MAE (He et al., 2022)), predictive methods (e.",1,neutral
"Following the literature, it can be categorized into three main types: generative methods (e.g. Autoencoder (Vincent et al., 2010) and MAE (He et al., 2022)), predictive methods (e.g. predicting rotation angles (Gidaris et al., 2018)) and contrastive methods (joint embedding architectures with or without negative samples).",1,neutral
"Pre-training is proven effective on many high-level vision tasks [15], [78], [79].",1,neutral
"Vision models later incorporated a BERT-like pre-training inspired approach for images [20, 21] to capture the relationship between patches to achieve state-of-the-art performance in Self-supervised learningbased image recognition benchmarks.",2,positive
SAM uses a fine-tuned pre-trained MAE [6] (Masked AutoEncoder) ViT [3] (Vision Transformer) which runs only once for each image.,2,positive
", 2023), has become pivotal in vision (He et al., 2021; Zang et al., 2022b), natural language processing (Rethmeier & Augenstein, 2023), and biological (Yu et al.",2,positive
Masked image modeling has made a resurgence as an effective self-supervised learning (SSL) technique [8] [26] using a transformer-based encoder-decoder architecture.,1,neutral
"Compared with convolutional neural networks (CNNs), which encourage locality, weight sharing, and translation equivariance, transformers build global dependency through self-attention layers, bringing more possibilities for feature exaction and breaking the performance ceiling of CNNs in return [2,3,4,5,6].",1,neutral
"For mask image modeling, we explore several state-of-the-art methods as baselines,
including MAE, Maskfeat, SimMIM, ConvMAE, BEiT v2, and BootMAE.",2,positive
"While the most direct approach involves comparing absolute reconstruction losses, such as the absolute reconstruction loss Lpred calculated by feeding an image patch into the reconstruction network (e.g., using L2 loss as in MAE), it is insufficient when evaluating the importance of a single patch among all patches.",1,neutral
"MAE (He et al., 2022) utilizes an asymmetric encoder-decoder structure, dividing the image into equal-sized blocks and predicting the masked block based on the unmasked block of the image.",2,positive
This approach is based on the observation that MAE consistently employs a masking ratio of 75% throughout training.,0,negative
"In contrast to the commonly used high fixed masking ratio found in studies like MAE and SimMIM, we introduce a novel adaptive masking ratio strategy.",2,positive
"SimMIM (Xie et al., 2022) extends the work of MAE by adjusting decoder weights and incorporating visible and masked patches as input, achieving results comparable to MAE while expediting the pre-training process.",2,positive
"The initial masking ratio, denoted as σ0, is set at 25%, a value determined based on experimental findings in MAE.",0,negative
"ConvMAE (Gao et al., 2022) enhances semantic information acquisition through MAE-based multi-scale encoding operations.",2,positive
"BootMAE (Dong et al., 2022) introduces a supervised Bootstrapped Masked Autoencoder, using Masked Autoencoding objectives for prediction tasks on patch-level features of image blocks and introducing an additional supervised bootstrapping signal.",2,positive
"Secondly, reconstruction-based methods like MAE, MaskFeat and SimMIM exhibit finer segmentation details and improved edge alignment over contrastive counterparts.",1,neutral
"In the context of medical images, FreMAE (Wang et al., 2023) addresses the challenge of distinguishing formal foreground from less useful background.",1,neutral
"To investigate the effectiveness of AMLP, we conduct experiments on two datasets and compare its performance with two state-of-the-art baselines: Fully Supervised Baseline (i.e., Fully Supervised) and Self-Supervised Baselines (i.e., SimCLR, BYOL, SwAV, MAE, Maskfeat, SimMIM, ConvMAE, BEIT V2, BootMAE).",2,positive
"Previous studies have largely adopted strategies from the vision and language domains [7, 8], where atom attributes are randomly masked with a predetermined ratio.",1,neutral
"A standard error function used for masked autoencoders within computer vision [7, 32, 33] is the cross-entropy loss, whereas previous GNN solutions utilize mean squared error (MSE) [12, 34, 35, 36].",1,neutral
"Masked autoencoders have found success in vision and language domains [7, 8] and have been adopted as a pre-training objective for graphs as the reconstruction task is able to transfer structural pattern knowledge [4], which is vital for learning specific domain knowledge such as valency in material science.",1,neutral
"Different from the direct reconstruction methods [11, 25] with pixel loss, we aim to reconstruct the mask regions that have the most similar feature map with the original image.",2,positive
"For mask strategy, we follow the setting of MAE [11], and only the unmasked token is used during pre-train.",1,neutral
"In detail, we use a pre-trained ViT-b/16 (with MAE [11] on ImageNet 1K for 1600 epochs) as initial parameters and refine on LAIONFACE-cropped for 16 epochs with our Mask Contrastive Face.",2,positive
"Instead of minimizing the pixel loss as previous methods [11, 25], we minimize the feature map difference between the original face image I and the pseudo image outputted byM0.",1,neutral
"Different from other image reconstruction mask image modeling [11, 25], instead of using pixel loss as a training objective function, the training goal of our method is to fit the output image feature map with the input image feature map.",2,positive
"(1) L2, a mask image modeling with L2 loss, which is close to MAE [11], and in order to learn the globe face identity, we remove the random resize crop operation during data augmentation; (2) L2 + Lmim , which directly adds theM1 decoder at the end of MAE decoder; (3)Lmim , a mask image model with 8-layerM0 and aM1 decoder; (4) Lmim , Table 7: Comparison with previous representation learning.",2,positive
SimMIM [25] and MAE [11] propose a simple auto-encoder structure for model pretrain and achieve impressive results with less computation resource consumption than the previous method.,2,positive
"In addition, with the popularity of [40, 41], We will further investigate how to pre-train the Transformer-based model on unlabelled medical images by a self-supervised approach and combine it with our proposed methods.",2,positive
"and corresponding GSR sequence can vary with each iteration, implying that new training samples can be generated regardless of data augmentation [55].",1,neutral
"In this work focus on joint-embedding (involving two transformed views) self-supervised encoders [3, 9, 10, 11, 12, 13, 18, 48] and not masked image models [22].",1,neutral
"While few-shot learning is widely studied in the computer vision community [26], video few-shot learning is less explored [4].",1,neutral
"OTAM [4] uses temporal alignment to improve few-shot classification for videos, using a distance metric to compare frames of the queries and the support set.",2,positive
"S1), we used a diverse set of training dataset and employed the Masked Autoencoders (MAE) method [He et al., 2022] as the Pre-training method.",2,positive
This choice was motivated by the fact that the masking strategy in MAE significantly reduces the required memory and training time.,1,neutral
"Upon adopting this strategy, various methods are available for its implementation, and in this study, we chose the Masked Autoencoders (MAE [He et al., 2022]) method (Supplementary Fig.",2,positive
"The recent emergence of models with large number of parameters, like GPT-4 [OpenAI, 2023], Pathways Language Model (PaLM, [Driess et al., 2023]), Vision Transformer-22B [Dehghani et al., 2023], MAE [He et al., 2022], Swin Transformer V2 [Liu et al., 2022], VideoMAE V2 [Wang et al., 2023b], CLIP [Radford et al., 2021], Segment Anything Model (SAM, [Kirillov et al., 2023]), and DINO V2 [Oquab et al., 2023], has highlighted the potent feature extraction capabilities.",2,positive
", 2023], MAE [He et al., 2022], Swin Transformer V2 [Liu et al.",2,positive
Training a DEM-specific MAE is likely to result in enhancing the model performance as there is evidence that scaling the model up provides significant gains He et al. [2021]. Access to an adequate amount of DEM data and compute environments will enable pre-training large Geo-AI models to provide a data-efficient DEM encoder and make possible low-resource learning for Geospatial tasks.,2,positive
"For example, Wei and Ji [2021] applied weakly supervised training successfully for the task of road segmentation. Similarly, Soliman et al. [2022] evaluated an existing dataset to train models to extract building footprints from DEM and Wang et al.",2,positive
"For example, Wei and Ji [2021] applied weakly supervised training successfully for the task of road segmentation.",1,neutral
"For instance, Masked autoencoders He et al. [2022] have demonstrated to learn strong pretext tasks through learning to reconstruct holistic visual concepts.",1,neutral
NNCLR Dwibedi et al. [2021] has extended this instance discrimination task to include non-trivial positives between augmented samples of the same images and among different images.,1,neutral
"NNCLR along with other methods, e.g. MoCo Chen et al. [2020b], has adopted memory banks in their scheme to maintain the support set of nearest neighbors.",1,neutral
"It can be clearly observed that the model performance slightly deteriorates when decreasing the spatially masking ratio with increased τs, which is consistent with the finding in MAE [24].",1,neutral
"we reconstruct the missing feature embeddings rather than the raw pixels of the masked areas, which is different from traditional MIM methods like SimMIM [19] and MAE [5].",2,positive
"Note that
we reconstruct the missing feature embeddings rather than the raw pixels of the masked areas, which is different from traditional MIM methods like SimMIM [19] and MAE [5].",2,positive
"The ImageNet-C Hendrycks and Dietterich [2019] is a benchmark for evaluating the robustness of the models against common corruptions and perturbations that can occur in real-world scenarios. It consists of more than 30,000 images derived from the ImageNet dataset, with each image being corrupted in one of 15 different ways, including noise, blur, weather conditions, and digital artifacts. CIFAR-10/100 Krizhevsky [2009] are subsets of the tiny images dataset. Both datasets include 50,000 images for training and 10,000 validation images of size 32× 32 with 10 and 100 classes, respectively. The Oxford 102 Flower Nilsback and Zisserman [2008] consists of 102 flower categories, each class including between 40 and 258 images.",2,positive
The ImageNet-C Hendrycks and Dietterich [2019] is a benchmark for evaluating the robustness of the models against common corruptions and perturbations that can occur in real-world scenarios.,2,positive
"This outcome is particularly notable on large-scale datasets such as ImageNet-O and ImageNet-C, underscoring the scalability and effectiveness of our method in real-world, high-dimensional settings.",2,positive
"The ImageNet-C Hendrycks and Dietterich [2019] is a benchmark for evaluating the robustness of the models against common corruptions and perturbations that can occur in real-world scenarios. It consists of more than 30,000 images derived from the ImageNet dataset, with each image being corrupted in one of 15 different ways, including noise, blur, weather conditions, and digital artifacts. CIFAR-10/100 Krizhevsky [2009] are subsets of the tiny images dataset.",2,positive
"The ImageNet-C Hendrycks and Dietterich [2019] is a benchmark for evaluating the robustness of the models against common corruptions and perturbations that can occur in real-world scenarios. It consists of more than 30,000 images derived from the ImageNet dataset, with each image being corrupted in one of 15 different ways, including noise, blur, weather conditions, and digital artifacts. CIFAR-10/100 Krizhevsky [2009] are subsets of the tiny images dataset. Both datasets include 50,000 images for training and 10,000 validation images of size 32× 32 with 10 and 100 classes, respectively. The Oxford 102 Flower Nilsback and Zisserman [2008] consists of 102 flower categories, each class including between 40 and 258 images. The images have large scale, pose, and light variations. In addition, there are categories that have large variations within the category and several very similar categories. iNaturalist-2018 Van Horn et al. [2018] (iNat) comprises a vast collection of 675,170 training and validation images, classified into 5,089 distinct fine-grained categories found in the natural world.",2,positive
"These augmented views are generated by sampling τ, τ ′ from a distribution of suitable data transformations, such as partially masking image patches [He et al., 2022] or applying image augmentation techniques [Chen et al.",1,neutral
"Such methods have shown to be robust in self-supervised learning and control settings (Xiao et al., 2022; Radosavovic et al., 2022; He et al., 2022).",1,neutral
"This includes using inpainting losses (Pathak et al., 2016; He et al., 2022), contrastive learning (He et al.",1,neutral
"As the Transformer [38]-based mechanisms have emerged for various tasks in computer vision [9, 14], several methods [21, 13] have been proposed to effectively exploit Transformer [38] in 3D interacting hand mesh recovery.",1,neutral
"Transformer [38]-based methods have become the de-facto standards in multiple tasks for both natural language processing [7, 3] and vision [9, 4, 14].",1,neutral
"Inspired by the success of Transformer-based models in the field of natural language processing (NLP) [4, 7] and computer vision (CV) [2, 14], we make the first endeavor to explore the idea of latent representation learning for sequential volumetric design, where the inputs are a sequence of voxel-based representations.",1,neutral
MAE [16] has demonstrated that mask modeling can be effectively used in unsupervised image representation learning in the field of computer vision.,1,neutral
"Finally, Ibanez et al. [15] propose MAEST, which pre-trains a transformer backbone by predicting masked wavebands with Masked-AutoEncoders [13] prior to labeled fine-tuning.",1,neutral
"[15] propose MAEST, which pre-trains a transformer backbone by predicting masked wavebands with Masked-AutoEncoders [13] prior to labeled fine-tuning.",1,neutral
[34] paved the way for self-supervised vision pre-training with masked-imagemodeling approaches.,1,neutral
", introducing knowledge distillation [27], [49], [50], [51], [52], improving data augmentations [53], [54], [55], exploring self-supervised learning in Contrastive Learning [56], [57], [58], [59] and Mask Image Modeling [60], [61], [62], [63], [64], [65], and scaling ViTs to larger models [66], [67], [68], [69].",1,neutral
"Masked Modeling approaches emerged with transformers and showed better performance than contrastive learning to learn local features [2,26,42,64].",1,neutral
"It masks a part of the input and reconstructs the signal either at pixel-level like MAE [20,26,52], at features-level [22,57], or by predicting visual tokens [50, 56].",1,neutral
Hyper-parameters generally follow [8].,1,neutral
Backbone APbox APmask Params with Mask R-CNN: MAE-B [8] 49.,1,neutral
"At the lowest level of the last top-down column, we use mean-square error (MSE) loss to reconstruct raw images, similar to [8].",1,neutral
"A typical MIM method, masked autoencoders (MAE) [8], employs an encoder to embed the masked images into semantic features and a decoder to reconstruct unseen patches, as shown in Figure 1 (b).",1,neutral
"Typical series of methods focus on self-supervised learning, such as contrastive learning [6, 7] and masked image modeling (MIM) [8, 9].",1,neutral
"Our method is different from raw pixel reconstruction methods like MAE [8] or mask distillation works like MaskDistill [31], which modeling homogeneous features at the output of the network.",2,positive
"trained with MIM [8, 9, 14, 40, 32] by a large margin using only raw pixels as reconstruction target.",1,neutral
"Similar as MAE [8], output features of the last bottom-up column are first normalized and concatenated with learnable mask tokens, then input into the top-down columns.",1,neutral
"3% top-1 accuracy outperforms ConNeXt V2 [12], MAE [8], and CAE [14] counterparts.",2,positive
"We initialize weights using MIM pre-trained models, and fine-tune on ImageNet-1K with class label, similar to [8, 12, 9].",2,positive
"We benchmark four representative methods—MoCo [6], DINO [7], MAE [8], and data2vec [9]—on the proposed dataset.",2,positive
"Specifically, we evaluate four representative SSL algorithms—namely: MoCo [6], DINO [7], MAE [8], and data2vec [9]—on three different downstream tasks: scene classification, semantic segmentation, and change detection.",2,positive
"Meanwhile, recent developments in masked image modeling reveal promising results in generative methods, which reconstruct the masked input at pixel- [8] or feature[9] level.",1,neutral
"SELF-SUPERVISED PRETRAINING We performed pretraining using four representative SSL methods: MoCo-v2/v3 [15], [16], DINO [7], MAE [8], and data2vec [9].",2,positive
"(2022b) used VIT masked self-supervised learning (He et al., 2021) (MAE).",2,positive
"Liu et al. (2022b) used VIT masked self-supervised learning (He et al., 2021) (MAE).",1,neutral
"Therein, Point-MAE [50] and PointM2AE [89] introduce masked autoencoders [26] into 3D point clouds pre-training, achieving competitive results on different 3D tasks.",2,positive
"For example, compared to M3PT that uses additional masked pre-training strategy [19], RigNet++ still achieves 15.",2,positive
"For viewport reconstruction, we choose the MAE architecture [17], which is a well-known encoder-decoder architecture implemented by Vision Transformers (ViT) [47], to reconstruct the viewport frames.",2,positive
"Reconstruction technologies are widely used in images [15], [16], [17] and conventional videos [18], [19], [20] because they can infer missing regions from partial observation, which",1,neutral
"Fortunately, reconstruction technology can alleviate this problem, among which the masked autoencoders (MAE) approach [17] is the most advanced.",1,neutral
"The mainstream methods based on masked image modeling can be categorized into approaches aiming to reconstruct visual tokens [2, 14, 23, 30, 55, 56, 60] or features [49, 51].",1,neutral
"Even though a lot of attention has been dedicated to refining the pretext-task [1, 2, 13, 23, 32, 33, 55, 57], comparatively less attention has been paid to the token selection strategy [6, 30, 31].",1,neutral
"We conduct nearest neighbor, linear probing and fewshot linear probing experiments on five downstream image classification tasks, comparing the representation learning capabilities of MAE [23] and CL-MAE, upon selfsupervising both models on ImageNet [39].",2,positive
"[23] showed that randomly masking a large number of image patches, i.",1,neutral
• We introduce curriculum learning into the MAE framework [23] to learn robust representations.,2,positive
"Our curriculum masking module (CMM) and the MAE [23] are trained in alternating steps, similar to how generative adversarial networks [20] are trained.",2,positive
"Masked image models can be divided into two main categories with respect to reconstructing the target either as visual tokens [2,14,23,23,30,30,55,55,56,60] or features [49, 51].",1,neutral
"To this end, we propose a novel masking module, which is trained in an end-to-end fashion along with the MAE backbone [23].",2,positive
"We introduce curriculum learning as the core element of our proposed method, while using MAE [23] as the underlying backbone for representational learning.",2,positive
"Specifically, we adopt the state-of-the-art human pose estimation model ViTPose [75] trained on COCO [38], with the ViT-L backbone [14] initialized with MAE [19] pretrained weights.",2,positive
"To improve generalization [38, 59], self-supervised representation learning methods [23, 39, 55, 54, 17, 31] pre-train visual encoders that compress visual observations.",1,neutral
"In computer vision tasks, certain portions of the image are randomly masked, and then the model reconstructs the masked pixels [31].",1,neutral
"Inspired by the content reasoning ability of masked autoencoders (MAE) [9], our framework endows the holistic learning process of deep unfolding with the explicitly integrated with inherent physical mechanism underlying the pan-sharpening task.",2,positive
"To learn crossmodal context, three types of tasks are commonly used, i.e., masked language modeling (MLM) [15], masked image modeling (MIM) [43], [44], and image-text matching (ITM) [45].",1,neutral
", masked language modeling (MLM) [15], masked image modeling (MIM) [43], [44], and image-text matching (ITM) [45].",1,neutral
"For instance, masking was adopted for visual unsupervised learning in [63] where the use of validation dataset was limited to the tuning of masking ratio.",1,neutral
"This emergence of segmentation capability has been corroborated by subsequent self-supervised learning studies [6, 18, 35].",1,neutral
"In deep learning, the self-attention mechanism has been widely employed in processing visual data [11] with state-of-the-art performance on various visual tasks [6, 18, 35].",1,neutral
"in generative self-supervised pretraining is based on denoising autoencoders [9], which use an encoder-decoder architecture to reconstruct corrupted images [10]–[12].",1,neutral
MAE [10] with a self-distillation scheme that used tokens from visible histopathology patches.,1,neutral
Luo et al. [28] enhanced the encoder of MAE [10] with a self-distillation scheme that used tokens from visible histopathology patches.,1,neutral
"Foundation Models and Prompt Learning Leveraging publicly available pre-trained foundation models [1, 13, 19, 18, 41] to downstream tasks has emerged as a prominent scheme in centralized learning.",1,neutral
", LAION5B [43]), deep learning has demonstrated remarkable achievements across computer vision [19, 18, 41] and natural language understanding [12, 54, 2].",1,neutral
"[19], [20], natural language [21], or multimodal inputs [22], [23].",1,neutral
"• We fine-tuned DINO-ViT, DINO-ResNet-50 [30] and ViT-MAE models [31] 108 within their respective self-supervised frameworks, using CheXpert [32] X-ray 109 image dataset.",2,positive
"ViT-MAE [31] is a ViT trained using masked autoencoding 188 (MAE), a simple self-supervision method that involves masking and 189 reconstructing a large proportion of the image.",1,neutral
"2019), partially masking image patches (He et al. 2022), or applying image augmentation techniques (Chen et al.",1,neutral
"Moreover, the use of self-supervised or weakly-supervised learning methods [218], [260] can help alleviate the reliance on accurate annotations, as they can learn from the inherent structure and correlations present in the data itself.",1,neutral
Achieving state-of-the-art performance in image classification tasks often employs models that are initially pretrained on auxiliary tasks and then fine-tuned with crossentropy loss (Dosovitskiy et al. 2020; Wang et al. 2022; He et al. 2021).,1,neutral
"Self-supervised representation learning generates its own supervision signal by exploiting the implicit patterns or structures present in the input data [3, 4, 14, 15].",1,neutral
UBVMT is trained by employing masked autoencoding [63] and contrastive modeling [22] to learn effective emotion representation.,1,neutral
"Following MAE [63] and TVLT [22], a random masking strategy is applied, where 75% of the face and ECG/PPG patches are randomly masked.",1,neutral
UBVMT is pre-trained by employing masked autoencoding [63] and contrastive modeling [22] to learn effective emotion representation.,1,neutral
"For a grid with all-zero pixels, we use the one mask embedding p[M ] ∈ R(3) [20] as a replacement:",1,neutral
"This challenge is similarly experienced by self-supervised learning algorithms, where the methods effective for larger models frequently produce limited gains for smaller models [440, 441].",1,neutral
", relative position embedding (MSA-RP [17]), learnable position embedding (MSA-LP [7]), and 2D sine-cosine position embedding (MSA-SP [8]).",1,neutral
"Fine-tuning on AudioSet-20K, achieving 34.8%, CMMixer-Solo significantly outperforms concurrent MAE-AST [59], which trained with an additional 1,000 hours of speech in Librispeech while we only fine-tune without off-domain pretraining.",0,negative
"However, we observe that we are still a little short of the best model AudioMAE, which is both pre-trained and fine-tuned on AudioSet and has a larger size of the model.",2,positive
"Compared with VideoMAE with 1600-epoch pretraining in Kinetics-400, the CMMixer with 400-epoch pre-training in M2M obtains accuracy improvement.
art self-supervised frameworks of the action classification task that rely on pretraining.",2,positive
"The autoencoder network presented in BEiT [12], SimMIM [13] and MAE are methods for the masked image modeling (MIM) task in CV.",1,neutral
"Following the architectures of
ViT, BERT and MAE, the encoder consists of piles of Transformer blocks and takes only visible mixture tokens Ir processed by CMMixer as input.",2,positive
MixMIM [14] and i-MAE [15] studied the separability and the degree of semantics on the latent features of the mixed images.,1,neutral
"Second, because of the heavy spatial redundancy in images [10], we randomly drop part of the data to improve encode efficiency after mixing them to gain a mixture.",1,neutral
"Therefore, we draw lessons from the successes of BERT [9] in NLP and MAE [10] in CV, innovating an effective pre-processing method of multimodal data and leveraging a mixer to emphasize the frame-level synchronization.",2,positive
"On the one hand, the CMMixer-Solo achieves 0.6%, 0.2%, and 0.4% higher accuracy than BEVT, VideoMAE-B and ST-MAE-B respectively.",2,positive
"• We develop a new multimodal pre-training framework based on Masked
Autoencoder (MAE) that uses a single encoder-decoder structure.",2,positive
The MAE architecture in the “mask-then-regenerate” training method consists of two networks: an encoder and a decoder.,2,positive
"Based on VideoMAE [33] and AudioMAE [31], we divide the video into n clips (with a default setting of 8) and extract one frame and 2.6 seconds waveform with a 16,000 sampling rate from the middle of each clip.",2,positive
"cope with the lack of real labeled data [3, 8, 14].",0,negative
"The best two results are shown in red and blue. large-scale transformers [18, 82] pretrained on ImageNet-22k [17] or by MAE [28] are used.",0,negative
"large-scale transformers [18, 82] pretrained on ImageNet22k [17] or by MAE [28] are used.",1,neutral
"generic architecture once a token has been defined [57], this approach has also been used in computer vision [26] and AtmoRep extended it to space-time neighborhoods.",1,neutral
"An accurate and robust representation of the dynamics is learned in AtmoRep using a novel self-supervised training protocol that extends existing ones [8, 26] to four-dimensional space-time and that is one of the keys to AtmoRep’s intrinsic capabilities.",2,positive
"This is an extension of masked token models used in natural language processing [8, 9] and computer vision [26].",1,neutral
"Lastly, in the video domain, OmniMAE [14] jointly trained a self-supervised ViT model with images and videos using sample replication and higher masking ratios.",2,positive
"In MAE, mask tokens are incorporated within the encoder and decoder, resulting in an asymmetric design that significantly reduces the computational overhead of the encoder.",1,neutral
"Similarly, GreenMIM [19] adapts MAE for hierarchical architectures by dividing local windows into mul-
tiple equally-sized groups and proposing an optimal grouping algorithm to determine the ideal group size.",2,positive
"Conversely, MAE [18] adopts a different approach by considering only the visible patches as input to the encoder.",1,neutral
UM-MAE [20] introduces a novel masking strategy tailored for pretraining pyramidbased ViTs.,2,positive
"In the domain of hierarchical ViTs, there are concurrent efforts exploring the application of MAE.",1,neutral
"MixMAE [21] focuses on rearranging the inputs and targets, rather than designing specific masking or grouping strategies.",2,positive
"VideoMAE [11, 39] bypasses this requirement by applying image MAE [16] to reconstruct 3D RGB video patches, achieving promising results on video benchmarks.",2,positive
"The optimal random masking ratio for video MAE [11, 39] is higher than that for image MAE [16] (0.",1,neutral
"Previous work [2, 11, 16] argues that generative approaches such as MAE generally perform worse on linear probing tasks as there is a larger gap between the reconstruction task and downstream evaluation compared to other pretraining methods.",1,neutral
"This work uses the same asymmetric encoder-decoder design as [11, 16, 39].",1,neutral
"ImageMAE [16] reconstructs normalized image patches and focuses on studying decoder design, discovering that larger decoders enable the encoder to process only visible patches, thus enabling the use of a larger masking ratio to achieve both better performance and computational efficiency.",2,positive
"Image MAE [2, 16] has achieved state-of-theart in image domain via learning to reconstruct randomly masked image patches.",1,neutral
These existing video MAE works all inherit random masking from image MAE [16] where the mask is either discontinuous such as in ST-MAE [11] or random tubes such as in VideoMAE [39].,1,neutral
"We compare Mean Squared Error (MSE), a loss function previously used in masked autoencoders [5], against Mean Absolute Error (MAE) and the recently proposed Barlow Twin Loss [52].",2,positive
"Although both of these properties have been examined in the past [5], [8], [14], we take this opportunity to re-explore any effect adjusting these values may have in the context of a generative network.",1,neutral
’s application of masked autoencoding in computer vision [5] has recently produced particularly notable results from a generative perspective.,1,neutral
"ViT-based self-distillation models [38], [39] and masked autoencoding methods [5], [6] are nearing performance on-par with supervised methods.",1,neutral
"Contrary to previous related, vision-based approaches [5], [6], our decoder plays an important role during both training and testing.",2,positive
"learnable parameter in their proposed design [5], [15], [60],",2,positive
"exceeded prior contrastive achievements [5], [6] for new SSL",1,neutral
"Previously proposed masked autoencoders [5], [6] used randomly sampled batches during training, however, few-shot techniques work with episodic data, traditionally.",1,neutral
’s Masked Autoencoder for image classification [5].,1,neutral
Later studies [59] show higher mark ratio (e.,0,negative
2022a) scheme into the transformer model and pre-trains the network using the masked autoencoder (He et al. 2022) technique.,2,positive
It consists of three modules: a large masked autoencoders (MAE) [5] pre-trained ViT as image encoder.,2,positive
"From Table 3, we can first conclude that it is necessary to retrain the image encoder when applied to the nuclei segmentation task as retrained Tiny-ViT outperforms pre-trained MAE ViT.",2,positive
"To reduce the parameters of image encoder, the MAE ViT of SAM is replaced with a TinyViT [16].",1,neutral
"For the image encoder, SAM adopts a large-sized MAE [5] pre-trained ViT to extract features from input images.",2,positive
"Our unlabeled dataset might also facilitate general unsupervised representation learning [2, 9, 11, 24, 29].",2,positive
"MAE pretrained weights [43], and Multi-Modal pretrained weights [44], and evaluate them on relatively large synthetical and authentical datasets, KADID-10k and KonIQ-10k.",2,positive
"With respect to distinct backbones, we adopt the widely used residual networks including ResNet(R)-50/-101/-152 and ResNeXt(RX)-101/-152 [60] pretrained by InfoMin [55], as well as the vision transformers including ViT-Base (ViT-B)/-Large (ViT-L) [14] pretrained by MAE [22].",2,positive
"(a) Linear evaluation: A standard protocol consisting in learning a linear classifier on top of frozen SSL features [6,20].",1,neutral
"2g) shows that ImageNet-1k DINO is a solid starting point compared to other alternatives [20,34,31,9].",1,neutral
"The introduction of the transformer architecture and the self-attention mechanism has led to a an explosion of machine learning models for NLP and vision with a wide variety of model architectures, model sizes, varying sparsity, training paradigms and training data sets [2, 3, 11, 7, 15, 1, 2].",1,neutral
"As explained in [6], the most decisive difference between visual signal and natural language lies in data redundancy.",1,neutral
"Recently, large-scale pre-trained models illustrate the potential performance among different fields including computer vision (CV) [23, 24, 11] and natural language processing (NLP) [15, 43, 32].",1,neutral
"We implement a common pretext task that reconstructs pixels for masked regions (Atito, Awais, and Kittler 2022; He et al. 2022).",2,positive
"2021; Mo, Sun, and Li 2021, 2022) and masked image modelling (MIM) based methods, such as (Atito, Awais, and Kittler 2021; Bao et al. 2021; Xie et al. 2022; He et al. 2022).",2,positive
We adopt the Vision Transformer (ViTbase) [34] model pre-trained using the MAE [17] method as the backbone for extracting visual features.,2,positive
The joint visual features V ∈ RH×W×C of the target and the search image are extracted by the Vision Transformer (ViT-base) [34] model pre-trained with the MAE [17] method.,2,positive
"Following the same way, [27] trains masked autoencoders [22] with contrastive learning [10] on video datasets and shows a decent performance on both image and video tasks.",1,neutral
"4 Masked Autoencoders (MAE) Masked autoencoders (He et al., 2022) are similar to denoising autoencoders, where the input images are corrupted and the autoencoder tries to reconstruct the original image.",1,neutral
"Building upon MAE, ViTDet (Li et al. 2022b) and MIMDet (Fang et al.
2022), etc, have made significant advancements in the development of ViT for object detection.",2,positive
"We employ the checkpoints of ViT-small/-base (Dosovitskiy et al. 2020) and HiViT-base (Zhang et al. 2022c), which are all pre-trained using the MAE (He et al. 2022) selfsupervised strategy.",2,positive
"2022c), which are all pre-trained using the MAE (He et al. 2022) selfsupervised strategy.",2,positive
We discover that employing the imTED structure (Zhang et al. 2022b) alone (i.e. MAEBBoxHead only) struggles to surpass the baseline performance.,2,positive
"MAE (He et al. 2022) proposes a novel pre-training mode that deviates from the classic fully supervised pre-training era of CNNs (He, Girshick, and Dollár 2019).",2,positive
"Please note that the ViT-small backbone is obtained from the MAE pretrained encoder, and the 4-layer Transformer block is derived from the pre-trained decoder, which forms the MAEBBoxHead module.",2,positive
"MAE (He et al. 2022) proposes a novel pre-training mode that deviates from the classic fully supervised pre-training era of CNNs (He, Girshick, and Dollár 2019).",2,positive
"In subsequent experiments, the pre-trained MAEBBoxHead is used as the baseline method by default.",1,neutral
"Therefore, we adopt a design inspired by the imTED (Zhang et al. 2022b) detector and substitute the backbone as well as head modules of the two-stage detector with Vision Transformer blocks pre-trained using the MAE method.",2,positive
We primarily draw upon the training recipes in MAE (He et al. 2022) for stable training.,2,positive
Some suggest removing random (as in MAE (He et al. 2022)) or unimportant (as in A-vit (Yin et al.,1,neutral
Some suggest removing random (as in MAE (He et al. 2022)) or unimportant (as in A-vit (Yin et al. 2022)) tokens.,1,neutral
"Moreover, we distinctly remove the strategies of random erasing and exponential moving average (EMA) in MAE.",1,neutral
"Motivated by recent patch-based image representation [16, 21, 69], we measure the importance scores of patches and save the most impor-",2,positive
"Motivated by the Masked Auto-Encoder (MAE) [21], which recovers images with only some patches of them, we drop less important patches to reduce the number of pixels utilized for describ-",2,positive
"At the training stage, we reconstruct training images via important patches and a pre-trained MAE [21] model.",0,negative
"When the data is required for training, the patches are passed through a strong pre-trained MAE decoder to reconstruct the images.",0,negative
"At the training stage, we employ a strong pre-trained MAE decoder to reconstruct the dropped patches and the original images.",0,negative
"As pointed out in Masked Auto-Encoder (MAE) [21], with a pre-trained decoder, some image patches can be dropped without affecting the reconstruction quality of the image.",1,neutral
The samples are originally selected by ResNet-18 and reconstructed with MAE.,0,negative
"Motivated by the Masked Auto-Encoder (MAE) [21], which recovers images with only some patches of them, we drop less important patches to reduce the number of pixels utilized for describ-
Algorithm 1 Data bin generation.",2,positive
"Specifically, we experiment with a ViT-B/16 model pre-trained unsupervisedly by Masked Auto-encoder [13].",2,positive
"It’s important to note that object recovery mirrors Masked Autoencoding [17], treating a random object as absent and necessitating reconstruction from contextual cues.",1,neutral
"With the denoising objective, DAE has been one of the dominant approaches for unsupervised representations learning [68, 67, 18, 36].",1,neutral
"For the audio encoder and the visual encoder, inspired by the excellent performance and generalization capability of recent vision/audition selfsupervised pre-trained models [17, 31, 32, 36, 85], we apply",2,positive
"We provide the adversarial robustness results to l2-norm attack for ViTs that are equipped with different training methods, including the supervised baseline, MoCo v3 [8], BEiT [3], PeCo [16] and MAE [25].",2,positive
"MAE[25] takes a big step forward, which directly uses the normalized raw pixel values as y,",1,neutral
"MAE [25] takes a step forward to directly use the raw pixel values as the target, achieving surprisingly good results.",2,positive
"BERT pretraining for vision transformers [3, 25, 16, 60, 20] ignites rising interests recently and shows great po-",0,negative
"From the quantitative results listed in Table 7 and Figure 3, we can surprisingly find that, though MAE [25] pretrained model performs better or comparably on clean images, it encounters much larger performance drop on adversarial samples.",0,negative
"One typical example can be seen with Masked Autoencoders [7], where they developed an asymmetric encoderdecoder architecture.",1,neutral
Visualization to show how self-supervised learning is learned using hidden tokens in MAE [7].,1,neutral
"Self-supervised learning has shown its application on multiple fronts, from Language models [5] to computer vision models [7], [3] and protein folding models [13].",1,neutral
"In the uni-modal realm, the community has harnessed the versatility of Transformers [62] in conjunction with the abundance of large-scale web data, yielding notable successes in the development of general-purpose foundation models for Natural Language Processing [12, 41, 57, 58, 5, 49] and Computer Vision [13, 28, 4, 53, 17, 16].",2,positive
"Following the asymmetric design of the vision MAE [20], different mask tokens are added to the decoder’s input sequence and later used to reconstruct the masked trajectories and lane segments with simple prediction heads.",1,neutral
The masked autoencoder (MAE) [20] has garnered significant attention due to its recent achievements in imagebased self-supervised learning.,1,neutral
"Recently, there emerge a host of fully unsupervised methods to learn robust representations for semantic discrimination [14, 38, 33, 70, 29, 37, 12] or spatiotemporal correspondence [44, 81, 86, 55, 52, 43], which achieve promising performance.",1,neutral
"The decoder consists of 8 Transformer blocks with a width of 512-d [14], and a linear projection is used as the last layer of the decoder to predict pixel values while keeping the output channels equal to the number of pixels (N ) in a given patch.",2,positive
"In self-supervised visual representation learning, the masked autoencoder (MAE) [14] is used to reconstruct randomly masked image regions using an encoder-decoder architecture.",1,neutral
Directly training the MAE with insufficient data often brings trivial benefits and may even have harmful effects on the model.,1,neutral
"Self-supervised learning and revised the masked autoencoder Following the MAE [14], we auto-encode the images by reconstructing the original images with only partial observations.",2,positive
The decoder includes 8 Transformer blocks with a 512-d width [14].,2,positive
"However, MAE pre-training requires a large volume of training data (usually more than 1 million) that may not be feasible to obtain for ultrasound breast images.",0,negative
"For example, because of the canonical structure, images can be easily segmented into multiple patches for masking [8, 15], which in the case of video are extended to patch tubes [13, 37].",1,neutral
"learning has been applied to images [15], videos [13, 37] and static point clouds [29, 47], it has not been promoted on 4D signals, such as point cloud videos.",1,neutral
"Networks pre-trained with self-supervised learning usually yield higher performance than when solely trained in a supervised manner [5, 15, 16, 19].",1,neutral
"The design of the masking operation is related to the information redundancy of input [13, 15, 37].",1,neutral
"According to prior experience, the masked autoencoder needs to be fed a considerable amount of data in the pretraining stage to learn useful knowledge [13, 15].",0,negative
"Following [13, 15, 37], we implement end-to-end fine-tuning, semi-supervised learning, and transfer learning to evaluate the pre-trained MaST-Pre.",2,positive
"Mask prediction has been proven to be an excellent selfsupervised task for visual representation learning [8,15,45].",1,neutral
[15] proposed MAE as a scalable vision learner to predict the pixels of masked patches.,1,neutral
"LibreFace (Ours) [16, 17, 44] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓",0,negative
", ResNet [17], Swin-Transformer [43], and Masked Auto-encoder (MAE) [16].",1,neutral
", ResNet [17], Swin Transformer [27] and MAE [16], which have recently achieved resounding success in other computer vision tasks.",1,neutral
Another noteworthy work is the self-supervised Masked Auto-encoder (MAE) [16].,1,neutral
", Vision Transformer (ViT) [11], Swin Transformer [27] and Masked Auto-encoder (MAE) [16].",1,neutral
"These models typically follow the design of auto-encoding frameworks [14] consisting of an encoder that learns to map the input graph [16, 21] into latent representations, and a decoder ar X iv :2 30 8.",1,neutral
"Self-supervised generative models, exemplified by MAE [14] and BERT [7], have demonstrated remarkable performance in acquiring generalizable representations in various domains such as computer vision [22, 54] and natural language processing [58].",1,neutral
"In self-supervised learning, this would make the pre-training task more challenging and valuable, encouraging the encoder to learn more generalizable patterns from the input [14, 16].",1,neutral
"Drawing inspiration from the effective representation learning of masked image autoencoders [16], we introduce masked motion modeling, a technique that involves temporally masking a random portion of the input motion sequence at a ratio r , and subsequently requiring the model to reconstruct the entire motion sequence.",2,positive
"Recently, the paradigm of pre-training transformer-based models on large-scale corpus and then fine-tuning them for downstream tasks has achieved great success in various domains, such as natural language processing (NLP) [52, 9, 34, 28, 47, 2], computer vision (CV) [10, 36, 37, 35, 13, 1], and vision-and-language (VL) [6, 29, 24, 30, 8, 44, 53].",1,neutral
"It has been suggested that the learned 449 representation in autoencoders is likely to be the compression of image content, while 450 MIMs have a deeper semantic understanding of the scene[18, 48].",1,neutral
STEP [37] adapts MAE [14] and proposes a pipeline to pre-train a model.,2,positive
"First, a traffic patch encoder is pre-trained by the data of source cities in the fashion of the Masked Autoencoder [14, 37].",1,neutral
"(5) Compared with the cross-city traffic forecasting methods, TPB achieves a performance enhancement of 6.52% and 7.71% in RMSE and MAE respectively on average and outperforms these methods on all datasets.",2,positive
"5.3 (4) Compared with the pre-training traffic forecasting
method, TPB achieves a performance enhancement of 4.26% and 8.96% in RMSE and MAE respectively.",0,negative
"Here, we propose a traffic patch encoder pre-training framework based on SOTA methods STEP [37] and MAE [14].",2,positive
"We set the mask ratio to 75% according to the original papers [14, 37].",0,negative
"Metric: Two commonly used metrics in traffic forecasting are used to evaluate the performance of all baselines, including Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).",1,neutral
"(3) Compared with the typical deep-learning traffic forecasting methods implemented in the Reptile framework, TPB achieves a performance enhancement of 9.92% and 13.13% in RMSE and MAE respectively on average.",2,positive
"The encoder of SRMAE, similar to the encoder of MAE [23], processes only these sampled high-resolution patches.",2,positive
"Masked Image Modeling (MIM)[5, 23, 71] self-supervisely learns deep representations by masking a portion of input signals and predicting these masked signals.",1,neutral
"On the other hand, the remaining patches are called high-resolution (HR) ones, similar to the MAE [23], where our",1,neutral
We randomly mask out 75% of total image patches following MAE[23].,2,positive
"Several target signals have been developed for the mask-prediction pretext task in MIM, such as normalized pixels[23, 71], discrete tokens[5, 16], HOG features[69], deep features[4, 78], and CLIP[74].",1,neutral
We follow the default finetuneing parameters of the MAE[23].,2,positive
"Masked Image Modeling Masked Image Modeling (MIM) has become a popular pretext task for visual representation learning, inspired by BERT[14] for Masked Language Modeling[5, 23, 71].",1,neutral
Encoder Only visible patches xp = {xp|M i = 1} are fed to the encoder following[23] and mapped to the high resolution patches xe across a stack of transformer blocks.,2,positive
"As a result, some works have accelerated the process by employing the asymmetric encoder-decoder strategy[23, 27] or reducing the input patches[8, 37].",1,neutral
Only the loss generated by the parts replaced by xl is calculated[23].,1,neutral
"Compared to previous mean angle error (MAE) [37, 48], the MPJPE reflects larger degrees of freedom of human poses and covers larger ranges of errors for a clearer comparison.",0,negative
"Moreover, previous methods using masking/denoising strategies mostly focus on data types of images [25, 45], videos [16, 53], languages [14, 50, 67] and point clouds [49, 59], but rarely on motion sequences, especially human motions.",1,neutral
"Comparing to previous popular methods based on masking/denoising autoencoding like masked autoencoder (MAE) [25] and denoising autoencoder (DAE) [56], which are mainly designed for model pre-training, we treat the denoising and masking prediction as auxiliary tasks to aid the primary fully-supervised motion prediction task and jointly learn all the tasks together.",2,positive
[25] presents the masked autoencoder (MAE) to accelerate model pre-training by masking a high proportion of input images and developing an asymmetric encoder-decoder architecture.,2,positive
"1) Comparisons with Prior Arts: First, we use the 3D-Unet [37] as the segmentation network backbone, our MLM based on [29].",2,positive
"Different from most MIM models [29]–[33] which need millions of images to train, our MLM only need a small number of label mark for training.",0,negative
"Unlike previous mask strategy [29]–[33] such as random mask, using a large patch, or per-frame masking, we sample the patches which contain the organ pixel which contains the organ local shape feature.",2,positive
1) Mask label modeling: Since most MIM models [29]–[33] have been shown effective in the pretext task of reconstructing the original image from only partial observations.,1,neutral
Inspired by the popular masked image modeling (MIM) methods [29]–[32] we introduce the MLM for modeling shape.,2,positive
"We implement the above two pipelines into our method with the following experiments: 1) meaning the output representations Er ∈ RN×D into a vector and projecting it to regress the distortion parameters, and 2) reshaping the output Er to form a reconstructed image, following MAE [28].",2,positive
"In recent years, unsupervised representation learning methods [25, 8, 21, 24, 10] have shown promising results in learning pretraining representations from large-scale unlabeled data.",1,neutral
"Self-supervised learning [9,10,22,42] trains models by large-scale unlabeled datasets, and the learned embeddings can be applied to the downstream tasks in different domains.",1,neutral
"There are two main types of selfsupervised learning (SSL) based on the pretext task used: generative and discriminative SSL, with generative SSL reconstructing altered or distorted data to its original input [9, 28, 31, 59, 65, 71]and early discriminative SSL predicting easily designed labels and task-specific representations that are not very generalizable [25, 57, 75].",1,neutral
It is a necessary augmentation for training the CNN to predict missed data using its surrounding available data (He et al. 2022).,2,positive
"The mask operation is inspired by (He et al., 2022).",2,positive
"Before the official training, an MAElike [32] unsupervised warming-up phase is deployed to upgrade robustness as described in Appendix B.",0,negative
"SkeletonMAE [53] first introduces the idea of MAE[17] into transformer-based 3D action representation learning, where the original joint coordinates of masked regions are predicted.",1,neutral
"Recently, as transformers flourish in computer vision, masked autoencoder (MAE) [17] has attracted a surge of research interest for its exceptional performance.",1,neutral
"Similar to the BERT [11] pre-training in NLP, the input tokens are randomly masked and corresponding objectives are predicted, which can be the raw pixels [17], HOG features [52], or token ids from offline learned dVAEs [2].",2,positive
"(3), which is normalized by its segment-wise mean and standard deviation as in [17].",1,neutral
"We follow the encoder-decoder design in MAE [17], where the transformer encoder focuses on representation learning, while the decoder is responsible for the implementation of the pre-training pretext.",2,positive
"Mask Sampling Strategy: In our approach, we employ the vanilla transformer as the backbone network, where embedding features at any spatio-temporal location can be freely masked as in MAE [17].",2,positive
"Different from MAE [17] that reconstructs the original signal for representation learning, in MAMP, a motion prediction head is adopted, which takes decoded features as input and predicts the temporal motion of the input skeleton sequence.",1,neutral
"Inspired by this, methodologies like MSM-MAE [32], MaskSpec [33], MAE-AST [34], and Audio-MAE [35] apply this approach in the audio domain.",2,positive
CAV-MAE [16] extends the MAE from single-modal to audio-visual multi-modal scenarios by combining cross-modal contrastive learning and masked data modeling to learn joint audio-visual representations.,1,neutral
"While BEATs also utilizes the accelerated training via MAE, it explores a self-supervised audio pre-training method involving masked discrete label prediction objectives.",2,positive
"The Masked Autoencoder (MAE) employs an asymmetric encoder-decoder architecture, where a significant portion of the encoder inputs is masked.",2,positive
"For multi-modal MAE, a new proxy task is introduced in MAViL for predicting
aligned and contextualized audio-visual reconstructions, surpassing the performance of reconstructing single-modal raw inputs.",1,neutral
"2) Transformer-based approaches from the image domain: such as ViT [7], Swin-Transformer [8], and MAE [9], as backbones.",1,neutral
"Methods like MBT [14], UAVM [15], CAV-MAE [16], and MAViL [17] fall under this category.",1,neutral
MAViL [17] not only utilizes MAE and cross-modal contrastive learning but also explores intra-modal contrastive learning and multi-modal masked data reconstruction.,1,neutral
"the recent success of masked autoencoders in reconstructing images (He et al. 2021; Bao, Dong, and Wei 2021), and videos (Tong et al.",1,neutral
"C V
] 1
4 A
ug 2
02 3
the recent success of masked autoencoders in reconstructing images (He et al. 2021; Bao, Dong, and Wei 2021), and videos (Tong et al. 2022), we adapt the motion synthesis as a reconstruction problem: to recover a sequence of masked human skeletons regardless of the masking…",2,positive
"Unlike previous works (He et al. 2021; Tong et al. 2022; Li et al. 2022), we do not neglect the masked patches in the encoder phase, as all patches E have relevant information for the model (due to the adaptation of X into Xfill).",2,positive
"(He et al. 2021) proposed an asymmetric encoderdecoder design for masked image modeling, and (Tong et al.",1,neutral
"(He et al. 2021) proposed an asymmetric encoderdecoder design for masked image modeling, and (Tong et al. 2022) adapted this pre-training framework for videos.",2,positive
Voxel-MAE [42] and ALSO [43] propose predicting occupancy for LiDAR perception as the pretext task.,1,neutral
"Additionally, methods like MAE [40] and BEiT [41] employ a random patch masking approach where missing pixels or features are reconstructed using a simple autoencoder framework.",1,neutral
"As humans, it is a straightforward task to determine what lies behind the mask, since the global context provides enough information to deduce spatial relationships [53].",1,neutral
"Masked Modeling (MM) recently achieves widespread success in various vision challenges by reconstructing masked visual patches [5, 6, 23].",1,neutral
"We employ MAE [5], a representative masked modeling method.",2,positive
"MS-COCO
tr@1 ir@1
real 58.1 44.2
VQGAN [64] 35.6 32.0
DALL-E2 [65] 44.5 38.6
Stable [34] 52.3 40.9
w/ ours 54.9 (+2.6) 43.8 (+2.9)
mix w/ ours 60.8 46.2
Table 4: MAE [5] downstream results.",0,negative
"(ii) Masked Modeling [5, 50, 51, 52, 23], such as MAE [5], SimMIM [6], and iBOT [50], use masked patch reconstruction in combination with basic data augmentation to effectively learn robust representations.",1,neutral
"In the past few years, several unsupervised learning techniques have emerged, including contrastive learning [1, 2, 3, 4], masked modeling [5, 6], and vision-language pretraining [7, 8, 9], etc.",1,neutral
3 Table 4: MAE [5] downstream results.,0,negative
"4, MAE [5] achieves consistent trend results when compared to MoCo-v2.",2,positive
"For classification tasks, we utilize the Masked Auto Encoder (MAE) [5].",1,neutral
"…and thus, provides direct comparisons with NLA, 2) LFADS has been frequently employed in this field, and 3) NDT is chosen since the masked autoencoder has been proposed for a de facto standard of representation learning in many domains (Devlin et al., 2018; He et al., 2022; Tamkin et al., 2022).",1,neutral
"The rationale for this selection is that 1) SeqVAE shares the backbone autoencoder with NLA, and thus, provides direct comparisons with NLA, 2) LFADS has been frequently employed in this field, and 3) NDT is chosen since the masked autoencoder has been proposed for a de facto standard of representation learning in many domains (Devlin et al., 2018; He et al., 2022; Tamkin et al., 2022).",2,positive
SAM utilizes a Vision Transformer (ViT) pretrained with MAE [20] as Image Encoder.,2,positive
"Image Encoder
SAM utilizes a Vision Transformer (ViT) pretrained with MAE [20] as Image Encoder.",2,positive
"[11], we leverage a self-supervised learning strategy based on masking.",1,neutral
"SAM provides three scale-specific pre-trained image encoder configurations: ViT-B (91M parameters), ViT-L (308M parameters), and ViT-H (636M parameters) [56], [57].",2,positive
It can be used as valuable auxiliary task for learning representation for language [31] or vision [32] task and works as regularization like in the well known Dropout [33].,1,neutral
"Note, the loss (Mean Square Error, MSE) is computed only between the masked patches, following [13].",1,neutral
"More recently, [13] demonstrated improved performance with a Masked AutoEncoder (MAE) strategy that reconstructs sets of masked-out image patches using only the remaining unmasked patches.",1,neutral
Pre-training network Similarly to [13] the sMAE architecture follows an asymmetric encoder-decoder design (Fig 1).,2,positive
The Masked AutoEncoder (MAE) [13] frames self-supervision of vision transformers as an image reconstruction problem.,1,neutral
"Such observations are aligned with those in [13], where it was noted that masking a high percentage of patches is necessary to reduce redundancy and create a challenging self-supervisory task which leads to learning more meaningful weights.",1,neutral
"Foundation models [3, 14, 23] have received significant interests in recent years, owing to their exceptional performance across a multitude of diverse tasks These models typically consume billions of parameters, trained on expansive web-scaled datasets for fundamental tasks such as next token prediction [6] or masked region completion [7].",1,neutral
"The capability of ViTs is further enhanced when it is pre-trained on a large corpus of image [54], [55], video [56], [57], [58] or multi-modal data [59], [60], or when the size of the model is increased [61], [62], or both [63], [64].",2,positive
"We considered the following three pretext tasks: our custom 1) Vertex Normal Prediction (VertNormPred), 2) µCT volume Reconstruction (ReconCT), and 3) Masked µCT volume Reconstruction (MaskReconCT) tasks.",2,positive
"3(b) for ReconCT and MaskReconCT tasks, where we minimize the L1 loss between the generated and original 5×5×5 µCT node-wise subvolumes.",1,neutral
"The pretext tasks: VertNormPred, ReconCT, and MaskReconCT, improved the model in capturing the µCT bony labyrinth structure for the mainstream task of prediction/classification of nodewise SD.",2,positive
"In the MaskReconCT [8] task, we focus on reconstructing pixel-wise randomly masked 5 × 5 × 5 μCT subvolumes using an encoder-decoder network shown in Fig.",2,positive
"In the MaskReconCT [8] task, we focus on reconstructing pixel-wise randomly masked 5 × 5 × 5 µCT subvolumes using an encoder-decoder network shown in Fig.",2,positive
"3. a)Vertex Normal Prediction (VertNormPred) network predicts the node-wise vertex normals given the 5 × 5 × 5 µCT subvolume centred at each node. b) (i) CT volume Reconstruction (ReconCT) network and (ii) Masked CT volume Reconstruction (MaskReconCT) network reconstructs the 5×5×5 µCT subvolume given µCT or pixelwise randomly masked µCT subvolume centred at each node, respectively.
learning to fill these gaps, the model becomes more adept at estimating SDs, especially when parts of the µCT are incomplete.",1,neutral
"Even contrastive implementations using augmentations tailored to 12-lead ECG data, such as CLOCS [11], are outperformed by the MAE, highlighting our decision to integrate MDM into our proposed MMCL.",2,positive
"Recent works investigating masked data modeling, such as iGPT [12], ViT [21], BEiT [22], or most recently the masked autoencoder (MAE) [16], are relying on Transformers [23] as backbone architecture and thus follow the example of BERT [15] in NLP. Transformers have shown great success in modeling local and global dependencies, enabling the model to capture relationships even in long sequential data [24].",1,neutral
"Using contrastive techniques for unimodal pre-training, the prediction performance is inferior to the MAE.",1,neutral
"Recent works investigating masked data modeling, such as iGPT [12], ViT [21], BEiT [22], or most recently the masked autoencoder (MAE) [16], are relying on Transformers [23] as backbone architecture and thus follow the example of BERT [15] in NLP.",1,neutral
2) Training Strategy: We first pre-train our unimodal signal encoder fs(·) using the MAE [16].,2,positive
"(1)
We train the MAE by optimizing the mean squared error (MSE) loss, which can be lower bounded by a global alignment loss with respect to the embeddings fs(xv) and fs(x′v), under the assumption that g(·) is L-bi-Lipschitz [36]:
LMSE = Ep(x)Ep(xv|x)||g(fs(xv))− x|| 2 (2a)
≥ − 1 2L Ep(xv,x′v|x)fs(xv) ⊤fs(x ′ v)− ϵ+ const.",1,neutral
We also employ a MAE during the initial step of our framework and further introduce unstructured masking similar to [18] to learn temporally encoded local and global ECG features.,2,positive
"With heavy masking ratios, we find the MAE able to eliminate redundancy present in the 12-lead ECG across signal channels and time.",2,positive
"Let fs(·) and g(·) be the signal encoder and decoder, respectively, such that the MAE can be represented as h(·) = (g ◦ fs)(·), where
x̂ = h(xv) = g(fs(xv)).",1,neutral
"When comparing unimodal pre-training strategies, masked data modeling (MDM) with the MAE [16] is most effective in learning ECG embeddings,
outperforming all other unimodally pre-trained models across all diseases.",1,neutral
We base our MDM implementation on the MAE [16].,2,positive
"When comparing unimodal pre-training strategies, masked data modeling (MDM) with the MAE [16] is most effective in learning ECG embeddings, outperforming all other unimodally pre-trained models across all diseases.",1,neutral
"language processing (NLP) [15], masked data modeling has been extended to other modalities such as imaging [16], video [17], and audio [18].",1,neutral
"Furthermore, to evaluate the effectiveness of the MAE [16] for learning rich ECG embeddings, we compare it to a fully supervised model and multiple unimodally pre-trained contrastive models.",2,positive
"In the medical domain, Zhang et al. [10] have recently applied the MAE to detect arrhythmia from ECG.",2,positive
"The auto-encoding model is composed of an encoder and a decoder, which has achieved remarkable success in speech and image processing [48, 49], due to its powerful ability of feature representation.",1,neutral
"supervision, instead of using the original samples (commonly used in the pre-training stage of speech/image processing [48, 50]).",1,neutral
"We note that the masking ratio is set to 50% since it achieves the best BP estimation performance in our experiments when it ranges from 15% to 75% following the existing experience [48, 50].",2,positive
"[35] employs Vision Transformer (ViT) [36], which is built upon a self-attention mechanism, as an encoder DNN.",1,neutral
[35] propose masked autoencoding suitable for learning contexts among local patches of a 2D image.,1,neutral
We adopt the vanilla ViT-Base [16] model pre-trained with MAE [25] on ImageNet-1k to initialize the backbone of our ROMTrack.,2,positive
"Since OSTrack [57] also adopts MAE pretrain, we would like to compare with it.",2,positive
We adopt the vanilla ViT-Base [16] model pretrained with MAE [25] on ImageNet [15] as the backbone of our ROMTrack.,2,positive
"Specifically, motivated by the recent successes of vision Transformers, e.g., ViT [16], MAE [21], and Swin Transformer [39], we employ Transformers for both image and text representation learning steams.",2,positive
", faster-RCNN [50] or ResNet [22], inspired by the recent advances of vision Transformers [21, 39], we employ Transformerbased model to encapsulate the images for dense representations and unify the backbones of different modalities.",2,positive
"• It is well known that powerful feature representations usually lead to impressive performance in many tasks [21, 22, 26], but have not been well discussed in previous cross-modal retrieval",1,neutral
"The MAE also reveals that masking a high proportion of the input image could yield nontrivial results, as well as accelerating the training procedure.",0,negative
"More recently, He et al. [21] devised a simple framework of masked autoencoders (MAE) with an asymmetric encoder-decoder architecture to operate only on the visible subset of patches and reconstruct the original image from the latent representation and masked tokens.",1,neutral
[21] devised a simple framework of masked autoencoders (MAE) with an asymmetric encoder-decoder architecture to operate only on the visible subset of patches and reconstruct the original image from the latent representation and masked tokens.,1,neutral
", ViT [16], MAE [21], and Swin Transformer [39],",1,neutral
"After that, numerous Transformer-based models have since emerged in image classification [15, 16, 17, 18, 19], object detection [9, 20, 21], and semantic segmentation [10, 22, 11, 23].",1,neutral
"Inspired by the success of large-scale pre-training in NLP [15, 41, 42, 6, 52, 31], CV [22, 17, 21, 25, 38], and 2D-VL [30, 2, 34, 40], we propose to pre-train 3D-VisTA on 3D scene-text data, aiming for better performances on 3D-VL tasks.",2,positive
"Recently, the masked modeling has achieved remarkable success in self-supervised learning [13, 46] for image representation learning.",1,neutral
"Inspired by the masked autoencoder [13], Wu et al.",1,neutral
"Self-supervised learning could be divided into three groups: contrastive learning, selfdistillation [6, 70] and masked image modeling [25, 11, 53].",1,neutral
"Several self-supervised techniques have emerged recently [7], encompassing simple pretext task-solving approaches [40,6,5], contrastive learning methods [3,10,38], and generative approaches [28,43,9].",1,neutral
"We use masked autoencoders (MAEs) as our SSL algorithm of choice (He et al., 2022).",2,positive
He et al. (2022) show that using only random crops works well in MAEs.,1,neutral
"I would like to thank the authors of the MAE paper (He et al., 2022) for making their code available.",2,positive
"Contrastive learning [9, 2, 15] and masked image modeling [16, 17], the two most prevalent pre-training methods in self-supervised learning, are naturally the main exploration goals in this work.",1,neutral
"We choose MoCo-v3 [9] and MAE (Masked AutoEncoder) [16] for contrastive learning and masked image modeling, respectively.",2,positive
Model Feature Vector SAM The image embeddings from the image encoder ( MAE pre-trained Vision Transformer (ViT)[24]) of the SAM[12].,2,positive
"For network architecture, we use ViT-Base [15] as feature extractor containing lm = 11 MSA blocks and lt = 1 TSA block, where the parameters are initialized via [22].",2,positive
"[18] introduce Vision Transformer (ViT) [15, 46, 22] into class-incremental learning (CIL).",1,neutral
"As illustrated in [29], the asymmetric structure does not reduce the performance on final task results but turns more lighted-weighted compared with the symmetric codec based on transformers.",1,neutral
"In addition, some self-supervised methods of masked image modeling [2, 17, 40, 43, 47] randomly mask out some input image tokens and then recover the masked content by conditioning on the visible context.",1,neutral
The image encoder of SAM has a Vision Transformer [20] backbone and is pretrained with the Mask Auto-Encoder (MAE) strategy [25].,2,positive
"self-supervised learning (Chen et al., 2020; He et al., 2021; Moskalev et al., 2022b), remains an interesting future di-",2,positive
"Firstly, the generalizability of our findings to other types of neural networks, other data modalities and other training regimes, e.g. self-supervised learning (Chen et al., 2020; He et al., 2021; Moskalev et al., 2022b), remains an interesting future di-
rection to explore.",2,positive
"It is designed with an asymmetric encoder-decoder structure[14] with Fourier-based attention blocks[15], which means the sequence lengths of the",2,positive
"From Table 4, we can see that directly fine-tuning the vanilla ViTMAE pre-trained model with 16× 16 patch numbers has lower accuracy.",1,neutral
"Motivated by ViTMAE, we adopt a masked image modeling (MIM) task to help the diagram encoder effectively extract geometry features and learn geometry primitives.",2,positive
"Similar to ViTMAE [11], we calculate the mean squared error (MSE) between the reconstructed and original image patches at pixel-level as the loss function Lmim for the MIM task, which can be formulated by",2,positive
"In addition, we enhance the geometry diagram understanding ability via a self-supervised learning method with the masked image modeling auxiliary task [11].",2,positive
"To verify the effectiveness of our proposed auxiliary tasks for the training diagram encoder, we conduct a series of experiments: (1) training the encoder by themasked imagemodeling (MIM) task from a pre-trainedmodel with the vanilla ViTMAE configuration (i.e., 16 × 16) on ImageNet; (2) training the encoder by the masked image modeling (MIM) task from random initialization with 8 × 8 patching; (3) training the encoder by the multi-label classification (MLC) task from random initialization; (4) training the encoder by a combination of MIM and MLC tasks.",2,positive
"To effectively train ViT, [11] proposed the ViTMAE method which randomly masked a number of patches in the pixel space, then learned to reconstruct these patches by an autoencoder method.",1,neutral
"In the original configuration of ViTMAE, an image is split into 16 × 16 patches and the mask ratio is set as 0.75.",2,positive
"Inspired by ViTMAE, we utilize a masked image modeling task to pre-train our Transformer-based diagram encoder, where we specifically design the mask percentage as our diagram images mainly contain straight lines and annotated characters without those sufficient textures in natural images.",2,positive
"Similar to ViTMAE [11], we calculate the mean squared error (MSE) between the reconstructed and original image patches at pixel-level as the loss function L𝑚𝑖𝑚 for the MIM task, which can be formulated by
L𝑚𝑖𝑚 = ∑𝑁 𝑖=1 𝐿𝑖 ·𝑀𝑖∑𝑁 𝑖=1𝑀𝑖 ,where 𝐿𝑖 = 1 𝐶 𝐶∑︁ 𝑗=1 ( 𝑃𝑖 𝑗 −𝑇𝑖 𝑗 )2 .",2,positive
"Outside of the NLP domain, VideoMAE [33], inspired by MAE[13], presented video tube masking with an extremely high ratio for video reconstruction, which significantly improved performance for self-supervised video pre-training.",2,positive
"where xvs , xvt are data samples from source and target cities,LMaskedAE denotes the loss of masked autoencoding [14], and λd is a hyperparameter.",1,neutral
The source feature network is pre-trained with masked autoencoding [14] to reconstruct masked patches using unmasked ones in a long input sequence x ∈ R ·P .,1,neutral
"We use the default setup, described in Section 5.4 of the main text, to adopt a MAE [20] pre-trained ViTBase as the backbone and train the model for ∼50 epochs.",2,positive
"While MIM pre-training has been shown to moderately improve the performance of other detectors [20, 50], its impact in plain settings is profound.",1,neutral
"For the fair comparison, We use a MAE [20] pretrained ViT-Base as the backbone and train the object detector for ∼50 epochs.",2,positive
"The pre-training methods [20, 51, 1] that follow the path of masked image modeling have drawn increasing attention due to their strong performance on various core vision tasks such as object detection and semantic segmentation.",1,neutral
"We leverage the recent advances of masked image modeling pre-training[1, 20, 51, 28] which have shown better locality[49].",2,positive
"Both methods have demonstrated strong empirical results on downstream tasks [7, 8, 10, 13].",1,neutral
"Therefore, we investigate the influence of model scale on social biases, using iGPT [8] and ViT-MAE [13], as both have been trained using selfsupervised methods and are available in three different model sizes.",2,positive
"In contrast, we observe the opposite effect on ViT-MAE, where it comes with a small increase in gender bias.",1,neutral
"Models In our experiments, we use BEiT [3], ViTMoCo [10] and ViT-MAE [13], which use a standard Transformer as the backbone network (12 layers, 12 attention heads, 768 hidden size).",2,positive
"Input Resolution and Patch Size In addition, input resolution and patch sizes have been discussed as important model parameters [3, 13].",1,neutral
"To this end, we adopt the standard contrastive learning objective for ViT-MoCo [10] and masked image modeling training objective for BEiT and ViT-MAE with a masking ratio of 40 % [3] and 75 % [13], respectively.",2,positive
"To evaluate whether the observed effects on ViT-MoCo and ViT-MAE are a result of their pre-trained checkpoints, we train them from scratch on ImageNet-1k and our counterfactual data (2-sided CDA).",0,negative
"Generative methods [3, 10, 19, 65] reconstruct the original input sample from the corrupted one.",1,neutral
"of robotics, Transformers have found practical applications in diverse areas such as path planning [15], [16], object recognition [17], and grasping [18].",1,neutral
"Recent works [63, 4] reveal the visual in-context ability through a simple Mask Image Modeling [15] scheme.",1,neutral
"To simplify pre-training and reduce computational overhead, MAE [14] only feeds the visible tokens into the encoder and encourages the decoder to reconstruct the raw pixels of masked patches.",2,positive
"2, we apply MFF to two MIM baselines, namely MAE [14] and PixMIM [30], and show the improvements brought by such design.",2,positive
"These methods cater to diverse inputs, including images[5, 1, 14, 54], videos[29, 21], and multi-modality inputs[36, 3].",1,neutral
"We evaluate it on MAE[14] and PixMIM[30], and more detailed results are shown in Section 5.",0,negative
"Among the various MIM methods available, pixel-based approaches such as MAE [14] are particularly interesting because of their simple pre-training pipeline and minimal computational overhead.",1,neutral
experiments based on the representative work of MAE [14] to uncover its neglected design aspects.,2,positive
"In order to investigate whether being biased towards low-level features is the sole and inherent drawback of pixel-based MIM, we introduce multi-level feature fusion to EVA[10] and supervised ViT[14].",2,positive
"We employ multi-level feature fusion to enhance MAE [14], resulting in MFFMAE.",2,positive
"3, we apply the first pilot experiment introduced in Section 1 to EVA [10] and supervised ViT [14], to investigate whether they too require lowlevel features and to confirm that the bias towards low-level details is the unique and inherent drawback of pixel-based MIM.",2,positive
"As shown in Figure 5, the expected hessian max eigenvalue of MFFMAE is smaller than that of MAE[14].",1,neutral
We choose the MSE loss instead of MAE to implement better robustness.,2,positive
and a Vision Transformer(ViT) based module [18].,2,positive
"7, which is usually composed of MSHA; besides, we remove the mask operation, and the
decoder is retained in MAE.",2,positive
"To extract the global feature information of the image, we employ the encoder and decoder in the Masked Autoencoders Are Scalable Vision Learners (MAE) [18], as shown in Fig.",2,positive
"Outperforming its supervised counterpart for pre-training, visual SSL [He et al., 2020; He et al., 2022] has become an active research field.",2,positive
"To this end, MAE [He et al., 2022] experiments with end-to-end training of masked autoencoder.",2,positive
", 2008] and recent masked autoencoder [He et al., 2022] both reconstruct a clean input from a corrupted one by predicting masked input content from unmasked input content.",2,positive
"Notably, we use MAE to refer to the method in [He et al., 2022] not as shorthand for masked autoencoder to avoid confusion.",2,positive
"In other words, the success of masked autoencoder in vision paves a path: SSL in vision“may now be embarking on a similar trajectory as in NLP” [He et al., 2022].",2,positive
"Specifically, their proposed MAE [He et al., 2022] directly predicts masked patches from the unmasked",2,positive
"Therefore, this section starts with introducing BEiT with its improved variants and then discusses the seminal work MAE [He et al., 2022] .",1,neutral
"Despite high similarity regarding pretext task, the masked autoencoder introduced in [He et al., 2022] differs from early denoising autoencoder [Vincent et al.",2,positive
"The success of MAE [He et al., 2022], outperforming jointembedding methods, revives this straightforward visual pretraining method.",2,positive
"More recently, MAE [He et al., 2022] simplifies this",1,neutral
"There exist three broad classes of self-supervised foundation models that extract task-agnostic visual features: (a) joint embedding methods (DINO [27], DINOv2 [28]), (b) contrastive learning methods (CLIP [29]), and (c) masked autoencoding approaches (MAE [35]).",1,neutral
"In our initial experiments, we found all these models to perform better than MAE [35], which only has token-level selfsupervision.",2,positive
"initial experiments, we found all these models to perform better than MAE [35], which only has token-level selfsupervision.",2,positive
"Recently, masked autoencoding using transformers [19] has emerged as an efective and scalable representation learning framework.",1,neutral
"visual representations [40], [41], [42].",1,neutral
"In this work, we take the visiontransformer model (ViT) as the backbone model, which is pre-trained in ImageNet-1K by masked auto-encoder (MAE) [31].",2,positive
"The gating part of Gate-DAP is:
ZIt = ViT|MAE(It), (4) Z′It = SpaG(Z I t ), (5)
HIt = MemoG(H I t−1, [Z ′I 1, ...,Z ′I t ]), (6)
[M′It ,M ′F t ,M ′S t ,M ′D t ] = MU-InfoG(H I T ,H S T ,H F T ,H D T ), (7)
M′t = Stack[M′ I t ,M ′F t ,M ′S t ,M ′D t ].",1,neutral
", 2021) ViT-B self-supervised MAE (He et al., 2022) ViT-B self-supervised CLIP (Radford et al.",2,positive
"In our approach, we adopt the masked autoencoder [19] to improve the channel estimation performance.",2,positive
"According to [19], prediction performance improves when the pretrained weights learned through the pre-training process with 75% of the masked image is used for training for downstream tasks.",1,neutral
"The Masked Autoencoders (MAE) architecture [24], for example, efficiently reconstructs missing patches in pixel space and achieves strong performance on large labeled datasets.",1,neutral
"The idea behind masked image modeling [24, 47, 5] has emerged as a way to address image denoising.",1,neutral
"Masked Auto-Encoders (MAE) [24], which are trained to minimize a reconstruction error in pixel space, have demonstrated competitive performances in fine-tuning with respect to SSL methods relying on handcrafted image augmentations.",1,neutral
"Clevr/Count Clevr/Dist
MIM methods, without view data augmentations data2vec [3] ViT-L/16 72.7 53.0
MAE [24] ViT-B/16 86.6 70.8 ViT-L/16 92.1 73.0
I-JEPA [1] ViT-B/16 82.2 70.7 ViT-L/16 85.6 71.2
Low-level vision.",0,negative
"This approach has shown remarkable progress in various domains, including natural language processing [16, 8, 15], speech recognition [4, 2, 44], and computer vision [50, 35, 10, 24].",1,neutral
"Epochs Top-1
MIM methods, without view data augmentations data2vec [3] ViT-L/16 1600 53.5
MAE [24] ViT-B/16 1600 68.0 ViT-L/16 1600 76.0
I-JEPA [1] ViT-B/16 600 72.9 ViT-L/16 600 77.5
Method Arch.",0,negative
"CIFAR100 Places205 iNat18
MIM methods, without view data augmentations data2vec [3] ViT-L/16 59.6 36.9 10.9
MAE [24] ViT-B/16 68.1 49.2 26.8 ViT-L/16 77.4 54.4 33.0
I-JEPA [1] ViT-B/16 69.2 53.4 43.4 ViT-L/16 83.6 56.5 48.4
Dense prediction.",0,negative
"Figure 1: Given a partial image of a dog, can you precisely determine the location of its tail? Existing Masked Image Modeling (MIM) models like [24, 1] are deterministic and predict masked tokens conditioned on fixed positions (a), while FlexPredict predicts masked tokens conditioned on stochastic positions (b).",1,neutral
"This is contrary to past works that use a deterministic mapping to determine the positional embedding of a token [1, 24].",1,neutral
"J-Mean F-Mean J&F Mean
MIM methods, without view data augmentations MAE [24] ViT-B/16 49.4 52.6 50.9 ViT-L/16 52.5 54.3 53.4
I-JEPA [1] ViT-B/16 56.1 56.2 56.1 ViT-L/16 56.1 55.7 55.9
Method Arch.",0,negative
"Following past works, we focus on evaluating the (target) encoder representations [24, 1], and use the standard VISSL [21] evaluation protocol like in [1].",2,positive
"MAE: (He et al., 2022) MAE or Masked Auto Encoders are Vision Transformers that have been pre-trained using the masked image-filling task.",1,neutral
", 2023) and self supervised (Chen et al., 2020b; Caron et al., 2020; Zbontar et al., 2021; He et al., 2022) techniques have extensively investigated , while for NLP, the widely used training objective is next token prediction (Brown et al.",2,positive
"…models, supervised (Touvron et al., 2021; Dehghani et al., 2023) and self supervised (Chen et al., 2020b; Caron et al., 2020; Zbontar et al., 2021; He et al., 2022) techniques have extensively investigated , while for NLP, the widely used training objective is next token prediction (Brown et al.,…",2,positive
"IGPT [3] proposes a generative pre-training technique and shows promising results on classification tasks, while MAE [10] adopts a similar pre-training scheme as BERT and predicts the masked regions of an image with unmasked ones.",2,positive
"To further explore the efficacy of the learned features, we employ a partial fine-tuning method based on the protocol proposed in [26].",2,positive
"Masked Image Modeling (MIM) [57, 3, 26, 50, 16, 51] methods are new paradigms of self-supervised learning which randomly mask a portion of the input image and reconstruct the masked parts via the reasoning of other unmasked parts.",1,neutral
"The knowledge of masked images can be learned in alterable manners, including dVAE codebooks in BeiT [3], raw RGB pixels in SimMIM [51] and MAE [26], HOG features in MaskFeat [50], etc.",1,neutral
"Previous works [57, 3, 26, 50, 16, 51] have demonstrated that MIM-based methods can learn better local and global representation than conventional self-supervised methods based on contrastive learning [40, 10].",1,neutral
"In addition, transformer-style models [21, 22, 43] have the capability to learn enriched representations through large-scale unsupervised training [44, 45], which is more efficient compared to CNN-based models.",1,neutral
"To accomplish this, we employ a Masked Autoencoder (MAE) model [30], a self-supervised learning approach that utilizes the ViT model as its backbone.",2,positive
"During the exploration, we found an innovative application of MAE on the limited dataset, which is not studied by the previous work [30].",2,positive
"3 TRANSFER LEARNING ON DOWNSTREAM TASKS After training, the Mask Transformer has learned powerful geometric knowledge through generative reconstruction (He et al., 2022), which enables VPP to serve as a self-supervised learning method for downstream representation transferring.",1,neutral
"After training, the Mask Transformer has learned powerful geometric knowledge through generative reconstruction (He et al., 2022), which enables VPP to serve as a self-supervised learning method for downstream representation transferring.",2,positive
"in 2D domain such as contrastive learning [19, 7] and mask modeling [18, 3] have also been adopted to 3D point cloud analysis [61, 31, 24] in recent research.",1,neutral
"PointContrast [57] embraces the principle of contrastive learning, whereas PointBERT [61] and PointMAE [31] integrate reconstruction pretext tasks.",1,neutral
"The other method entails utilizing generative tasks to restore the data from partially or disrupted inputs, such as MAE [18] and BEiT [3].",1,neutral
"Recently, inspired by MAE [18] in the image domain, generative pre-training in 3D domain mainly focuses on implementing random masking as T and utilizing
Transformers model as M for reconstruction [61, 31, 24, 63].",1,neutral
"In object-level analysis, generative pre-training inspired by MAE [18] has been studied thoroughly, but their performances still lag behind architecture-based methods like PointNeXt [36].",1,neutral
"Recently, inspired by MAE [18] in the image domain, generative pre-training in 3D domain mainly focuses on implementing random masking as T and utilizing Transformers model as M for reconstruction [61, 31, 24, 63].",1,neutral
"For the former stage, we resort to masked autoencoders (MAE) [31] and image mixing techniques to enhance representation learning of self-supervised depth estimation models.",2,positive
• Masked image modeling: The masking-based image reconstruction approach [31] exhibits potential for improving OoD robustness; this simple operation encourages the model to learn more robust representations by decoding masked signals from remaining ones.,1,neutral
"In fact, MAE-based data processing can serve as a powerful data augmentation technology [31].",1,neutral
"In this challenge, we directly load a pre-trained MAE model [31] for image reconstruction of the input image x.",2,positive
We initialized its parameters with ImageNet-1k MAE [11] pre-training; 2) a simple feature pyramid [18] network to introduce multiscale supervision; 3) a morphology-based edge loss strat-,2,positive
"This suggests that MAE pre-training is indispensable for ViT-based image manipulation localization, and demonstrates that MAE is a powerful and effective method for downstream tasks with limited datasets.",2,positive
MAE pre-train We initialize the ViT with parameters pretrained on ImageNet-1k [5] with Masked Auto Encoder (MAE) [11].,2,positive
"We initialized its parameters with ImageNet-1k MAE [11] pre-training; 2) a simple feature pyramid [18] network to introduce multiscale supervision; 3) a morphology-based edge loss strat-
egy is proposed to ensure edge supervision.",2,positive
"As shown in w/o MAE aspects in Table 3, the use of Xavier initialization to train the model resulted in complete non-convergence.",1,neutral
"We initialize ViT-B with MAE pre-trained weights on ImageNet1k and used the AdamW optimizer [21] with a base learning
1We noticed some resolution errors in the public CASIAv2 dataset.",2,positive
"For initialization, besides full setup with MAE pre-training on ImageNet-1k, we test Xavier initialization and ordinary ViT pre-training on ImageNet-21k by classification.",2,positive
The results indicate that MAE greatly alleviated the problem of non-convergence and over-fitting of ViT on small datasets.,2,positive
"In conclusion, our findings are:
MAE pretrain is mandatory.",0,negative
"The common weakness of all the above works is that they do not evaluate their SSL models in linear or non-linear probing setups, even though these setups are de-facto standards for evaluation of SSL methods in natural images [8,13,23].",1,neutral
"In recent years, self-supervised learning in computer vision has evolved from simple pretext tasks like Jigsaw Puzzles [22], Rotation Prediction [17], and Patch Position Prediction [10] to the current SotA methods such as restorative autoencoders [13] and contrastive [8] or non-contrastive [9] joint embedding methods.",1,neutral
"SSL, such as that by DINO [7], MoCoV3 [11], BeiT [4], and MAE [16], is also able to achieve similar performance using ImageNet-1k for pre-training.",2,positive
"MAE [16] also show a high performance, even when trained on ImageNet-1k.",1,neutral
"[15], P2C groups input points as patches that represent a small but possibly continuous region on the underlying surface, where we expect the network to predict masked patches based on unmasked regions.",1,neutral
"Conventionally, BVQA is regarded as a regression task, which can be supervised by the mean absolute error (MAE) and the mean squared error (MSE) [9], [13], [15], [16], [19], [20] to predict absolute quality
(i.e., MOSs).",1,neutral
"BVQA is commonly formulated as a regression problem, optimized for MAE or MSE as the loss function.",1,neutral
"We conjecture that these augmentations as visual distortions are not common in LSVQ, and thus may not hurt the quality prediction performance as measured on LSVQ. MAE attempts to reconstruct the masked patches of an image as a way of encouraging semantic-related distortion-unaware features, hence not beneficial to the BVQA task.",2,positive
", resulting from unsupervised learning [58], [59], [60]) will be ablated in Section 5.",0,negative
"2) initializations from unsupervised pre-training, including MoCo [58], MAE [59], and CLIP [60].",0,negative
Here we compare MAE and MSE with the default PLCC.,2,positive
"Specifically, we initialize ResNet-50 with weights supplied by MoCoV2 and CLIP, ViT-B by MAE and CLIP, and Swin Transformer-B by ImageNet-22k.",2,positive
"Here, we further explore other possibilities: 1) initializations from pre-training on larger computer vision datasets (e.g., ImageNet-22k) and
2) initializations from unsupervised pre-training, including MoCo [58], MAE [59], and CLIP [60].",2,positive
"We first arrange the features in descending order based on their Fisher Score, and then select a predetermined proportion of top-ranked features to retrain a classifier to probe the quality of features [30], [31], [32].",2,positive
"a probe [30], [31], [32] on the masked features, denoted as z m, to quantitatively assess their quality.",1,neutral
"the authors perform a 2D-sine-cosine linear embedding on the patches which are fed as input to the multimodal ViT encoder which operates only on the visible tokens, tremendously reducing the cost of computation [179].",2,positive
"In particular, [178] proposes Multi-Task MAE (MultiMAE), a pretraining strategy reconstructing diverse image modalities.",1,neutral
"Once these labels are obtained, similar to original MAE [179], the authors sample a large portion of the image modalities divided into 16x16 patches.",2,positive
[178] leverages the recent the success of Masked AutoEncoders (MAEs) [179].,2,positive
"MAEs [179] are asymmetric encoderdecoder models in which the encoder only operates on a small
portion (about 15",1,neutral
"Once these labels are obtained, similar to original MAE [179], the authors sample a large portion of the",1,neutral
MAEs [179] are asymmetric encoderdecoder models in which the encoder only operates on a small,1,neutral
Respective image modalities are patch-wise masked in a similar way as MAE [179].,1,neutral
"For example, He et al. (2022) observed that a supervised ViT-H/14 overfits on ImageNet1k (Russakovsky et al., 2014) without a model EMA, achieving an accuracy of 80.9%.",2,positive
"…model EMA inhabits wider minima than the target model, reducing overfitting and improving generalization (Huang et al., 2017; Izmailov et al., 2018; He et al., 2022); ii) compared to the target model, the model EMA moves slowly, making it useful as a stabilizer for networks governing Bellman…",2,positive
It is also typical to use a Batch Normalization layer without trainable affine terms before this linear layer as in He et al. (2022) to stabilize probe training.,1,neutral
"This recipe was developed by starting from a well-known supervised ViT-B/16 recipe (He et al., 2022) and performing a search over weight decay and learning rate hyperparameter choices.",2,positive
"This functional copy, which we call the model EMA, has a number of desirable properties: i) the model EMA inhabits wider minima than the target model, reducing overfitting and improving generalization (Huang et al., 2017; Izmailov et al., 2018; He et al., 2022); ii) compared to the target model, the model EMA moves slowly, making it useful as a stabilizer for networks governing Bellman updates in reinforcement learning, (Lillicrap et al.",1,neutral
"Additionally, the pre-training weights of both the pure image pre-trained ViTs (i.e. , DINO, DINOv2 and MAE) and the image encoder ViTs of vision-language models (i.e. CLIP and BLIP) are compatible with MGP-STR.",2,positive
"Besides DeiT-Base, we also initialize MGP-STR with various recent pre-trained ViT backbone models (i.e. , DINO [96], DINOv2 [97], MAE [98] CLIP [14] and BLIP [99]) to verify
the effectiveness of our method.",2,positive
", DINO [96], DINOv2 [97], MAE [98] CLIP [14] and BLIP [99]) to verify",1,neutral
"(3) We verify various recent ViT backbone models (such as those from MAE, DINO and BLIP) and show that, once adequately trained with enough domain-specific data (text images in this work), these ViT models can work equally well on the task of scene text recognition (see Sec.",2,positive
"Self-supervised pretraining paradigms [3, 10, 24] has also been explored, leading to state-of-the-art results.",1,neutral
We also apply E(2)VPT to two self-supervised objectives: MAE [24] and MoCo v3 [10].,2,positive
Image Classification accuracy for different pretrained objectives — MAE [24] and MoCo v3 [10] with ViT-Base [12] as backbone.,1,neutral
"We respectively examine the performance and robustness of E(2)VPT on ViT [12], Swin [54], and two selfsupervised objectives — MAE [24] and MoCo v3 [10].",2,positive
Pretrained objectives MAE [24] MoCo v3 [10] Tuned/ VTAB-1k [96] [19] Tuned/ VTAB-1k [96] [19] Methods Parms & Data Total Natural [7] Specialized [4] Structured [8] Total Natural [7] Specialized [4] Structured [8],0,negative
"These methods involve freezing most of the backbone and only fine-tune a small portion of the parameters, such as linear [32] or MLP heads [9], or a few blocks/layers of the backbone [24, 65, 91, 99].",1,neutral
"We conducted experiments with two self-supervised objectives, MAE [24] and MoCo v3 [10], on backbones pretrained without labeled data, following the approach of VPT [34].",2,positive
"Despite the development of self-supervised learning [3], and the utilization of vast amounts of Internet images and descriptions [4], densely annotated datasets are still scarce and obtained through manual annotation, which is not only a labor-intensive and",1,neutral
"Recent major successes are broadly based on momentum-based contrastive learning [13], [30], [31], and masked auto-encoding [3], and natural language supervision [4].",1,neutral
"In light of the current state of computer vision, two upstream tasks have been promising for pretraining: self-supervised learning (SSL) [327], [328] and image–text retrieval [304].",1,neutral
MAEs learn to reconstruct input data after randomly masking certain input features.,1,neutral
"In the field of medical image analysis, MAE pre-training has also been found to be effective [32].",1,neutral
"Following [11], we adopt Mean-SquareError(MSE) to compute the reconstruction loss:",1,neutral
One representative methodology for self-supervised learning is the masked autoencoder (MAE) [11].,1,neutral
"Following [11], we divided the 3D images into sub-volumes of the same size and randomly masked a portion of them, as demonstrated in Figure 2.",2,positive
"Recently, masked autoencoder (MAE) [13] based models demonstrated great scalability and substantially improved several self-supervised learning benchmarks [30].",1,neutral
"Furthermore, We consider an additional baseline for model adaptation based on the patch reconstruction objective in [13] on pooled normal and unlabeled images, denoted as (AMAE - Stage 2 (Mask Rec.",2,positive
"As for Stage 1 of our method, (i) AMAE creates synthetic anomalies from only normal training images, and the usefulness of pre-trained MAE [13] is evaluated by training a lightweight classifier using a proxy task to detect synthetic anomalies.",2,positive
"2, Top), using the same MAE architecture and pixel-wise mean squared error (MSE) optimization in [13].",1,neutral
"Despite sharing the same reconstruction principle, MAE has not been explored in k-space interpolation of Cartesian undersampled data.",1,neutral
"For that, we use a ViT/MAE [5,11] encoder E consisting of alternating blocks of multi-head selfattention and multi-layer-perceptrons.",2,positive
"In the recent past, masked image modeling [34,11] has emerged as a promising method for learning rich generalizable representation by reconstructing the whole image from a masked (undersampled) input.",1,neutral
Masked Autoencoders (MAE) [11] are one such model that leverages the global dependencies of the undersampled input using Transformers and learns masked-based rich feature representation.,1,neutral
", ViTMAE (He et al., 2022) and MultiMAE (Bachmann et al.",1,neutral
"To do this, we employed a pre-trained feature extractor model, either a supervised model such as a pre-trained CNN (e.g., EfficientNet (Tan & Le, 2019)) or a pre-trained model in an unsupervised fashion such as a masked autoencoder (e.g., ViTMAE (He et al., 2022) and MultiMAE (Bachmann et al., 2022)).",2,positive
"Recently, masked autoencoders (MAE) (He et al., 2022; Bachmann et al., 2022), a class of autoencoders, have attracted attention in computer vision.",1,neutral
"…do this, we employed a pre-trained feature extractor model, either a supervised model such as a pre-trained CNN (e.g., EfficientNet (Tan & Le, 2019)) or a pre-trained model in an unsupervised fashion such as a masked autoencoder (e.g., ViTMAE (He et al., 2022) and MultiMAE (Bachmann et al., 2022)).",2,positive
"Finally, some methods are based on masking and patch-reconstruction (Bao et al., 2022; He et al., 2022; Zhou et al., 2022; Assran et al., 2022; 2023).",1,neutral
"Given the final video frame-level representations X and text word-level X, we first randomly replace a noun phrase or verb phrase representations with mask embeddings [75], where each mask token is a shared, learned vector.",1,neutral
"1 Introduction Many of the most successful machine learning systems for computer vision [13, 34] and natural language processing [9, 16] leverage large amounts of unlabeled or weakly-labeled data.",1,neutral
"[34] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll’ar, and Ross B.",0,negative
"For self-supervised objectives (i.e., MAE and CLIP), we use ViT-Base/16, ViT-Large/14, ViT-Large/16 and ViT-Huge/14 as basic backbones.",2,positive
"To explore the effect of different pre-training strategies, we conduct experiments by using various backbones under supervised (SUP [10]) and self-supervised (MAE [15], CLIP [39]) settings.",2,positive
"Additionally, we employ ViT-Base/16, ViT-Large/14, ViT-Large/16 and ViT-Huge/14 for assessing generalization to different pre-training strategies (e.g., MAE [15] and CLIP [39] ).",2,positive
"In both of natural language processing and computer vision communities, transferring pretrained large models to downstream tasks has long been a popular paradigm [9, 2, 50, 4, 15, 39, 5, 7], which al-",1,neutral
"Benefiting from the emergence of huge-scale datasets [8, 42, 41], the rapid development of neural network architectures [10, 17, 43, 35] and self-supervised learning [15, 39, 5], large-scale pre-trained models dependent on sufficient computational resources show the great potential of",1,neutral
"For fine-tuning models pre-trained by MAE [15], we follow its official configurations of LP on ImageNet-1K, which adopt a linear scaling rule [13].",2,positive
"Since MAE aims to optimize parameters for image reconstruction, which suffers from a clear gap for classification task and may require to tune amount of parameters for per-Figure",2,positive
"Moreover, this learning-to-reconstruct idea naturally links to recent advancements in masked data modeling [9, 7, 24], and enables our model to recover complete observation from even more corrupted inputs obtained, e.",1,neutral
"It is learned in a supervised rather than self-supervised [9, 24, 7, 16] manner, because we can easily obtain the supervision by aggregating raw sensor data of all agents.",1,neutral
"B.1 Video Recognition
For video recognition, we follow VideoMAE [111] to modify the tokenizer by replacing the 2D embedding layer with a 3D embedding layer to simultaneously encode the spatial-temporal information from input frames.",1,neutral
"Method Modalities Share Parameters Unpaired Data Transformer [11] 8 8 ViT [13], Swin Transformer [19], MAE [4] 8 8 Point Transformer[9], PCT [22], Point ViT [21] 8 8 AST [6], SSAST [23] 8 8 CLIP [24], Flamingo [25], VLMO [2], OFA [10] 8 8 BEiT-3 [3] Several Layers 8 ImageBind [26] 8 8 Meta-Transformer [ours] Whole Backbone 4",2,positive
"In contrast, Meta-Transformer-B16F delivers the train and validation MAE scores of 0.8034 and 0.8863, which reveals the limited ability of current Meta-Transformer architecture for structural data learning.",2,positive
"Among all the methods, Graphormer shows the best performance with the lowest train and validation MAE scores of 0.0582 and 0.1234, respectively.",0,negative
"Method Modality UCF101 Params OPN [109] V 59.6 - SimCLR [110] V 88.9 86.9M VideoMAE V1 [111] V 96.1 86.9M VideoMAE V2 [112] V 99.6 86.9M ViT [13] (from scratch) V 51.4 86.9M Meta-Transformer-B16F V 46.6 1.1M
cantly reduced trainable parameter count, suggesting the potential benefit of unified multi-modal learning and less architectural complexity.",2,positive
"For instance, images exhibit a high degree of information redundancy due to densely packed pixels, which is not the case with natural language [4].",1,neutral
"We choose MoCo-v3 [24] and MAE [14] as archetypes of contrastive and restorative SSL respectively, using a ViTB architecture (with 12 layers) for both methods to ensure a fair comparison.",2,positive
"restorative pre-training approach is the masked autoencoder (MAE) [14], which employs an asymmetric encoder-decoder architecture and masks 75% of the image.",2,positive
Masked autoencoder (MAE) [14] learns representations from unlabeled images by reconstructing the original image based on a partially masked input.,1,neutral
"In this section, we set up strong contrastive and restorative self-supervised baselines for medical imaging based on the robust self-supervised learning methods MoCo-v3 [24] and MAE [14].",2,positive
"Here, we provide a brief overview of the MoCo-v3 [24] and MAE [14] methods, which we use to build strong contrastive",2,positive
"ing [5]–[10] aims to bring representations of similar images closer together in the embedding space while pushing representations of dissimilar images farther apart, whereas restorative learning [11]–[14] focuses on reconstructing missing regions from partially masked inputs.",1,neutral
There is a long history of restorative pre-training in computer vision [11]–[14].,1,neutral
We adopt the same fine-tuning settings as MAE [14] and fine-tune on a single V100 GPU.,2,positive
"In this paper, we leverage the MAE method [14] to develop optimal fine-tuning strategies that effectively utilize restorative pre-trained features for medical imaging analysis.",2,positive
"…into effective utilization of task-specific unlabeled data is needed, with a promising starting point in approaches that achieve SOTA performance on ImageNet without much extra data such as Data-efficient Image Transformers (Touvron et al., 2021) and Masked Autoencoders (He et al., 2022).",2,positive
"Further investigation into effective utilization of task-specific unlabeled data is needed, with a promising starting point in approaches that achieve SOTA performance on ImageNet without much extra data such as Data-efficient Image Transformers (Touvron et al., 2021) and Masked Autoencoders (He et al., 2022).",2,positive
"For instance, Masked Autoencoders achieved SOTA performance
on ImageNet-1K and ImageNet-C through MIM (He et al., 2022).",1,neutral
"In computer vision, masked image modeling (MIM) has emerged as an effective generative task (Bao et al., 2022; He et al., 2022; Z. Xie et al., 2022).",1,neutral
"…Supervised is pre-trained using supervised learning on ImageNet-1k, SWAG is Supervised Weakly from hashtAGs (Singh et al., 2022), CLIP is Contrastive Language-Image Pretraining (Radford et al., 2021), and MAE is for masked-autoencoder (He et al., 2022), a generative pre-training technique.",2,positive
"MIM does not require specific data augmentation and more robust for downstream tasks [1, 71, 64, 23].",1,neutral
", 2020) or an autoregressive denoising approach (Vincent et al., 2008; He et al., 2022).",1,neutral
", ImageNet21K and JFT-300M) increase, the scale of vision models have increased significantly, from million-scale to billionscale [45, 25, 47, 7, 38, 15].",2,positive
"This philosophy of mask-reconstruction has led to a series of visual pre-training methods such as BEiT [3], BEiT-v2 [37], and MAE [14].",1,neutral
"…been shown to enhance generalization performance (Dittadi et al., 2022; Locatello et al., 2020), but this approach has primarily been explored in the context of pretraining or incorporating auxiliary loss (He et al., 2022; Rasmus, Berglund, Honkala, Valpola, & Raiko, 2015; Sabour et al., 2017).",1,neutral
", 2020), but this approach has primarily been explored in the context of pretraining or incorporating auxiliary loss (He et al., 2022; Rasmus, Berglund, Honkala, Valpola, & Raiko, 2015; Sabour et al., 2017).",1,neutral
"Previous research has similarly argued for benefits from a reconstruction-based masking approach, but in the context of augmenting datasets for improving the learning of robust visual representations for use in downstream tasks (M. Chen et al., 2020; He et al., 2022).",1,neutral
The first is the limited precision of the ViTMAE tokenization of the patches; here we have adopted 256 dimension latent vector.,1,neutral
"We thereby construct Enki, a ViTMAE model trained on SST ocean model outputs that may then be applied to actual remote sensing data.",2,positive
"While [8] advocates t%=75 to insure generalization, we generated Enki models with t%=[10,20,35,50,75].",2,positive
"In this study, we use the fine-scale (1/48◦, 90-level) ocean simulation from the Estimating the Circulation and Climate of the Ocean (ECCO) project and referred to as LLC4320 to train an implementation of the ViTMAE.",2,positive
"dogs, landscapes) can be described by a language and therefore analyzed with natural language processing (NLP) techniques and (2) one can frequently recover the sentiment of sentences that are missing words and then predict these words, [8] demonstrated the remarkable effectiveness of ViTMAE to reconstruct masked images.",1,neutral
"We have designed and trained a machine learning model, inspired by ViTMAE [8], named Enki to reconstruct masked SST fields [12].",2,positive
"Guided by the intuition that (1) natural images (e.g. dogs, landscapes) can be described by a language and therefore analyzed with natural language processing (NLP) techniques and (2) one can frequently recover the sentiment of sentences that are missing words and then predict these words, [8] demonstrated the remarkable effectiveness of ViTMAE to reconstruct masked images.",1,neutral
"In this manuscript, we introduce a novel approach, inspired by the vision transformer masked autoencoder (ViTMAE) model of [8] to reconstruct masked pixels in satellite-derived fields.",2,positive
"As described in Section 3, we trained Enki with a range of training mask percentiles expecting best performance with t%=75 as adopted by [8].",0,negative
The ViTMAE approach of Enki offers a qualitative advance over traditional deep-learning vision models.,2,positive
"Central to ViTMAE’s success was its training on a large corpus of unmasked, natural images.",1,neutral
"A primary hyperparameter of the ViTMAE is the training percentage (t%), i.e. the percentage of pixels masked during training (currently a fixed value).",1,neutral
’) against (a) a hide-and-predict objective defined in the pixel space with per-patch normalization as in MAE [37] (‘Pixels’); and (b) a hide-predict-objective in which the decoder must directly regress the cenc-dimensional output teacher-token embeddings (before any code assignObjectives LIMG +hide-and-predict variants Targets Assign.,1,neutral
"Although they reach strong results when fine-tuning on a downstream task with sufficient training data [6, 37], they do not provide high-level “ready-to-use” image representations –which are typically evaluated in linear probing and k-NN classification settings.",1,neutral
"Translated in the vision paradigm as Masked Image Modeling (MIM), such tasks have rapidly gained popularity through their simplicity and their direct compatibility with ViTs processing images as sequences of patch tokens [6, 16, 37, 47, 78, 88].",1,neutral
"Most applications in the image domain, usually named Masked Image Modeling (MIM), define low-level image information, such as pixels [37,78], as prediction targets.",1,neutral
"MIM approaches come with different reconstruction targets for the masked input tokens: RGB pixels [37, 78], handcrafted HOG descriptors [74] or token features computed by a teacher network [3, 5, 6, 25, 28, 47, 88].",1,neutral
Setup MAE [37] DINO† [14] iBOT† [88] MOCA (ours),0,negative
"As expected, MAE [37] struggles in these setups.",0,negative
"Self-supervised representation learning for Vision Transformers (ViT) [23, 73] has attracted a significant amount of attention over the last years [3, 6, 37, 47, 60, 78, 84, 88].",1,neutral
predicting token ids [6] or reconstructing pixels [37]).,1,neutral
"While simple in structure, this recipe led to the development of surprisingly effective Large Language Models (LLMs) (4), able to process and generate text with outstanding human-like capability, Vision Transformers (ViTs) (25; 13) able to extract meaningful representations from images and videos with no supervision (6; 18), and VisionLanguage Models (VLMs) (2; 36; 28), that can bridge those data modalities describing visual inputs in language, or transforming language descriptions into visual outputs.",1,neutral
"Following the same group-based principle [64], [203], SegCLIP [204] adds a reconstruction loss [217] and a superpixel-based KL loss to the normal image-text contrastive loss.",1,neutral
[18] introduced masked autoencoders (MAE) as a scalable self-supervised learning approach for computer vision aim to reconstruct missing patches in images.,1,neutral
"Compared to the standard MAE for images [27], the image decoder has access to two additional types of context information: (1) The encoded patch embedding from the unmasked image patches of the neighboring frames; (2) The encoded slot tokens from a subset of context frames.",2,positive
"When no context frame is used, we essentially utilize only patch-level representations to perform reconstruction with the temporal transformer (simulating a perframe MAE followed by a temporal transformer).",1,neutral
"The pretraining objective is inspired by masked autoencoding (MAE) for unlabeled video frames, where the aim is to reconstruct a subset of “masked” image patches given the “unmasked” image patches as context.",2,positive
"Masked Autoencoders (MAE) [27], on the other hand, simply regress to the pixel values of these tokens.",1,neutral
"Our MAE baselines are pretrained with the same hyper parameters (e.g. optimization and mask ratio) as IV-CL, which we have observed to be optimal based on the validation set performance.",2,positive
"Second, the slot tokens are provided as context information via a temporal transformer network for other images in the same video, where the goal is to perform video reconstruction via the masked autoencoding [27] objective with the temporal context.",1,neutral
Our reimplementation of image and video MAEs achieve very similar performances on their original benchmarks.,2,positive
"MAE has been extended to regress features [57] and to learn video representations [53, 20].",1,neutral
"However, for video-based MAE, we observe that the “un-factorized” backbone leads to training collapse on CATER.",1,neutral
"As most of the prior work require explicit object detection and are not end-to-end trained, we reimplement an image-based MAE [27] and a video-based MAE [53] baseline and analyze the impact of inductive biases (using slot tokens or not) as well as pretraining objectives (predictive coding given compressed context, or autoencoding the original inputs) on the reasoning performance.",2,positive
"%) employed in vanilla MAE [27], we found that the optimal masking ratio was 37.5% on downstream CATER accuracy.",0,negative
"We use the same number of layers, hidden size, and other hyperparameters as recommended by [27].",2,positive
We confirm empirically that the proposed method outperforms MAE and its video extension by large margins.,2,positive
"Masking ratio: Contrary to the large masking ratio (75%) employed in vanilla MAE [27], we found that the optimal masking ratio was 37.",1,neutral
Image Decoder for Pre-training: We use the same image decoder as in [27].,2,positive
"Masked Autoencoder (MAE) [He et al., 2021] is a generative-based SSL method with the training objective of reconstructing masked images.",2,positive
"Unfortunately, existing methods [Misra and Maaten, 2020, Chen et al., 2020a, Grill et al., 2020, He et al., 2020, 2021] are designed for discriminative tasks and not suited for generative modeling.",1,neutral
"There are mainly three categories, i.e., pretext task-based [Misra and Maaten, 2020], contrastive methods such as SimCLR [Chen et al., 2020a], MoCo [He et al., 2020], BYOL [Grill et al., 2020], SimSiam [Chen and He, 2021], and generative-based such as Masked Autoencoder (MAE) [He et al., 2021].",2,positive
MAEs are designed for discriminative tasks and their generation capability is yet to be explored.,2,positive
"In MAE, a high mask ratio and a relatively weak decoder have been verified as important factors for good discriminative performance, which is also corroborated by our perspective of preserving distributions in different dimensions.",1,neutral
", 2020], SimSiam [Chen and He, 2021], and generative-based such as Masked Autoencoder (MAE) [He et al., 2021].",2,positive
"MAE [10], for example, trains their linear layer for 100 epochs with 16,384 images per batch.",1,neutral
"Method Accuracy
Supervised 82.5% MoCo v3 84.1% MAE 84.9% MAGE 84.3% GD (Linear, pool 2×2) 73.17% GD (Linear, pool 4×4) 73.50%
Table 4: Stable Diffusion linear probe results.",0,negative
"Despite these preconceptions, we suggest that the early success of BigBiGAN is endorsed by recent approaches such as MAE [10] and MAGE [8], where the model must tend to low-level pixel information, but learns models which are also very good for classification tasks.",2,positive
"MAE [10] and iBOT [66] train an autoencoder via masked image modeling, and several other transformer-based methods have been built under that paradigm [67–69].",1,neutral
"Self-supervised learning has enabled substantial development in computer vision [60, 13, 14, 5], and several related works have also emerged in the field of STR [34, 61, 37, 1].",1,neutral
We adopt MAE [13] framework to pre-train the ViT backbone in MAERec.,2,positive
We choose Vision Transformer (ViT) [8] as the default backbone for its effortless applicability in masked image modeling [13].,2,positive
We adopt the framework of MAE [13] with minor modifications.,2,positive
"The pretrained visual representations from such methods have demonstrated superior performances on various downstream visual tasks, like image classification and object detection [7, 23, 24].",1,neutral
"The supervision signal can be found from various “proxy tasks” like colorization [63], spatial ordering or impaining [15, 24, 38, 43], temporal ordering [21, 36], contrasting similar instances [10, 23, 40], clustering [3, 6], and from multiple modalities [1, 45].",1,neutral
toencoders [13] (MAEs) proposes to learn representations by recreating the original images from patch-form masked images.,1,neutral
The ViT encoder is ViT-B/16 and we load pretrained weights from MultiMAE [3].,2,positive
"Examples include language models such as BERT [13], GPT-3 [7], T5 [38], and PaLM [11], as well as vision and vision-language models such as MAE [19], Multi-MAE [3], BiT [24], MuST [16], Flaminglo [15], and CLIP [35].",1,neutral
"Notable examples include BERT [13], GPT [36], MAE [19], CLIP [35] and Flamingo [1], etc. Recently, there has been a significant focus on developing such general-purpose models for sequential decision-making and control tasks, such as GATO [41].",1,neutral
"Notable examples include BERT [13], GPT [36], MAE [19], CLIP [35] and Flamingo [1], etc.",1,neutral
"We adopt ImageNet-V+ to evaluate 40 different models pre-trained on ImageNet, including models with different structures (the CNN-based VGG [46], ResNet [21], Inception [48, 47], DenseNet [26], EfficientNet [49], MobileNet-v2 [43], the transformer-based: ViT [14], DeiT [51], Swin Transformer [33], and the MLP Mixer [50]), different training paradigms (adversarial training [42] and mask-autoencoder [20]), different augmentation methods (AugMix [23], DeepAugment [22]).",2,positive
"On the other hand, inspired by the masked learning trend (He et al. 2021), we employ the masked multihead attention mechanism to obtain object features of the tth iteration Hi,j(t), which are highly responded to class probabilities:",1,neutral
"On the other hand, inspired by the masked learning trend (He et al. 2021), we employ the masked multihead attention mechanism to obtain object features of the tth iteration Hci,j(t), which are highly responded to class probabilities:
Hci,j(t) = Attention(WqryM(Fi,j(t)) ,W⊤keyM(Ei,j(t))⊤)…",1,neutral
"R O
] 1
4 Ju
l 2 02
3
In this paper, we propose a tactile representation method based on the Masked Autoencoder [8], named TacMAE, to simulate the contact area’s absence of incomplete tactile data caused by partial contact.",2,positive
"Specifically, our TacMAE receives two components as the input: latent features from the encoder and trainable vectors that illustrate the existence of the missing patches for reconstruction [8].",2,positive
"In [8], patches of raw pixels are masked by a high masking ratio which are then reconstructed by using visible patches.",1,neutral
"based on the Masked Autoencoder [8], named TacMAE, to simulate the contact area’s absence of incomplete tactile data caused by partial contact.",2,positive
"[13] proposed the masked autoencoder framework, a self-supervised method to pre-train the ViT model on small data sets.",1,neutral
"In order to train the MAE framework, we use the official PyTorch implementation1 provided by the authors [13].",2,positive
The MAE framework [13] works as follows.,2,positive
"Masked data modeling which employs the Transformer architecture has been vastly adopted by the deep learning community ranging from text [10] to images [13], videos [29] and sound [14].",1,neutral
[13] designed an asymmetric encoder-decoder model that operates on patches.,1,neutral
"[13] which has the embeddings dimension equal to 512, the number of Transformer blocks set to 8, and 16 attention heads.",1,neutral
"former networks have been proposed in the field of computer vision [18, 19, 21, 22, 43].",1,neutral
"In this work, inspired by vision transformers [18, 19, 22] and point cloud transformers [23, 24, 26], we try to apply transformer to feature extraction of triangular meshes to achieve better performance in 3D shape analysis tasks.",1,neutral
"Deep learning has revolutionized many research fields by empowering machines to learn complex patterns from vast amounts of data, instances include computer vision [15, 19, 33], natural language processing [5, 8, 29], and speech recognition [13, 14, 16].",1,neutral
"Another line of work pursues reconstruction losses for supervision, where certain regions get masked from an input image, and backbones get trained to reconstruct them [21, 28, 64, 72], also known as Masked Image Modeling (MIM).",1,neutral
Vision Transformer Backbone Supervised [28] - ViT-B 300 82.,1,neutral
"Following common practice in the literature [28, 58], we pre-train image backbones unsupervised on ImageNet-1k.",2,positive
"2020b), and masked autoencoders (MAEs) (He et al. 2022) have been proposed.",2,positive
"Comparison with sota pretraining methods: To further demonstrate the superiority of our customized ”pre-training” approach for few-shot medical image segmentation, we present a comparison between our method and three stateof-the-art pre-training methods: Swin-SimMIM (Xie et al. 2022), ViT-MAE (He et al. 2022), and Res50-SimCLR (Chen et al. 2020b) in Table 3 and Table 4.",2,positive
"6, we visualize the learned features of DenseMP, Swin-SimMIM, ViT-MAE, and Res50-SimCLR.",2,positive
"2022), ViT-MAE (He et al. 2022), and Res50-SimCLR (Chen et al.",2,positive
"…of our customized ”pre-training” approach for few-shot medical image segmentation, we present a comparison between our method and three stateof-the-art pre-training methods: Swin-SimMIM (Xie et al. 2022), ViT-MAE (He et al. 2022), and Res50-SimCLR (Chen et al. 2020b) in Table 3 and Table 4.",2,positive
"In contrast, both SwinSimMIM and ViT-MAE demonstrate some degree of dense activation, with different pixel points exhibiting varying activation levels and a certain degree of clustering charac-
Table 4: Comparison with SOTA pertaining methods on setting 2.",1,neutral
"In the image processing domain, methods like MoCo (He et al. 2020), SimCLR (Chen et al. 2020b), and masked autoencoders (MAEs) (He et al. 2022) have been proposed.",1,neutral
"Abd-CT Dice Abd-MRI Dice
Method RK LK Liver Spleen Mean RK LK Liver Spleen Mean
Swin-SimMIM 32.08 ± 16.04 24.89 ± 6.49 64.84 ± 1.87 45.49 ± 7.65 41.83 ± 8.01 51.25 ± 4.54 51.06 ± 6.14 67.5 ± 3.86 52.73 ± 9.86 55.64 ± 6.10 ViT-MAE 44.85 ± 8.12 75.64 ± 5.40 70.24 ± 6.58 40.85 ± 7.55 57.90 ± 6.91 54.53 ± 15.13 49.33 ± 16.02 74.32 ± 3.57 57.57 ± 9.02 58.94 ± 10.93 Res50-SimCLR 19.63 ± 10.95 14.49 ± 5.19 54.19 ± 4.42 20.96 ± 7.43 27.32 ± 7.00 28.93 ± 21.02 21.02 ± 5.08 62.58 ± 3.21 39.1 ± 13.19 37.91 ± 8.05 DenseMP 64.10 ± 6.87 65.95 ± 5.82 73.21 ± 3.81 70.30 ± 8.35 68.39 ± 6.21 82.78 ± 2.50 79.61 ± 4.29 72.71 ± 2.99 72.33 ± 8.68 76.86 ± 4.62
teristics.",0,negative
"We further expanded the experiment by utilizing visual foundation models such as SimCLR-ResNet-50, BeiT-ViT-B16, MAE-ViT-B16, and MoCo-ViT-B16, as well as multimodal models CLIP-ResNet-50, and CLIP-ViT-B16.",2,positive
"Although models like MoCo [11], SimCLR [5], MAE [10], and BEiT [2] have been developed, their prominence falls short of the widespread acceptance observed in the NLP realm.",2,positive
"Results are presented in Table 2, evaluated on MAE metric where lower number indicates better performance.",0,negative
"This strategy follows a conceptually simple mask-then-predict paradigm that removes a portion of the data and learns to predict the missing content, such as in BERT [15] and MAE [23].",1,neutral
"We follow [34] to choose the most representative 2D and 3D pretraining baselines: AttrMask [28], ContextPred [28], InfoGraph [54], MolCLR [63], GraphCL [70], as well as recently published method Mole-BERT [65] and GraphMAE [26] as 2D baselines.",2,positive
"The X-axis shows the training steps, while the Y-axis shows the MAE of the training set.",1,neutral
"Research initially explored random token dropping (Akbari et al., 2021; Li et al., 2023; He et al., 2022).",1,neutral
"This model is based on a transformer vision model, and it has been pre-trained with a masked autoencoder (MAE) [6].",2,positive
"Unlike the previous methods [10, 22], we do not utilize reconstruction loss [23], but use this only as a data augmentation method.",2,positive
Masked Autoencoders (MAE) [12] are also shown to be scalable self-supervised learners for computer vision.,1,neutral
Point-MAE [19] adapts the concept of masked autoencoders to point clouds.,2,positive
It then appends a learnable embedding for the masked audio tokens to g and passes it through a shared audio-visual transformer decoder [28].,2,positive
"Further, different from the existing MAE-style models [26, 24, 33], we propose a specialized masking strategy that better learns spatial audio-visual cues.",2,positive
"Given an egocentric video clip with binaural audio, we mask segments of it and train a model based on a new form of masked autoencoding (MAE) [16, 28, 16, 72, 34, 6] to predict the missing segments on the basis of the video and the unmasked segments in the audio.",2,positive
"They explore using both modalities to construct pretext tasks based on synthesis [58, 57], alignment [4, 40, 56, 2, 19], and masked auto-encoding (MAE)[26, 24, 33], with downstream tasks focused on audio-visual event classification and retrieval.",1,neutral
"To solve our pretext task of binaural audio inpainting in egocentric videos, we propose an approach based on the masked autoencoding framework [28], which has been shown to learn meaningful semantic features from audio-visual data [26, 33, 24].",2,positive
"Previous research [26] has suggested that a higher mask rate is required to achieve better performance in masked image modeling tasks, whereas a lower mask rate is sufficient for masked language modeling tasks.",1,neutral
"Therefore, we expect that our method can be combined with Masked Autoencoders [16] or other scalable self-supervised method.",2,positive
"We evaluate a representative sample of vision foundation models based on their pre-training algorithms: (1) a masked autoencoder (MAE) (He et al., 2022), a canonical masking-based method that fills image portions during pre-training, (2) SimCLR (Chen et al.",2,positive
"MAE (He et al., 2022) trains on images with patch-wise masking and reconstructs missing pixels.",2,positive
"BERT[7], T5[23], GPT2[22], GPT3[3], have motivated the advancement of vision [8, 19, 11] and multimodal [21, 1, 30, 17] in terms of architecture design and learning paradigm.",1,neutral
"It is worth noting that the motivation and objective of the proposed masking strategy are considerably different from those of the well-known masked image modeling [20, 55, 25].",1,neutral
"However, when we reduce the dataset size significantly, such as in the case of the masked autoencoder [22] trained on 400 times less data from ImageNet [11], its performance significantly declines.",2,positive
94 197 x 1024 X MAE-L [22] ImageNet [11] 19.,1,neutral
65 257 x 1280 X MAE-H [22] ImageNet [11] 18.,1,neutral
"Opting for MAE over alternatives like CLIP is informed by two primary considerations: 1) Our scenario lacks textual annotations, and 2) The MAE model is widely used with its robust image inpainting abilities.",2,positive
"Despite the input image being part of a generated composite action, we encourage the model to reconstruct the original sub-actions, denoted as v̂imae, v̂ j mae = fmae(v̂
i, v̂j), where fmae denotes the MAE model.",2,positive
"3D to 2D rendering: In this study, we utilize the pretrained MAE [10] as our self-supervised model.",2,positive
"Finally, we propose Decoupling Refinement, which leverages a self-supervised pre-trained model MAE to ensure semantic consistency between the sub-actions and compositional actions.",2,positive
"We then compute the value for each pixel in the 2D attention map A by aggregating these decayed attention scores from all N joint points:
Ai(pix) = N∑ n=1 Ein ||pix− pixin||2 , (6)
where pix denotes each pixel of the rendering image v. Lastly, we divide the image into segments of 16×16 regions, following the MAE model’s configuration.",2,positive
Constraints: The masked images are input into a pretrained MAE model for the inpainting process.,1,neutral
"Regarding the MAE model, we utilize a larger vision that also incorporates a GAN loss to enhance the in-painting capabilities.",2,positive
"However, there is a gap between the 3D action sequence and the images (used for MAE).",1,neutral
We then employ a self-supervised pre-trained model Mask AutoEncoder (MAE) [10] to reconstruct the complete image from sub-segments.,2,positive
"This refinement process involves rendering generated 3D actions into 2D space, decoupling these images into two sub-segments, using the MAE model to restore the complete image from sub-segments, and constraining the recovered images to match images rendered from raw sub-actions.",2,positive
"Motivated by the recent success of sequence modeling for representation learning in NLP [9, 37] and CV [22, 12], both the trajectory learning function and the policy learning function can be implemented with Transformer neural networks as fφ(·) and πθ(·).",1,neutral
"The tremendous success of the Transformer model for natural language processing [9, 37] and computer vision [22, 32] has inspired numerous works that seek to apply such architectures for decision-masking, and similarly motivates our work.",1,neutral
"TrajNet aims to learn trajectory representations with self-supervised sequence modeling objectives, such as masked autoencoding [22] or next token prediction [37].",1,neutral
"Recent work in NLP and CV has demonstrated masked autoencoding (MAE) as an effective task for self-supervised representation learning [9, 6, 3, 22].",1,neutral
"We see these methods as they are learning trajectory representations and the policy jointly, with training objectives inspired by MAE [22] and GPT [37].",2,positive
"By leveraging advanced sequence modeling objectives from language and visual domains [9, 3, 22], such methods have shown competitive performance in various challenging tasks [14, 18].",1,neutral
"To accelerate convergence, we initialize our backbone with MAE-pretrained weights [12].",2,positive
"space [12], leading to significant learning inefficiency in VL",1,neutral
"Prior works use different objectives to learn transferrable representations, including contrastive learning [7, 4, 16, 17], siamese similarity [9, 5, 8], and masked self-reconstruction [6, 18].",1,neutral
"Given the success of visual representation learning [4, 5, 6, 7, 8, 9] on a wide range of computer vision tasks, a natural solution ar X iv :2 30 7.",1,neutral
"To evaluate its effectiveness, we replace the ViT model pretrained by DINO with MocoV3 [107] and MAE [106] pretraining model.",2,positive
"Moreover, according to [6, 18], this setting can increase the difficulty of the main task learning and improve the optimization effect.",1,neutral
"Recent studies on masked autoencoders [4, 6, 40] have demonstrated the effectiveness of this approach in enabling models to acquire useful implicit semantics by masking important information during the reconstruction of missing knowledge.",1,neutral
"During training, FCMAE only operates on the visible data points, allowing the network to better focus on the unmasked part, avoiding problems such as overfitting caused by feature mis-learning which affects accuracy.",2,positive
"(5)
The MAE calculation is simple and easy to understand, with a lower sensitivity to outliers.",1,neutral
"In order to assess their prediction performance, this paper selects two objective evaluation metrics: the mean absolute error (MAE) and the root mean square error (RMSE).",1,neutral
The structure of the FCMAE applied to a time series is displayed in figure 3.,1,neutral
"Definitions of MAE andRMSE are as follows, where RULi denotes the actual value of RUL at time i, and ∧
RULi denotes the predicted value of RUL at time i:
MAE= 1 n n∑ i=1 ∣∣∣∣RULi− ∧RULi∣∣∣∣ (4)
RMSE= √√√√1 n n∑ i=1 ( RULi− ∧ RULi )2 .",1,neutral
"Building upon the traditional ConvNeXt structure and adapting the training strategy and forward inference process of Transformers, the model introduces a masked autoencoder (MAE) [24] to mask the input and convert it into a sparse format.",2,positive
"ConvNeXt V2 model also constructs a novel Fully Convolutional Masked Auto Encoder (FCMAE) structure based on the traditional masked auto encoder [24], using a random mask to obtain a sparse matrix based on the original data.",2,positive
"Notably, masked autoencoder (MAE) [21] in computer vision develops an asymmetric encoder-decoder architecture for masked image modeling.",1,neutral
"Considering that there are massive unlabeled facial videos on the Internet, a natural question arises in the mind: can we exploit them to fully unleash the power of deep neural networks for better DFER? The recent progress of self-supervised learning in many deep learning fields [1, 12, 21] indicates that there is a positive answer.",1,neutral
MAE [21] improves BEiT by designing an asymmetric encoder-decoder architecture to enable efficient endto-end pre-training.,2,positive
VideoMAE [53] is a simple extension of MAE [21] in the video domain.,1,neutral
"g DAE [29, 30], MAE[31], BeiT[32, 33]) is a self-supervised learning method that learns representations from the image itself.",1,neutral
"We assess the performance of the previous state-of-the-art approach (UNet [19]) and fine-tune the latest pre-training methods based on vit (MAE [31], BEiT [32]) on our AxonCallosumEM dataset.",2,positive
the pre-trained MAE [31] and BEiT [32] models yields enhancements of 0.,1,neutral
"The input size for the pre-trained ViTs from MAE [31] and BEiT [32] was 224x224, while for EM-SAM [9], it was 1024x1024.",2,positive
"We compared the publicly available pre-trained ViTs, including MAE [31] and BEiT [32].",0,negative
"Furthermore, the fine-tuned EM-SAM [9] achieves state-of-the-art performance due to its original enormously large training dataset and 4× larger input size, comparing with MAE [31] and BEiT [32].",2,positive
"Please noted that, only nature images were fed to the model of MAE [31], BEiT [32], and EM-SAM [9] before our fine-tuning procedure.",0,negative
"We are using a ViT pre-trained on ImageNet-21k using the generative, self-supervised learning method of Masked Autoencoders(MAE) [14] that has exhibited major amounts of effectiveness in generalization.",2,positive
The adopted method MAE [18] considers random masking.,1,neutral
"Recently, vision transformers (ViTs) have been adopted for federated learning [13] such as MAE [18], BEiT [19], and Swin Transformers [20] to overcome the data heterogeneity issues.",2,positive
"MAE [18] is leveraged as our self-supervision module, specifically for performing the augmentive modeling.",2,positive
"For the augmentive modeling, we leverage MAE [18] method into our SelfFed framework.",2,positive
"In this work, we propose the use of Swin transformer along with MAE [18] to perform augmentive modeling as self-",1,neutral
"Using MAE as a SSL pretraining method, the proposed approach is evaluated on few downstream tasks for remote sensing image scene classification.",2,positive
"In [17], a large-scale unlabeled dataset for SSL in Earth observation (SSL4EO-S12) is presented and evaluated using several SSL algorithms, such as MoCo [8], DINO [2] and MAE [7].",2,positive
"Although the deep learning community has used overparameterized functions for much more than vanilla supervised learning [9][35][38], supervision is still of great importance for most applications [9][20].",1,neutral
"We hypothesize that while being more powerful than ResNet in general, U-Net tends to perform better when trained on high-resolution data [67], and ViT often suffers from overfitting when trained from scratch [26, 50].",2,positive
"[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Additionally, Encoder represents an MAE [34] pre-trained Vision Transformer (ViT) [35] .",2,positive
"We consider pre-trained classifiers with architectures such as Vision Transformers (ViT) [17], ResNet-18/50 [21] trained with different pretraining methods including supervised training [21], adversarial training [38], SimCLR [8], MoCo [20], SwAV [7], CLIP [35], and MAE [19].",2,positive
"Since large pre-trained models are becoming the cornerstone of machine learning [35, 8, 9, 7, 16, 19], understanding when the knowledge gathered by these models improves the performance of a downstream task is becoming crucial.",1,neutral
"4) with large pre-trained models trained with different architectures (ResNet [21], ViT [17]), pretraining methods (CLIP [35], MAE [19], SimCLR [8], DistilBERT [39]), and datasets (Imagenet [15], CIFAR10/100 [23], Aircraft [27], Pets [33] and DTD [10]).",2,positive
The image encoder Ev is an MAE [53] pre-trained ViT backbone [35].,2,positive
The image encoder E v is an MAE [53] pre-trained ViT backbone [35].,2,positive
Masked autoencoder (MAE) [38] is a straightforward selfsupervised technique that learns feature representations by randomly masking patches and then reconstructing the missing pixels.,1,neutral
"We extract similarity matrices for the visual features, obtained through a pretrained MAE [18] or our fine-tuned one, and for the text features, produced using a pretrained BERT [6] or fine-tuned one.",2,positive
"Examples of pre-trained visual models include ViT [29], Swin Transformer [30], VideoMAE V2 [31] and others [32–41].",1,neutral
"Subsequent endeavors in computer vision began to improve and extend ViT model, such as DeiT [73], Swin Transformer [74], TNT [75], MAE [76], MoCo-v3 [77], BeiT [78], etc.",1,neutral
"To provide the attention maps for IDM, we suggest the Contrastive Attentional Masked Auto-encoder (CAMAE) framework.",2,positive
"Moreover, previously proposed masking approaches like grid or block masks both fail to achieve effective semanticwise masking [4].",1,neutral
We found that the PLB tends to capture noisy attention maps with large reconstruct loss in the MAE framework without CL.,1,neutral
"For this process, we use the same regularization strategies as [4] and set weight decay as zero.",1,neutral
"Meanwhile, previous works [4] found that the MIM requires much more training epochs, and it only captures limited high-level semantics within local details.",2,positive
"CMAE [6] designs a multi-task framework to coordinate the learning of pixel-level and conceptual representations, but the training efficiency problem is still unsolved.",2,positive
"In contrast, MAE [4] only inputs unmasked visual tokens to the encoding, which greatly reduces the computation.",1,neutral
"In Table III, the CAMAE outperforms other self-supervised methods and achieves a new state-of-the-art performance.",2,positive
"Specifically, IDM samples an attention map from CAMAE with multiple attention heads in each step, and it sorts the visual patches based on their activation values of the sampled attention map.",2,positive
CMAE [6] combines CL and MAE as multi-task framework to boost the performance.,2,positive
The MIM experiments are similar to [4].,1,neutral
"CL-based methods [1]–[3] distinguish whether two input images come from the same instance, while MIM methods [4] reconstruct the masked image from visible patches that capture semantic features.",1,neutral
"In the first image, there is a bird standing on the grass ground, in the previous CL methods, such as iBOT or MAE, the object-related attention maps always contain the background grass, and the attention map of our method mainly focuses on the bird itself.",2,positive
This result indicates that the attention maps captured by CAMAE decouple the background and object information much better when compared to other methods.,1,neutral
"For a fair comparison, our pre-training and linear classification experiments are conducted on the ImageNet100/1K dataset, following the same protocol as [4].",2,positive
"Moreover, training epochs and hours in Table III represent the efficiency of pre-training, and we can see that CAMAE does not require too many training epochs, achieving efficient representation learning.",2,positive
"However, we find the PLB is not helpful for MAE framework.",2,positive
"ConvMAE [13] performs multi-scale coding operations based on MAE, which makes the model learn richer semantic information.",1,neutral
"Specifically, i) the segmentation results of the contrastive learning methods SimCLR, BYOL, and SvAW are highly inaccurate and even over-segmented; ii) Context, SimMIM, MAE, and ConvMAE have better results but unsatisfactory segmentation performance in the edge region; and iii) the proposed AMP-DRL segmentation results are closer to the ground truth and retain more details in the foreground region.",2,positive
"MAE [11] uses an asymmetric encoder and decoder structure to divide the image into a number of the same size patches, and the masked patches are predicted directly based on the unmasked image patches.",2,positive
"SimMIM [12] lightens the weight of the decoder based on MAE and takes all visible and masked patches as input, which allows it to achieve similar results as MAE while speeding up the pre-training process.",2,positive
"Specifically, Context, SimMIM, MAE, and ConvMAE generally outperform SimCLR, BYOL, and SvAW on both datasets.",2,positive
"A mask ratio below 60% is no different from no masking in terms of fine-tuning accuracy, while in MAE [1], the masking rate interval that can be chosen is very large (40- 80%).",1,neutral
"A mask ratio below 60% is no different from no masking in terms of fine-tuning accuracy, while in MAE [1], the masking rate interval that can be chosen is very large (4080%).",1,neutral
"the input data first and then learns to reconstruct the original data, in 2D vision [1, 2], several works [3, 4, 5] have investigated such self-supervised learning approaches on point cloud data and achieved impressive performance using specialized designs.",1,neutral
"Thus, to embed the mask into the encoder, the specific structure like Transformer in Point-BERT [4] and Point-MAE [5] is not necessary.",1,neutral
"As claimed in MAE [49], auto-encoder models may be comparative with contrastive models, so VGNAE tries to explore auto-encoder mechanism further and achieve the",2,positive
"As claimed in MAE [49], auto-encoder models may be comparative with contrastive models, so VGNAE tries to explore auto-encoder mechanism further and achieve the best performance on some tasks.",2,positive
"Inspired by MAE [46], we then introduce masked filter modeling to construct PCA-like knowledge by aligning the outputs between the intermediate features of the pre-trained teacher and the decoder added to the student, which guides the filter sampling based on the Straight-Through Gradient Estimator.",2,positive
"In addition, different from MAE [46] through reconstructing the masked image patches to learn feature representation, our PCA-like knowledge is extracted by reconstructing the features of the student to approach those of the pre-trained teacher.",2,positive
MAE is a new type of autoencoder model [23].,2,positive
"In series association [6], we mask [8] the points themselves to ensure that the reconstruction does not rely on their own information.",0,negative
"Moreover, text modality is usually highly semantic and information-dense [1], while visual and audio modalities are relatively redundant in sentiment representation.",1,neutral
"In computer vision, large pretrained plain ViTs such as MAE [20], DINO [5, 32] and DeiT [44, 45] are widely adopted as backbones for tackling downstream tasks.",1,neutral
"Benefit from the large-scale datasets [39, 53] and powerful scaling laws [25], recent pretrained plain ViTs [20, 2, 14, 12] have achieved strong performance on many visual benchmarks.",2,positive
"In Figure 12 (b), we report the results of stitching different pretrained weights based on the base and large variants of ViTs, including MAE [20], SAM [26], AugReg [41] and BEiTv2 [34].",0,negative
• MAE [20].,1,neutral
"Prevalent approaches in this area include contrastive learning [5, 8, 52, 35] and masked image modeling (MIM) [20, 2, 58, 13, 18, 50].",1,neutral
ImageNet-1K finetuned MAE-B4 and MAE-L5.,2,positive
"Different from SkexGen and previous work on masked learning (He et al., 2022), we apply masking on a skip-connection from the encoder input to the decoder input.",2,positive
"Tang et al. [40] combined inpainting, contrastive learning and rotation prediction for SSL. Chen et al. [8] proposed Masked Image Modeling (MIM) that is a 3D extension of MAE.",1,neutral
"In addition, some image restoration methods have been proposed, such as image colorization [47], denoising [10], inpainting [30] and Masked Auto-Encoder (MAE) [16].",1,neutral
"Using image restoration [10,16,52] as the pretext task in SSL can alleviate this problem by pretraining the encoder and encoder for pixel-level intensity prediction.",1,neutral
"In addition, compared with image restoration [10, 16, 52], the gap between pretraining and downstream tasks are minimized, as both of them are formulated as segmentation tasks, which makes the features learned during pretraining be more transferable to downstream segmentation.",1,neutral
"The paradigm of finetuning [254, 255] or prompt learning [163], optimization in the form of selfsupervised reconstruction [256] or contrastive pairs [161], and data pipelining, etc.",1,neutral
", 2022); (3) Language has a higher information density compared to images (He et al., 2022) and tables, the transformation from low-density to high-density can be done losslessly for tables and with minimal loss for images using",1,neutral
"In recent years, supervised learning methods have made remarkable advancements in computer vision (CV), natural language processing (NLP), and human-level game playing [1, 2, 3, 4, 5, 6, 7].",1,neutral
MUST [31] tackles unsupervised adaptation of VLMs from the perspective of Masked Image Modelling [16] that heavily relies on Transformer backbones [8].,2,positive
"Except for the common supervised pre-training [16, 10, 24], contrastive learning (CL) [4, 14, 6, 12] and masked image modeling (MIM) [1, 44, 13] dominate the recent research.",1,neutral
"ViT [10] is adopted for all the models in Hybrid Distill, and Tm is provided by MAE [13] while Tc is provided by DeiT [36] or CLIP [30].",2,positive
"While the latter, including MAE [13] and SimMIM [44], aims to reconstruct the masked image patches and has become mainstream due to its efficiency brought by mask operations.",2,positive
"In MAE [29], certain patches in an image are masked, and the model is trained to reconstruct the pixel information in these masked patches.",1,neutral
"This is mainly due to two reasons (i) with the adaption of transformer based models to vision tasks like image classification [9, 20, 34] (ii) with the availabilty of large scale video-text datasets like HowTo100M [40], WebVid-2M [3] and YT180M [57].",2,positive
"SAM employs a pre-trained Masked AutoEncoder (MAE) [60] based on Vision Transformer (ViT) [80] to process images into intermediate features, and encodes the prior prompts as embedding tokens.",2,positive
"SAM utilizes the MAE pre-trained ViT as its image encoder, which is available in three versions: base, large, and huge.",2,positive
"Drawing inspiration from BERT’s training regimen, MAE [60] was the first to propose a Transformer-based visual masking training strategy, which yielded significant results across numerous downstream tasks.",2,positive
1: The overall framework of our masked compression model (MCM) which unifies pre-trained MAE [23]-based MIM and LIC for extremely low-bitrate image compression.,2,positive
"1, MCM is a two-stage framework that unifies pre-trained MAE [23]based MIM and LIC in an end-to-end manner.",2,positive
"Since different patches contain different information, simply implementing such a random mask sampling as existing MIM methods [23,12,37] may preserve",1,neutral
"After that, receiving the visible tokens by the tokenization on the visible patches pv, similar to MAE [23], we also apply a pre-trained standard ViT [32] as the encoder E to learn latent representations, resulting in encoded tokens x of pv:",2,positive
"1, our MCM unifies pre-trained MAE [23]-based MIM and LIC for extremely lowbitrate image compression.",2,positive
The representative method of MIM is masked autoencoder (MAE) [23] which applies a standard ViT [32] as the,2,positive
"Recently, masked autoencoder (MAE) [23] has exhibited great success on various computer vision",1,neutral
"In response, the general purpose representation learning method—masked image modeling and specifically masked autoencoders (MAE)—has become a popular default self-supervised mechanism for such tasks [24].",1,neutral
"Today, dense vision tasks—depth prediction, semantic segmentation, surface normals, and pose estimation—rely on pretrained representations [24, 2].",1,neutral
We use two masked image modeling objectives: Masked Autoencoders [24] and CroCo [49].,2,positive
"To understand the potential of MIMIC for the high-level classification tasks, we evaluate MAE [24] and CroCo [49] pretrained with MIMIC-3M on ImageNet-1K [18].",2,positive
"Amongst masked image modeling, BEiT [3] proposes the pretext task of recovering the visual tokens from a corrupted image, MAE [24] learns by masking patches of an image and inpainting the masked patches; MultiMAE extends MAE to a multi-task formulation [2].",1,neutral
: MAE [24] masks out a large portion (75%) of the input patches of an image and uses an asymmetric encoder-decoder architecture to reconstruct the masked-out pixels.,1,neutral
This process is akin to reconstructing an image from patches [4].,1,neutral
"Random masking [13, 7] is used as the image token reduction strategy.",1,neutral
Masked Autoencoders is a popular self-supervised method proposed by [2] and gained state-of-the-art performance on various downstream tasks in 2D image domain.,1,neutral
"These pretext tasks are designed to learn the spatial features from RGB data, such as inpainting of the data [28], disrupt the spatial order of the data and random",1,neutral
Generative based Denoising AE [51] Reconstruct clear image from noisy input Masked AE (MAE) [28] Reconstruct randomly masked patches GANs [52] Adversarial training with a generator and a discriminator Wasserstein GAN [53] Train the generator to produce samples that are as close as possible to the real data distribution Relative position [45] Predict the relative positions of random patch pairs Rotation [29] Predict the rotation angle of the random rotated image puzzle [46] Predict the correct order of the puzzle,1,neutral
"In this experiment, the features we use are: 1) the RGB values of image itself, the most basic feature of pixels, 2) the CNN features obtained from supervised CNN (ResNet (He et al. 2016)), and self-supervised CNN (MoCov3 (Chen, Xie, and He 2021)), 3) transformer features obtained from supervised transformer (ViT (Dosovitskiy et al. 2020)), and self-supervised transformers (DINO (Caron et al. 2021) and MAE (He et al. 2022) which is known to outperform DINO in down-stream tasks).",2,positive
"Hence, the ViT and MAE may not pay much attention to the details of images, thereby generating highly similar features on objects.",1,neutral
"…and self-supervised CNN (MoCov3 (Chen, Xie, and He 2021)), 3) transformer features obtained from supervised transformer (ViT (Dosovitskiy et al. 2020)), and self-supervised transformers (DINO (Caron et al. 2021) and MAE (He et al. 2022) which is known to outperform DINO in down-stream tasks).",2,positive
This is because the ViT is trained to classify images which forces the network to recognize the object itself and MAE is trained to predict the masked regions which forces the network to understand overall context of images.,1,neutral
2021) and MAE (He et al. 2022) which is known to outperform DINO in down-stream tasks).,1,neutral
One notable point is that we need to set τ to high value when we use the features of ViT and MAE.,2,positive
"We train RetinaNet (Lin et al. 2017b) detectors with ResNet as backbones, and explore vision transformer detection and segmentation quantization using ViT and Swin Transformer (Liu et al. 2021a) pretrained with the newly proposed selfsupervised method MAE (He et al. 2022).",2,positive
"We tried ViT (Dosovitskiy et al. 2021) and Swin Transformer (Liu et al. 2021a) pretrained on ImageNet1k and ImageNet21k, respectively, using the self-supervised learning methods MAE (He et al. 2022).",2,positive
"2021a) pretrained on ImageNet1k and ImageNet21k, respectively, using the self-supervised learning methods MAE (He et al. 2022).",2,positive
2021a) pretrained with the newly proposed selfsupervised method MAE (He et al. 2022).,0,negative
"The most popular approaches of visual unsupervised pretraining are contrastive learning [17, 5] and masked image modeling (MIM) [1, 16, 48].",1,neutral
"Models trained in this fashion have shown competitive results on detection, classification, and segmentation tasks, in some cases outperforming even their fully supervised counterparts [15, 35].",1,neutral
"Much recent effort has also been expended around contrastive learning and their variants [14] with the resulting models’ performance shown to be at par or even outperforming their supervised counterparts [15, 35].",1,neutral
"…(Vaswani et al., 2017) there has been a widespread adoption of the underlying architecture in many areas of Machine Learning research such as NLP (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020), computer vision (Dosovitskiy et al., 2021; He et al., 2022; Radford et al., 2021; ?",1,neutral
", 2020), computer vision (Dosovitskiy et al., 2021; He et al., 2022; Radford et al., 2021; ?; Ramesh et al., 2021; Rombach et al., 2022), speech recognition (Radford et al.",2,positive
"2020) and masked autoencoders (MAE) with Vision Transformer base model (ViT-B) (He et al. 2022) as the representative contrastive and generative methods of SSL, respectively.",2,positive
"To convey the fundamental knowledge of the SSL’s efficacy to clients, we select SimCLR with ResNet50 (Chen et al. 2020) and masked autoencoders (MAE) with Vision Transformer base model (ViT-B) (He et al. 2022) as the representative contrastive and generative methods of SSL, respectively.",2,positive
"Interactive Partitioned Convolution Unlike humangenerated language sequences which are information-dense, most natural signals are not highly semantic (He et al. 2021).",1,neutral
"These findings align with those reported in masked autoencoders [19], indicating that a higher masking ratio effectively reduces image redundancy.",1,neutral
"Inspired by masked autoencoders [19], we create a reconstruction task that cannot be easily addressed by the model with a high masking ratio.",2,positive
"Unlike masked autoencoders [19], the mask in our method is not completely empty but contains Gaussian noise.",2,positive
"Recent progress of large language models and foundation models[3, 10, 14, 22, 37, 47, 54, 55] demonstrates the amazing generalization ability of deep learning models, trained on large amounts of data and prompted (with few-shot samples) for downstream applications.",1,neutral
"Transformers have demonstrated the ability to generalize to a vast array of tasks, such as language modeling, image generation, and representation learning [Vaswani et al., 2017, Devlin et al., 2018, He et al., 2022, Parmar et al., 2018].",1,neutral
", 2009) training set, as commonly used in SSL methods for both MIM (He et al., 2021) and contrastive learning (Chen et al.",1,neutral
"are MIM-based methods MAE (He et al., 2021), BEiT (Bao et al.",1,neutral
"Standard SSL protocols is to either learn a linear classifier on frozen features (Chen et al., 2020a; He et al., 2020) or to finetune on downstream tasks (He et al., 2021; Chen et al., 2022).",2,positive
"The compared baselines
are MIM-based methods MAE (He et al., 2021), BEiT (Bao et al., 2021), CIM (Fang et al., 2022) as well as contrastive methods (Caron et al., 2021; Zhou et al., 2022; Chen et al., 2021) and the combination of the two techniques: CAE (Chen et al., 2022) which is emerging.",2,positive
"We conduct self-supervised pre-training on the ImageNet-1K (Deng et al., 2009) training set, as commonly used in SSL methods for both MIM (He et al., 2021) and contrastive learning (Chen et al., 2020a).",2,positive
"Differences. iBOT learns the local information by adding MIM objective, which is now widely developed in MAE (He et al., 2021), SimMIM (Xie et al., 2021d).",2,positive
", 2020) or to finetune on downstream tasks (He et al., 2021; Chen et al., 2022).",2,positive
"iBOT learns the local information by adding MIM objective, which is now widely developed in MAE (He et al., 2021), SimMIM (Xie et al.",2,positive
"Linear Layer The simple classification head is always used to evaluate the generalization of feature representation learned in the pre-training task [7,10,11,19].",1,neutral
"The image encoder has the same architecture as Vision Transformer (ViT) [8], and is pre-trained with MAE [10] on their own collected SAM-1B dataset.",2,positive
It is capable of utilizing semantic-level features that maintain a higher degree of finegrained information [29] than those identity-specific features of commonly-used identity embeddings.,2,positive
the MAE [29] framework and was pre-trained on a largescale face dataset [9]–[12] consisting of 2.,2,positive
The image encoder employs an MAE [25] pre-trained ViT network [6] to extract image features.,2,positive
The image encoder uses the Vision Transformer (ViT) (10) as its backbone and is pre-trained using the masked strategy from the masked autoencoder (MAE) (11).,2,positive
"For masked image modeling, we follow the setting of SimMIM (Xie et al., 2022b) and MAE (He et al., 2022) using their official code repositories12 where the masking ratio is 0.6 and 0.75, respectively.",2,positive
"SimMIM (Xie et al., 2022b) and MAE (He et al., 2022) adopt an encoder-decoder structure that zeroes out random spatial patches in each patchfied image and learns representations by predicting pixel values in masked patches.",2,positive
", 2022b) and MAE (He et al., 2022) adopt an encoder-decoder structure that zeroes out random spatial patches in each patchfied image and learns representations by predicting pixel values in masked patches.",1,neutral
"We simply adopt l1 regularization to minimize the distance between predicted patches and the targets, followed by earlier reconstruction-based works (Xie et al., 2022b; He et al., 2022), and after completing a sequential training, the obtained encoder hθ can be utilized for many different downstream tasks.",2,positive
"Inspired by our above observations, we propose a new UCL framework based on Masked Image Modeling (MIM) (Xie et al., 2022b; He et al., 2022) that improves task-generic representation across all layers during training.",2,positive
"We follow Siamese network structure by Madaan et al. (2022) and
implement a MIM-based continual self-supervised learning framework under SimMIM (Xie et al., 2022b) and MAE (He et al., 2022) for UCL.",2,positive
"…a new UCL framework for a better generalizable representation model across all layers, we survey Masked Image modeling (MIM) (Pathak et al., 2016; He et al., 2022) that self-trains input representation by minimizing regression loss to predict RGB pixel values in randomly zeroed patches in…",2,positive
"To build a new UCL framework for a better generalizable representation model across all layers, we survey Masked Image modeling (MIM) (Pathak et al., 2016; He et al., 2022) that self-trains input representation by minimizing regression loss to predict RGB pixel values in randomly zeroed patches in patchified input images.",2,positive
"…adopt ℓ1 regularization to minimize the distance between predicted patches and the targets, followed by earlier reconstruction-based works (Xie et al., 2022b; He et al., 2022), and after completing a sequential training, the obtained encoder hθ can be utilized for many different downstream tasks.",2,positive
"Hence, naively training masked AEs to reconstruct randomly masked tokens in video anomaly detection is suboptimal.",1,neutral
"Since training is carried out only on normal examples, it is expected for AEs to exhibit high reconstruction errors when anomalies occur.",0,negative
"As observed in other studies [5, 33], AEs tend to generalize too well to out-of-distribution data.",1,neutral
"Lastly, we describe how to create training videos with synthetic anomalies and train the masked AEs to jointly predict the anomaly maps and overlook (not reconstruct) the anomalies from training frames.",2,positive
"We elaborate the connection to seem-
ingly related masked AEs in the supplementary [10, 88].",2,positive
"This behavior is not desired in anomaly detection, since methods based on AEs rely on having high reconstruction errors for abnormal examples and low reconstruction errors for normal ones.",1,neutral
"With the same purpose in mind, we propose to employ masked auto-encoders [29] in anomaly detection, introducing new ways to regulate their generalization capacity.",2,positive
[29] proposed masked auto-encoders as a pretraining method to obtain strong backbones for downstream tasks.,1,neutral
"Indeed, our masked AE is specifically designed for abnormal event detection in video, while the masked AEs proposed in [10, 88] are focused on improving the pretraining procedure.",2,positive
Learning to reconstruct the static background via masked AEs is both trivial and useless.,1,neutral
Our masked AE pursues the architectural principles proposed in [29].,2,positive
"Thus, to better leverage the reconstruction error of AEs in anomaly detection, researchers explored a few alternatives, from the use of dummy [33] or pseudo-anomalies [5, 26] to the integration of memory modules [27,46,57].",1,neutral
"However, several researchers observed that AEs generalize too well [5, 33], being able to reconstruct anomalies with very high precision.",1,neutral
"Moreover, we go beyond applying standard masked AEs, proposing several modifications leading to superior performance levels: emphasizing tokens with higher motion, augmenting training videos with synthetic anomalies, and employing self-distillation.",2,positive
Masked AEs [29] have been originally applied on natural images.,1,neutral
"A large body of work on video anomaly detection has focused on employing auto-encoders (AEs) to address the task [5, 21, 26–28, 33, 46, 77, 80], relying on the poor reconstruction capabilities of these models on out-of-distribution
1https://en.wikipedia.org/wiki/Vehicle-ramming_ attack
data.",1,neutral
"[29], we replace the ViT [19] blocks with CvT blocks [83], aiming for higher efficiency.",2,positive
"Transfer Learning on Downstream Tasks To further evaluate the transferability of our proposed PatchMix, we conduct transfer learning experiments on downstream tasks: object detection and instance segmentation on COCO [23] by Mask RCNN [17] with FPN [22] as MAE [15].",2,positive
"However, this protocol is not suitable for the evaluation of non-linear representations [1, 11, 15].",1,neutral
"To further evaluate the transferability of our proposed PatchMix, we conduct transfer learning experiments on downstream tasks: object detection and instance segmentation on COCO [23] by Mask RCNN [17] with FPN [22] as MAE [15].",2,positive
"Furthermore, the success of masked image modeling [1, 34, 15] demonstrates that small portion of patches from image, e.",1,neutral
"Meanwhile, compared to the previous leading self-supervised learning methods, MAE and SIM, our method requires significantly fewer pretraining epochs but achieves significantly transfer learning perfor-
mance on object detection and instance segmentation task.",2,positive
"assigning two pretraining tasks of text box segmentation and mask image modeling (MIM) [64], [65] to the output features of the encoder and decoder, respectively, the performance of text removal can be effectively enhanced.",1,neutral
"[50], [64], [65] and natural language processing [51].",1,neutral
"A typical choice of the pre-trained parameter is the minimizer of the average expected risk over the T upstream tasks [43, 18, 29], i.",1,neutral
"Macro accuracy or F1-score are commonly used and can show a high correlation [28,16,2,45,24,30].",1,neutral
"Recently, self-supervised learning (SSL) has emerged as a promising paradigm that offers supervisory signals to real-world problems while avoiding the extensive cost of manual labeling, leading to great success in advancing NLP [4, 5] as well as computer vision tasks [6, 7].",1,neutral
"We implement AugMask using MAE style token drop [13], allowing us to inherit the computational cost reduction by skipping network computation for the masked region.",2,positive
"Also, AugMask has the lowest computation costs due to MAE [13]-style computation reduction.",2,positive
"We utilize three finetuning recipes: MAE [13], BEiT v2 [29], and Finetune CLIP [30].",2,positive
"Generally, self-supervised learning, such as MAE [13] and BEiT [17, 29], does not use supervised labels at pretraining, which makes AugSub inapplicable for pretraining.",1,neutral
"Random masking is an augmentation technique for BERT-like self-supervised learning [17, 13].",1,neutral
"We construct AugSub utilizing three in-network drop-based techniques: dropout [1], drop-path [6, 7], and input masking [13, 17].",2,positive
"AugMask is applied on various supervised learning cases including DeiT-III [11], ResNet-RSB [9], MAE finetuning [13], and Swin transformer [19].",2,positive
"We select three drop-based techniques for AugSub: dropout [1], drop-path [6], and random masking [13].",2,positive
MAE [13] is a representative method of masked image models (MIM).,1,neutral
regularization is random masking (as done in MAE [13]).,1,neutral
"previous methods [36-38], each layer is equally important in MCIAT, so layer-decay is not used in fine-tuning.",1,neutral
"For example, Kong et al. [19] proved that MIM pretraining is equivalent to learning occlusion-invariant visual features.",1,neutral
"Numerous foundation models have emerged in recent years, such as SimCLR [2], MAE [3], Florence [4] SAM [5] for computer vision, the BERT model [6] and GPT [7], [8] series for natural language processing, and CLIP [9] and Flamingo [10] for vision-language learning, etc.",2,positive
"While self-supervised learning (SSL) and Masked Image Modeling (MIM) have led to promising results in building such foundation models for remote sensing, these models primarily learn lowlevel features, require annotated data for fine-tuning, and not applicable for retrieval and zero-shot applications due to the lack of language understanding.",1,neutral
"Jun Zhou is with the School of Information and Communication Technology, Griffith University, Nathan, Queensland 4111, Australia.
models are primarily inspired by the success of self-supervised learning (SSL) in computer vision, particularly the Masked Image Modeling (MIM) [3], [11], [12] method.",2,positive
"For example, Billion-scale MAE [16] proved that ViT with 2B scale can be successfully applied
12
13
[16] K. Cha, J. Seo, and T. Lee, “A billion-scale foundation model for remote sensing images,” ArXiv, vol. abs/2304.05215, 2023.",2,positive
Park et al. [20] showed that MIM methods prefer to learn high-frequency texture features instead of capturing longer-range global patterns.,1,neutral
"models are primarily inspired by the success of self-supervised learning (SSL) in computer vision, particularly the Masked Image Modeling (MIM) [3], [11], [12] method.",1,neutral
"We compare RemoteCLIP with a variety of baselines, including the vanilla CLIP model (ViT-Base32 and ResNet-50), Self-supervised Learning (SSL-based) foundation visual models (SwAV, Barlow Twins, VICReg),
ImageNet pretrained models (ViT-Base-32 and ResNet-50), and existing remote sensing foundation models (ViTAE and SatMAE).",2,positive
"Nevertheless, recent studies have reveal that the MIM method, the basis of most current remote sensing foundation models, primarily learns low-level visual features instead of high-level semantics.",1,neutral
"Several recent works, including SatMAE [13], Scale-MAE [14], ViTAE [15], Billion-scale MAE [16], RingMo [17], GFM [18], have employed MIM on large Vision Transformers (ViT) and largescale satellite imagery datasets, yielding encouraging results.",2,positive
"Another line of works attempted to scale up the MIM model, such as RingMo [17], billion-scale MAE [16], VITAE [15], etc.
3) Vision Language Models: As detailed in Section I, SSLbased remote sensing foundation models mainly learn lowlevel features and lack an understanding of high-level semantics.",2,positive
"Improvements upon these methods includes introducing momentum contrast [65], rotation (0◦, 90◦,180◦,270◦) invarience [66], knowledge distillation [67], etc.
2) MIM-based SSL Models: MIM methods mask a proportion of the patches input, and train the vision transformer to reconstruct these masked patches.",2,positive
"Some remote sensing models based such MIM method mainly focus on adding new properties to the model, including scale-invarince (ScaleMAE [14]), temporal information (SatMAE [13]), temporal invarince (SeCo [68]), etc.",1,neutral
"The remote sensing community is also attempting to develop such foundation models, and currently the main methodology follows the self-supervised learning (SSL) approaches in general computer vision, such as augmentation-based models (e.g., SimCLR [2]) and Masked Image Modelling (MIM)-based models (e.g., MAE [3]).",2,positive
Recently this training method has been used to train image recognition models [9] and tabular models [1].,1,neutral
"Point-M2AE uses a pyramid-like backbone that gradually downsamples the input point cloud, obtaining multi-scale features.",2,positive
"While hierarchical architectures such as M2AE are typically associated with slightly better performance, they require parameter tuning when transferred to other domains or scaled to larger datasets.",1,neutral
"Point-MAE (Pang et al., 2022) and Point-Multiscale MAE (M2AE) (He et al., 2022) both utilize masked autoencoding to pretrain their transformer backbones, by reconstructing the actual points of the masked neighborhoods directly.",2,positive
"Inspired by masked autoencoders (He et al., 2022), they mask out parts of the road environment and/or past trajectory points and train to reconstruct them.",2,positive
"During training, the image is first divided into patches, and randomly masked at 75% according to [7] before input to the encoder.",1,neutral
[7] introduced the ViT into the autoencoder model as the encoder and used a transformer as the decoder.,1,neutral
[39] designed powerful masked autoencoders by randomly covering the grids of the input image and reconstructing the missing pixels.,1,neutral
The recent works like Bert [36] or masked autoencoders [39] have the similar idea by using masked to predict in pre-training.,1,neutral
"Task-specific fine-tuning has become a standard paradigm and demonstrated remarkable performance in various applications from vision [9, 17] to natural language processing [3, 7, 24, 34].",1,neutral
"Here, we focus on two evaluation modes that are most common in the self-supervised learning literature in language and computer vision [5, 3, 4, 26].",1,neutral
Previous work on masked visual pre-training [3] finds a high masking ratio is helpful.,1,neutral
"We use the pre-trained models from [7] which were trained via MAE [3] on a collection of 4.5M images from Ego4D [29], Epic [30], Something-Something [31], 100 Days of Hands [32], and ImageNet [33].",2,positive
We use the pre-trained models from [7] which were trained via MAE [3] on a collection of 4.,0,negative
"Consistent with prior work [3], we find that fine-tuning is more effective than linear probe evaluation.",1,neutral
"We instantiate this idea through a masked reconstruction task, similar to the BERT [5] and MAE [3] counterparts in language and computer vision.",1,neutral
"Over the last few years, inspired by vision [1, 2, 3] and language [4, 5, 6], there has been an increased interest in pre-training for robotics.",1,neutral
"Representation learning has shown significant success in computer vision [12, 18, 32, 35, 46, 52, 19] by learning generic visual representations through large-scale pretraining, which can then be reused in multiple downstream tasks such as detection and segmentation [11, 45, 47, 50].",1,neutral
"In the case that x represents generated images, a valid choice is a deep kernel built upon a pre-trained NN-based image encoder h (e.g., a ViT trained by the objective of MAE [47] or CLIP [48]).",1,neutral
", a ViT trained by the objective of MAE [47] or CLIP [48]).",0,negative
Papers like He et al.(1) and Cao et al.,1,neutral
"Mask autoencoder (MAE) is a structure widely used in language models and vision models in recent years [11], [70].",1,neutral
We feed the obtained masked image into the image encoder and employ the same decoder structure as MAE to reconstruct the full image area based on the unmasked area.,2,positive
"Hard instrument area reinforcement: mask ratio rt. MAE [14] originally employs a high masking ratio (0.75), but in our study, we argue that a lower threshold (0.25) is more suitable: surgical instruments occupy a relatively small portion of the image, a low masking ratio will make the model concentrating on the hard area.",1,neutral
Ours w/o HAM; the HIAR module is then rather similar to a typical MAE module.,1,neutral
"We propose a hard instrument area reinforcement module intertwined with the popular image reconstruction approach, masked autoencoder (MAE) [14].",2,positive
"Unlike MAE, which pays equal attention to all areas, our objective is to focus the visual encoder on discriminating the challenging instrument area.",2,positive
We seek to reinforce its performance on the hard-predicted area in an image by utilizing a MAE-like structure [14] to reconstruct the image especially on hard-predicted area for representation enhancement.,2,positive
"On the other hand, MAE [17] applied masked image modeling to directly predict the continuous tokens (without a discrete tokenizer).",1,neutral
"In the representation learning community, masked training is widely used to improve training efficiency in domains such as natural language processing [10], computer vision [17], and visionlanguage understanding [26].",1,neutral
"Masked training As transformers become the dominant architectures in natural language processing [51] and computer vision [12], masked training has also been broadly applied to representation learning [10, 17, 26] and generative modeling [34, 8, 7] in these two domains.",1,neutral
"Our decoder has the same architecture with MAE [17], except that we add the adaptive layer norm blocks for conditioning on the time and class embeddings [33].",2,positive
"Similar to MAE [17], we apply an asymmetric encoder-decoder architecture: 1) the encoder has the same architecture as the original DiT except without the final linear projection layer, and it only operates on the unmasked patches; 2) the decoder is another DiT architecture adapted from the lightweight MAE decoder, and it takes the full tokens as the input.",2,positive
"We hypothesize that the training in the late stage has been gradually dominated by the MAE reconstruction task, and the MAE training alone does not produce photorealistic images [17].",1,neutral
"The decoder takes both the encoded unmasked tokens in addition to new mask tokens as input, where each mask token is a shared, learnable vector [17].",1,neutral
"Masked training significantly reduces the overall training time and memory, especially in vision applications [17, 26].",1,neutral
"However, with the advent of versatile transformerbased models [42, 10] and large-scale vision-language pre-training [35, 4, 1, 14], there’s a growing shift towards developing comprehensive, multi-purpose, open-vocabulary vision systems, known as Visual Foundation Models (VFMs) [49, 26, 28].",1,neutral
"Additionally, SAM uses an Masked Autoencoder (MAE) pre-trained vision transformer (ViT).",2,positive
"This approach uses a MAE [14] pre-trained Vision Transformer (ViT) [11] image-encoder and a set of prompts that are either points, text, or bounding boxes to mask desired objects.",1,neutral
"12 True _ False GroundedSAM [29, 24] GroundingDINO[29],MAE[14]+ViT[11] 834.",1,neutral
Masked autoencoders (MAE) [86] provide a selfsupervised pre-trained backbone for developing fully trained models with a small labeled dataset.,2,positive
Masked autoencoders (MAE) [86] provide a self-supervised pre-trained backbone for developing fully trained models with a small labeled dataset.,2,positive
"During pre-training, a MAE segments the input into a top-down 2D grid (patches) and masks over 75% of randomly selected patches.",1,neutral
"Among various types of SSL methods, we identify reconstruction-base learning with masked autoencoders (MAE) [He et al., 2022] as one of the most suitable SSL approaches for training DP foundation vision models.",2,positive
"Vision SSL methods can be broadly categorized as either joint embedding-based learning (JE) [Chen et al., 2020a, He et al., 2020, Grill et al., 2020, Zbontar et al., 2021, Chen and He, 2021] or reconstruction-based learning (REC) [Bao et al., 2021, Xie et al., 2022, He et al., 2022].",2,positive
"Non-private training of SSL models often require a significant number of training epochs, much larger than what is required in supervised learning [Chen et al., 2020a, He et al., 2022, Balestriero et al., 2023].",1,neutral
"For (Syn)-ViP pre-training, we follow the training setup outlined in [He et al., 2022]: we apply the training parameters specified in Table 8 of He et al. [2022] and pre-train pre-train (Syn)-ViP on the S21k dataset developed in Baradad et al. [2022], which comprises of 1,300,000 training samples,…",0,negative
dense neural network based on a masking mechanism [37].,1,neutral
"To partially demonstrate it, we use the MAE pretrained model [99] as the teacher for local spatial feature representation.",2,positive
"Self-supervised learning (SSL) has demonstrated remarkable success in representation learning across various fields, including computer vision [1, 2], natural language processing [3, 4], and speech processing [5, 6].",1,neutral
"SimCLR [44], MoCo [45], MAE [46] are representative works.",1,neutral
"We also predict the visual modality by reconstructing the image crops for the masked tokens in MMLM, in a way similar to MAE (He et al., 2022).",2,positive
"2 Masked Crop Modeling (MCM) We also predict the visual modality by reconstructing the image crops for the masked tokens in MMLM, in a way similar to MAE (He et al., 2022).",2,positive
"Stemmed from the image vision community, most 3D self-supervised learning methods focused on object-centric point clouds [84, 77, 83, 14, 97, 45, 110, 29, 91] or indoor scenes [38, 15, 22, 108, 10, 39, 57, 58, 109] by either pretext task learning [21, 70, 71, 112, 30], contrastive learning [11, 32, 12, 34, 27, 13, 46, 98, 104] or mask modeling [105, 31, 24, 61], where the scale and diversity are much lower than the outdoor driving scenes [67, 6].",1,neutral
"Our proposed combination of multiobjective self supervised tasks performs better than existing masking based self-supervised pretraining approaches [37], [38] and shuffling order prediction alone.",1,neutral
"Our work aims to systematically evaluate such representations for perceptual similarity, also including OpenCLIP (an open-source implementation of CLIP) [38] and pre-trained masked autoencoders (MAE) [30].",2,positive
"LPIPS [90] and DISTS [23] use CNN backbones, whereas DINO [9], CLIP [64], OpenCLIP [16], and MAE [30] use transformer-based backbones [24].",2,positive
"Following standard practice, distance D ( x, ˜ x ; f θ ) = 1 − cos f θ ( x ) , f θ (˜ x ) is taken as the cosine distance between the CLS tokens taken from the last layer for DINO and MAE (before and after the layer normalization, respectively), and the embedding vector for CLIP and OpenCLIP.",1,neutral
"We present an algorithm for mining a dictionary of Rosetta Neurons across several popular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50, BigGAN, StyleGAN-2, StyleGAN-XL.",2,positive
"We search for Rosetta Neurons across eight different models: Class Supervised-ResNet50 [13], DINOResNet50, DINO-ViT [4], MAE [12], CLIP-ResNet50 [24], BigGAN [3], StyleGAN-2 [15], StyleGAN-XL [29].",2,positive
"Recent contributions to foundational vision models (Dosovitskiy et al., 2021; He et al., 2022) and a wider availability of computational resources has enabled many of these applications.",1,neutral
MAE [11] then directly reconstructs the raw pixel values of masked tokens and achieves high efficiency with a high mask ratio.,2,positive
"Inspired by Painter [33] and MAE [11], we explore two different baselines for PIC and name them PIC-Sep and PIC-Cat.",2,positive
"Both visual prompt [3] and Painter [33] combine two pairs of images that perform the same task into a grid-like image and randomly mask portions, following MAE [11].",1,neutral
"The above methods adopt Mask Image Modeling (MIM) architecture [11, 2] for in-context task transfer.",1,neutral
"In addition, leveraging the mask point transformer architecture, we explore two distinct baseline methodologies for PIC, which encompass separating inputs and targets akin to the Painter strategy [33] and concatenating inputs and targets in a manner analogous to the MAE approach [11] for reconstruction.",2,positive
"It has been widely used in various fields, including graph learning [13], computer vision [14], and natural language processing [28].",1,neutral
"Our findings indicate that a masking ratio of 75% and a patch size of 16 achieve the best transfer performance, which is consistent with MAE for natural images [8].",0,negative
"These two conclusions are consistent with other works [26,8].",1,neutral
"Recently, as representative of generative self-supervised learning (SSL) methods, masked autoencoder (MAE) [8] has achieved great success in many vision tasks [11,10,24].",1,neutral
"Contrastive learning (CL) [24], [140], [141], [142] offered the first methodology to surpass supervised learning in downstream tasks, and masked image modeling (MIM) [25], [52], [143] pushed the performance of pre-trained models to a higher level.",1,neutral
"A pre-training stage with either selfsupervised representation learning [25], [52] or largescale datasets (e.",1,neutral
"The recently published Segment Anything Model (SAM) [13] uses a combination of these techniques, namely a computationally-heavy MAE-ViT as its image encoder, a flexible prompt encoder using positional encodings, and a significantly lighter Transformer block as its mask decoder.",2,positive
"Concurrently, Masked Autoencoders (MAE) [12] have become popular for their ability to efficiently learn representations in a self-supervised manner by randomly masking out a portion of the input image and then training to reconstruct the missing pixels.",1,neutral
"0 is based on multi-mask training that considers M different masked versions of the training sample, similar to Masked Autoencoders (MAE) [28].",2,positive
"We apply the loss function to all patches, rather than the visible ones only [24], to optimise for reconstruction.",1,neutral
Interested readers are encouraged to refer to the paper [24] for more details.,0,negative
The underlying assumption is that a masked autoencoder (MAE [24]) trained on healthy samples is capable of learning their distribution and reconstructing healthy regions of the image.,1,neutral
"Following the principles of reconstruction-based detection, we directly use the well-designed training pipeline of MAE [24] and serve the reconstruction error to assign an anomaly score.",2,positive
"[24] for the image classification task, where a much higher masking ratio performs better.",1,neutral
"[23] presented a memory-augmented selfattention encoder and a multi-level cross-attention decoder based on a masked autoencoder [24] with a large masking ratio, aiming to obtain high reconstruction error for the anomalous regions.",2,positive
"As a directed result, GANs are difficult to be equipped with some standard modules, such as dropout [65] and masking operator [9, 22], which strengthen the model generalization via introducing randomness into the training process, but further decrease the training stability.",1,neutral
"We argue that such dynamically masking at time intervals can break the original dependency of the discriminator on some local features that are important to distinguish historical samples, inspired by [65, 22, 14, 43].",1,neutral
"Note that self-supervised learning methods like image-only constrative learning [6, 8, 21] and masked image modeling [2, 14, 20, 68] are not shown for comparison.",1,neutral
"Note that there is also a vast amount of literature on exploring image-only self-supervised learning methods, such as image-only contrastive learning [6, 21], non-contrastive learning [4,7,19] and masked image modeling [2,20,52,56].",1,neutral
"Very recently, inspired by the success of pre-training and finetuning paradigm in natural language processing [7, 38] and computer vision [4, 14], researchers started to apply self-supervised learning (SSL) to molecular property prediction [16, 17, 27, 37, 43, 57].",1,neutral
EVA advanced the state-of-the-art by combining the ideas of MAE and CLIP.,2,positive
"A new era in deep learning-based computer vision has begun with the introduction of numerous vision foundation models, such as CLIP [37], Bamboo [63], SEER [17], MAE [18], UniCL [57] and many others [23, 27, 60, 62].",1,neutral
"Following the recent success in the area of natural language processing with models such as BERT [11] and GPT-3 [5], some of these groundbreaking results were achieved by scaling up the model size [15, 18, 60], adding more training data [17, 18], employing promising self-supervised learning schemes [17, 18] or supervising vision encoders from natural language [23, 37, 57, 62].",1,neutral
Masked Auto-Encoder (MAE) [18] is another unsupervised training regime that uses asymmetric auto encoders to train Visual Transformers on unlabeled data collections.,1,neutral
The SEER RegNet [17] and the MAE ViT-L were used with huggingface [54].,1,neutral
"We build upon masked autoencoders (MAEs) (He et al., 2022) and train vision transformers (ViTs) (Dosovitskiy et al.",2,positive
"We build upon masked autoencoders (MAEs) (He et al., 2022) and train vision transformers (ViTs) (Dosovitskiy et al., 2020) to reconstruct masked LiDAR data from fused LiDAR and camera features.",2,positive
"Our proposed fusion method builds upon masked autoencoding (He et al., 2022), which is a recent form of denoising autoencoding.",2,positive
"To encode a richer geometry in our interpolation, we embed the datasets using a masked auto encoder (MAE) [He et al., 2022] and learn the OTDD map in this (∼200K dimensional) latent space.",2,positive
"datasets using a masked auto encoder (MAE) [He et al., 2022] and learn the OTDD map in this (∼200K dimensional) latent space.",1,neutral
We use ResNet-18 as the model architecture and pre-train the model on decoded MAE images (interpolated dataset) or original images (single dataset).,2,positive
"Inspired by Mae [16] and BERT [9], we mask random patches of the input image and labels at the training stage.",2,positive
"Inspired by the success in natural language processing (NLP) [3, 26] and computer vision (CV) [5, 8] communities, Transformer [35] has get an impressive result in long multivariate time series forecasting (LMTF) on the strength of powerful ability to capture long-term dependence.",1,neutral
"With the rise of Transformer in NLP and CV [3, 5, 8, 26], it has also aroused great interest in the time series community.",1,neutral
"[8] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Namely Masked Image/Language Modeling [9, 15, 35] and language-image pretraining [22].",1,neutral
al [15] proposed a similar masked learning process in vision transformer.,1,neutral
He et al. (2021) state that masking autoencoders (including ViT) are scalable self-supervised learners due to this approach.,1,neutral
"3 [17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Finetuning large pretrained models on downstream tasks has been increasingly popular nowadays [3, 11, 17].",1,neutral
"Some recent studies [16, 19, 22, 25, 37] have utilized pixel information as the low-level supervisory signal for self-supervised pre-training.",1,neutral
"Some of these methods [7, 15, 19] have introduced an encoder-decoder architecture to separate uncorrupted encoded information from masked tokens, which are employed directly as input to the decoder.",1,neutral
"While several recent studies have refined the MIM approach [7, 15, 19], there has been little exploration of alternative pre-training objectives in the visual domain.",1,neutral
"A straightforward approach is to employ foundation models trained from natural images He et al. [2022], Zhai et al.",1,neutral
"However, its out-of-the-box performance on complex medical tasks such as pancreas or cell nuclei segmentation is not satisfactory He et al. [2023], Deng et al. [2023], Mazurowski et al. [2023]. SAM can be further tuned to achieve state-of-the-art performance by leveraging high-quality downstream data and performing proper fine-tuning Ma and Wang [2023], adding adapters with specially designed architectures Wu et al. [2023a], or effective prompts Huang et al. [2023a], Cheng et al.",2,positive
"A straightforward approach is to employ foundation models trained from natural images He et al. [2022], Zhai et al. [2022], Wang et al.",1,neutral
"However, its out-of-the-box performance on complex medical tasks such as pancreas or cell nuclei segmentation is not satisfactory He et al. [2023], Deng et al. [2023], Mazurowski et al. [2023]. SAM can be further tuned to achieve state-of-the-art performance by leveraging high-quality downstream data and performing proper fine-tuning Ma and Wang [2023], adding adapters with specially designed architectures Wu et al. [2023a], or effective prompts Huang et al.",2,positive
"However, its out-of-the-box performance on complex medical tasks such as pancreas or cell nuclei segmentation is not satisfactory He et al. [2023], Deng et al.",2,positive
"A straightforward approach is to employ foundation models trained from natural images He et al. [2022], Zhai et al. [2022], Wang et al. [2022], Oquab et al.",2,positive
"However, its out-of-the-box performance on complex medical tasks such as pancreas or cell nuclei segmentation is not satisfactory He et al. [2023], Deng et al. [2023], Mazurowski et al. [2023]. SAM can be further tuned to achieve state-of-the-art performance by leveraging high-quality downstream data and performing proper fine-tuning Ma and Wang [2023], adding adapters with specially designed architectures Wu et al.",2,positive
"A straightforward approach is to employ foundation models trained from natural images He et al. [2022], Zhai et al. [2022], Wang et al. [2022], Oquab et al. [2023], and then design sophisticated algorithms to solve downstream medical tasks.",2,positive
"However, its out-of-the-box performance on complex medical tasks such as pancreas or cell nuclei segmentation is not satisfactory He et al. [2023], Deng et al. [2023], Mazurowski et al.",2,positive
"47.8 - - - 42.6 IN-1k DINO 48.9 32.9 52.2 62.4 43.7 IN-1k MAE 51.2 34.9 54.7 66.0 45.5 IN-1k FLSL 53.1 36.9 56.2 67.4 47.0
Table 2: VITDET-B/16 WITH MASK R-CNN ON COCO
Pretrain Backbone APVOC IN-1k DINO ViT-S/16 48.9 IN-1k DINO ViT-B/16 49.1 IN-1k DINO ViT-S/8 51.1 IN-1k FLSL ViT-S/16 53.1 IN-1k FLSL ViT-B/16 53.5 IN-1k FLSL ViT-S/8 55.2
Table 3: FASTER R-CNN FPN ON UAVDT
Protocol for hyperparameter tuning Standard instance-level SSL evaluation protocols typically utilize one of the two approaches: employing a k-NN classifier or training a linear classifier on fixed features.",0,negative
FLSL-pretrained ViT on ImageNet-1k (IN1k) demonstrates superior performance compared to the state-of-the-art ADCLR-IN1k [61] and MAE [33] pretrained counterparts.,2,positive
"FLSL also outperforms the SOTA generative approach, MAE, by +1.7% and +1.4% in the two tasks, respectively.",2,positive
"In general, SSL for ViT can be classified into two categories: the joint-embedding strategy epitomized by DINO [8] and MoCov3 [13], and the generative approaches represented by MAE [24].",1,neutral
"Baselines We compare FLSL with various existing SSL approaches that are based on the ResNet [27] and ViT [19] architectures: (a) self-supervised ResNet: MoCo-v2 [12], DetCo [55], DenseCL [51], BYOL [23], and SCRL [43]; and (b) self-supervised ViT: MoCo-v3 [13], MoBY [57], DINO [8], MAE [24], SelfPatch [60], and ADCLR [61].",2,positive
"FLSL pseudo-code, complete training details, and settings of augmentation pipeline are provided in Appendix D. Baselines We compare FLSL with various existing SSL approaches that are based on the ResNet [27] and ViT [19] architectures: (a) self-supervised ResNet: MoCo-v2 [12], DetCo [55], DenseCL [51], BYOL [23], and SCRL [43]; and (b) self-supervised ViT: MoCo-v3 [13], MoBY [57], DINO [8], MAE [24], SelfPatch [60], and ADCLR [61].",2,positive
"However, MAE deals with high-level vision tasks, while we are dealing with low-level vision tasks, our optimal masking ratio should be lower than that of MAE.",2,positive
"When adopting the random mask strategy, the optimal masking ratio is a key parameter to tune as it depends on the redundancy of the data used [18].",1,neutral
"For example, BERT [34] uses the rate of 15% for the language masking, while MAE [18] uses 75% for the image masking, which indicates that there is more redundancy information in images than languages.",1,neutral
"1(a), they apply the same detection head on both domains, inevitably leading to performance degradation on the target domain [10, 5, 29].",1,neutral
"Recently, several efforts [27, 63] have also shown the exceptional capability of ViTs in self-supervised learning of surrogate tasks such as masked image modeling which may significantly enhance the performance of downstream applications.",1,neutral
These works show that MAE effectively releases the transformer power and do not require extra labeled data.,2,positive
"In essence, the proposed masked cost-volume autoencoding (MCVA) has unique designs compared with conventional MAE methods, which encourages the cost-volume encoder 1) to construct high-level holistic representation of the cost volume, more effectively encoding long-range information, 2) to reason about occluded (i.e., masked) information by aggregating faithful unmasked costs, and 3) to decode task-specific feature (i.e., larger cost patches at required locations) to better align the pretraining process with that of the finetuning.",2,positive
Masked Autoencoding (MAE).,0,negative
The conventional MAE methods aim to reconstruct input data at fixed locations and use the positional encodings as query features to absorb information for reconstruction (the first row of Tab.,1,neutral
"As a self-supervised learning technique, MAE, e.g., BERT [21], achieved great success in NLP. Based on transformers, they mask a portion of the input tokens and require the models to predict the missing content from the reserved tokens.",1,neutral
Randomly masking cost values [22] can lead to information leakage and makes the model biased towards aggregating local information.,1,neutral
We show that the naive adaptation of MAE scheme to cost volume does not work due to the redundant nature of cost volumes and the incurred pretraining-finetuning discrepancy.,2,positive
"Pretraining with MAE is also introduced to these modalities, e.g., image [22], [59], [60], [61], video [62], point cloud [63], [64], [65], [66].",1,neutral
"We are inspired by the recent success of masked autoencoding, such as BERT [21] in NLP and MAE [22] in computer vision.",2,positive
Pretraining with MAE encourages transformers to build effective long-range feature relationships.,2,positive
", image [22], [59], [60], [61], video [62], point cloud [63], [64], [65], [66].",1,neutral
MCVA-FlowFormer is the first work that introduces the transformers with MAE pretraining paradigm to optical flow and sheds light on improving optical flow estimation with unlabeled data.,2,positive
"In the field of CV, such methods continue to develop and have proven effective (Pathak et al., 2016; Dosovitskiy et al., 2021; He et al., 2021; Wei et al., 2022; Xie et al., 2022).",1,neutral
"To address these challenges, we propose to use masked autoencoders (MAE) (He et al., 2021) as a pre-training strategy",2,positive
"As shown in Figure 1, our method is an extension of MAE (He et al., 2021) to 3D electron microscopy image data.",2,positive
"The best masking ratio we observed for 3D MAE (He et al., 2021) on SEM images can reach 90%.",2,positive
", 2019) uses a masking ratio of 15% for languages, while MAE (He et al., 2021) uses a masking ratio of 75% for images, indicating that images are more information redundancy.",1,neutral
", 2021) is also used to enhance the representation ability of self-attention mechanismmodels (He et al., 2021;Wei et al., 2022; Xie et al., 2022).",1,neutral
"As a feasible alternative, self-supervised learning acquires supervised information from the data itself and has recently been shown to successfully address the need for data and be able to learn dense representations of the input (Hung et al., 2018; Lin et al., 2020; He et al., 2021; Mittal et al., 2021).",1,neutral
", 2017), Masked autoencoder (MAE) (He et al., 2021) is also used to enhance the representation ability of self-attention mechanismmodels (He et al.",1,neutral
", 2019) and two-dimensional (He et al., 2021;Wei et al., 2022) methods.",1,neutral
"Unlike the 2D MAE (He et al., 2021) design, due to the different spatial resolutions during imaging, we do not use downsampling in the z-direction, which ensures the 3D resolution of the voxel is close to a cube.",2,positive
"Our study utilizes two well-performing self-supervised Vision Transformers, MAE (He et al., 2022) and MoCo v3 (Chen et al.",2,positive
"As a result, self-supervised models demonstrate superior scalability across various vision tasks compared to supervised ones (Chen et al., 2020; He et al., 2020; Grill et al., 2020; He et al., 2022; Caron et al., 2021; Bao et al., 2021).",1,neutral
"Masked image modeling (He et al., 2022; Bao et al., 2021; Zhou et al., 2021; Xie et al., 2022), which learns representations by recovering randomly masked patches, is",1,neutral
", 2021) and MAE (He et al., 2022) retain rich information across the blocks.",0,negative
", 2011) classification benchmark, MAE (He et al., 2022) and MoCo v3 (Chen et al.",2,positive
"For instance, a large performance gap exists between full fine-tuning and linear probing when employed as a transfer method for the Masked Autoencoder (MAE) (He et al., 2022).",1,neutral
"Currently, self-supervised learning (SSL) with Vision Transformers (ViTs) (Bao et al., 2021; He et al., 2022; Chen et al., 2021; Caron et al., 2021) have exhibited remarkable results across diverse visual recognition tasks such as classification and semantic segmentation.",1,neutral
"Self-supervised Vision Transformers have proven to be an excellent pretrained backbone for computer vision tasks (Bao et al., 2021; He et al., 2022; Chen et al., 2021; Xie et al., 2022; Zhou et al., 2021; Caron et al., 2021).",2,positive
"Utilizing these models for diverse computer vision tasks is a promising strategy, as they have demonstrated excellent transferability and high performance (He et al., 2022; Chen et al., 2021; Caron et al., 2021; Bao et al., 2021; Zhou et al., 2021).",2,positive
"Deviating from prior practices [7, 32], we develop RAE and R-MAE by pre-training on COCO train2017 [45].",0,negative
"Reconstructive learning is the dominating paradigm in pre-training natural language representations [23, 10], and while steady progress is made [17, 32], computer vision models are still lagging behind.",1,neutral
"We begin with MAE [32] as a representative baseline, and explore the use of pre-computed regions [25] in an MAE-style.",2,positive
"Different from prior practices [7, 32], we develop R-MAE by pre-training on COCO train2017 [45], for its scenecentric images and ground-truth regions as potential oracles.",2,positive
"Using the default fine-tuning recipe from ViTDet [43] and MAE [32], our RAE shows significant improvement (47.",2,positive
"RAE as a task is fully compatible with MAE, which can be optimized in parallel by simply restoring the pixel decoder [32].",2,positive
"Besides supervised classification, un- or self-supervised learning methods [23, 10, 18, 32] have recently emerged as powerful alternatives for pre-training representations.",1,neutral
MAE [32] is the foundation and baseline of our RAE and R-MAE.,2,positive
"On the other hand, reconstructive methods [32, 7, 58, 19] as denoising autoencoders [56] preserve the 2D structure.",1,neutral
"Meanwhile, reconstructive pre-training such as Masked Autoencoding (MAE) [32] has proven even more effective, improving the upper-bound of detection accuracy beyond faster convergence [44, 64].",1,neutral
Masked autoencoder (MAE): A state-of-the-art self-supervised ViT-based model for ImageNet[21].,2,positive
"Due to the lack of labeled medical images, many studies pre-
train their models on large-scale natural image datasets (such as ImageNet) and transfer the pre-trained models to medical imaging tasks[48] [49].",1,neutral
"[55] proposed a generic autodidactic model and demonstrated that their method outperforms any 2D approaches, both models transferred from ImageNet and the 2D version of their model.",1,neutral
performs superiorly on a variety of tasks[21-23].,1,neutral
"We now show how similar ideas can be extended to other popular self-supervised learning objectives, such as non-contrastive learning [6, 92] and masked pre-training [20, 28].",1,neutral
"[28] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Masking: Conceptually, masking [20, 28] can be interpreted as leveraging unmasked regions in the same modality to predict masked regions or leveraging the other modality to predict the masked region.",1,neutral
"Many works focus on image- or pixel-level contrastive learning for semantic tasks [56, 30, 12, 55, 31].",1,neutral
"In most papers such as [22, 16, 43, 31] the autoencoders attempt to reconstruct the original image directly.",1,neutral
"Although the ReID model incurs additional computational costs, the ReID model obtained by self-supervised pre-training [24]–[28] can effectively against the poor appearance of occluded targets and provide high-quality instance appearance embeddings for the tracker.",2,positive
"[30], where a self-supervised learning method in CV is proposed.",1,neutral
"According to the setup of the proposed models, we also evaluate SSAST and MAE-AST with a temproal resolution of 40 ms per frame, by applying average pooling to the frame-level representations.",2,positive
"Supervised Methods PANN [42] 81M 43.9 27.8 - - - PSLA [47] 14M 44.4 31.9 - - 55.4 AST [25] 86M 45.9 34.7 98.1 - - HTS-AT [48] 31M 47.1 - 98.0 - - PassT [49] 86M 47.1 - - - 65.3 KD-AST [33] 86M 47.1 - - - 62.9
Self-supervised Methods SSAST-PATCH [6] 89M AS+LS - 31.0 98.0 64.2 - SSAST-FRAME [25] 89M AS+LS - 29.2 98.1 80.8 - Conformer [8] 88M 67K hours * 41.5 27.6 - - - MAE-AST-PATCH [7] 86M AS+LS - 30.6 97.9 - - MAE-AST-FRAME [7] 86M AS+LS - 23.0 98.0 63.3 - ASiT [30] 85M AS - 35.2 98.8 63.1 - data2vec [24] 94M AS - 34.5 - - - MaskSpec [26] 86M AS 47.1 34.7 97.6 - - MSM-MAE [14] † 86M AS - 36.7 98.4 95.3 - Audio-MAE (local) [9] 86M AS 47.3 37.0 98.3 94.8 - BEATsiter3 [10] 90M AS 48.0 38.3 98.3 - - BEATsiter3+ [10] ** 90M AS 48.6 38.9 98.1 - - M2D [15] 86M AS - 37.4 98.5 94.4 -",0,negative
SSAST [6] and MAEAST [7] have shown that the patch-wise strategy and framewise strategy perform differently for different downstream tasks.,1,neutral
"Among the comparison models, BEATs performs the best in terms of both PSDS1 and PSDS2, and MAE-AST-PATCH achieves
close performance with BEATs.",2,positive
"AST [7], MaskSpec [26], MSM-MAE [14] and MAE-Audio [9] follow the asymmetric encoder-decoder structure of vision SSL pre-training method MAE [27], in which the encoder encodes the unmasked region, while the decoder processes both the masked and unmasked regions and reconstructs the unmasked region.",2,positive
"This kind of method has been first applied to speech self-supervised pre-training, e.g. MockingJay [20], wav2vec2 [21], Hubert [22], Tera [23] and data2vec [24], and then to audio self-supervised pre-training, e.g. SSAST [25], Conformer [8], MAE-AST [7] and MAE-Audio [9].",1,neutral
"Data augmentation is critical for adjusting the matching difficulty; ii) data2vec constructs the teaching representation by taking the average of the last eight transformer blocks, while our ATST-Frame uses the asymmetric structure of the BYOL [16], where an extra predictor network is set for the student branch. iii) M2D organizes spectrograms patch-wisely and uses a MAE structure
3 in the student branch, while ATST-Frame adopts a frame-wise strategy and uses a standard transformer encoder architecture.",2,positive
"1) Comparison Methods: We compare with six SSL pretrained models: BYOL-A-v2 [11], SSAST [6], MAE-AST [7], Audio-MAE [9], BEATs [10] and M2D [15].",2,positive
"The framewise models, e.g. SSAST and MAE-AST, can also be directly used for this task, with a temproal resolution of 20 ms per frame.",2,positive
"For SSAST [6] and MAE-AST [7], increasing the temproal resolution of their frame-wise models from 20 ms to 40 ms largely improves the performance.",2,positive
"From the perspective of model architecture, MAEAST [7], MaskSpec [26], MSM-MAE [14] and MAE-Audio [9] follow the asymmetric encoder-decoder structure of vision SSL pre-training method MAE [27], in which the encoder encodes the unmasked region, while the decoder processes both the masked and unmasked regions and reconstructs the unmasked region.",2,positive
We pretrain all models from scratch for 400 epochs using ResNet50 [20] (except ViT-B [12] for MAE [17]) as our backbone with synchronized batch normalization [18] during pretraining.,2,positive
"For MAE, we strictly follow its original pre-training settings [17].",0,negative
"Since our cropping strategy is orthogonal to SSL methods, we now validate its adaptability to the non-contrastive SSL method MAE [17].",2,positive
"ViT-Base pretrained (800ep) on COCO with MAE [17], and finetuned on COCO using Mask RCNN FPN (‘Mask’) and Cascade RCNN FPN (‘Cas.",2,positive
"Thus we include six self-supervised methods from a variety of families [3] – contrastive (SimCLRv2 [17]), self-distillation (DINOv2 [73]), canonical correlation analysis (SwAV [13]), masked image modelling (MAE [40] & data2vec [2]), and language-image (CLIP [80] – we use the OpenCLIP implementation [45] trained on DataComp-1B [32]) – and design experiments to qualitatively and quantitatively understand their respective feature spaces.",2,positive
"Interestingly MAE achieves high linear-classification accuracy but performs very poorly on kNN evaluation protocols [40], perhaps also indicating a lack of clustering based on class-specific semantic information.",2,positive
"MAE and data2vec, both trained using masked image modelling, have a widespread focus on textures and shapes, with an often smaller importance for the semantic information related to the main object.",1,neutral
"CLIP VIT-L/14, DINOv2, and MAE display far greater alignment with the human experiment baseline.",2,positive
"MAE Masked autoencoder (MAE) [40] is from the masked image modelling family, and learns to directly reconstruct masked image patches.",2,positive
"Figure 18 shows the results across 4 encoders -
Inception, CLIP, DINOv2, and MAE.",0,negative
"Therefore, it is counterintuitive to obtain very low values, such as those observed in MAE, data2vec, SimCLRv2, and even with simple resizing, where there are only a few effective samples among the 50k images.",1,neutral
"The masked models, MAE and data2vec, have a intra-class diversity that is highly correlated with each other, but also with the diversity as measured in image-space.",2,positive
These results are perhaps unsurprising given their respective self-supervised training procedures - the contrastive SimCLR relies on a set of heavy augmentations while MAE and data2vec work closer to pixel space and do not use augmentations - but such results point to clear differences between perceptual spaces as a direct result of the self-supervised training objective.,2,positive
MAE is similar to CLIP except with the roles of FLS-POG and CT perhaps flipped. data2vec is somewhat similar to SwAV with the correct directionality – particularly for CT score – but with a slightly weaker trend for the other metrics.,2,positive
"Obvious choices for more universally applicable representation spaces are modern self-supervised learning (SSL) models [3] trained on large and diverse datasets, as they have proven to extract representations that excel at a number of generalized downstream tasks [17, 13, 18, 37, 14, 80, 40, 73].",1,neutral
We find that SimCLR ranks worst on the perceptual score while the masked methods of MAE and data2vec score the highest.,2,positive
"In this work, we choose masked autoencoder (MAE) [12] and contrastive learning [3, 11] to pre-train models, as they generalize well on multiple tasks and show an impressive performance in previous research.",2,positive
"Instead of using a pre-training strategy specific to one downstream task, we apply masked autoencoder (MAE) [12] and MoCoV2 [3], a successful method for contrastive learning, to evaluate the generated data.",2,positive
"Recent works [17, 18, 21, 39] apply the successful masked autoencoder [12] on point clouds.",1,neutral
"MAE [12] is employed to pre-train vision transformers [7, 30].",1,neutral
"In the general paradise of DAE, noise δ in Function 6 is randomly patched to the tokens or positional embeddings [14, 21], and its additive proportion can be adjusted to simulate different levels of noise pollution.",1,neutral
"Generative methods such as Masked Autoencoder [21] mainly adopt dimensionality reduction and reconstruction to compile the most important feature of data into an encoder, as well as filter the noise contained in data [14, 55].",1,neutral
Then He generalizes this method to pre-train an encoder for an image task [21].,1,neutral
We leave other structures such as Masked Autoencoder [21] adopting Vision Transformer [15] for further exploration.,2,positive
[9] showed that reconstructing pre-trained features obtained from self-supervised methods like DINO [10] or MAE [11] leads to state-of-the-art object discovery on complex real-world images.,1,neutral
"The most promising avenue so far in terms of scaling to the real-world is to reconstruct features from modern self-supervised pre-training methods [10, 11, 37, 38].",1,neutral
"In Figure 7d, we show that VideoSAUR works well with 4 different self-supervised representations, with MSN [37] and DINO [10] performing slightly better than MAE [11] and MOCO-v3 [38].",2,positive
"In particular, Seitzer et al. [9] showed that reconstructing pre-trained features obtained from self-supervised methods like DINO [10] or MAE [11] leads to state-of-the-art object discovery on complex real-world images.",1,neutral
"We use the powerful pipeline of MAE [53] with the following modifications: (1) Patches are not dropped, (2) The loss function used is Gumbel-Softmax",2,positive
"We observe that: (1) The style of reconstructed images is very different from original images and (2) Privacysensitive patches such as faces and texts are blurred, and thus the reconstruction with MAE still cannot reveal the original identity of faces or the contents of texts.",1,neutral
"We use a recently proposed powerful Transformerbased framework, MAE [53] (tiny), to recover the original clean images from the images encrypted by MI.",2,positive
"We adapt MAE with two modifications: (1) Patches are not dropped and (2) The linear patch embedding is replaced by a nonlinear patch embedding, which is consistent with the patch embedding used in PEYOLOS.",2,positive
"We use the powerful pipeline of MAE [53] with the following modifications: (1) Patches are not dropped, (2) The loss function used is Gumbel-Softmax
proposed in [54], and (3) The positional encoding is removed, which is necessary as the patch orders are randomly shuffled by RS.",2,positive
Much of the motivation behind goal-conditioned RL is that it will allow RL researchers to realize the same sorts of scaling laws seen in self-supervised methods for computer vision [116] and NLP [51].,1,neutral
"In NLP and computer vision, self-supervised learning is typically done via one objective (often denoising), while the downstream tasks use a different objective (e.g., linear regression).",1,neutral
"Scaling model performances with the amount of data have been successfully demonstrated in CV and NLP [7, 44], motivating us to study whether contrastive RL offers similar scaling capabilities in the offline RL setting.",2,positive
"Our work is only a small step in this direction; relative to the model sizes and dataset sizes in NLP and CV, ours are tiny.",2,positive
"Second, because much of the work on self-supervised learning has been focused on CV and NLP, not RL, the recent innovations in those field may not transfer to the RL setting; the best objectives, architectures, and regularizers for the RL setting may be considerably different from the best choices in other fields.",1,neutral
"E.5 Dataset Size
Scaling model performances with the amount of data have been successfully demonstrated in CV and NLP [7, 44], motivating us to study whether contrastive RL offers similar scaling capabilities in the offline RL setting.",2,positive
"Self-supervised learning serves as the bedrock for many NLP and computer vision applications, leveraging unlabeled data to acquire good representations for downstream tasks.",1,neutral
"One natural next step is thus to scale to large-scale transformer architectures, where recent work has corroborated masked autoencoding as an effective strategy on a token basis He et al. (2022).",1,neutral
"In the past few years, researchers have developed various pretraining methods (Chen et al., 2020; Khosla et al., 2020; Grill et al., 2020; He et al., 2022; Baevski et al., 2022) that enable models to learn from massive real world datasets.",2,positive
"Different from contrastive learning, MIM task follows the idea of masked language modeling task (MLM) [26, 27,
ar X
iv :2
30 6.",1,neutral
"The existing MIM methods attempt to map the highly masked image into a fixed target, which inevitably introduces large fitting error, even if the prediction is a plausible solution for the given input.",1,neutral
4 MAE [17] ViT-S/16 ViT decoder 1 invisible pixel 1600 79.,1,neutral
"However, it fails to evaluate the quality of non-linear features [1, 17, 25].",1,neutral
"Due to the heavy spatial redundancy of image, the highly random masked images in MIM task can still effectively retain the semantics of the original images [17], which achieves very promising performance in self-supervised learning.",1,neutral
"We call it as non-unique target issue, which substantially limits the flexibility of the MIM models.",2,positive
"Inspired by the above observations, we propose a novel asymmetric patch sampling strategy, to introduce more asymmetry for contrastive learning and alleviate the nonunique target issue suffered by the existing MIM methods at the same time.",2,positive
"For the first view, we conduct sparse patch sampling [17] to obtain highly sparse patch sequences, which only contain small portion of patches from the original image, e.",1,neutral
8 MAE [17] ViT-B/16 ViT decoder 1 invisible pixel 1600 83.,1,neutral
"In this task, the masked raw pixels [17, 14, 36] or their tokens [1, 12, 34] are used as the targets for model training.",1,neutral
"Second, compared to MIM methods, we replace the reconstruction objective with contrastive one, which provides more flexible targets for training.",2,positive
"Based on the difference of pretext tasks, the popular branches contain contrastive learning (CL) [35, 5, 18, 16, 9, 37, 33] and masked image modeling (MIM) [36, 12, 17, 15, 7].",1,neutral
CAE [7] adopts the representation alignment between visible patches and invisible patches before decoder in MIM task.,2,positive
"For example, generative-based SSL such as the Masked autoencoder (He et al., 2021) learns to reconstruct images with only a small fraction of the pixels.",1,neutral
"Generative Modeling for Images: There exists a large body of work in deep learning for image generation, including methods based on Variational Autoencoders (VAEs) [26], Generative Adversarial Networks (GANs) [16, 23–25, 41], Transformer networks [11, 17], and Diffusion models [8].",1,neutral
"VQGAN is a state-of-the-art method that generates images of better quality efficiently at higher resolutions than other counterparts such as StyleGAN [23] and Vision Transformers [11, 17].",2,positive
"They utilize a masked autoencoder (MAE) backbone [38] to encode the image into a latent space, before using an attentionbased decoder to generate segmentation masks from a learned encoding of prompts, in contrast to our work, which operates clustering directly on the encoder outputs.",2,positive
"More specifically, for the image encoder, it uses a MAE [8] pre-trained Vision Transformer (ViT [7]) to extract image embedding.",1,neutral
"[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Supervised learning methods have seen large strides in recent years in computer vision (CV), natural language processing (NLP), and human-level game playing [18, 26, 42, 8, 49, 13, 43].",1,neutral
"Aligned with the success of transformer-based [56] language pre-training models [43, 62], image-text pretraining [41, 23] and video-text pre-training [29, 20] usually use masked visual modeling and have shown promising results on short videos clips.",1,neutral
"MAE [26] introduced an asymmetric encoder and decoder architecture, where masked tokens are skipped in computation-heavy encoder and only pass all tokens through a light-weight decoder.",1,neutral
"Self-supervised learning methods [58], [26] use data itself",1,neutral
"The proposed rPPG loss is more suitable for pre-training than the original pixel reconstruction loss adopted in vanilla MAE [26], which enables",2,positive
"Despite the advancements in SSL-enhanced recommender systems, a fundamental question remains poorly understood: What information is crucial and should be preserved for self-supervised augmentation in recommendation? Motivated by the recent success of masked autoencoding (MAE) techniques in advancing selfsupervised learning [3, 4, 7], this work explores the above question from the perspective of generative self-supervised augmentation with rationale-aware invariant representation learning.",1,neutral
"While our model effectively leverages the partially labeled data, its performance can be potentially improved by addressing missing labels from self-supervised [19] or semi-supervised learning [47, 46].",2,positive
"Here we consider three pretrained models: ResNet34, MAE[9] and SimCLR[4]",2,positive
"Various feature extractor selections setting: In addition to the WideResNet28 model (used as feature extraction module in this paper), we also consider three additional pre-trained models: ResNet34, SimCLR, where we use ResNet18 as backbone and MAE, where we use ViT-tiny as backbone, as a supplement to prove the effectiveness of the online version.",2,positive
"From CIFAR-100, we observe that across all models, the representations of the Original images are most similar to the Foreground variants with similarity scores ranging from 0.24 for the MAE model, to the 0.4 for the Supervised model.",2,positive
"Given the pretrained task of the MAE model, it was expected that it would be more robust to the masking strategies.",2,positive
"Nonetheless, all the models except MAE reach higher balanced accuracy on the Original image than on the Foreground, and the Center variant leads to higher performance compared to the Foreground variant.",0,negative
This highlights that MAE leads to disjointed representation from different variants.,1,neutral
"Furthermore, the MAE model also shows really low similarity between the Background and Original, and Border and Background.",0,negative
"Meanwhile, in the MAE representation space the different variant are mostly matching their neighbors.",1,neutral
"When evaluated on only the Background version of the dataset, all models except the MAE-based were able to achieve relatively high balanced accuracy, above 70% balanced accuracy.",0,negative
"Examples include autoencoders [10, 7], clustering [11, 12], instance-level discrimination [13, 14], and contrastive learning [15].",1,neutral
"Recently, it has been demonstrated that SSL methods such as SimCLR [5], VICReg [6], MAE [7], and DINOv2 [8] can achieve superior performance in many downstream vision tasks, including image classification, object detection, and semantic segmentation.",1,neutral
"According to Balestriero et. al, in [4] SSL can be classified into four broad families: the Deep Metric Learning family (e.g., SwAV [22] and SimCLR [5]), the Self-Distillation family (e.g., DINOv2 [8]), the Canonical Correlation Analysis family (e.g., VICReg [6]), and the Masked Image Modeling family (e.g., MAE [7]).",1,neutral
"To obtain the visual analogue of words, part-based image representations are investigated in SemMAE [94].",1,neutral
Masked AutoEncoders (MAE) [104] mask random patches from images and reconstruct missing pixels.,1,neutral
"To this end, video masked autoencoders (VideoMAE) [97] are proposed to conduct video tube masking with a high masking ratio, making this task challenging.",1,neutral
"To explore whether the performance can be improved with the multi-scale backbone and local and global operations, MCMAE is proposed in [98] by designing multi-scale hybrid convolution-transformer architecture via the mask autoencoding scheme.",2,positive
"In masked prediction, due to the spatial feature redundancy [104] in computer vision, it is important to define a proper masking strategy, including how much and where to mask.",1,neutral
"The effective data augmentation strategies for the MAE remains to be further explored, which are different from the those used in constrastive learning.",2,positive
"Ta rg et s MaskCo [102] ICCV21 Contrastive mask prediction, mask prediction head BEIT [103] ICLR22 Masked image modeling MAE [104] CVPR22 Asymmetric encoder-decoder MaskFeat [105] CVPR22 Space-time mask and predict MFM [106] ICLR23 Masked frequency modeling CIM [107] CVPR23 Correlation modeling LocalMIM [108] CVPR23 Local multi-scale reconstruction .",2,positive
Mixed Autoencoder (MixedAE) [100] studies the mixing augmentation for MAE.,1,neutral
"Following this paradigm, image representation learning [30, 38, 40] has been widely studied.",1,neutral
", mask image modeling [38, 4] and contrasted learning [60].",1,neutral
"…learning methods gave rise to the need for self-supervised learning (SSL) in both image (Chen et al. 2020; He et al. 2020; Grill et al. 2020; He et al. 2022; Bao, Dong, and Wei 2021) and video (Han, Xie, and Zisserman 2020; Feichtenhofer et al. 2021; Wang et al. 2021; Wei et al. 2022)…",2,positive
"Recently, there has been increasing interest in masked target prediction-based SSL in the image domain (He et al. 2022; Bao, Dong, and Wei 2021; Dong et al. 2021; Xie et al. 2022) and video domain (Wei et al. 2022).",2,positive
"Following the recent masked target prediction-based SSL model (He et al. 2022), we build an SSL model suitable for video summarization tasks with an adaptation of asymmetric encoder-decoder design.",2,positive
"Our autoencoder is built as a variant of masked autoencoders (He et al. 2022; Bao, Dong, and Wei 2021).",2,positive
"Recently, there has been increasing interest in masked target prediction-based SSL in the image domain (He et al. 2022; Bao, Dong, and Wei 2021; Dong et al. 2021; Xie et al. 2022) and video domain (Wei et al.",2,positive
"Such limitations of previous unsupervised learning methods gave rise to the need for self-supervised learning (SSL) in both image (Chen et al. 2020; He et al. 2020; Grill et al. 2020; He et al. 2022; Bao, Dong, and Wei 2021) and video (Han, Xie, and Zisserman 2020; Feichtenhofer et al.",1,neutral
"C V
] 2
J un
2 02
3
We further show that the NN scene understanding capabilities of canonically-pretrained vision transformers (such as MAE [30] and DINO [15]) vary greatly, despite similar finetuned performance.",2,positive
"Similarly to NLP, image-based SSL has witnessed great success in recent years, notably with the advent of contrastive methods [14, 15, 16, 23, 33], self-distillation [12, 18, 28], and masked auto-encoding [30].",1,neutral
"We further show that the NN scene understanding capabilities of canonically-pretrained vision transformers (such as MAE [30] and DINO [15]) vary greatly, despite similar finetuned performance.",2,positive
We follow the finetuning protocol of MAE [30] and use UperNet [76] as a decoder.,2,positive
"We also note that Hummingbird scales well with increasing the dataset size from ImageNet-1k to ImageNet-22k, which does not hold for all other methods (e.g. MAE, consistent with [51]).",2,positive
"†indicates results are taken from [30], using UperNet [76] as the decoder.",0,negative
"For ViTDet, we use the ImageNet-1K MAE pretrained checkpoint [19] with the layerwise lr decay [34]; the pixel decoder is the simple feature pyramid inside ViTDet, whose outputs are upsampled and added together to get the high resolution feature map F .",2,positive
"Because deep learning models can learn good representations from self-reconstruction of the original image [39], the U-Net decoder learnt to reconstruct the original image from the augmented image.",1,neutral
"We empirically evaluate the pretrained weight from ImageNet-1K [40], ImageNet-22k [42] and recent proposed MAE [21, 41] method.",2,positive
4 show that MAE [21] pretraining outperforms the other pretraining methods (73.3% vs.70.4%).,0,negative
"However, we also observe that VideoMAE [41] pretraining does not perform well which only achieves 64.1% AO, comparing to the 70.4% AO from
image classification pretraining.",0,negative
4 show that MAE [21] pretraining outperforms the other pretraining methods (73.,1,neutral
"Masked Autoencoder (MAE) [28] is a self-supervised approach with a vision transformer encoder and a small transformer decoder, which randomly masks a large portion of input patches, and then reconstructs the masked patches according to the visible patches.",1,neutral
"The works include framework design [13, 23, 28, 70], prediction targets [2, 19, 66, 77], and integration with visionlanguage representation learning [42, 43, 75].",2,positive
"To validate the effectiveness of GAN-MAE framework, the used ViT architecture and most hyper-parameters are exactly the same to [28,60], i.",2,positive
"Similar to [28], we further evaluate the robustness of classification performance on the four ImageNet variants, i.",2,positive
"For example, compared with the recent work MAE [28], our GAN-MAE in ViT-L network achieves 86.",2,positive
"The optimal ratio for MAE-GAN is identical to 75%, showing a obviously better classification performance compared with same masking ratio in previous work [28].",0,negative
"Similarly, vision transformer [21, 41, 50] based masked image modeling (MIM) approaches [1,3,28,70,77] for computer vision tasks have also been developed.",1,neutral
"Our experiments follow the same architecture, settings, and pre-training recipe as MAE [28], and we find that the simple incorporation of a discriminator consistently outperforms MAE in variant models, e.",2,positive
", visual tokens [3,19], semantic features [1, 77] and raw pixels [28, 70].",1,neutral
"Particularly, masked image modeling (MIM) in SSL for vision transformers has shown remarkably impressive downstream performance in a wide variety of computer vision tasks [3, 28], attracting increasing attention.",1,neutral
"The original MAE pretraining recipes (He et al., 2022; Feichtenhofer et al., 2022) explicitly do not use drop path (Huang et al.",0,negative
"Following (He et al., 2022), we adjust learning rate, training epochs on each dataset.",2,positive
"ablated here, we find the defaults in (He et al., 2022; Feichtenhofer et al., 2022) to be appropriate.",2,positive
"In the main paper, we show that we can replace the spatial biases offered by specialized modules in a hierarchical vision transformer with a strong pretext task like MAE (He et al., 2022), thereby teaching these spatial biases instead.",2,positive
"On images, Hiera is faster and more accurate than even the most recent SotA (He et al., 2022; Gao et al., 2022; Woo et al., 2023), offering 30-40% speed-up compared to the best model at every scale.",2,positive
"This brings the decoder for video in line with images (He et al., 2022).",2,positive
"Moreover, MAE pretraining is sparse and can be 4− 10× as fast as normal supervised training, making it an already desirable alternative across many domains for more than just accuracy (He et al., 2022; Feichtenhofer et al., 2022; Huang et al., 2022b).",2,positive
"Moreover, their simplicity unlocks the use of powerful pretraining strategies such as MAE (He et al., 2022), which make ViTs computationally and data efficient to train.",2,positive
"Several masked image modeling approaches (He et al., 2022; Wei et al., 2022) have found benefits from longer pretraining schedules, often using up to 1600 epochs.",1,neutral
"Hiera consistently outperforms ViT pretrained with MAE (He et al., 2022), indicating that our Hiera-L and Hiera-H architectures backbone pretrain acc.",2,positive
"Masked Autoencoders (MAEs): Recently, [23] proposed Masked Autoencoders for learning self-supervised image representations.",1,neutral
Masked Autoencoders (MAEs) by [23] are a recent addition to the masked predictive modelling family.,1,neutral
"Masked Autoencoders (MAEs): Recently, [23] proposed Masked Autoencoders for learning selfsupervised image representations.",1,neutral
"A key characteristic of the Masked Autoencoder paradigm is its asymmetric design, which allows pairing small decoders with large encoders while scaling favourably for linear probe performance [23].",1,neutral
"As an example, our default configuration yields np = 250, and thus the window sizes for each MW-MHA module in all decoder blocks will be [2, 5, 10, 25, 50, 125, 250, 250] for a total of 8 attention heads, which is a reasonable number of attention heads inline with previous research [23, 37].",2,positive
"Recently, several works have been proposed for learning audio representations in a self-supervised manner.",1,neutral
"Downstream tasks: Recently, several standardized benchmarks have been proposed to evaluate audio representations thoroughly across a wide variety of domains, such as SUPERB [51] and HEAR [48].",1,neutral
"This is consistent with previous works [23, 31, 37].",0,negative
"Encoder: In line with previous work [23, 31, 37], we use a Vision Transformer (ViT) [17] based encoder, which only processes non-masked patches (20% in this work).",2,positive
"Additionally, to improve the efficiency of training, our model also incorporates a masked autoencoder [21].",2,positive
"As shown in [21], the mask ratio for the encoder input plays an important role affecting both the representation quality and the efficiency.",1,neutral
"Note that this is lower than the 75% mask ratio used in the MAE paper [21], suggesting that it requires seeing more pixels (i.",1,neutral
"We build our ZeroSeg model based upon the recent masked autoencoder (MAE) work [21], which aims to learn semantically meaningful representations through reconstructing masked-out image pixels.",2,positive
"ZeroSeg incorporates a masked encoder [21] as the main backbone, and it has two different heads, the first one is the reconstruction decoder for reconstructing the masked patches.",2,positive
MAE also improves training efficiency by reducing the number of input tokens in the encoder.,2,positive
Our ZeroSeg also builds upon the success of MAE and incorporates a masked autoencoder to improve the training efficiency and semantic representation for those segments.,2,positive
"Among these strategies, MAE [21], or masked autoencoder, stands out for its ability to reconstruct missing patches with superior performance.",2,positive
"Note that this is lower than the 75% mask ratio used in the MAE paper [21], suggesting that it requires seeing more pixels (i.e., lower mask ratio) to better learn pixel-level tasks.",1,neutral
", masked autoencoding [21]), while the outputs from the segmentation head are transformed into several segment tokens {g} to learn semantic segmentation via distillation.",1,neutral
"Denoising autoencoders [21, 9, 3, 15] have gained popularity as a means of reconstructing original images from corrupted inputs.",1,neutral
"Similar to MAE, ZeroSeg leverages an asymmetric encoder-decoder architecture (Fig.",2,positive
"Synthetic images significantly improve over real for MAE, DINO, and SimCLR, and performs on par with real for BYOL, and slightly worse for MoCo-v3 (which could be attributed to not tuning the guidance scale w).",2,positive
"In particular, Masked Autoencoder (MAE) [26] has demonstrated significant improvements in downstream finetuning performance.",1,neutral
"For our study, we choose SimCLR [10] from the former family and MAE [26] from the latter due to their simplicity and strong performance.",2,positive
"When fine-tuning pre-trained MAE models on ImageNet, we found synthetic images are still able to outperform real images.",1,neutral
"The optimal guidance scale for MAE is 6, and this is different from SimCLR where the accuracy peaks at 8 or 10.",2,positive
MAE [26].,1,neutral
"While the linear probing accuracy of MAE is lower than that of contrastive methods, its effectiveness often comes with fine-tuning.",1,neutral
"Following the default hyperparameters in MAE [26], we train a ViT-B/16 model for each guidance scale w.",2,positive
"Following the default hyperparameters in MAE [26], we train a ViT-B/16 model for each guidance scale w. Figure 2(right) reports the linear probing results.",2,positive
"To leverage the power of large model, we adopt VideoMAE [24] as the based model of our clip-level distracted action classifier.",2,positive
MAE [9] is the recently top-performance image pre-training algorithm which proposes a mask modeling pretext task for self-supervised learning.,1,neutral
"More specifically, our backbone is ViT-L/16 and we initialize the model with learned VideoMAE on Kinetics-710 [13].",2,positive
"In [24], the authors presented a self-supervised learning method called Video-MAE for video transformer pretraining.",1,neutral
VideoMAE [24] extends MAE [9] to spatio-temporal space and shows excellent performance on various video understanding benchmarks.,2,positive
MAE [9] is the recently top-performance image pre-training algo-rithm which proposes a mask modeling pretext task for self-supervised learning.,2,positive
"As an alternative, MAE [13] proposes a more straightforward method for image modeling, directly reconstructing the pixels of masked patches.",1,neutral
"Our model is a Vision Transformer (ViT) [10] pretrained using a self-supervised learning approach, which involves the masking-then-reconstruct procedure from Masked Autoencoder (MAE) [13].",2,positive
"As suggested by [13], we use a masking ratio of 75%, which speeds up the pretraining process since only 25% of the patches need to be processed by the MAE encoder.",2,positive
"Drawing inspiration from Masked Autoencoders [13], MAE-Face learns robust visual representa-",2,positive
"Different from supervised learning from human annotations, these methods push the model to solve a no annotation needed pretext task to learn representations, for example: predicting relative patches [5,10], image inpainting [30], solving jigsaw [29], contrastive learning [8], masked image model [10], and so on.",1,neutral
"They pre-trained the Masked Autoencoder (MAE) model on various large-scale facial image datasets
and used it as a visual feature extractor.",2,positive
"Differently, MAE [10] try to reconstruct a masked image to learn semantic features.",1,neutral
Li et al. [27] also use MAE pre-trained weights combined with AffectNet supervised pre-trained weights and ranked 2nd in ABAW4.,0,negative
It also used MAE pre-trained weights to enhance its performance.,0,negative
"For example, various works [27,41] in ABAW competitions have adopted MAE to pre-train their models on a combination of numerous facial recognition databases and achieve promising performance.",1,neutral
"[6] During the training stage, FOVs of size F × F are randomly sampled from the training image volume which then gets divided into non-overlapping n× n multi-pixel patches.",1,neutral
The masked autoencoder (MAE) is a novel selfsupervised representation learning paradigm [6].,1,neutral
"Following MAE [6], our model is trained via the mean squared error (MSE) loss between the reconstructed image and the original image on masked patches 1 D ∑D i=1(xi − yi)(2) where D is the total number of reconstructed pixels in the FOV.",2,positive
"Furthermore, taking inspiration from the Masked Autoencoder (MAE) [6] learning paradigm, we incorporate the surrogate task of multi-pixel patch masking and reconstruction via a light weight ViT decoder for each sampled FOV of a given image to simultaneously learn semantically meaningful token representations of all patches in the FOV.",2,positive
", MAE [18], BEiT [2] and SimMIM [38] with masking probabilities consistent with those reported in the papers.",0,negative
", l2-distance [18] or cross-entropy [2].",1,neutral
MAE [18] is one of the representative works that utilise an asymmetric autoencoder to recover a randomly masked input.,1,neutral
Models in ablation experiments are built upon the ViTS backbone [14] and asymmetric MAE architecture [18].,2,positive
"Some works like MAE [18] and SimMIM [38] do not differentiate visual cues in images, and randomly mask local regions or patches.",1,neutral
"For object detection, we adapt the ViT to take the place of the vanilla FPN backbone [25] in Mask R-CNN [20] following [18] and fine-tune the model for 15 epochs.",2,positive
The pipeline of proposed evolved part masking using MAE [18] as an example.,1,neutral
"75 following the original model [18], K is linearly reduced from 40 to 10 and γ is set to 2, which are ablated in Sec.",1,neutral
", MAE [18], BEiT [2] and SimMIM [38] and evaluate performance on imageNet-1K classification [12] and ADE20K segmentation [40].",2,positive
"Classification performances are validated on imageNet-1K with end-to-end fine-tuning or linear probing following the common evaluation protocol [18, 23, 38].",0,negative
"Masked encoder: Similar to the original MAE work [18] our positional embedding, Pos (.",2,positive
"Recent methods incorporating masked autoencoders [18], have shown great promise in similar image processing tasks [46, 16], where they efficiently extract spatial features that are representative of the original data.",1,neutral
"Pioneered with stacked autoencoders [40] using convolutional neural networks (CNNs), while numerous methods [2, 18, 42] have employed the mask-word approach derived from natural language processing (NLP) to randomly obscure image patches within the spatial domain through the utilization of vision transformers [12, 25].",1,neutral
We employ an asymmetric design as outlined in Figure 2 inspired by [18] in that it differs from classical encoder-decoder designs.,2,positive
Table 10 shows our backbone with supervised pre-training weights improves performance compared to self-supervised pre-training weights by MAE [22].,0,negative
We tabulate comparison results with MAE [22] pre-trained weights in Table 10.,0,negative
Methods Pre-train Train DF FS MAE [22] 99.,0,negative
"Getting inspiration from the masked image modeling [20], Liu et al.",1,neutral
"Great success of unsupervised pre-training has been achieved by even showing better downstream performances [7,20] against supervised pre-training.",0,negative
", rotation prediction [15], reconstruction [20,55], colorization [30,59, 72] and contrastive learning [7, 8, 21, 32, 57, 63].",1,neutral
"Model pre-training on image [4, 7, 20, 21, 31] or video [13, 32, 35–38, 46, 47, 55,61,68] data without human annotations is a fundamental research topic.",1,neutral
This technique is widely used in language and vision models and shows promising improvements [19].,1,neutral
"Researchers have made several improvements to the visual transformer, including the Transformer architecture[66, 22], more advanced self-attention mechanisms[54, 68, 47, 11], and pre-training techniques[23], among others.",1,neutral
Transfer learning is an important part of realizing downstream tasks [45].,1,neutral
[45] is a highly influential work that drives the development of MIM self-supervised learning.,1,neutral
"Nowadays, self-supervised learning using masked image modeling has slowly gained attention [44] [45] [46] [47] [48].",1,neutral
"Also included are models trained using masked autoencoding (MAE,[He et al., 2022]).",0,negative
"Large-scale pre-trained models have achieved great success across various domains and applications [1, 2, 3, 4, 5, 6, 7, 8].",1,neutral
"Most recently, masked autoencoders (He et al., 2022) also adopted masked pre-training by predicting pixel values for each masked patch, and BEiT3 (Wang et al.",1,neutral
"Most recently, masked autoencoders (He et al., 2022) also adopted masked pre-training by predicting pixel values for each masked patch, and BEiT3 (Wang et al., 2022) performs MLM on texts, images, and image-text pairs, obtaining state-of-the-art performance on all-vision and vision-language tasks.",1,neutral
"This is important because it has been shown that pre-training transformers on large – sometimes even multimodal – datasets [1,5,16,23,36] leads to significant improvements on downstream tasks such as semantic segmentation.",1,neutral
"ViT-based models now achieve state-of-theart results for various tasks, and it is found that ViTs are especially well-suited for pre-training on large datasets [1,16], which in turn yields significant improvements on down-",1,neutral
"Earlier self-supervised works used “pretext tasks” such as rotation prediction [15], solving jigsaw puzzles [28], colorization [44], or missing data completion [17].",1,neutral
SatMAE [10]: is a Masked autoencoder [17] that uses ViTLarge as backbone.,2,positive
"Computer vision has been continuously inspired by NLP in recent years, including the visual transformer series [5,13,29,32] and self-supervised MAE series [15,19,60].",1,neutral
"In vision tasks, the decoder reconstructs pixels that have lower-level semantics compared to common recognition tasks, which means the network design is crucial for determining the semantic level of the learned latent representations [18].",1,neutral
"Several works [18, 57] have also leveraged the masked image modeling (MIM) [1, 5, 69] paradigm and explored a generative pretrained framework for high-level representation learning.",1,neutral
"Inspired by successes in Masked AutoEncoders (MAE), where the pretraining model on ImageNet can efficiently adapt to the high-level representative vision benchmarks such as recognition and detection [18, 57], we argue that pretraining is still a potential solution for BID task.",2,positive
"In an approach to address this issue, recent research has explored the use of transformer models, which have shown excellent capabilities in natural language processing (NLP) [7] and image segmentation [9, 14], to model sequential data for video action recognition [1, 18, 22].",1,neutral
"Unlike similar approaches that utilize encoder-decoder architectures for masked pre-training [19], the small linear reconstruction head in this approach forces the encoder to focus its capacity on modeling the masked tokens, rather than leaving this task to the decoder.",1,neutral
Masked image modeling approaches for remote sensing data utilize masked autoencoding [19] with multi-,1,neutral
This pre-training approach results in strong visual representations when combined with autoencoder networks [19] or by estimating pixel values from latent representations with a simple linear layer [48].,1,neutral
"Compared with currently popular generative models like MAE [17,28,90], discriminative methods are more propitious to temporal and multimodal information, and have better applicability for structures of point cloud models.",1,neutral
We believe the reason is that non-linear features are important here as [28] shows.,1,neutral
"Moreover, generative frameworks [17,28,90] achieve great progress in unsupervised representation learning, but they are more suitable for Transformer-based backbones instead of others like convolution.",1,neutral
"On the one hand, compared with only using the data of image modality, it is relatively more effective to detect the sarcasm expressed in the highly semantic text [24].",1,neutral
"MAE-based Features MAE-based features are extracted by an MAE model pre-trained on DFEW [14], Emo-tionet [3], FERV39k [41] datasets.",2,positive
"The visual feature extractors include the DenseNet-based [13] facial expression model, the IResNet100-based [6] facial expression model, the IResNet100-based [13] facial action unit (FAU) detection model, the MobileNet-based [11] valence-arousal estimation model, the MAE-based [9] facial expression, and action unit model.",2,positive
"[4] successfully adapted the highperformance Masked Image Modeling technique [10, 18], originally developed for 2D images, to 3D images, leading to significant improvements in accuracy.",1,neutral
"The study of conventional self-supervised learning methods mainly involves data-related pretext task designs, such as test time training [53], data augmentation prediction [40, 52, 54], cycle consistency loss [17, 41], masked autoencoder [13], self-distillation loss [4].",1,neutral
Our framework starts with pre-training a masked autoencoder (MAE) [9] from a set of face databases.,2,positive
Masked autoencoder (MAE) [9] is a self-supervised learned model which reconstructs original images from a set of masked images.,1,neutral
We adopt ViT-S [13] pre-trained on ImageNet-1K [12] with MAE method [19] as the backbone network.,2,positive
"We also note that, over the last few years, the ’pre-training’ and ’fine-tuning’ paradigms [21,24,39,46,50,65,67,68,77] have become revolutionary in both NLP and CV communities.",2,positive
"Inspired by its success, many works [12, 18, 33, 49] introduced this architecture in computer vision tasks, called Vision Transformers (ViTs).",1,neutral
"This makes the performance of machine learning in many vision tasks continue to reach new levels [3, 18, 33].",1,neutral
"Our pipeline consists of feature extraction based on pre-trained feature backbone [5, 17, 50], asymmetric feature learning, and matching flow super-resolution.",2,positive
"We find that the proposed method works quite well in conjunction with trans-former feature backbones, such as MAE [17], DINO [5], and iBOT [50], so we call the proposed method asymmetric correspondence transformer, written as ACTRans-former, and train it end-to-end.",2,positive
"We find that the proposed method works quite well in conjunction with transformer feature backbones, such as MAE [17], DINO [5], and iBOT [50], so we call the proposed method asymmetric correspondence transformer, written as ACTRansformer, and train it end-to-end.",2,positive
"The encoder can be a general image encoder that encodes pixels into hidden feature representations, such as ConvNet [25, 51], vision Transformer (ViT) [17, 24, 58], or a hybrid architecture [60].",1,neutral
We follow the setup in MAE [7] to evaluate linear classification performance.,2,positive
"[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Consequently, CLIP embeddings consistently outperform other visual pre-training approaches such as SimCLR [8] or MAE [22] across various downstream tasks [27].",1,neutral
"Medical Image Linear Classification To assess the quality of the visual representations learned by Med-UniC, we employ linear classification [50] on CheXpert [23], RSNA [43], and COVIDx [44].",2,positive
"Motivated by the success of masked autoencoder [19] in 2D, the MAE-style self-supervised method became popular in point cloud [28, 50].",1,neutral
"Recently, self-supervised learning (SSL), which was born out of natural language processing and was later successfully applied to computer vision [16, 11], has been demonstrated to aid in graph representation learning.",1,neutral
"Given the advantages observed when applying Mask Image Modeling (MIM) in conjunction with VAE frameworks [24], [25], [26], such as enhanced generalization capabilities",1,neutral
Voxel-MAE [26] and ALSO [27] propose predicting occupancy for LiDAR perception as the pretext task.,1,neutral
"Additionally, methods like MAE [24] and BEiT [25] employ a random patch masking approach where missing pixels or features are reconstructed using a simple autoencoder framework.",1,neutral
"Inspired byMasked AutoEncoder [13], we leverage a shared learnable token 𝑒 ∈ R𝑑0 as embeddings for target nodes, while using an embedding layer with the parameter 𝑊 ∈ R𝑑𝑦×𝑑0 to get embeddings for context nodes.",2,positive
"Inspired byMasked AutoEncoder [13], we leverage a shared learnable token e ∈ R0 as embeddings for target nodes, while using an embedding layer with the parameter W ∈ Rdy×d0 to get embeddings for context nodes.",2,positive
"Furthermore, our work demonstrates the value of self-supervised representation learning tasks for Vision Transformers (ViTs) [51, 18, 1, 32, 7].",1,neutral
C.1 and for the MAE [13] in Sec.,1,neutral
"For example, DiffRate can compress an off-the-shelf ViT-H model pre-trained by MAE [13] with 40% FLOPs reduction and 50% throughput improvement with only 0.",1,neutral
"For example, DiffRate can compress an off-the-shelf ViT-H model pre-trained by MAE [13] with 40% FLOPs reduction and 50% throughput improvement with only 0.16% accuracy drop, outperforming previous methods that require tuning the network parameter.",2,positive
"Notably, in contrast to DeiT models, MAE models tend to retain more tokens in the deep block.",1,neutral
"Due to the flexibility in handling various input formats, ViT has also been widely applied to self-supervised learning [13] and other modalities [9, 35].",1,neutral
"In this section, we conduct a series of experiments on ImageNet-1k [6] using DeiT [30], MAE [13], and LVViT [16].",2,positive
"This is because DeiT classifies solely based on the class token, while MAE classifies based on the average of all image tokens.",1,neutral
"MAE Models
Also, Table 9 presents the complete outcomes of MAE [13], encompassing both off-the-shelf models and fine-tuned models.",2,positive
Similar results are also observed in ViT-B (DeiT) and ViT-B/L/H (MAE).,1,neutral
"Despite utilizing a simple ViT (ViT-CLS) as the backbone, EVP demonstrates good results, particularly when using two other pre-trained weights: the self-supervised model (ViT-MAE [88]) and the recent foundation model (ViT-SAM [69]).",2,positive
"More recently, this has been studied extensively as masked autoencoders [He et al., 2022, MAE].",1,neutral
“Simple algorithms that scale well are the core of deep learning” [34].,1,neutral
"The transformer was first proposed in natural language processing [28] and has since gained popularity with great success in other fields, such as computer vision [5][11] and bioinformatics [27][14].",1,neutral
"This observation also aligns well in the vision domain (He et al., 2022).",2,positive
"For the second type, we use the official opensource code of ATS and ToMe to implement them on various pre-trained Transformer backbones, including DeiT [16], LV-ViT [18], MAE [25], AugReg [26], and SWAG [27].",2,positive
"Our work establishes a new milestone in the adversarial robustness of object detection and encourages the community to explore the potential of large-scale pre-training on adversarial robustness, which has shown great success in improving benign accuracy of downstream tasks [21, 22, 40].",2,positive
"In contrast, our method achieves better quantitative results.. Compared to the VSD methods, our approach outperforms them in terms of MAE, Fβ , IoU, and BER metrics.",2,positive
"Table 1 summarizes the quantitative comparison results between our method and other state-of-the-art methods in terms of MAE, Fβ , IoU, and BER.",2,positive
"Specifically, the image encoder uses an MAE [15] pre-trained Vision Transformer (ViT) [11].",2,positive
"Following previous methods [4, 10, 20], we use four commonly used validation metrics to evaluate the effectiveness of the method, including: Mean Absolute Error (MAE) [4], F-measure (Fβ) [4, 19], Intersection over Union (IoU) [4], and Balaence Error Rate (BER) [4, 65].",2,positive
"For MAE and BER, the lower the better, while for the metrics of Fmeasure and IoU, the higher are preferred.",1,neutral
"Specifically, we improve the MAE by 27.3% compared to TVSD [4].",2,positive
"Our rationale is as follows: for the image encoder, it is pre-trained on millions of natural images using MAE [15], and the trained encoder is capable of extracting good natural image representations, which are completely suitable for video shadow detection.",2,positive
"Large-scale pretraining has achieved phenomenal success in the fields of computer vision (Chen et al., 2020; He et al., 2022) and natural language processing (Devlin et al., 2019; Radford et al., 2018), where fast adaptation to various down-
Work done during the first author’s internship at Tencent…",2,positive
"Similar to what we have witnessed in the fields of computer vision (Chen et al., 2020; He et al., 2022) and natural language processing (Devlin et al., 2019; Radford et al., 2018), it is a promising direction to leverage highly sub-optimal and unlabeled training data to conduct RL pretraining.",2,positive
"Similar to what we have witnessed in the fields of computer vision (Chen et al., 2020; He et al., 2022) and natural language processing (Devlin et al.",1,neutral
"Large-scale pretraining has achieved phenomenal success in the fields of computer vision (Chen et al., 2020; He et al., 2022) and natural language processing (Devlin et al.",1,neutral
"We train a selfsupervised model using a modified version of the Masked-Auto-Encoder (MAE) [38], for details see appendix B.",2,positive
"We train a selfsupervised model using a modified version of the Masked-Auto-Encoder (MAE) [38], for details see appendix B.5.",2,positive
"We train a self-supervised model using a modified version of the Masked-Auto-Encoder (MAE) [38], a self-supervised model trained on ImageNet for object detection.",2,positive
"We define the Mean-Squared-Error between the ground truth image and reconstruction of masked patches as loss, equivalent to [38].",1,neutral
"Inspired by and upgraded upon the idea of self-training by ""random masking-reconstructing"" with autoencoders [31], this study proposed to incorporate a pre-training phase with masked sequential autoencoders to pre-train the lane detection models and facilitate their capabilities in aggregating contextual information for feature extraction through continuous frames.",2,positive
"Different from the original masked autoencoders [31] implemented by the vision transformer, the proposed upgrade version of masked sequential autoencoders for pre-training is implemented under the ""CNN-ConvLSTM-CNN"" or ""CNNAttention_LSTM-CNN"" architecture, which can further aggregate valuable image contextual information and spatialtemporal features.",2,positive
[31] show that taking advantage of a pre-training strategy by randomly masking a high proportion of input image and reconstructing the original image from the masked patches using the latent representations can improve accuracy and accelerate training speed for downstream tasks.,1,neutral
"In phase 2, we use the pre-trained ViT-MAE-base [47] model as the image auto-encoder.",2,positive
"Decoder Layers Following [6, 47, 51], for both the fMRI and the image auto-encoder, we build asymmetric architectures where the decoder is much smaller than the encoder.",1,neutral
"Compared to the MAE pre-trained ViT, FINOLA achieves comparable performance but costs lower FLOPs.",0,negative
"Relation to MAE [25]: Masked FINOLA has a similar architecture to MAE, with differences in masking and prediction.",1,neutral
"For instance, we compare the performance of FINOLA+MobileFormer with MAE+ViT in the context of ImageNet classification.",2,positive
Fine-tuning on ImageNet-1K: Table 7 compares FINOLA with MAE and MoCo-V3 on finetuning results.,2,positive
"BEiT [2] and PeCo [17] predict on tokens, MaskFeat [48] predicts on HOG, and MAE [25] reconstructs original pixels.",1,neutral
"Notably, FINOLA achieves a remarkable accuracy of 62.8% on a Mobile-Former model with 14 million parameters, surpassing the performance of MAE with ViT-S (22M parameters) and BEiT with ViT-B (86M parameters).",2,positive
"Firstly, Masked FINOLA utilizes a regular masking design by grouping all unmasked patches into a single block, in contrast to MAE’s random unmasked patches.",2,positive
"This finding diverges from the observations in MAE [25], where longer training is essential for fine-tuning improvements.",1,neutral
"2021) as in the convolutional case, and others applying novel loss functions such as a pixel reconstruction loss (He et al. 2022).",1,neutral
"…due to its computational efficiency and flexible inductive bias which can be learned during training, with some algorithms using instance differentiation (Caron et al. 2021) as in the convolutional case, and others applying novel loss functions such as a pixel reconstruction loss (He et al. 2022).",1,neutral
"ViT [17], self-supervised pre-training [23] or contrastive pretraining [42]).",1,neutral
The encoders are initialized with MAE-Base[38] pretrained weights.,2,positive
"The image encoder uses a pre-trained Vision Transformer (ViT) adapted to handle high-resolution inputs (23,24).",2,positive
"Masked image modeling is proven to be a scalable method [3] that as the model size scales up, the performance improves considerably.",1,neutral
"On the other hand, MIM-based methods, represented by MAE [3], SimMIM [4], and BEiT [5], aim to capture knowledge about local relationships within an input image through a reconstruction task.",1,neutral
"In this work, we tend to evaluate the data scaling ability under MAE [3], which is the simplest framework of masked image modeling, with only the reconstruction target changed.",1,neutral
"Masked Image Modeling Self-supervised learning, which focuses on acquiring powerful and transferable representations by leveraging data without human-labeled annotations, has garnered increasing attention [5, 3, 13, 14, 4, 15, 16] in recent years.",1,neutral
"We follow the design of MAE [3], except for the reconstruction target.",2,positive
"The batch size is always set as 4096 during pre-training, and the masking ratio is set as 75% following [3].",0,negative
"In this work, we investigate various target encoders, including MAE [3], DINO [28], CMAE [13], CLIP [29], etc.",2,positive
"With a large portion of patches masked, these models directly predict the missed pixels [5, 3] or discrete visual vocabularies [5].",1,neutral
"In our experiments, we adopt the MAE [3] configurations, with the exception of the reconstruction target, which is produced by a target encoder.",2,positive
"Recently, inspired by the success of masked language modeling in Natural Language Processing, masked image modeling methods like BEiT [5], SimMIM [4], and MAE [3] have shown remarkable capabilities of representation in a “mask-and-predict” manner.",1,neutral
"initialization, we initialize ViTMatte-S and ViTMatte-B using DINO [6] and MAE [22] pretrained weights respectively.",2,positive
"models with self-supervised pretraining, such as MAE [22] and DINO [6], yield the best performance for ViTMatte.",1,neutral
We observe that ViTMatte gets best results with self-supervised pretraining DINO [6] and MAE [22].,0,negative
"These models [6, 18, 19, 22, 63] are typically pretrained using advanced techniques, extensive data, and powerful computing resources.",1,neutral
"Many of them, such as MAE [22], DINO [6], and iBOT [63] mainly target plain vision transformer structures and pretrained models in a selfsupervised manner.",1,neutral
"As previously discussed, one of the advantages of ViTMatte is its ability to inherit various pretraining weights [6, 22, 63] from ViTs.",2,positive
"For the model
initialization, we initialize ViTMatte-S and ViTMatte-B using DINO [6] and MAE [22] pretrained weights respectively.",2,positive
"When using ViT as a backbone for vision tasks, Simple Feature Pyramid (SFP) introduced by [31] is a commonly technique to convert the single-scale features of ViT into multi-scale features [18, 19, 22].",1,neutral
"Our experiments reveal that ViT
models with self-supervised pretraining, such as MAE [22] and DINO [6], yield the best performance for ViTMatte.",2,positive
"[22]), there has been a renewed focus on this nonhierarchical structure.",1,neutral
"is advocated in [14]), a random sampling strategy is used to mask(remove) (k = 75% m) embeddings at positions",1,neutral
"Therefore, to alleviate the above shortcomings of visual representations in ZSL models, we consider pre-trained Masked Autoencoders (MAE) [14] to capture more discriminative semantically rich visual features of images for classification.",1,neutral
• We propose a pre-trained Masked Autoencoders (MAE) [14] based multi-head self-attention model for solving the ZSL task.,2,positive
We initialize the backbone with MAE [14] pre-trained on ImageNet-1k without labels.,2,positive
"We train embedding models with three different visual SSL algorithms: DINO (Caron et al., 2021), Mugs (Zhou et al., 2022), and masked autoencoders (MAE) (He et al., 2022).",2,positive
"MAE: Masked autoencoders (MAEs) use reconstruction of masked image patches as the self-supervised learning objective (He et al., 2022).",1,neutral
"The DINO algorithm performs the best in our evaluations, with Mugs coming in second and MAE third.",2,positive
"MAEs use a much lighter data augmentation pipeline than other algorithms, requiring only random resized crops and horizontal flips.",2,positive
"As recommended (He et al., 2022), we use a large masking ratio of 75% during training, i.",2,positive
", 2022), and masked autoencoders (MAE) (He et al., 2022).",2,positive
"To answer it, we conduct a rigorous comparative study for the adapter-based and fine-tuning based TransRec on two item modalities (i.e., texts and images) with two popular recommendation architectures (i.e., SASRec [21] and CPC [43]) and four powerful ME (i.e., BERT, RoBERTa [27], ViT and MAE [14]).",2,positive
"Specifically, we run experiments on eight combinations: {SASRec+BERT, CPC+BERT, SASRec+RoBERTa, CPC+RoBERTa, SASRec+ViT, CPC+ViT, SASRec+MAE, CPC+MAE}, where BERT, RoBERTa, ViT, and MAE are the most
6The Food, Cartoon, and Dance vertical channel dataset are denoted by Bili_F, Bili_C, and Bili_D, respectively 7Bili: https://www.bilibili.com/; TN: https://news.qq.com/; KS: https://www.kuaishou. com/new-reco; DY: https://www.douyin.com/.",2,positive
"However, for image recommendation, the performance gap between FTA and AdaT is relatively large, regardless of the training strategies used (SASRec/CPC+ViT/MAE).",2,positive
", BERT, RoBERTa [27], ViT and MAE [14]).",1,neutral
"Then, a patching technique [Dosovitskiy et al., 2021, Bao et al., 2022, He et al., 2021, Nie et al., 2023] is applied, which divides the series into either overlapped or non-overlapped segments.",1,neutral
"Secondly, unsupervised pretraining typically correlates better with non-semantic tasks than supervised counterparts [61, 22, 38].",1,neutral
"2018) and MAE (He et al., 2022) that only require single-modality data, VLP models rely on largescale aligned image-text datasets (Lin et al., 2014; Sharma et al., 2018; Ordonez et al., 2011; Krishna et al., 2017) to bridge the gap between the two modalities, which requires either extensive manual…",2,positive
"2018) and MAE (He et al., 2022) that only require single-modality data, VLP models rely on largescale aligned image-text datasets (Lin et al., 2014; Sharma et al., 2018; Ordonez et al., 2011; Krishna et al., 2017) to bridge the gap between the two modalities, which requires either extensive manual annotations or heavy data cleaning processes (Lin et al., 2014; Sharma et al., 2018).",2,positive
"2018) and MAE (He et al., 2022) that only require single-modality data, VLP models rely on largescale aligned image-text datasets (Lin et al.",2,positive
Our work studies a simple extension of MAEs [24] to videos.,2,positive
Our training settings follow [24] and we build on the open-source implementation of MAEs (https://github.,2,positive
"To that end, we study a simple extension of MAE [24] to video data (Fig.",1,neutral
"Similar to the findings in the image [24] and video [27] domains, we find that SiamMAE does not require extensive data augmentation to achieve competitive performance.",2,positive
"Finally, the output sequence of the decoder is used to predict the normalized pixel values [24] in the masked patches.",1,neutral
"To create a challenging predictive self-supervised learning task, MAEs randomly mask a high percentage (75%) of image patches [24] and extensions to videos [28, 27] use an even higher masking ratio (90%).",1,neutral
"The training paradigm based on contrastive learning has been proven to have strong generalizability and zero-shot transferability in the field of image classification [27, 43, 44, 65].",1,neutral
"To explore which contrastive paradigm is most suitable, we evaluate three famous paradigms, namely, MoCo [44], MAE [43], and CLIP [65], on their abilities to extract generalizable Choice Textual Labels",1,neutral
"Different training paradigms (MoCo [44], MAE [43], and CLIP [65]) lead to different generalizability.",1,neutral
"To explore which contrastive paradigm is most suitable, we evaluate three famous paradigms, namely, MoCo [44], MAE [43], and CLIP [65], on their abilities to extract generalizable
representations for discriminating unseen real (Danbooru [3]) and synthetic (Latent Diffusion [69]) painting images.",1,neutral
"From the perspective of training, a large body of works [43, 44, 65, 73] (and references therein) have shown that contrastive learning can enhance the general representation ability of neural networks.",1,neutral
"Meanwhile, the requirement of an external tokenizer for BEiT was alleviated by iBOT (Zhou et al., 2021) which introduced an online tokenizer trained with the distillation routine of BYOL, thus leveraging the advances made on the side of discriminative SSL. Xie et al. (2022b) got rid of the tokenizer of BEiT and proposed SimMIM, which directly operates over pixel values and predicts them.",2,positive
"The results obtained by Tian et al. (2023) indicate that ViTs, which have been considered a prerequisite for MIM, are not irreplaceable and that CNNs can still compete with ViTs in generative SSL.",0,negative
"With a unique take on MIM, He et al. (2022) proposed MAE, an asymmetric autoencoder framework that directly learns to reconstruct image patches.",2,positive
"To mitigate the high costs of training, researchers have started exploring techniques for efficient training and evaluation (Li et al., 2021a; Garrido et al., 2022a; He et al., 2022).",2,positive
"This is primarily due to the poor results obtained with linear probing with generative frameworks that only use MIM as the pretext task (see MAE, BEiT, SimMIM, iGPT in Figure 7).",1,neutral
"He et al. (2022) further argues that linear probing misses the opportunity to utilize strong but non-linear features, and this sentiment is repeated by a number of follow-up research efforts (Yi et al., 2022; Chen et al., 2022a).",1,neutral
"He et al. (2022) also evaluated the effectiveness of different reconstruction targets and found no statistically significant difference between reconstructing DALL-E tokens and pixels, suggesting simple pixel reconstruction to be a viable reconstruction target.",2,positive
"More recently, MSE has also been adopted to measure the correctness of reconstruction targets for MIM-based generative frameworks (He et al., 2022; Hou et al., 2022; Tian et al., 2023).",2,positive
Chen & He (2021) and He et al. (2022) argue that there is no correlation between the accuracy of linear probing and fine-tuning or downstream transferability.,2,positive
"(Dosovitskiy et al., 2020; Bao et al., 2021; He et al., 2022).",2,positive
"…Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020), and similar techniques have been applied in computer vision (Chen et al., 2020a, 2021b; He et al., 2022; Bao et al., 2022)
and vision-language communities (Li et al., 2019a; Chen et al., 2020b; Radford et al., 2021; Kim et al., 2021; Li…",1,neutral
", 2019) as well as in computer vision (Chen et al., 2021b; He et al., 2022).",2,positive
"Masked path modeling is inspired by the masked data modeling pretraining methods in the language and vision communities (Devlin et al., 2019; He et al., 2022), where the general idea is that the model is trained to reconstruct an original input (e.g., a sentence or an image) given parts of the…",1,neutral
"To address these challenges, researchers have adopted the pretrainingthen-finetuning paradigm, which has been successful in natural language processing (Peters et al., 2018; Devlin et al., 2019) as well as in computer vision (Chen et al., 2021b; He et al., 2022).",1,neutral
", 2020), and similar techniques have been applied in computer vision (Chen et al., 2020a, 2021b; He et al., 2022; Bao et al., 2022) and vision-language communities (Li et al.",1,neutral
"ing methods in the language and vision communities (Devlin et al., 2019; He et al., 2022), where the general idea is that the model is trained to reconstruct an original input (e.",1,neutral
"In this line of research (e.g., SimCL (Chen et al., 2020b), MoCo (He et al., 2020), SwAV (Caron et al., 2020), BYOL (Grill et al., 2020), DenseCL (Wang et al., 2021), DINO (Caron et al., 2021), iGPT (Chen et al., 2020a), iBOT (Zhou et al., 2022), BeiT (Bao et al., 2022) and MAE (He et al., 2022)), matching N features of the student to N features of the teacher by referring to dense features of different image patches has been explored.",2,positive
"…2020), DenseCL (Wang et al., 2021), DINO (Caron et al., 2021), iGPT (Chen et al., 2020a), iBOT (Zhou et al., 2022), BeiT (Bao et al., 2022) and MAE (He et al., 2022)), matching N features of the student to N features of the teacher by referring to dense features of different image patches has been…",2,positive
", 2022) and MAE (He et al., 2022)), matching N features of the student to N features of the teacher by referring to dense features of different image patches has been explored.",1,neutral
"Besides, iBOT, iGPT, BeiT and MAE with masked image modeling are particularly used to train vision transformers in unsupervised learning regime, while our method is mainly for training efficient convolutional neural networks in supervised learning regime.",2,positive
"Secondly, in formulation, many existing self-supervised learning methods (e.g., SimCL, MoCo, SwAV, BYOL, DenseCL and DINO) define a contrastive predication task that encourages the encoder (backbone) to attract similar (positive) sample views and dispel different (negative) sample views with their corresponding contrastive losses leveraging data augmentation, and some others (e.g., iGPT, BeiT and MAE) define a masked reconstruction task that uses a decoder to predict the original image (pixel-wise/patch-wise) given the representation learnt by the encoder (backbone) with their corresponding reconstruction losses leveraging masked image modeling. iBOT further combines masked image modeling into self-supervised contrastive learning for improved performance.",1,neutral
"GPT (Radford and Narasimhan, 2018), pre-training followed by fine-tuning has become the predominant trend for several years and has also influenced other research fields, such as computer vision (He et al., 2022; Bao et al., 2022) and biology (Rives",2,positive
"…BERT (Devlin et al., 2019) and GPT (Radford and Narasimhan, 2018), pre-training followed by fine-tuning has become the predominant trend for several years and has also influenced other research fields, such as computer vision (He et al., 2022; Bao et al., 2022) and biology (Rives et al., 2021).",2,positive
"These models are trained using various paradigms in natural language processing, such as masked language modeling [57, 58], denoising autoencoder [59], replaced token detection [60], and sentence prediction tasks[61], as well as in computer vision, including data generation [23, 62, 63], data reconstruction [64], and data contrastive learning [10, 12, 13, 20–22].",1,neutral
"Cross-Entropy loss is particularly important in image classification tasks [1, 2, 3, 4, 5], while Mean Square Error (MSE) loss is commonly used in regression tasks [6, 7, 8].",1,neutral
"Our second proposed objective aims at following the progress that masked predictions have had in NLP (e.g., Devlin et al., 2019; Zhang et al., 2019) and Computer Vision (e.g., Bao et al., 2022; He et al., 2022).",2,positive
"For example, they can reconstruct an image even if 75% of the pixels are missing [15, 27].",1,neutral
"Optimizing the loss on all tokens reduces reconstruction performance, as previously observed [15].",1,neutral
"Transformers have gained widespread popularity in computer vision and natural language processing tasks for predicting missing image patches or words [9,10,15].",1,neutral
"Previous work has shown allocating additional encoder capacity to be beneficial for smaller granularity inputs, both for characters and bytes (Cherry et al., 2018; Xue et al., 2022b) and other modalities (He et al., 2021; Zhang et al., 2017).",2,positive
"While prior work has studied how the amount masked influences model learning (He et al., 2022), Figure 1: DIFFERENCE-MASKING automatically selects what to mask based on what makes the task domain different from the pretraining domain.",1,neutral
"While prior work has studied how the amount masked influences model learning (He et al., 2022),
most masking approaches choose which parts of the data to mask randomly.",1,neutral
We also conduct comparison experiments on CLIP [RKH+21] ViT-L/14 and MAE [HCX+22] ViT-H/14.,2,positive
"Motivated by masked language modeling [DCLT19, LOG19] in natural language processing, MAE [HCX22] uses an asymmetric encoder-decoder and conducts masked image modeling to effectively and efficiently train scalable vision Transformer [DBK20] models.",2,positive
", DINOv2 [ODM23], CLIP [RKH21], and MAE [HCX22]) and a class-agnostic segmentation model (SAM) [KMR23].",2,positive
We also conduct comparison experiments on CLIP [RKH21] ViT-L/14 and MAE [HCX22] ViT-H/14.,2,positive
"Notably, semi-supervised learning exploits unlabeled data [33, 24], transfer learning exploits prior knowledge obtained from different tasks [7, 39], and self-supervised learning exploits data augmentations for label agnostic representation learning [3, 11].",1,neutral
"SAM has three components: (a) an image encoder that is based on a MAE [15] pre-trained Vision Transformer [9], (b) a flexible prompt encoder that is divided into sparse prompts (i.",2,positive
"SAM has three components: (a) an image encoder that is based on a MAE [15] pre-trained Vision Transformer [9], (b) a flexible prompt encoder that is divided into sparse prompts (i.e., points, boxes, text)) and dense prompts (i.e., masks), and (c) a fast mask decoder that is based on Transformer decoder and can efficiently map the image embedding, prompt embeddings, and an output token to a mask.",2,positive
Generative self-supervised methods aim to learn the generalized representation by mask image modeling (Bao et al. 2021; Xie et al. 2022; He et al. 2022).,1,neutral
"An asymmetric network structure is also explored to only process the visible patches by the encoder, which significantly improves computation efficiency (He et al. 2022).",1,neutral
"Therefore, current research efforts try to mitigate this issue by using unlabeled data [54,4,55] or forms of self-supervision [33,19,22,18].",1,neutral
proposed a novel unsupervised learning approach using masked autoencoders to reconstruct inputs from partially masked versions of themselves [23].,1,neutral
"Data augmentation has been a critical component of recent advances in deep vision models (He et al., 2022; Bai et al., 2022; Liu et al., 2021).",2,positive
"Learning efficiency can be achieved through label efficiency, transfer learning [16, 22, 31], pretraining through self-supervision [13], or efficient finetuning with compression [17, 39].",1,neutral
"Transformers, which have gained far-flung fame in natural language processing (NLP) area [8, 28], are also attracting increasing attention in lots of computer vision (CV) tasks, such as object detection [4], image classification [9] and many others [13, 31], impelling the widespread research on vision transformers (ViTs).",1,neutral
"With tokenizers pretrained on relative smallscale dataset (i.e., ImageNet-1k [35] with 1.28M images), DeiT demonstrates better image captioning performance (65.8 CIDEr) than self-supervised models DINO (45.0) and MAE (37.3), without jointly tuning the visual tokenizer.",2,positive
"Similar to object counting benchmarks, we report Mean Absolute Error (MAE) and Root Mean Square Error (RMSE).",1,neutral
"On GVTBench, we evaluate visual tokenizers with the same architecture (ViT-B [34]) but different pretraining strategies, including fully-supervised (DeiT [16]), self-supervised (DINO [19], DINOv2 [20], MAE [18]) and text-guided weakly supervised (CLIP [17]) pretraining.",2,positive
"Among all the self-supervised models, MAE achieves the best performance, indicating the patch-based supervision is particularly effective for improving fine-grained visual understanding.",2,positive
"Based on this benchmark, we comprehensively evaluated existing visual tokenizers with same architecture but different pretraining methods, including fully supervised (DeiT [16]), text-guided weakly supervised (CLIP [17]) and self-supervised (MAE [18], DINO [19], DINOv2 [20]) models (Section 2).",2,positive
"Recently, auto-encoder based [18] methods ask the model to directly generate the masked patch in the continuous space.",1,neutral
"In CV, Vision Transformers (e.g., Dosovitskiy et al. 2020; Liu et al. 2021; Bao et al. 2022; Ding et al. 2022; He et al. 2022) split an image into patches and then feed into the Transformer models.",1,neutral
"architecture similar to [4], to predict unseen frames based on the self-attention mechanism [13, 14].",1,neutral
"Its self-attention mechanism is exploited to model relationships across neighbouring frames as in [13, 14].",1,neutral
"• VC-1 [Majumdar et al., 2023], which is a very recent ViT trained on 7 different egocentric video sources (over 5.6 million frames, including Ego4D) relevant to sensorimotor skills, using a self-supervised masked autoencoding (MAE) [He et al., 2022] objective.",2,positive
This is based on the observation that dynamically-equipped R3M models best matched DMFC neural dynamics compared to the MAE objective of VC-1 and goal-conditioned objective of VIP.,2,positive
"6 million frames, including Ego4D) relevant to sensorimotor skills, using a self-supervised masked autoencoding (MAE) [He et al., 2022] objective.",2,positive
"Based on the above analysis, we propose a novel SSL framework for point clouds, called PointGPT.",2,positive
"Our main contributions can be summarized as follows: (I) A novel GPT scheme, termed PointGPT, is proposed for point cloud SSL. PointGPT leverages a point cloud auto-regressive generation task while mitigating positional information leakage, outperforming other single-modal SSL methods.",2,positive
"Consequently, our PointGPT surpasses other single-modal SSL methods with comparable model sizes.",2,positive
"The core idea of SSL is to design a pretext task to learn the distribution of the given data, obtaining beneficial features for the subsequent supervised modeling tasks [21; 13].",1,neutral
The success of SSL in NLP and image processing has motivated researchers to develop SSL frameworks for point cloud representation learning.,1,neutral
This is in line with the previous SSL methods [37; 64; 66; 24] to allow for a direct comparison with these prior approaches.,1,neutral
"Current point cloud SSL methods directly fine-tune pre-trained models on the target dataset, which may result in potential overfitting due to the limited semantic supervision information [55].",2,positive
"In natural language processing (NLP) and image analysis domains, self-supervised learning (SSL) [11; 18; 45; 5; 44] has emerged as a promising approach for acquiring latent representations without relying on annotations.",1,neutral
"(2)
Embedding: Following Point-MAE [37], a PointNet [38] network is employed to extract rich geometric information for each point patch.",1,neutral
"However, generative SSL methods [4; 18; 7; 11; 47; 44] have recently achieved more competitive performance.",1,neutral
Point-MAE extends the MAE by randomly masking point patches and reconstructing masked regions.,1,neutral
"In the computer vision field, BEiT [4] and MAE [18] randomly mask input patches, and pre-train models to recover the masked patches in the pixel space.",1,neutral
"The experimental results are presented in Table 1, our PointGPT-S surpasses other single-modal SSL methods.",2,positive
"The results are presented in Table 1, our PointGPT-S, which has similar capacities and training data to previous methods like Point-MAE, outperforms other single-modal SSL methods.",2,positive
"Meanwhile, others have tried to enlarge the receptive field globally [110, 116, 5] but have overlooked important local (spatial) information, which is conventionally essential for recovery tasks [31, 101, 10, 11].",1,neutral
"Masking ratio High value of masking ratio can lead to a student model producing good representation, as it has less information to infer with [10, 31].",1,neutral
"This has been boosted by a very comprehensive body of research [4, 5, 6, 7, 8, 9, 10] that has proposed ways to pre-train deep learning models, such as ResNet [11] and Vision Transformers [12], employing different types of SSL methods, primarily contrastive [13, 14], clustering [15, 16], distillation [2, 17], information maximisation [18, 10], and masking [19, 20].",1,neutral
"SSL learns without labels by the construction of proxy tasks that are informed by the data itself such as predicting an occluded region of the image [27, 19].",1,neutral
"Similarly, DMAE [1] aligns the intermediate features between teacher model and student model, studying the potential of distilling knowledge from MAE.",2,positive
"Instead of feeding masked tokens as input to the encoder, MAE [8] develops a straightforward decoder to reconstruct image patches, resulting in a significant decrease in pre-training computational costs.",2,positive
"To break the limitation that MAE-based methods can only be performed on
the standard vision Transformers [6], a lot of works [12, 24, 36] have been proposed.",1,neutral
"With the same setup, an asymmetric vision-transformer-based autoencoder [25, 14] is trained on the Human Connectome Project [26] with the visual cortex (V1 to V4) defined by [20].",2,positive
"It learns general features of fMRI by trying to recover masked data from the remainings, similar to the GPT [13] and MAE [14], after which knowledge can be distilled and transferred to a downstream task with limited data and a few-step tuning [15, 13, 14, 6].",2,positive
"The video classifier based on VideoMAE [34] is trained on Kinetics-400 [35], an annotated video dataset with 400 classes, including motions, human interactions, etc.",2,positive
VideoMAE [31] and MAE [10] extend the work of ImageMAE [13] to the video domain.,2,positive
"Recently, contrastive learning [23,6,12,22] approaches and masked autoencoder [31,10,33,13,11] are mainly used in SSL to learn better visual representations from large scale unlabeled datasets.",1,neutral
MAE for images [13] aims to reconstruct the masked patches in images.,2,positive
MAE Masked Autoencoders Are Scalable Vision Learners (MAE) [3] is an autoencoder model that learns the hidden features of data by encoding it into a low-dimensional vector and then decoding it back to the original data.,1,neutral
"As depicted in Figure 4 (A), the results indicated that the performance of AdCo performed significantly superior to MAE.",0,negative
"Specifically, the predictive performance of AdCo-extracted features yielded a correlation coefficient of 0.30, while MAE reached only 0.17.",0,negative
We speculated that the transformer-based [42] MAE is more adept at capturing global associations such as in natural images.,2,positive
"2.7 Contrastive learning and attentive pooling improve performance We also conducted a comparative analysis of two self-supervised learning models, namely masked autoencoder (MAE) and adversarial contrastive learning (AdCo), with respect to their performance in extracting tile-level features.",2,positive
"For feature extraction, we tested two types of self-supervised learning, adversarial contrastive learning (AdCo) [30] and masked autoencoder (MAE) [31], and compared their performance.",2,positive
(A) Performance comparison in predicting tumor immune infiltration level by AdCo and MAE feature extractors.,1,neutral
"With the development of unified architectures [26, 27, 28, 29] and efficient pretraining tasks [15, 1, 2, 30], recent works have achieved promising results in vision-language learning [31, 32, 33, 34, 35, 36] and audio-language learning [37, 38, 39, 40].",1,neutral
"Following [2], we only input the unmasked units to the modality fusion encoder to reduce computation costs and save memory.",2,positive
"Representation models have received considerable attention in computer vision [1, 2, 3, 4, 5, 6, 7, 8, 9], speech processing [10, 11, 12, 13], natural language processing [14, 15, 16, 17, 18], etc.",1,neutral
"The second one is intra-modal denoising contrastive learning, it can be viewed as a combination of masked prediction [15, 1, 2, 10] and contrastive learning [30, 7, 4], where we perform contrastive loss between the fine-grained masked features and visible features, such as image patches, language tokens, or audio waveform features.",1,neutral
DiffMAE [98] combines MAE [29] and DMs to learn better inpainting.,1,neutral
"Literature on self-supervised representation learning [29, 38] suggests that a (reasonably) more difficult pretext task usually leads to better representations.",1,neutral
"For example, MAE [29] leverages supervised fine-tuning to verify the effectiveness of their learned representations.",1,neutral
"[29] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"More recently there has been greater thought placed into masking strategies of these approaches with the aim to learn better representations through prediction or invariance to the missing regions [55, 48, 3, 32, 20].",1,neutral
"Recently, PTMs, particularly Transformer-based PTMs, have achieved remarkable performance in various Computer Vision (CV) [23], [24] and Natural Language Processing (NLP) [25] applications.",1,neutral
"DAE’s “mask and predict” mechanism has been successfully used in pre-training NLP [119] and CV models [24], [120], [121].",2,positive
Using DAE to study PTMs [24] demonstrates the potential of denoising strategies in pre-training.,1,neutral
"In addition to adopt linear probing [12], full-finetuning [18], or zero-shot [23] to the base models, many new strategies or methods have been proposed.",1,neutral
The image encoder employ the Vision Transformer [53] pretrained with MAE [16] to extract the image features with the shape of C ×H ×W from the input images.,2,positive
"However, deep autoencoders have demonstrated remarkable capabilities in fitting nonlinear manifolds, even in high-dimensional settings [4, 5, 6].",1,neutral
"Following a similar approach to ResNet, this is the most parameter efficient MAE.",2,positive
"For the Semi-ViT model [2], we also use the same MAE ViT-Base model.",2,positive
"In the ViT architecture, we have relied on Masked Autoencoders (MAE) [9] ViT-Base model.",2,positive
further improve) with larger unlabeled datasets [2] as well as provide unique opportunities for new mechanisms such as masking [18].,1,neutral
"Our image encoder is a MAE [22] pretrained Vision Transformer (ViT) [12], following SAM [29].",2,positive
"Masked Image Token Prediction Similar to text infilling, we build an image denoising method to model image patches inspired by previous studies (Bao et al., 2021; He et al., 2022).",2,positive
"to model image patches inspired by previous studies (Bao et al., 2021; He et al., 2022).",2,positive
"Unlike 2D MIM methods which adopt image classification task as the benchmark to evaluate the effectiveness of their pre-training methods, we do not have a classification task for scene-level 3D point clouds.",2,positive
"Compared with SimMIM, MAE is more pre-training efficient because it only takes the visible token as the input of the encoder and passes all tokens through a light-weight decoder.",2,positive
A2MIM [24] introduce to learn the frequency component of the masked patch features.,1,neutral
"Based on evidence from 2D masked modeling methods [18], we choose a high mask ratio (70%) when removing tokens.",1,neutral
MAE [18] and SimMIM [47] both propose to predict the raw pixels of the masked patches.,1,neutral
"Motivated by the success of BERT [12] for masked language modeling, Masked Image Modeling (MIM) [1, 3, 6, 13, 18, 24, 44, 47, 54] becomes a popular pretext task for self-supervised visual representation learning.",1,neutral
"Motivated by the success of BERT [12] for masked language modeling, Masked Image Modeling (MIM) [1, 3, 6, 13,18,24,44,47,54] becomes a popular pretext task for selfsupervised visual representation learning.",1,neutral
"Similar to the original MAE work [18], visible tokens are encoded by a Transformer encoder.",1,neutral
"On a high level, implementation of the MIMEx model is similar to that of MAE [15].",2,positive
"First, masked autoencoding relies on less domain knowledge compared to methods like contrastive learning, and has proven success across many different input modalities [9, 10, 15, 29, 40].",1,neutral
"Moreover, we can leverage standard architectures such as those used in masked language modeling [9] and masked image modeling [15], for which the scalability and stability have been tested.",2,positive
"We apply random masking with uniform distribution to the latent vector z, following the same protocol used in MAE [15].",1,neutral
"These errors or noisy labels can be addressed with bigger datasets [11] and more network parameters [12], [13], [14].",1,neutral
Similar conclusions are also reported recently in masked image model self-supervised learning [13].,1,neutral
"achieved success in natural language processing [47], it is not common in computer vision until very recently [13].",1,neutral
"…et al., 2022) and ViCHA (Shukor et al., 2022) use image and text masking combined with multimodal masking and/or image-text matching, but the masking approach is out of the scope of this work as it usually requires fine-tuning, as shown by He et al. (2022), while we focus on zero-shot transfer.",2,positive
"…specified or adaptive limited number of layers, modules, weights and biases of the model (Lee et al., 2019; Zhao et al., 2020; Guo et al., 2021; He et al., 2022; Ben Zaken et al., 2022), this type of methods can be the most memory-efficient for training because no additional parameter is…",1,neutral
"Specification: fine-tuning only specified or adaptive limited number of layers, modules, weights and biases of the model (Lee et al., 2019; Zhao et al., 2020; Guo et al., 2021; He et al., 2022; Ben Zaken et al., 2022), this type of methods can be the most memory-efficient for training because no additional parameter is involved.",2,positive
"of three parts, a powerful image encoder (MAE [91] pretrained ViT [92]); a prompt encoder, which is divided into",1,neutral
"Following MAE [12], we obtain the image representation by masking a large ratio of the image (i.",1,neutral
"One is to improve the pre-training of cross-modal alignment based on ConVIRT [3], and the other is the recently proposed joint image-text reconstruction for self-supervised pre-training, which uses the currently popular MAE method.",2,positive
"The reconstruction-based self-supervised pre-training methods were mainly inspired by MAE [12] and the self-supervised pre-text task in BERT [13], which used self-supervised reconstruction of images and reports for representation learning.",1,neutral
"Specifically, image reconstruction task follows MAE [12] to mask image patches, which reduces the redundancy of visual information by masking patches in a large proportion; The report reconstruction task applies WordPiece [36] as the tokenizer, and we propose a memory-augmented cross-modal fusion module to fuse visual and report features to obtain cross-modal representations.",2,positive
"A. Image Reconstruction
Following MAE [12], we obtain the image representation by masking a large ratio of the image (i.e., 75",1,neutral
Masked Image Modeling (MIM) [35] masks a certain proportion of random 2D patches and optimizes the encoder and decoder for pixellevel reconstruction.,1,neutral
"MVP [85] introduces multi-modality guided teacher models in MIM, utilizing CLIP [69] features as the reconstruction target.",2,positive
"In the first stage, inspired by the significant success of masked image modeling, such as BeiT [26], MAE [27], and SimMIM [28], we try to build a pre-trained model that supports arbitrary spatial mask input.",2,positive
"Motivated by the great success of masked image modeling, such as BeiT [26], MAE [27], and SimMIM [28], we incorporate the idea of pretraining a model with random masks and design a two-stage training strategy.",2,positive
"Since our tasks are similar to [35], the model also inherits the advantages of the MAE unsupervised model, which is convenient for processing downstream tasks.",2,positive
Reference [35] verified the scalability and generalization of the visual transformer with the design,2,positive
"We insert a cls token as the feature vector for downstream classification tasks, as it will automatically aggregate the features from other tokens when fine-tuned [1, 14].",2,positive
"Audio MAE methods [2, 3, 4] originate from image MAE [14] and typically take the log-mel spectrogram as input.",1,neutral
"Unlike conventional Transformer methods in speech processing, which consider the time axis in raw waves or spectrograms as the direction of input sequences [10, 11, 18], ViT divides log-mel into patches (each patch contains 16-by-16 time-mel bins following [3, 4, 14, 15]), and conducts sequential modelling in accordance with the patch’s position in the original log-mel spectrogram, as shown in Fig.",1,neutral
"Audio MAE can be simply pretrained by ground truth labels, and is able to reduce resource consumption by not inputting masked tokens to the model [14].",2,positive
"Most existing contrastive methods are CNN-based, while ViT backbones have been popular recently with generative approaches [5, 22] and has shown promise with contrastive approaches [6].",1,neutral
"supervised tasks [5, 6, 22], it is important to understand how to build capable open-vocabulary detectors with vision transformers [12, 35].",1,neutral
"PIXEL is built using ViT-MAE (Masked Auto Encoding Visual Transformer) (He et al., 2022).",2,positive
"proposed MAE [19], which employs an asymmetric encoder-decoder architecture and performs masked image modeling (MIM) tasks by encoding the image with missing pixels and then reconstructing the missing pixels.",2,positive
"Unlike other masking strategies [17, 19], SAM can precisely mask the diseased tissue, as shown in Fig.",1,neutral
"In the past year, masked autoencoders (MAEs) [19] have emerged as the powerful tools for the classification [45] and semantic segmentation [46, 30, 26, 49, 53] of medical images.",1,neutral
MAEs [19] provide an efficient training scheme for vision transformers.,2,positive
Self-supervised MAE [19] ViT-B [15] 72.,1,neutral
"Recent notable examples include MAE [76, 77] in vision; CLIP [30], ALIGN [31], Florence [78], BEiT [79], GATO [32], CoCa [10], SWAG [35], etc. for visual-language models.",1,neutral
"Recent notable examples include MAE [76, 77] in vision; CLIP [30], ALIGN [31], Florence [78], BEiT [79], GATO [32], CoCa [10], SWAG [35], etc.",1,neutral
"Therefore, self-supervised learning for tabular data (He et al., 2022; Devlin et al., 2018), particularly one that is able to bootstrap the learning on new tables, is still an open problem.",1,neutral
"While deep learning has achieved tremendous success in the fields of computer vision (CV) (He et al., 2022; Liu et al., 2021) and natural language processing (NLP) (Devlin et al.",1,neutral
"Recently, self-supervised learning (SSL) provides a promising learning paradigm that obtains supervision from the data itself by leveraging the underlying structure in the data, and has been widely studied in CV and NLP [22], [23], [24], [25], [26], [27], [28].",1,neutral
"It is largely different from MAE [22], where the best masking ratio is 0.75.",1,neutral
The results also demonstrate that our model is a successful application of masked autoencoding from CV and NLP domains to the brain network.,2,positive
2) Masked Autoencoder: Masked Autoencoder (MAE) [22] is a pre-training method that has been proven to be effective on the image domain.,1,neutral
"Due to the large amount of redundant information in images and the sparse semantics, MAE employs a large masking ratio to increase the difficulty of the pretext task and forces the network to learn high-level semantic features.",1,neutral
A comparison of brain network classification results between our proposed graph self-supervised learning and GMAE on the ABIDE dataset is shown in Table I.,2,positive
"Zhang [19] propose Graph Masked Autoencoders (GMAEs), which is a self-supervised transformer-based model for learning graph representations.",1,neutral
"Most related to ours is the model of GMAE (Graph Masked Autoencoders with transformers) [19], which is a graph self-supervised learning method by simply applying the masked autoencoding idea on the graph data.",1,neutral
GMAE is a masked node reconstruction autoencoder by simply borrowing the masked autoencoding idea through graph transformer layers on the graph data.,1,neutral
"Similar to MAE, we randomly partition the nodes into two sets: visible nodes and masked nodes with a small random subset (e.g., 25",1,neutral
"In summary, compared with previous MAE for images, the proposed self-supervised learning with mask scheme possesses two distinctive properties:
Authorized licensed use limited to the terms of the applicable license agreement with IEEE.",2,positive
"Motivated by the masked autoencoding [22], [23], it naturally comes into minds how to design a self-supervised learning with the masked autoencoding scheme on the brain network.",1,neutral
MAE is an asymmetric encoder-decoder architecture consisting of multiple visual transformer layers which mask random patches of the input image with high masking ratio and reconstruct the missing pixels.,2,positive
It is non-trivial to transfer the pretext tasks designed for CV/NLP for brain network analysis.,1,neutral
"2) Rather than the transformer module used as the encoder and decoder in MAE, only local spatial convolution is unitized on the adjacent matrix in our masked graph autoencoder.",1,neutral
"It is largely different from MAE [22], where the best masking ratio is 0.",1,neutral
"5) We compared the self-supervised learning methods of GMAE [19], BrainGSL-AE and BrainGSL, all of which involve two branches: pre-training through self graph reconstruction and fine-tuning on downstream graph classification.",2,positive
"inpainting [8], or rotation prediction [5]) and tasks a neural network to solve it, with the aim of generating latent representations that capture high-level image semantics.",1,neutral
showed that transformers are able to confidently reconstruct from the context up to 95% of hidden data in a masked autoencoder fashion [10].,1,neutral
"Recently, masked image modeling (MIM) has recently gained increasing attention in computer vision, such as BEiT [8], MAE [45], SimMIM [164], as it eliminated the need for additional information, data augmentation, and selection of positive and negative pairs.",1,neutral
"Density/masking is the most frequent data augmentation method adopted in mask autoencoder (MAE) type SSL research (He et al., 2021; Pang, Wang, Tay, Liu, Tian and Yuan, 2022; Yu, Tang, Rao, Huang, Zhou and Lu, 2021).",2,positive
"Since the ViT pre-trained with MAE [6] contains 12 layers attention layers, we experiment with n = 3, 6, 9, 12 and report their results in Table 5.",2,positive
"Our feature extractor, pre-trained with MAE [6], consists of 12 transformer encoder blocks with a hidden dimension of 768, and each multihead self-attention layer contains 12 heads.",2,positive
"However, 1) we exclude the self-supervised [6] pre-training on the FSC147 dataset compared with CounTR, which leads to a complex training strategy, and 2) we do not require an additional CNN to extract features of exemplars, which will cause varied feature spaces.",2,positive
CounTR [16] need two-stage training regime [6].,0,negative
"Inspired by the success of generative self-supervised learning in language representation [8], masked autoencoders (MAE) have been proposed as effective learners for understanding images [11].",1,neutral
"In light of the aforementioned limitations, an interesting question naturally arises: How to build SSL-enhanced sequential recommender systems that are easily adaptable and noise-resistant? Inspired by the recent success of autoencoder-based masking techniques in image data augmentation [11, 13], generative self-supervised learning with the goal of masked data reconstruction can naturally alleviate the heavy reliance on manually constructing high-quality contrastive views for accurate embedding alignment.",1,neutral
"Again, most of the experiments are conducted with ViT-B, given the increasing popularity and efficacy of attention-based models in large-scale pre-training [9, 18].",1,neutral
"Convolutional Neural Networks (CNNs) [19, 33, 42, 58] and Vision Transformers (ViTs) [7, 9, 18, 41, 60] have brought massive empirical success in various vision tasks including image classification [19], object detection [59], semantic segmentation [26], and more.",1,neutral
2019) and computer vision (He et al. 2022) to effectively model semantic knowledge in high-dimensional space.,1,neutral
"Recently, Transformers (Vaswani et al. 2017) have been used in domains such as text generation (Radford et al. 2019) and computer vision (He et al. 2022) to effectively model semantic knowledge in high-dimensional space.",1,neutral
"We use the ViT-Base (ViT-B) and ViTHuge (ViT-H) as our backbone, which is proposed by [70] and pre-trained as MAEs [71] on ImageNet-1K [72].",2,positive
"Each mask token is a shared, learned vector that indicates the presence of a missing patch to be predicted [34, 36].",1,neutral
We employ the weights trained on ImageNet [34] as initialization and used the OCT2017 dataset [29] to fine-tune the model for 300 epochs.,2,positive
"For a fair comparison, in the implementation of the transfer learning, we use the vision transformer [32] trained on the ImageNet and fine-tuned on the OCT2017 dataset as the encoder.",2,positive
"Then the encoded visible patches and mask tokens are inputted into the Transformer decoder, which adopts the architecture of the MAE decoder in [34].",2,positive
The settings of the Transformer decoder follow the lightweight design used in [34] with an embedding size D of 512 and 8 consecutive sub-blocks.,2,positive
"Although the original OCT data is collected in gray-scale (C = 1), we use the default RGB channels used in the vision Transformer models [32,34] for simplicity.",2,positive
The output of the MAE decoder is reshaped to form a reconstructed image.,2,positive
"The advantages of our method can be summarized below: We leverage the emerging self-supervised generative learning [26, 27], which has been demonstrated to be effective in transferring to general computer vision tasks [34,35,49], here we use it to address the annotation efficiency problem in OCT segmentation.",2,positive
The model achieves the best accuracy when trained on the ImageNet and fine-tuned on the OCT2017 dataset.,2,positive
"We leverage the modeling capacity with self-attention of the Transformers [31–33], and the scalability and generalization capacity of masked self-supervised generative learning [34, 35].",1,neutral
"We follow the learning strategy of the masked autoencoders (MAE) [34], in which the objective is to reconstruct missing pixels after randomly masking patches of input images.",2,positive
"Mask prediction has demonstrated its effectiveness in numerous computer vision tasks and draws increasing interest [2,12,21,23,49,58].",1,neutral
"The contrastive learning paradigm is conducted by pulling positive samples while pushing negative ones [3, 6, 8, 19, 22, 51, 63], and the mask prediction paradigm learns representations by modeling the visible parts to infer the masked ones [2, 21, 39, 49, 58, 62].",1,neutral
"[6, 19, 21, 22, 58, 63] have shown the effectiveness of selfsupervised learning on images or static point clouds, these methods cannot be directly extended to point cloud videos due to the following three challenges: (i) Multiple-Granularity Information Matters.",1,neutral
"The mask prediction paradigm usually learns to reconstruct masked raw signals by modeling visible ones [2, 21, 39, 49, 58, 62].",1,neutral
"In contrast, the mask prediction paradigm [2, 12, 21, 23, 58] pays more attention to modeling local structures while ignoring global semantics.",1,neutral
The masked tokens are replaced with one single trainable vector [MASK] as in the original MAE [24].,1,neutral
"The present paper focuses on the masked autoencoder (MAE), a self-supervised generative model that uses an asymmetric encoder-decoder architecture with input masking [24].",2,positive
"As in [24] the encoder inputs are only the visible tokens, which resolves the quadratic complexity issue that is inherent in transformer models with respect to the sequence length [24].",1,neutral
It uses the same architecture as the vanilla MAE [24] but incorporates a masking process from video ViT (ViViT) [34].,2,positive
"autoencoder [24, 25, 26], which consists of two components: an encoder that maps the perturbed or noisy input to a latent representation and a decoder that generates the unperturbed or denoised input from the latent representation.",1,neutral
"The global tokens, denoted by w ∈ R1×(ta·d·ea) and w ∈ R1×(tv·h·w·ev), can be thought of as similar to the [CLS] tokens proposed in [24].",1,neutral
"The parameters of the optimizer, similar to [24], are β2 = 0.",1,neutral
"[24], improving the quality of MAE predictions can potentially lead to better representations for downstream tasks.",1,neutral
"To successfully reconstruct the masked tokens, the encoder must generate a semantic representation, which can then be effectively transferred to downstream tasks [24].",1,neutral
"Input pixel pruning has already been pioneered in the context of scalable self-supervised learning in computer vision, where random image patches are masked in the input image and the missing pixels are subsequently reconstructed [12].",1,neutral
"…al., 2019), and by encouraging the feature map to be invariant under data augmentations, self-supervised contrastive learning methods (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Chen and He, 2021; He et al., 2021) can achieve state-of-the-art performance for various downstream tasks.",2,positive
"By applying the above augmentations, self-supervised contrastive learning methods (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Chen and He, 2021; He et al., 2021) can achieve state-of-the-art performance for various downstream tasks.",2,positive
", 2019), and by encouraging the feature map to be invariant under data augmentations, self-supervised contrastive learning methods (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Chen and He, 2021; He et al., 2021) can achieve state-of-the-art performance for various downstream tasks.",2,positive
"Motivated by the recent advances in pre-training from natural language processing [5, 10] and computer vision [7, 14], researchers devote efforts to pre-training on GNNs [16, 17, 32, 42, 54].",1,neutral
", ImageNet [3]) with fully-/self-supervised learning [5,10] and fine-tuning on downstream tasks such as object detection [27] or semantic segmentation [19], has not been successfully adopted in SITS analysis yet.",1,neutral
"The recent Vision Transformer[5, 11, 12] has surpassed CNN architectures in the field of vision.",2,positive
"LIMITATION
When dealing with scenes like Cityscapes where objects are of different sizes, Vision Transformer[5] is still not as effective as CNN networks.",1,neutral
"But the plain Vision Transformer lacks scale variation, so we want to enhance the multiscale context fusion in the neck part to cope with the scenes with changing size objects and achieve better segmentation.",2,positive
", using a shallow decoder (Lu et al., 2021; He et al., 2021), to decrease the dependencies among generated tokens as well as improve the correlation between span representations and their associated knowledge.",1,neutral
"Notably, we restrict the capacity of the decoder, e.g., using a shallow decoder (Lu et al., 2021; He et al., 2021), to decrease the dependencies among generated tokens as well as improve the correlation between span representations and their associated knowledge.",1,neutral
"This framework is often used on top of deep learning applications to ease downstream learning and processing (Rombach et al., 2022; Turian et al., 2010; He et al., 2022) as well as a design principle for the network itself (Ronneberger et al.",1,neutral
", MAE (He et al., 2022), can be useful for linear probing performance.",1,neutral
"We observe that other methods, e.g., DINO (Caron et al., 2021), BEiT (Bao et al., 2022), and MAE (He et al., 2022), have consistent properties (See Figure C.1).",1,neutral
"Among them, SimMIM (Xie et al., 2022b) and MAE (He et al., 2022) are simple yet effective methods to reconstruct masked tokens without complicated pretext tasks.",1,neutral
"Deviating from CL, masked image modeling (MIM) (Bao et al., 2022; Xie et al., 2022b; He et al., 2022) has risen as a strong competitor of CL in the era of Vision Transformers (ViTs) (Dosovitskiy et al.",2,positive
"Therefore, MIM with a deep self-attention decoder, e.g., MAE (He et al., 2022), can be useful for linear probing performance.",1,neutral
", 2022), and MAE (He et al., 2022), have consistent properties (See Figure C.",1,neutral
"Deviating from CL, masked image modeling (MIM) (Bao et al., 2022; Xie et al., 2022b; He et al., 2022) has risen as a strong competitor of CL in the era of Vision Transformers (ViTs) (Dosovitskiy et al., 2021) with its impressive performances of downstream tasks.",2,positive
"MAE (He et al., 2022) addresses this problem by introducing deep explicit ViT decoders and reconstructing masked tokens only in the separate decoders.",1,neutral
We note that the optimal masking ratio in [25] is 75% for natural images.,1,neutral
"For common image classification tasks, they directly use the normal full-supervised method to train the decoder after training the backbone with selfsupervised learning [25], [40], [41].",1,neutral
"[25], which solves a similar problem in the image classification field.",1,neutral
"In Stage I, we introduce the MAE-style training paradigm [25], a recently proposed novel self-supervised learning method, to train the latent representation extractor with unlabeled samples.",2,positive
"On the other hand, BYOL [28], SimSiam [18] treat two crops from the same image as similar pairs instead of contrastive learning, while masked autoencoders [31] introduced a masking strategy to the age-old autoencoders for learning representation via reconstruction.",1,neutral
"typical sources, such as natural language [11], image [12] and",1,neutral
"The huge and rich labeled data has tremendously promoted he development of deep learning [1–4] , such as ResNet [5] , enseNet [6] , MAE (Masked Autoencoders) [7] , ViT (Vision Transormer) [8] and CLIP (Contrastive Language-Image Pre-training) [9] .",2,positive
"Introduction
The huge and rich labeled data has tremendously promoted he development of deep learning [1–4] , such as ResNet [5] ,
enseNet [6] , MAE (Masked Autoencoders) [7] , ViT (Vision Transormer) [8] and CLIP (Contrastive Language-Image Pre-training) [9] .
owever, in many specific scenarios, the annotation of data is
ostly and only limited labeled samples are accessible.",2,positive
"In Bert [34] and MAE [35], these models recover the occluded segments/images in a logical way according to the gradually learned context.",1,neutral
"of domains including computer vision (CV) [4, 10, 23], natural language processing (NLP) [26, 30, 32], recommender systems (RS) [25, 27, 28], etc.",1,neutral
"Inspired by MIM in CV [10], we devise three feature-level data augmentation operators as shown in Figure 2: (1) Random Mask, masking each feature in the augmented group with randomly sampled positions; (2) Span Mask, masking each feature in the augmented group with continuous positions beginning from the sampled starting position; (3) Uniform Noise, adding imperceptibly small noises to each feature in the augmented group.",2,positive
"Borrowing the idea of mask image modeling (MIM) in CV [10], we devise three feature-level data augmentation operators — randommask, spanmask, and uniform noise — in order to perturb song representations in different manners.",1,neutral
"To perform more comprehensive feature embedding, we design the GPT-Net based on anomaly-specific datasets and masked autoencoder [37] to pre-train the encoder.",2,positive
"In particular, Multi-modal Multi-task Masked Autoencoder (MultiMAE) [4] is a multi-modal pre-training
linear projections before a fixed positional embedding is added to them.",1,neutral
We use the pre-trained MultiMAE (RGB + Depth + SemSeg) made publicly available by Bachmann et al. [3].,2,positive
"[1] remedy this issue by encoding only a subset of all input tokens, similar to MAE [5].",1,neutral
We show that fine-tuning a pre-trained MultiMAE model can significantly increase VO performance using only 5% of the training data amount of previous methods [35].,2,positive
Our results show that MultiMAE pre-training provides useful multi-modal features for VO that fine-tuned outperform the ConvNet baselines.,2,positive
"The image encoder was, with the backbone of ViT, pre-trained by the masked autoencoder (MAE [83]) technique.",2,positive
"In addition, BERT introduces the next sentence prediction task, which can be used in conjunction with masked language model to pre-train text pair representations [25].",1,neutral
"In the self-supervised learning step, as described above, we use Masked Autoencoder (MAE) He et al. (2022) as the self-supervised learners.",1,neutral
"We decouple seismic data p(x, t) and velocity maps c(x, z) and train individual masked autoencoders (MAE) He et al. (2022) for each domain (shown in Figure 1-b).",2,positive
"In the selfsupervision step, we use the same hyper-parameters and the training schedule with the original MAE paper He et al. (2022), except we change the batch size to 512 and remove the pixel normalization.",2,positive
"In particular, we train two masked autoencoders (MAE) He et al. (2022) separately, i.e. one for seismic data and one for velocity maps (see Figure 1- b).",2,positive
"In particular, we train two masked autoencoders (MAE) He et al. (2022) separately, i.",2,positive
"In particular, the encoder and decoder can be learned separately in their own domains via MAE He et al. (2022), and the two corresponding latent spaces are linearly correlated.",1,neutral
"In particular, we train two masked autoencoders (MAE) He et al. (2022) separately, i.e. one for seismic data and one for velocity maps (see Figure 1b). Surprisingly, we observe a linear correlation between the two latent spaces. This means the self-pretrained encoder and decoder can be frozen, and we only need to learn a linear converter to connect them from the paired seismic data and velocity map. This introduces an interesting insight into FWI: the self-consistent representation within each domain is associated with simpler mapping across domains. We name this method SimFWI, as it simplifies the mapping (linear) in FWI between seismic data and velocity map via domain-independent self-supervised learning. Furthermore, SimFWI provides a better understanding of the relationship among multiple FWI datasets with different subsurface structures. We found that these datasets can share both encoders and decoders, but have different linear mappings between the latent spaces of two domains (i.e. seismic data and velocity map). Essentially, the two domains have a piece-wise linear relationship over multiple datasets. In addition, we found a correlation between the linear layer’s singular values and the complexity of the dataset. SimFWI achieves solid performance on multiple FWI datasets. It has comparable results to the InversionNet Wu & Lin (2019), a jointly trained model that uses paired data as supervision, with only half the model size (12.",2,positive
"In particular, the encoder and decoder can be learned separately in their own domains via MAE He et al. (2022), and the two corresponding latent spaces are linearly correlated. Table 1 lists notations of our method. 3.1 Domain-Independent Self-Supervised Learning We decouple seismic data p(x, t) and velocity maps c(x, z) and train individual masked autoencoders (MAE) He et al. (2022) for each domain (shown in Figure 1-b).",2,positive
where l is a distance metric (such as the mean squared error metric used by [16]).,1,neutral
"Unlike other masked-autoencoder methods [11, 16], structured masking strategies outperform random masking.",1,neutral
"Our approach is based on the masked autoencoding framework [16], in which the network architecture includes both an encoder (f ) and a decoder (g).",2,positive
"A key difference between Presto and other self-supervised approaches [11, 34, 16, 10] is that Presto is pre-trained using pixel-timeseries instead of images.",1,neutral
"Vision transformers typically process an input image by dividing the image into several patches of sizeH×W ×3 (orH×W ×11 for the multi-spectral SatMAE model) [11, 34, 16, 10].",1,neutral
", ground-level photography) – for example, by using a ResNet [15] backbone [12, 13, 14], or by adapting masked autoencoding for image classification [16] to satellite imagery [10, 11].",1,neutral
"We fill in the missing values with a shared and learnable embedding [10, 18] to indicate the absence of a user-specified motion in these pixels.",2,positive
"For image masking, unlike MAE [12], we aim to reconstruct the invisible patches features with visible image, text, and entity features to facilitate multi-modal information and knowledge fusion.",2,positive
The encoders are initialized with the MAE [18] pre-trained parameters.,2,positive
"However, BEiT and MAE only focus on image-based tasks.",1,neutral
"MAE [60] directly works in the continuous space, i.",1,neutral
"Among them, the major one is information density [60].",1,neutral
"Motivated by the success in NLP, some works attempt to leverage the idea of BERT into CV tasks [55], [56], [57], [58], [59], [60], which mainly focus on the RGB modality.",1,neutral
"MAE [60] directly works in the continuous space, i.e., masking and reconstructing the pixel values.",1,neutral
"In contrast with supervised learning, Self-supervised learning (SSL) (Chen et al., 2020; Chen & He, 2020; Zbontar et al., 2021; Bardes et al., 2022; Caron et al., 2020; He et al., 2022) aims to learn general representations of content-rich data without explicit labels by solving a pretext task.",1,neutral
"In general, the improvement of the above methods is accompanied by the introduction of improved backbones, such as [26, 27, 30, 44].",1,neutral
"MAE(He et al., 2022) builds a representative MIM framework.",2,positive
"Following previous works(He et al., 2022; Xie et al., 2022; Wei et al., 2022a; Bao et al., 2022; Huang et al., 2022), we use ImageNet-1K(Russakovsky et al.",2,positive
"Previous masked image modeling works mainly focus on the framework design(Chen et al., 2022; Dong et al., 2022; He et al., 2022), masking strategy(He et al., 2022; Li et al., 2022), or combining with contrastive learning(Huang et al., 2022).",2,positive
"While (He et al., 2022; Xie et al., 2022; Zhou et al., 2022) use a simple random strategy, AttMask(Kakogeorgiou et al.",1,neutral
"MAE(He et al., 2022) is the most representative pre-training method with MIM.",2,positive
", 2022), a random masking strategy with 60% mask ratio and 32× 32 mask block size is adopted, and other settings are kept the same as MAE(He et al., 2022).",2,positive
"The fine-tuning settings also follow MAE(He et al., 2022).",1,neutral
"While (He et al., 2022; Xie et al., 2022; Zhou et al., 2022) use a simple random strategy, AttMask(Kakogeorgiou et al., 2022) and SemMAE(Li et al., 2022) improve the masking strategy through the guidances of attention maps and semantic parts, respectively.",1,neutral
"Instead of using an 8-layer decoder in MAE(He et al., 2022), we use a lighter 2-layer decoder for ViT-Base and a 4-layer decoder for ViT-Large, respectively.",2,positive
"Following MAE(He et al., 2022), we employ Mask RCNN(He et al., 2017) with
FPN(Lin et al., 2017) as the detector.",2,positive
"Main settings follow the MAE(He et al., 2022).",0,negative
"Some works (e.g., MAE(He et al., 2022) and MaskFeat(Wei et al., 2022a)) use the pixel-level targets and reconstruct them from masked inputs.",1,neutral
"The pre-training setting follows MAE(He et al., 2022).",0,negative
"MAE(He et al., 2022) with this head achieves 51.7% bbox mAP and 45.9% mask mAP. iBOT adopts Cascade Mask R-CNN(Cai & Vasconcelos, 2018), which is also unfair.",0,negative
"Masked image modeling (MIM)(Bao et al., 2022; He et al., 2022; Xie et al., 2022) has become a new paradigm to pretrain vision transformer(Dosovitskiy et al.",1,neutral
"Selected teachers for fair comparisons are:
• MAE(He et al., 2022).",2,positive
"We adopt the same setting of MAE(He et al., 2022) to do linear probing.",1,neutral
"Following SimMIM(Xie et al., 2022), a random masking strategy with 60% mask ratio and 32× 32 mask block size is adopted, and other settings are kept the same as MAE(He et al., 2022).",2,positive
"Following previous works(He et al., 2022; Xie et al., 2022; Wei et al., 2022a; Bao et al., 2022; Huang et al., 2022), we use ImageNet-1K(Russakovsky et al., 2015) as the pretraining and fine-tuning dataset.",2,positive
"Same as the settings of MAE(He et al., 2022), we turn on relative position bias(Raffel et al., 2020) during transfer fine-tuning.",1,neutral
"Img2Vec achieves 85.1% top-1 accuracy with ViT-B, which surpasses MAE(He et al., 2022) by 1.5% absolutely.",2,positive
"The whole framework is similar to MAE(He et al., 2022) except for the reconstruction target, which is extracted from a well-trained teacher model with the entire image as input.",2,positive
"After obtaining the predictions and targets, we adopt the patch loss to optimize the model, which is similar to MAE(He et al., 2022).",2,positive
"Masked image modeling (MIM)(Bao et al., 2022; He et al., 2022; Xie et al., 2022) has become a new paradigm to pretrain vision transformer(Dosovitskiy et al., 2021; Bai et al., 2022) models.",2,positive
"For example, MAE(He et al., 2022), etc. reconstruct raw pixels or low-level (e.g., histograms of oriented gradients, HOG (Dalal & Triggs, 2005)) features of the masked part.",1,neutral
"REC learning does not require hand-crafted augmentations, but doesn’t transfer as well when directly using pre-trained features without any fine-tuning [9], and also requires a decoder in pixel space, increasing the computational cost relative to JE .",2,positive
[9] showed that a simple masking approach (MAE) tailored for ViTs followed by a pixel-level reconstruction objective outperforms all other methods for fine-tuning and scaling with dataset and ViT size.,1,neutral
[9] showed that simply fine-tuning the last few ViT blocks led to a significant improvement over linear probe transfer.,1,neutral
"In order to understand why fine-tuned MAEs can quickly exceed transfer performance of fine-tuned JE models even with partial fine-tuning [9], we need to develop a mechanistic understanding of what happens during fine-tuning.",2,positive
"Prior work has shown that linear probe transfer works better for JE models than for MAE [9], but did not consider the amount of class specific information in the final ViT layer relative to the probe, and how it is distributed across layers.",1,neutral
"Among SSL methods for learning ViT representations, two broad categories have emerged: joint embedding based learning [7, 3] and reconstruction-based learning [8, 9] (referred to as JE and REC respectively hereafter).",1,neutral
"Second is REC SSL family which rely on a reconstruction loss in the pixel space that doesn’t require handcrafted data augmentations but instead utilizes a decoder to reconstruct from the noisy representation [8, 19, 9].",1,neutral
"Recently, MIM has been shown to learn transferable, robust, and generalized representations from visual images, improving performance in downstream computer vision tasks [34].",1,neutral
"[34] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick, “Masked autoencoders are scalable vision learners,” Preprint at arXiv:2111.",2,positive
"Characteristics of pretrained FViTs: Existing works have shown that pretrained FViTs have two promising characteristics regarding their learned features: (1) pretrained FViTs can identify complex but meaningful features [16,41], even on unseen datasets without tuning [6, 30, 36]; (2) the learned features in FViTs can be reversely projected to the input image space using gradient-based methods [22, 44, 46].",1,neutral
"Existing efforts in developing FViTs mainly fall into two categories: (1) exploring how to scale up ViTs’ architectures to construct powerful FViTs [40, 69, 75]; (2) developing selfsupervised pretraining techniques to train FViTs so that their learned representations can be more effectively generalized to downstream tasks [4, 7, 17, 30, 36].",1,neutral
"Inspired by the various visualizations showing FViTs’ attention distributions in previous works [22, 30, 52, 66], we hypothesize that the evolution of attention distributions during the tuning process contains hidden traces for identifying the existence of over-fitting.",1,neutral
"Similarly, advanced training strategies, such as masked image modeling [8, 2] and image-text contrastive learning [28, 27, 6], have been adopted to improve the model representations.",1,neutral
"Specifically, we employed a ViT pretrained on ImageNet-21k using the generative, self-supervised learning method of masked autoencoders (MAE) [39], which exhibited major amounts of effectiveness in generalization.",2,positive
"This idea evolved into masked auto-encoding methods [He et al., 2022], in which the masked region is a union of image patches that can be predicted using a transformer.",1,neutral
"Somemethods like [He et al., 2022] are built with a specific decoder which make such visual
analysis easy, however most SSL methods aren’t shipped with a decoder.",1,neutral
"Somemethods like [He et al., 2022] are built with a specific decoder which make such visual",1,neutral
"However, it should be noted that these SSL algorithms explicitly demand localization in their objective functions, for example via masked autoencoding where patch features should contain information regarding the contents of the corresponding section of the image [He et al., 2022].",2,positive
"Other methods such as MAE, DINO, and iBot use the AdamW optimizer [Loshchilov and Hutter, 2017] with a smaller base learning rate of 1e− 5− 5e− 4.",1,neutral
"The most common benchmark on ImageNet runs the optimization over
100 epochs for ViT smaller than base, and for 50 epochs for larger models [He et al., 2022].",2,positive
"100 epochs for ViT smaller than base, and for 50 epochs for larger models [He et al., 2022].",1,neutral
"In SSL vision models, LayerDecay increases performance when fine-tuning on downstream tasks [Bao et al., 2021b, Zhou et al., 2022a, He et al., 2022].",2,positive
"One approach is to use a reconstruction-based objectives such as MAE [He et al., 2022] which uses a reconstruction loss in pixel space to avoid the need for defining precise invariances.",1,neutral
"Existing SSL methods designed specifically for transformers confirm that the trained models are effective for downstream detection and segmentation tasks, especially when fine-tuned [Li et al., 2021b, He et al., 2022].",2,positive
"Full Fine-tuning The Masked Auto-encoders (MAE) paper [He et al., 2022] re-introduced fine-tuning as the main evaluation metrics.",2,positive
"Note that BEiT, MAE, and SimMIM are deployed on downstream prediction problems by removing the decoder and replacing it with a prediction head.",1,neutral
"This is especially helpful when using masked prediction pretraining objectives such as MAE [He et al., 2022] or Masked Siamese Networks [Assran et al.",1,neutral
"This is especially helpful when using masked prediction pretraining objectives such as MAE [He et al., 2022] or Masked Siamese Networks [Assran et al., 2022b].",1,neutral
"For example, the ConvNextV2 architecture, which was state of the art on ImageNet (for models trained with only public data) when released, employs MAE pretraining [Woo et al., 2023].",2,positive
"In computer vision, analogous objectives exist with models such as MAE or BYOL learning to predict masked patches of an image or representation [Grill et al., 2020, He et al., 2022].",1,neutral
"In order to streamline MIM pre-training, two concurrent works [He et al., 2022, Xie et al., 2022] propose simplified algorithms, masked autoencoders (MAE) and SimMIM respectively, which directly reconstruct masked image patches rather than discrete image tokens extracted from an encoder as in BEiT.",2,positive
"Interestingly, the authors point out that simply pretraining a ConvNextV2 with the MAE framework is subpar.",2,positive
"Along with the spurt of deep learning and the self-supervised learning (pretraining) technology [14, 15, 16, 17, 18], pretrained MRL models are becoming prevalent.",1,neutral
"Pre-training was performed using masked auto-encoder modeling [18], followed by fully supervised training on the SAM dataset [4].",2,positive
"In particular, extended from MAE [17], we develop a masking strategy during the training to adaptively mask out visual tokens and learn strong pixel representations by reconstructing clean signals from corrupted inputs.",2,positive
"We compare our strategic masking with block-wise masking [1], random masking [17] and uniform masking [28].",2,positive
"Inspired by the recent MAE [17], we introduce a masking strategy in self-supervised pre-training that adaptively masks out patch tokens and learns pixel representations by reconstructing clean signals from corrupted inputs.",2,positive
"Specifically, we learn a score map for patch selection to choose the most informative patches as masked tokens under a determined ratio, as opposed to randomly masking in [17] or uniformly masking in [28].",1,neutral
"Unlike MAE [30], the decoder has cross-attention blocks in the beginning.",1,neutral
This advantage is similar to that of MAE [30].,1,neutral
"Following MAE [30], we ablate the decoder design in Tables 1a and 1b.",2,positive
"Following the ViT-B decoder in MAE [30], we set the number of blocks as 4.",1,neutral
"Self-supervised learning makes it possible to train deep learning models with infinite unlabeled data [30, 31].",1,neutral
MAE [30] reconstructs images given mask-out.,1,neutral
"First of all, we compare our VSA with recently proposed MAE [30].",2,positive
"Pre-training, on the other hand, is important for vision transformers [1, 30], since it is related to inductive bias.",1,neutral
"The loss is computed in a pixel-by-pixel manner, the same as MAE [30].",1,neutral
"Pre-training for vision transformers was extensively studied, such as DINO [6], BEiT [1] and MAE [30].",1,neutral
The MAE (masked autoencoder) [27] is a novel selfsupervised learning algorithm that demonstrates state-of-theart performance on various vision benchmarks.,2,positive
"As such, ViTs usually require long pre-training times, before being fine-tuned to the required task (He et al., 2021).",0,negative
"One of the most representative approaches is Masked Autoencoders (MAE) [26], which pre-trains the model by masking",1,neutral
"In summary, previous works [4,26,50] crucially require a certain trade-off between the local details and contextual semantics, which leaves room for further improvement.",1,neutral
"Although compared with other self-supervised learning (SSL) frameworks MAE can consistently help the model extract generally useful fea-1 tures even with few training samples (as proven by [31]), to some extent, MAE solely takes raw pixels as reconstruction targets mainly depending on local feature representation rather than fully utilizing the global information.",2,positive
"Previous works of reconstruction targets could be divided into three categories, including discrete tokens [4, 42], feature maps [50, 54], and raw image pixels [26, 51].",1,neutral
"MAE [26], SimMIM [51], DINO [9] and Swin UNETR [46]), among which MAE and SimMIM have achieved promising results on natural images, DINO is a representative contrastive learning method, and Swin UN-ETR is a representative of the previous efforts on SSL meth-ods for medical image analysis.",2,positive
"Since Masked Language Modeling (MLM) obtained great success in the field of Natural Language Processing (NLP) [18], numerous works [4,12,26,42,51,54] have transferred this idea to the vision domain, making Mask Image Modeling (MIM) an effective pre-training strategy.",1,neutral
"MAE [26], SimMIM [51], DINO [9] and Swin UNETR [46]), among which MAE and SimMIM have achieved promising results on natural images, DINO is a representative contrastive learning method, and Swin UNETR is a representative of the previous efforts on SSL methods for medical image analysis.",2,positive
"Some recent works applied MAE-based methods for medical image analysis [27, 46, 55] and achieved promising results across various benchmark datasets with different modalities, including computed tomography (CT) [38] images, magnetic resonance imaging (MRI) [28], to name a few.",1,neutral
"As experimentally illustrated in several previous works [4, 23, 26, 42, 47, 51], random masking strategy is not only simple but also effective for MIMbased self-supervised learning paradigm on large-scale natural images.",1,neutral
"Following-up works mainly focus on improving the accuracy and efficiency by introducing new designs, such as ConvMAE [23] and Siamese Image Modeling [47].",2,positive
We also verified the generality of FreMIM on RGB images dataset namely ISIC 2018 compared with the other seven well-6 [25] ViT-B/16 [20] MAE [26] 75.18 (-0.10) 88.95 ( +0.53 ) 78.47 (+2.14) 80.87 (+0.86) UNETR [25] ViT-B/16 [20] DINO [9] 75.22 (-0.06) 88.33 (-0.09) 75.89 (-0.44) 79.81 (-0.20) UNETR [25] ViT-B/16 [,0,negative
01 UNETR [25] ViT-B/16 [20] MAE [26] 75.,1,neutral
"One of the most representative approaches is Masked Autoen-coders (MAE) [26], which pre-trains the model by masking * Equal Contribution.",2,positive
"In comparison with MAE on UNETR and SimMIM on Swin UN-ETR, our FreMIM greatly improves model performance with the benefit of exploiting MIM in the frequency domain for global representation learning.",2,positive
"MAE cannot be adapted to Swin Transformer back-bone due to the token-dropping operation), for other meth-ods we kept their original backbone as in their papers to achieve a fair comparison, which implicitly demonstrates our method’s superior versatility to various backbones.",2,positive
"Without feeding masked to-kens into encoder, MAE [26] designed a simple decoder to reconstruct image patches, leading to a considerable reduction of computation complexity during pre-training.",2,positive
"Despite making methodological advancements and structural innovations, these methods have not essentially solved the key limitations of MAE.",1,neutral
"Without feeding masked tokens into encoder, MAE [26] designed a simple decoder to reconstruct image patches, leading to a considerable reduction of computation complexity during pre-training.",1,neutral
Swin Transformer and CNN-based models can not be directly integrated with MAE).,2,positive
Zhou et al. [55] applied MAE pre-training paradigm for medical image segmentation and significantly improved the results.,1,neutral
"Due to the random masking strategy of embedded image patches, MAE is only applicable for ViT [19] without the consideration of CNNs or hierarchical Transformer architecture), our FreMIM is a generic and flexible framework, which means both CNN-based and Transformer-based models can be easily integrated with our FreMIM for effective self-supervised pre-training.",2,positive
We follow MSN [14] for linear-probing and MAE [13] for full fine-tuning of standard models (see Sec.,1,neutral
"From works that demonstrate robustness in the fullshot regime, we seem to arrive at the following conclusions for robustness to natural distribution shifts in the fullshot regime: (1) Amongst ImageNet pre-trained initializations, SSL ViTs are more robust than their supervised and CNN counterparts, with the more recent ones being better [13, 14].",2,positive
"However, we find that these lead to poor performance for self-supervised (SSL) ViTs such as MSN ViTB-16, likely due to the absence of a classification head for such models.",0,negative
"We aim to address this gap in our work by evaluating some of the most recent SSL ViTs on a variety of datasets and distribution shifts, also comparing with CNNs and the supervised counterparts.",2,positive
It then optimizes a supervised ViT’s saliency maps [70] to resemble these offline segmentation maps while maintaining its classification accuracy.,2,positive
"Recent methods that leverage ViTs [13, 14] demonstrate superior robustness to some natural distribution shifts [9, 10, 11]",1,neutral
It can be seen that self-supervised (SSL) ViTs often perform better than SSL CNNs on ImageNet and supervised ViTs and CNNs on iWildCam and Camelyon datasets.,2,positive
"A : Self-supervised ViTs generally perform better than CNNs and the supervised counterparts (where applicable) on both ID and OOD shifts, but no single initialization or model size works better across datasets.",1,neutral
"From works that demonstrate robustness in the full-shot regime, we seem to arrive at the following conclusions for robustness to natural distribution shifts in the full-shot regime: (1) Amongst ImageNet pre-trained initializa-tions, SSL ViTs are more robust than their supervised and CNN counterparts, with the more recent ones being better [13, 14].",2,positive
"Recent works [71, 72] demonstrate the data specificity of ViTs and the global semantic invariance of SSL approaches such as DINO, which can be helpful for this purpose.",1,neutral
"Recent methods that leverage ViTs [13, 14] demonstrate superior robustness to some natural distribution shifts [9, 10, 11] compared to previous state-of-the-art methods without additional interventions.",1,neutral
"We assess the quality of the curve fit via mean absolute error (MAE) and coefficient of determination ( R 2 ) of the curve on these data points, as shown in table 4.",2,positive
"Researchers have shown selfsupervised learning (SSL) to be better or on-par with supervised learning for pre-training deep networks for various downstream tasks [49, 50, 20, 51, 13, 14] and we refer the reader to [52, 53] for thorough literature reviews.",1,neutral
"For full fine-tuning in the full-shot regime, we use the MAE codebase [13] and fine-tune for 20 epochs.",2,positive
"In the past decade, Computer Vision has made significant progress due to advanced architectures like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), large datasets, and sophisticated training strategies [1, 2, 3, 4].",1,neutral
"Researchers have also looked at the effect of architecture, i.e. CNNs and ViTs on robustness to different kinds of shifts [32, 33, 34, 35] and distortion robustness of several models in comparison to humans [36].",1,neutral
"Recent methods [13, 14, 15, 16, 17] use self-supervised or large-scale vision-language pre-trained models (such as CLIP [4]) and fine-tune them on fully labelled (in-domain) data to achieve impressive performance on such datasets.",1,neutral
"RobustViT [68] RobustViT uses an unsupervised localization method such as TokenCut [69] to dump offline segmentation maps and then optimizes a supervised ViT’s saliency maps [70] to resemble the offline ones while maintaining its classification accuracy to its improve robustness on the OOD shifts for ImageNet [7, 8, 9, 10, 11].",2,positive
"Thus, while SSL ViTs perform better than SSL CNNs and the supervised counterparts (where applicable) on both ID and OOD shifts in the low-shot regimes, no single initialization or model size performs the best across datasets.",2,positive
"Despite being amenable to low-shot finetuning, we find that it is non-trivial to implement RobustViT for self-supervised ViTs that perform better on OOD shifts and downstream tasks [13, 14].",2,positive
The MAE is an asymmetrical encoder-decoder architecture that relies on input masking [10].,2,positive
This article focuses on self-supervised SER with the masked autoencoder (MAE) approach [10].,1,neutral
Impact of the masking strategy The choice of the masking strategy in MAE-based self-supervised models can have a significant impact on the performance of auxiliary tasks [10].,1,neutral
"As in [10], the encoder inputs are only the visible tokens; this is to learn a representation that relies on the context.",1,neutral
"suggest [10], improving the quality of MAE predictions can potentially lead to better representations for downstream tasks.",1,neutral
"The parameters of the optimizer, similar to [10], are β2 = 0.",1,neutral
"Since most of the tokens are masked and only the visible tokens are fed to the encoder, this resolves the quadratic complexity issue inherent in transformer models with respect to the sequence length [10].",1,neutral
"of high resolution, which is not available in CTs), we proposed several additional loss terms in the objective function of VoxelMorph as well as a random masking strategy to greatly improved the quality of the synthetic CT images (a similar idea has been adopted by He et al.(93) to significantly",2,positive
"M3 is based on an asymmetric encoder-decoder architecture, similar to MAE [34], as shown in Fig.",2,positive
"ViT [5] is the first work to prove that a pure Transformer architecture requires significant computational resources and efficient pre-training, such as masked autoencoder [18], [19].",1,neutral
"Linear (He et al., 2022) and Partial (Yosinski et al.",1,neutral
"While ImageNet21k is larger than ImageNet1k, self-supervised learning adopted by MAE-ViT-B reduces overfitting the training classes, thus resulting in better performance than ViT-B (which is trained supervisedly).",2,positive
"CLIP outperforms ViT-B/MAE-ViT-B, thanks to a large amount of diverse multi-modal pre-trained data (i.e., learning the visual concept from language supervision).",1,neutral
"MAE-ViT-B (He et al., 2022) is only trained on ImageNet1k (not seeing more data than the downstream classification task), it performs better than ViT-B (Dosovitskiy et al.",2,positive
"Without exposure to the 1k classes, MAE-ViT-B achieves a noticeably better ECE.",0,negative
"We hypothesize that selfsupervised learning (MAE-ViT-B) plays a positive role compared to supervised learning (ViT-B), as it does not overfit the training classes and instead learn a holistic understanding of the input images beyond low-level image statistics aided by a well-designed loss (e.g., the masking and reconstructing strategy in MAE).",2,positive
"2 also compares four large-scale pre-trained models listed in Table 1 for computing RMD. Interestingly, while
MAE-ViT-B (He et al., 2022) is only trained on ImageNet1k (not seeing more data than the downstream classification task), it performs better than ViT-B (Dosovitskiy et al., 2020) trained on ImageNet21k.",2,positive
"In the case of self-supervised learning, the proxy task and loss are also formulated to learn a holistic understanding of the input images beyond low-level image statistics, e.g., the masking strategy designed in MAE (He et al., 2022) prevents reconstruction via exploiting local correlation.",1,neutral
"Large-scale pre-training has witnessed pragmatic success in diverse scenarios and pre-trained models are becoming increasingly accessible (Devlin et al., 2018; Brown et al., 2020; Bao et al., 2021; He et al., 2022; Radford et al., 2021).",2,positive
"2 also compares four large-scale pre-trained models listed in Table 1 for computing RMD. Interestingly, while
MAE-ViT-B (He et al., 2022) is only trained on ImageNet1k (not seeing more data than the downstream classification task), it performs better than ViT-B (Dosovitskiy et al., 2020) trained on…",2,positive
", the masking strategy designed in MAE (He et al., 2022) prevents reconstruction via exploiting local correlation.",2,positive
We follow MAE [37] and use the standard ViT architecture [28].,2,positive
Critical to our sequential approach are the two components MAE [37] and NNCLR [29] on which we build upon.,2,positive
"[37] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B.",0,negative
"We find that all MAE models, all MAE–CT models and the huge models of MAE–CTaug benefit from more regularization in this setting and therefore use the protocol from the original MAE publication [37], which uses Mixup [93], Cutmix [91], RandomErase [94], DropPath [44] (0.",2,positive
"MIM methods, like Masked Autoencoders (MAE) [37] and others [7, 87, 6] first mask out areas ar X iv :2 30 4.",1,neutral
MAE pre-training MAE pre-training follows the original work [37] to learn a rich but coarsely structured representation in a compute efficient manner by randomly masking out a large fraction of the input patches.,2,positive
"MIM approaches are able to train large ViT models, that lack the inductive bias of convolutional neural networks, and learn rich representations just from ImageNet [37, 6, 87, 73].",1,neutral
"We follow common linear probing protocols and sweep over learning rates [95, 12, 4] and insert a non-affine BatchNorm [48] layer after the fully frozen encoder [26, 37, 3].",1,neutral
"Additionally, MAE–CT is motivated by the reported effectiveness of partial fine-tuning [37].",1,neutral
"Various methods have shown that MIM can improve data efficiency and allows scaling of the model size [7, 37, 87, 6].",1,neutral
And Point-MAE [46] pre-trains point cloud models using the idea of MAE [39].,1,neutral
"At the same time, a number of unsupervised [38, 39, 40] and weakly supervised [41, 42, 43] pre-trained models also bring better visual feature representation and adaptability to downstream tasks for the transfer learning process.",1,neutral
"The transformer block can be used as part of more complicated systems e.g. in encoder-decoder architectures for sequence-to-sequence modelling for translation [Devlin et al., 2019, Vaswani et al., 2017] or in masked auto-encoders for self-supervised vision systems [He et al., 2021].",1,neutral
"By masking a portion of input tokens and reconstructing the RGB images, MAE achieves better results than supervised training.",1,neutral
"As a concurrent work, MaskFeat [142] mainly studies reconstructing targets of the MIM framework, such as HOG features.",2,positive
"Motivated by BERT, BEiT [141] proposes the BERT-like pertaining (Mask Image Modeling, MIM) of vision transformers.",1,neutral
"1) •Better ViTs Design SS / IS [21], [119], [120], [121], [122], [123], [124], [125], [126], [127] •Hybrid CNNs / transformers / MLPs SS / IS [23], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137] •Self-Supervised Learning SS / IS [24], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147]",2,positive
"the following works adopt improved designs on various vision tasks, including representation learning [23], [24], object detection [25], segmentation [26], low-level image processing [27], video understanding [28], 3D scene understanding [29], and image/video generation [30].",1,neutral
"MAE [24] SS/IS Image/Semantic Masks Pure transformer + CNN decoder a MIM pretraining framework for plain ViTs, which achieves better results than supervised training.",2,positive
"After BEiT, MAE [24] shows the ViTs can be trained with the simplest MIM style.",1,neutral
"Following MAE [24], several works [263], [264] simply the MIM pretraining process.",1,neutral
Point-M2AE [264] designs a multiscale MIM pretraining by making the encoder and decoder into pyramid architectures to model spatial geometries and multilevel semantics progressively.,2,positive
Point-MAE [263] divides the input point cloud into irregular point patches and randomly masks them at a high ratio.,1,neutral
"Motivated by MIM [24], MIC [294] proposes a masked image consistency to learn spatial context relations of the target domain as additional clues.",1,neutral
"The following works focus on improving the MIM framework [143], [144] or replacing the backbone of ViTs with CNN architecture [145], [208].",2,positive
"The objectives with 1) MSE and 2) the CS loss are defined respectively as follows:
arg min θ
1
n n∑ i=1 ∥∥x(i) − hθ (x(i))∥∥22 and arg minθ 1n n∑ i=1 `CS ( x(i), hθ ( x(i) )) ,
where `CS is given in Eq.",1,neutral
"[26] proposed Masked AE (MAE), where an AE is defined via Vision-Transformer (ViT) [15].",1,neutral
"Firstly, the dCS is based on the CS, while N2V is based on the MSE.",2,positive
N2N uses a set of pairs of noisy images to train a U-Net [54] with the MSE-based loss.,1,neutral
"As for the loss, in the vision domain, it is commonly defined via the Mean Squared Error (MSE) [41, 37].",1,neutral
"A naive approach is to learn an AE-based model by minimizing the CS between the reconstruction ŝ from a noisy data x and the corresponding clean data s, as some similar approach with the MSE-based loss has been investigated by the previous study [69].",1,neutral
"Krull et al. [37] proposed Noise2Void (N2V), which also employs a U-Net for predicting clean image, and the loss is defined via the MSE.",2,positive
"2 Further Details with Representation Learning AE based RL Several works [64, 35, 26, 43] have proposed AE-based RL methods, and many of them are applied to the vision domain.",1,neutral
"2) RL There are two popular methods: the AE-based methods [64, 26, 43] and the self-supervised learning methods that use data augmentation [7, 22, 8].",1,neutral
"The loss function is usually defined via the MSE [64, 26, 43].",1,neutral
The loss is defined via the MSE using the original image and the predicted image.,1,neutral
"In SDAE, a stacked AE is trained by minimizing an MSE-based loss, and its input is corrupted by an additive Gaussian noise.",1,neutral
"4.5.1 Setting in Expt3
Using ESC-50 [50] dataset, we compare 1) MSE, 2) CS, 3) N2V, and 4) dCS.",2,positive
"In this experiment, using Noisy-MNIST, we compare 1) MSE, 2) CS loss, 3) N2V loss, 4) SURE loss, and 5) dCS loss.",1,neutral
"Note that N2N and N2V are self-supervised denoising methods, i.e., the minimization of these losses is equivalent to that of an MSE-based loss defined via the clean data; for the detailed mathematical arguments
1This study is an extension of the denoising method proposed in Sanada et al. [55]; see the last paragraph of Section 2.3 for the comparison with Sanada et al. [55].
of N2N, see Zhussip et al. [71].",1,neutral
"We now present a few concrete examples of popular models that employ various combinations of generative architectures, joint embedding architectures, contrastive training, and noncontrastive training: The Denoising Autoencoder approach in generative architectures (Devlin et al., 2018; He et al., 2022; Vincent et al., 2008) using a triplet loss which utilizes a positive sample, which is a vector from the training set that should be reconstructed perfectly, and a contrastive sample consisting of data vectors, one from the training set and the other being a corrupted version of it.",1,neutral
"…contrastive training, and noncontrastive training:
The Denoising Autoencoder approach in generative architectures (Devlin et al., 2018; He et al., 2022; Vincent et al., 2008) using a triplet loss which utilizes a positive sample, which is a vector from the training set that should be…",1,neutral
[24] create a masked autoencoders (MAE) to reconstruct masked patches when an image is split into multiple patches.,1,neutral
"Inspired by the Mask Auto-Encoder (MAE), we also design a Masked Skeleton Cloud Repainting task to pretrain the designed auto-encoder framework, aiming for learning more discriminative and informative self-supervised representations.",2,positive
"Motivated by BERT [52] and MAE [53], we study the masked skeleton cloud modeling strategy for skeleton representation learning based on the introduced auto-encoder framework.",2,positive
"75, which is consistent with the value reported in MAE [23].",0,negative
"To model the spatial relationships in weather data, we employ the Vision Transformer (ViT) architecture [22] as the backbone network and apply the widely-used pre-training scheme, Masked AutoEncoder (MAE) [23], to propose a pre-trained weather model named W-MAE for multi-variable weather forecasting.",2,positive
"In particular, Masked AutoEncoder (MAE) [23], a recent state-of-the-art self-supervised pretraining scheme, pre-trains a ViT encoder [22] by masking an image, feeding the unmasked portion into a Transformer-based encoder, and then tasking the decoder with reconstructing the masked pixels.",2,positive
"We initialize image/text encoders in the same style as CLIP, except for one change: we use a sinecosine position embedding in ViT, like (Chen et al., 2021; He et al., 2022), and keep it frozen while training.",2,positive
"learning [19, 20] to achieve better performance.",1,neutral
"For pre-training the MRI encoder, we explore the selfsupervised masked auto-encoder (MAE) method [22] us-",2,positive
75 in our experiments) of input embeddings and trains models to reconstruct them [23].,1,neutral
") [26, 55, 43, 72, 32, 88] are interesting ways forward for the community, on the exciting task of VPA.",1,neutral
"Many proposed frameworks are constrained to a single domain, such as image or text (Azizi et al., 2021; Bai et al., 2019; Chen et al., 2019; He et al., 2022).",2,positive
SAM consists of a heavyweight image encoder that is based on Masked Autoencoders [44] and a pretrained Vision Transformer model [45].,2,positive
"• Autoencoders: these are trained to reconstruct their inputs by learning an efficient representation of the data (He et al., 2022).",1,neutral
"First, we run our evaluations for MAE (He et al., 2021), DINO (Caron et al.",2,positive
"First, we run our evaluations for MAE (He et al., 2021), DINO (Caron et al., 2021), SEERv2 (Goyal et al., 2022a), MSN (Assran et al., 2022), EsViT (Li et al., 2022a), Mugs (Zhou et al., 2022) and iBOT (Zhou et al., 2021).",2,positive
"Interestingly, our evaluation using +ms is on par with fully finetuning MAE with an Upernet decoder (53.0 versus 53.6 mIoU).",2,positive
"Recently, the emergence of patch-based architectures, like ViTs, has led to a revisit of inpainting for pre-training (He et al., 2021; Bao et al., 2021; El-Nouby et al., 2021), potentially in feature space (Assran et al.",2,positive
"An alternative to text-guided pretraining is self-supervised learning (Caron et al., 2018; Chen et al., 2020; He et al., 2021) where features are learned from images alone.",1,neutral
"Recently, the emergence of patch-based architectures, like ViTs, has led to a revisit of inpainting for pre-training (He et al., 2021; Bao et al., 2021; El-Nouby et al., 2021), potentially in feature space (Assran et al., 2023; Baevski et al., 2022).",2,positive
"This property of MAEs has been further validated on video (Tong et al., 2022), audio (Xu et al., 2022), and across other modalities (Girdhar et al., 2022).",2,positive
"Of particular interest, He et al. (2021) show that a masked auto-encoder (MAE) learns features that provide substantial improvements when finetuned on downstream tasks.",1,neutral
"Specifically, considering that the saturated regions can be regarded as masking the short LDR input patches, inspired by [6], we randomly mask a high proportion of the short LDR input and expect the model to reconstruct a no-saturated HDR image from the remaining LDR patches in the first stage.",2,positive
"More recently, also leveraging vision transformers, Reed et al. (2022) scale the Masked Auto-Encoder approach of He et al. (2022) and apply it to building segmentation.",1,neutral
"Masked-signal-modeling-based transformer models, such as BEiT [2] and the masked autoencoder [22], have been used for 3D pretraining [31, 62, 59, 35].",1,neutral
"The MAE learns these unknown regions to reconstruct the masked patches, and achieves excellent results.",2,positive
"1) Batch Random Mask Block: In order to obtain more robust features and make the prediction of the network adapt to complex and changeable environments, we propose a batch random mask (BRM) block inspired by the Masked AutoEncoders (MAE) [50], in which the random patches of the image are masked to reconstruct these missing regions by the model.",2,positive
"Existing pretrain pretext tasks can be divided into discriminative tasks [4, 9, 14, 48] and generative tasks [3, 11, 19, 25, 26, 32, 51, 55].",1,neutral
"Inspired by the works of SimMIM[14], MAE[13], BEiT[21], etc.",1,neutral
"Self-supervised learning has made remarkable progress in addressing the issue of small sample sizes in the domain of natural language processing (NLP) in recent times, the application of this learning algorithm has progressively extended to the domain of computer vision(CV) [12][13][14][15].",1,neutral
"Inspired by the works of SimMIM[14], MAE[13], BEiT[21], etc., our framework is based on self-supervised representation learning of recovered pixels, learns representations by mask
modeling, masks part of the input tongue image signal, masking a portion of the input tongue image signal and predict the raw signal in the masked region.",2,positive
"Our results in Section 5.2 shows that while off-the-shelf
CLIP representations can be poor (especially for RGB-stacking see Figure 6 Right), adapting them through our proposed adapters results in similar performance as other adapted representations (such as MAE ones).",2,positive
", 2020) and masked auto-encoder (MAE) for ViT (He et al., 2022).",2,positive
"Moreover, the adapted representations match the performance of more performant models (e.g. MAE).",1,neutral
"This advantage of MAE features for control is also observed in (Xiao et al., 2022).",1,neutral
", 2020) and for ViT we use masked auto-encoder (MAE) (He et al., 2022).",2,positive
"• Comprehensive evaluation of our approach across 3 different manipulation suites (35 individual tasks), 3 major model architectures (ViTs, NFNets, and ResNets) with supervised (imagenet classification) and self-supervised pretraining (CLIP, BYOL, Visual MAE).",2,positive
"Existing self-supervised pretrained visual models, such as SimCLR (Chen et al., 2020), BYOL (Grill et al., 2020) or Visual MAE (He et al., 2022), have also been shown to provide strong initializations for a wide range of visual downstream tasks.",2,positive
"For NFNet we use CLIP pretraining (Radford et al., 2021), for ResNet we use BYOL (Grill et al., 2020) and for ViT we use masked auto-encoder (MAE) (He et al., 2022).",2,positive
"In addition to imagenet pretraining across all three architectures, we also evaluate using ALIGN (Jia et al., 2021) for NFNet, BYOL for ResNet (Grill et al., 2020) and masked auto-encoder (MAE) for ViT (He et al., 2022).",2,positive
", 2020) or Visual MAE (He et al., 2022), have also been shown to provide strong initializations for a wide range of visual downstream tasks.",1,neutral
"image classification [71, 89, 82, 23], object detection [45, 77, 78, 6], semantic segmentation [87, 80, 69] and so on [70, 41, 86, 47, 24, 40].",1,neutral
"Recently, the prevailing trend has been disrupted by self-supervised learning based on contrastive learning [26, 8, 70] and masked image modeling [2, 25, 74], which learns better transferable representations with only unlabeled images.",1,neutral
"Recently, self-supervised learning [26, 8, 25, 2] has",1,neutral
"Inspired by Masked Auto Encoders (MAE) [11], Point-MAE [22] further explores pre-training methods for point cloud Transformers, implementing a pre-training pipeline for point cloud tasks with a completely standard Transformer structure.",2,positive
"By contrast, self-supervised pre-training methods (He et al., 2020; 2022; Radford et al., 2021; Jia et al., 2021) can be easily scaled to billions of unlabeled examples by designing an appropriate pretext task, such as solving jigsaw puzzles (Noroozi & Favaro, 2016), invariant mapping (Chen & He,…",1,neutral
MAE [10] applies the concept of reconstruction from BERT to the domain of computer vision.,1,neutral
"Because of its large model capacity and generalizing capability, this transformer-based architecture is widely used in LVMs. MAE [8] applies the concept of reconstruction from BERT to the domain of computer vision.",1,neutral
"To demonstrate that the performance of the foundation model improves with the increase in the number of model parameters when pretrained using the same number of datasets in the remote sensing field, we pretrain models with different
KEUMGANG CHA et al.: A BILLION-SCALE FOUNDATION MODEL FOR REMOTE SENSING IMAGES 5
numbers of parameters using MAE [5] and the large-scale remote sensing imagery dataset, MillionAID [44].",2,positive
This is because the amount of pretraining data (MillionAID) and the methodology used in pretraining (MAE) should be the same for fair comparison.,0,negative
"AID) [44], and pretraining method (MAE) [5].",1,neutral
"Clearly, models pretrained with MAE outperform those pretrained with IMP, with mAP differences ranging from 3.85 to 5.05.",0,negative
"However, since models with a large number of parameters can experience overfitting to the pretext task (MAE in this paper), the models are pretrained with 400 epochs using the AdamW optimizer [95] and a batch size of 2048.",2,positive
CV BYOL [1] ResNet200 2x 375 Million SimCLR v2 [2] ResNet152 3x w sk 795 Million DINO [3] ViT Base 84 Million iBOT [4] ViT Large 307 Million MAE [5] ViT Huge 632 Million ALIGN [6] EfficientNet-L2 800 Million CLIP [7] ViT Large 307 Million SEER [8] RegNety-256gf 1.,0,negative
The one is to predict the pixel values of the masked area by MSE loss [5].,1,neutral
"As expected, the performance of models pretrained with MAE is higher than those pretrained with IMP.",0,negative
numbers of parameters using MAE [5] and the large-scale,1,neutral
"In the original MAE, pretraining is applied with 1600 epochs [5].",1,neutral
The MAE means MAE on the MillionAID.,0,negative
"masked image modeling randomly masks parts of an image and learns to reconstruct the masked part [5], [53].",1,neutral
"learning papers using vision transformers in computer vision, only classification and semantic segmentation performance are introduced, and it is challenging to find object detection performance [3], [5], [55], [92].",1,neutral
"Specifically, the dataset, pretraining method, and foundation model structure are the same as in previous research, namely MillionAID [44], MAE [5], and vision transformer [43].",2,positive
"In this section, we discuss the details of the model architecture (vision transformer) [43], pretraining dataset (MillionAID) [44], and pretraining method (MAE) [5].",2,positive
2) MAE: MAE [5] learns representations by reconstructing randomly masked images using the encoder-decoder structure of the vision transformer.,1,neutral
"a Transformer[70], while features are learned through self supervision (such as masked input reconstruction) on large datasets [20, 49, 5, 28].",1,neutral
"For our default model, we re-use weights from the publicly available MAE model.",2,positive
"Following MAE [28], the encoder processes only the visible part of the inputs for all stages.",1,neutral
Stage 1: We follow settings from MAE [28].,0,negative
"Following [28], we adopt an encoder-decoder architecture during training and ask the model to reconstruct patches that are randomly masked out, at the pixel level.",2,positive
"We use masked input prediction[20, 28, 5, 75] objective for unimodal stages.",1,neutral
"The difference between our approach and [20, 49] is that we follow the encoder-decoder structure in [28], where masked tokens are removed for the encoder and are reconstructed through a separate decoder.",2,positive
[28] shows that ViT better generalizes under pixel-level supervision with aggressive masking.,1,neutral
"Self-supervised learning has proven to be a very effective training mechanism for learning representations that transfer well to various downstream tasks[20, 28, 57, 4].",1,neutral
"In Computer Vision, Masked Image Modeling (MIM) [11, 36, 3, 41, 2] also gains significant popularity for self-supervised representation learning.",1,neutral
"The success of MLM [8] and MIM [3] demonstrate that mask-based pretraining helps learn global and generalizable
features, which is beneficial to various downstream tasks.",1,neutral
"• RGB mask Following MIM [41, 11, 36, 14, 3], we randomly mask blocks of 2D input images and take an additional UNet-like decoder after the feature extractor to reconstruct the masked regions.",2,positive
"Note that the masked language modeling (MLM) [8] in natural language processing and the masked image modeling (MIM) [3, 41, 11, 14] in computer vision both mask the inputs to the model and predict the masked components in the output space, enforcing the model to learn global features from the neighbour words or pixels.",1,neutral
"Motivated by the fact that masked modeling [3, 8, 41, 14] , i.",1,neutral
"Pre-training techniques, as one of the self-supervised learning approaches, can leverage a big model to learn the general representations with amounts of unlabeled dataset [12, 19, 33, 43].",1,neutral
"Large pre-trained models have achieved substantial results in many areas including natural language processing [12, 33], computer vision [7, 19] and software engineering [2, 15, 17, 18, 64].",1,neutral
", 2018), denoising or recovering partiallymasked input (Vincent et al., 2008; He et al., 2022), or encouraging learned representations to be invariant to a set of handcrafted representations (Chen et al.",1,neutral
"…(Noroozi & Favaro, 2016), predicting rotations (Gidaris et al., 2018), denoising or recovering partiallymasked input (Vincent et al., 2008; He et al., 2022), or encouraging learned representations to be invariant to a set of handcrafted representations (Chen et al., 2020; Caron et al.,…",1,neutral
Understanding the inherent spatial redundancy in local representations proves beneficial for learning visual representations for segmentation tasks [4].,1,neutral
"Following (He et al., 2022), during linear probing we include BatchNorm without the affine transformation before the final classifier.",1,neutral
", 2021) or masked reconstruction (He et al., 2022) objectives.",2,positive
"This can be operationalized using contrastive (Radford et al., 2021; Jia et al., 2021) or masked reconstruction (He et al., 2022) objectives.",2,positive
", 2018), images (He et al., 2022), videos (Feichtenhofer et al.",2,positive
"For audio, we use a fixed 2D sinusoidal positional encoding as described in (Huang et al., 2022) and (He et al., 2022).",2,positive
"Most previous approaches have focused on single modalities, such as text (Devlin et al., 2018), images (He et al., 2022), videos (Feichtenhofer et al., 2022), and audio (Baade et al., 2022; Chong et al., 2022).",2,positive
"MAE (He et al., 2022) is pre-trained by the masked image modeling (MIM) task (Bao et al., 2021) inspired by the success of masked language modeling in NLP (Devlin et al., 2018).",2,positive
"Masked Image Modeling (MIM) (Bao et al., 2021) pursues a different direction by learning to predict removed pixels (He et al., 2022), discrete visual tokens (Peng et al., 2022), or pre-computed features (Wei et al., 2022).",1,neutral
"MAE (He et al., 2022) is pre-trained by the masked image modeling (MIM) task (Bao et al.",2,positive
", 2021) pursues a different direction by learning to predict removed pixels (He et al., 2022), discrete visual tokens (Peng et al.",1,neutral
"ViT-B/16: MoCo v3 (Chen et al., 2021b), DINO (Caron et al., 2021), MAE (He et al., 2022), iBOT (Zhou et al., 2021) and CLIP (Radford et al., 2021).",0,negative
"Because most largescale vision models are based on masked image modeling (MIM) [12, 3, 6, 2], different prompt fusion methods activate knowledge at different locations in the large-scale model, affecting the downstream task performance.",1,neutral
"While these models can achieve impressive results on many tasks [12], they often require massive amounts of data and computation to train, making them impractical for many real-world applications.",1,neutral
"Previous studies [35, 3] have focused on combining images and labels into a new image and using MIM for pre-training, resulting in models with in-context learning capabilities.",1,neutral
The efects of model capacity have attracted signifcant attention in other felds like CV [15] and NLP [4] as it is demonstrated that SSL can largely beneft from increasing model parameters.,1,neutral
"SAM has already shown remarkable potential in accurately segmenting objects in realworld scenarios; its extensive training and zero-shot learning allow it to respond appropriately to any prompt at inference time [17, 18].",2,positive
"Recently, masked autoencoding has become a very popular approach for self-supervised Visual Transformer (VT) pre-training [2,7,16,20,35,45,60,61], together with other alternatives including contrastive learning [8,9,23,47,68,72,74] and selfdistillation [6, 18, 28].",1,neutral
"Among the various self-supervised pre-training strategies, masked autoencoding [2, 16, 20] is a prominent approach that has been widely explored.",1,neutral
"Due to its natural compatibility with the tokenwise representation in VTs, masked autoencoding has been explored for pre-training VTs on data across many fields, such as RGB images [2, 20, 61], pose data [11, 33] and 3D data [71].",1,neutral
"These works [2, 6, 20, 60] generally pre-train VTs on a large dataset in a self-supervised manner, allowing them to extract semantically meaningful and generalizable features without the need for annotated",1,neutral
"In many cases, we have access to large-scale unlabeled data containing such corruptions and noise, and would like to exploit them to pre-train a VT that can be effectively adapted to a wide range of downstream tasks [2, 11,20, 61].",2,positive
"Having rapidly risen in popularity in recent years, Vision Transformer (ViT) [16] and its variants [19] have shown impressive performance across various computer vision tasks, such as image classification, video recognition and 3D action analysis [2, 20, 31, 51, 60].",1,neutral
"Amongst this wave of research on Visual Transformers (VTs), there has emerged a popular paradigm – self-supervised VT pretraining [2, 6, 20, 60] – which has attracted a lot of attention in the research community.",1,neutral
"Previous works [2,16,20] have explored masked autoencoding to train Transformers, which we follow.",1,neutral
"Masked autoencoding [2,11,16,20,33,60,61,71] works by randomly masking a portion of input tokens or patches, and letting the VT reconstruct them.",1,neutral
"Due to the popularity of pre-training methods and their potential widespread applications in realworld scenarios [2, 11, 20, 33, 35, 71], it becomes important to improve their robustness to corrupted data, but this is often overlooked by previous methods.",1,neutral
"There’s another line of self-supervised learning work [2, 25] based on vision transformers, which naturally uses fixed-size patch level representation due to the structure of the vision transformers.",1,neutral
"One popular solution to this problem is the pretraining-finetuning approach, which has gained widespread adoption in natural language processing [7, 8] and computer vision [10, 11].",1,neutral
A missing component in most previous SSL studies (except MAE [15]) is input normalization although it is a basic and indispensable preprocessing step for effective training.,1,neutral
"We conducted experiments on six SSL methods: DINO [4], MoCo v3 [7], iBoT [35], Mugs [36], MAE [15], and MSN [1], using the Vision Transformer architecture (ViT) [10].",2,positive
MAE [15] is one of the representative methods of the masked image modeling (MIM) approach.,1,neutral
"Many studies have proposed effective learning strategies: contrastive learning that performs instance discrimination based on randomly augmented views [30, 5, 7, 2], a teacher-student framework that trains representations by using outputs of a momentum encoder as supervision [13, 11, 4, 21], and masked image modeling [15, 3, 23, 35] that aims to reconstruct randomly masked patches.",1,neutral
"At the same time, the pre-training & finetune paradigm has broadly applied to various visual recognition tasks because loading a pre-trained model usually can boost training convergence and performance [1, 8, 33, 14].",1,neutral
"Self-supervised learning methods [9, 14, 15] aim to pretrain a visual backbone with rich semantic representation, while the other parts of detectors designed for downstream tasks are ignored and usually initialized randomly.",1,neutral
"[83] also use a similar training strategy to pretrain a CV model, which makes a great success on the downstream tasks in the CV community.",2,positive
"Masked autoencoder (MAE) [83] develops an asymmetric encoder-decoder architecture to couple the self-supervised reconstruction and backend training, yielding a promising transfer performance for the downstream tasks.",2,positive
"Compared to MAE, MB1 outperforms MAE by 2% in both UF1 and UAR, approximately.",2,positive
"We utilize the encoder and decoder parts of μ-BERT (without DMA and POI) to train previous SSL methods (MoCo V3 [4], BEIT [1], MAE [9]) and then continue learning the MER task on the large-scale database CASME3 [14].",2,positive
"Three self-supervised methods (MoCo V3, BEIT, and MAE) got better results when they were pretrained on CASME before fine-tuning to the recognition task.",1,neutral
"We utilize the encoder and decoder parts of µ-BERT (without DMA and POI) to train previous SSL methods (MoCo V3 [4], BEIT [1], MAE [9]) and then continue learning the MER task on the large-scale database CASME3 [14].",2,positive
"Especially, MAE [9] achieves 3.5% and 2% up on UF1 and UAR compared to ViT-S, respectively.",2,positive
"He et al., [9] presented a Masked Auto Encoder (MAE) that utilizes the BERT framework.",2,positive
"Transformers and deep learning have significantly improved results for many tasks in computer vision [1,7,9,22, 23, 26, 27, 30, 40, 41].",1,neutral
", [9] presented a Masked Auto Encoder (MAE) that utilizes the BERT framework.",2,positive
"Regarding the pixel prediction, the per-patch normalization [34] consistently helps the fine-tuning accuracy.",1,neutral
MAE [34] predicts pixel colors with an efficient asymmetric architecture.,1,neutral
"3), and is outperformed by concurrent self-supervised pre-training algorithms such as Masked Autoencoders (MAE) [34].",1,neutral
scratch pre-trained MAE [34] ViT-L 304M 82.,0,negative
"6% top-1 [34], which aligns with our observations on fine-tuning the pre-trained ADM.",0,negative
"Inspired by the work of SimpleClick [35], we employ large models for feature encoding, such as the widely used MAE-pretrained Vision Transformer (ViT) [21].",2,positive
"For instance, ViT-Base (ViTB) [21] patchifies the input image of size H ×W into a sequence of 16×16 patches, which are then projected intoC0dimensional vectors.",1,neutral
"As our ViT backbones are pre-trained on 224 × 224 pixel images by MAE [21], we resize the pre-trained absolute positional embeddings to match the size of our images.",2,positive
"More recently, FocalClick [10] employed SegFormer for interactive segmentation, and SimpleClick [35] introduced the MAEpretrained ViT [21] into interactive segmentation.",1,neutral
"In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 [19] (for MAE [16], we use ViT-base [11].) encoder Eg for 200 epochs.",2,positive
"In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 [19] (for MAE [16], we use ViT-base [11].",2,positive
"Recently, He et al. propose MAE [33] and yield a nontrivial and meaningful generative self-supervisory task, by masking a high proportion of the input image.",1,neutral
Global Pre- ImageNet Relative Rotation MAE BYOL SimSiam training Method [28] Loc [10] Pred [21] [16] [14] [7] w/ MoCo [17] 78.,1,neutral
"Inspired by the masked autoencoding module, et al. [30] generalized the concept of MAE [13] to 3D point cloud and achieved some improvements.",2,positive
[30] generalized the concept of MAE [13] to 3D point cloud and achieved some improvements.,1,neutral
"The masked attention is widely used [4, 9, 25] for invalid-token masking, self-supervised training [19, 44], image inpainting [25], etc.",1,neutral
"For self-supervised pretraining, we take inspiration from recent contrastive learning and masked image modeling methods [3, 8, 11, 13, 34, 118] as they can learn both objectlevel global representations and part-level local features.",2,positive
"• MAE: Masked Autoencoders [15]: ViT-B16, ViTL16.",2,positive
"Many examples exist in the image domain for the training of representation models via solving explicit proxy tasks [16,17,55,59,83], discriminating instances through contrastive learning [10,27,32,72], optimizing clustering and representation [2,7,8], bootstrapping knowledge with self-distillation [9,11,23] or image reconstruction with masked autoencoders [3, 26].",1,neutral
"Originally inspired by the way human learning works, pre-training is now a fundamental element of high performance DL [194, 195].",1,neutral
"Motivated by scalability and powerful pretraining methods, we use an MAE [47] pre-trained Vision Transformer (ViT) [33] minimally adapted to process high resolution inputs [62].",2,positive
"Unless otherwise specified: (1) SAM uses an MAE [47] pre-trained ViT-H [33] image encoder and (2) SAM was trained on SA-1B, noting that this dataset includes only automatically generated masks from the final stage of our data engine.",0,negative
"Motivated by scalability and access to strong pre-training, we use an MAE [47] pre-trained Vision Transformer (ViT) [33] with minimal adaptations to process high resolution inputs, specifically a ViT-H/16 with 14×14 windowed attention and four equally-spaced global attention blocks, following [62].",2,positive
We initialize SAM from an MAE [47] pre-trained ViT-H.,2,positive
"While our model is initialized with a selfsupervised technique (MAE [47]), the vast majority of its capabilities come from large-scale supervised training.",2,positive
"For a comprehensive comparison, we also compare TDMR with MRKD which means a simple combination of model reprogramming (MR) and knowledge distillation (KD), and transfer learning method linear probing [7], [80].",2,positive
"For the foundation model application to downstream tasks, a common transfer method is the linear probing [7], [80] which just modifies the output dimension of the teacher classifier to the total number of categories of the target data.",1,neutral
"Subsequent work such as ViT [13] and MAE [21] have adapted this approach to the computer vision domain with great success, and transformer models trained on large scale data have become the de facto computer vision backbone.",2,positive
"Subsequent work such as ViT [13] and MAE [21] have adapted this ap-1 Work mainly done while author was at Allen Institute for AI proach to the computer vision domain with great success, and transformer models trained on large scale data have become the de facto computer vision backbone.",2,positive
Backdoored MAE vs. defended MoCo-v3.,0,negative
We compare finetuned ViT-B models trained with MoCo-v3 and MAE in Table 5.,2,positive
"We find that MoCo-v3 defended with PatchSearch and i-CutMix is better than MAE both in terms of Acc and FP for 1% labeled
finetuning data, but MAE quickly catches up in the 10% regime.",2,positive
"Restrictions apply.
from [44] indicate that MAE is robust against backdoor attacks.",0,negative
"In addition to ResNet-18 [27], we also conduct experiments on ViT-B [17] with MoCo-v3 [13] and MAE [25].",2,positive
NGswin Dense connection [20] Merged multi-scale encoder features Asymmetric [17],2,positive
"As this recovery process requires the information in the surrounding areas of each pixel [7, 17, 82] and CNN is conventionally good at extracting local features, proper use of CNN is essential.",2,positive
", computer vision [42, 63, 100], which suggests that Transformer has become a unified backbone architecture for both NLP and computer vision.",2,positive
"In contrast, self-supervised vision models [4, 14, 13] learn to encode pixels by keeping the representation of different augmented views being consistent.",1,neutral
"Compared to the above text-supervised models, selfsupervised vision models show some emerging properties on grouping pixels into spatially-consistent regions [4, 13, 14, 6].",1,neutral
"Self-supervised learning, also known as unsupervised learning, aims to learn good visual representations of images without any human-defined labels [4, 13, 14, 6].",1,neutral
"Self-supervised vision methods [4, 13, 14] learn to encode pixels into semantic features by keeping different augmented views consistent, and the consistency can be further enhanced with the self-distilling process [12].",1,neutral
"Considering that the pixels in images have heavy spatial redundancy [13], they are encoded inconsistently, resulting in coarse and spurious groupings of regions in Fig.",1,neutral
"Lots of methods [14, 15, 41] are built to enhance the representation learning of models and achieve superior results in downstream tasks.",1,neutral
"Following previous pre-training approaches [14, 25], we use the default image input size of 224×224.",2,positive
Masked autoencoder (MAE) [14] randomly masked patches and reconstructed the missing region.,1,neutral
"Motivated by the success of unsupervised learning in NLP, some self-supervised learning methods [2, 6, 14, 15, 28, 38, 40, 41] are introduced for vision tasks.",1,neutral
Comparison with Hiera [52]: We show class-level performance (average precision and relative gain) of Hiera [52] (pre-trained on using MAE [24]) and ours.,2,positive
"Overall our method has a gain of +2.8 mAP compared to Video MAE [15, 49].",0,negative
"To further explicitly supervise motion feature generation, inspired by MAE [18], we design a decoder (Figure 3(b)) to reconstruct pixel motion dynamics.",2,positive
"MaskDP, on the other hand, leverages masked auto-encoding (He et al., 2022), a bi-directional sequence modeling technique to improve the generalization of BC.",1,neutral
"While the most straightforward way to augment diverse visual features is to employ discrete hard masks as recent trends [13, 45], which aim to reconstruct images, our approach suppresses information in images using the soft masks with real-valued ar X iv :2 30 4.",2,positive
"As the core of VLM, various vision-language pre-training objectives [14], [18], [20], [26], [81], [82], [83], [84] have been designed for learning rich vision-language correlation.",2,positive
9: Illustration of masked image modelling [82].,1,neutral
"The detailed pre-training hyper-parameters are in Table 7, which mainly follows the training settings used in the original MAE [37].",0,negative
"Following the autoencoding pipeline in the original MAE [37], the encoder only takes visible embedded patches as input, and the decoder is input with all the embedded patches for masked patch reconstruction.",2,positive
"Inspired by masked language modeling [6,23], masked image modeling (MIM) approaches are proposed for learning unsupervised image [37, 92] or video representations [31, 74], which have been shown to be effective for many downstream tasks including image classification, object detection and video action recognition.",1,neutral
The masked autoencoder (MAE) model [37] consists of an encoder and a decoder.,2,positive
"As can be seen, the implementation is simple and neat, which could be flexibility incorporated into existing approaches like MAE [37].",2,positive
", 75%) and training hyper-parameters of MAE [37] to pretrain the TwinMAE and DropMAE models.",2,positive
"Inspired by the great success of self-supervised learning in NLP, recent advances [37, 92] in computer vision suggest that training large-scale vision transformers may undertake a similar trajectory with NLP.",1,neutral
The seminal work MAE [37] reconstructs the input image from a small portion of patches.,1,neutral
"MAE [12] exploited an autoencoder architecture to reconstruct the raw normalized RGB pixels of the masked patches, without the need of passing masked tokens into the encoder.",2,positive
"Take MAE[12] for example, it masks patches of the source images, feeds the
ar X
iv :2
30 4.",1,neutral
"Recently, Masked Image Modeling(MIM)[12, 2, 29, 31, 1] methods have aroused great interest in the community.",1,neutral
"Take MAE[12] for example, it masks patches of the source images, feeds the ar X iv :2 30 4.",1,neutral
"In recent years, Autoencoder (AE)-based methods predominantly rule the DR space [20, 21, 22] where multiple variations of AE have been developed to address the problem of ‘Curse of Dimensionality’ in several application domains including computer vision and computational biology.",1,neutral
"SpectralMAE requires performing masking operations in the spectral dimension, in contrast to imageMAE [36], which applies random masking operations in the spatial dimension.",1,neutral
"Additionally, some other self-prediction methods are designed to drop a part of the input signal and recover the missing input component for the predicted output in training (Devlin et al. 2018; Bao et al. 2022; He et al. 2022).",1,neutral
"The recently proposed MAE [10] follows the high-level idea of masked auto-encoding meanwhile carefully designing the masking strategy, the encoder and the decoder according to the properties of images.",2,positive
"For example, BERT [9] in NLP and MAE [10] in CV adopt self-supervised learners through pre-training to leverage the inherent co-occurrence dependencies of data so that they can capture the underlying general and universal patterns of sentences and images, respectively.",1,neutral
"As proven in [10], a narrow decoder is enough for the MAE task, so we set L′′ to 1.",1,neutral
The computational challenges introduced by global attention mechanisms were later addressed by Masked Autoencoders (MAE) through high image masking strategies [3].,1,neutral
", the LaCViT-trained MAE [3], achieves an increase of 10.",1,neutral
"Transformers have significantly advanced the field of computer vision, particularly in tasks such as image classification [1, 2, 3].",1,neutral
"While several works have attempted to mitigate these issues by either incorporating convolutional neural networks or modifying the transformer architecture [8, 9, 10], these approaches often negate the native advantages of transformers such as training efficiency and scalability [2, 3].",1,neutral
"• Extensive experiments on eight image classification datasets, shows that LaCViT significantly outperforms baseline models (e.g., the LaCViT-trained MAE [3], achieves an increase of 10.78% on Top-1 Accuracy compared with the original MAE on CUB-200-2011).",2,positive
"Since VC-1 was trained with MAE (He et al., 2021), it captures features that are generally useful for reconstructing images.",2,positive
"In this work, we use one such pre-training algorithm (MAE (He et al., 2021)) to explore scaling and adapting pre-trained visual representations (PVRs).",2,positive
"We now ask: how much does the relevance and diversity of the pre-training dataset and the model size matter? To study this, we fix the pre-training objective – MAE (He et al., 2021) – and then vary the composition of the pre-training dataset and the size of the visual backbone (ViTB with 86M parameters and ViT-L with 307M parameters).",2,positive
"Recently, a flurry of works have proposed using the vision transformers (ViTs) (Dosovitskiy et al., 2021) with masked image modeling (He et al., 2021; Baevski et al., 2022a; Yao et al., 2022), which among other benefits reduces the computation time required for pre-training.",2,positive
"The last few years have seen increasing interest in the self-supervised learning (SSL) of visual representations (He et al., 2021; Caron et al., 2020; Baevski et al., 2022b; Chen et al., 2020; 2021).",2,positive
"Experiment Details of Training PVRs
To train the MAE models, we use the official codebase released by the authors on GitHub (He et al., 2021) and use the default hyperparameters provided by the repo to train the ViT-B and ViT-L models.",2,positive
", 2022b), or reconstructive (Bao et al., 2021; He et al., 2021) objectives for training.",2,positive
"Specifically, in MAE adaptation we continue training the backbone network with the MAE (He et al., 2021) pre-training objective on task-specific data.",2,positive
"These algorithms use contrastive (Chen et al., 2020; 2021), distillation-based (Caron et al., 2020; Baevski et al., 2022b), or reconstructive (Bao et al., 2021; He et al., 2021) objectives for training.",2,positive
"We train vision transformers (ViT-B and ViT-L) (Dosovitskiy et al., 2020) on these 4 datasets using Masked AutoEncoding (MAE) (He et al., 2021), and systematically analyze their performance on CORTEXBENCH.",2,positive
", 2020) on these 4 datasets using Masked AutoEncoding (MAE) (He et al., 2021), and systematically analyze their performance on CORTEXBENCH.",2,positive
"End-to-end (E2E) fine-tuning with a task-specific loss function can in-principle capture both of the aforementioned benefits of adaptation, and is widely used in computer vision literature (He et al., 2020; Caron et al., 2021; He et al., 2021; Baevski et al., 2022b).",1,neutral
", 2021) with masked image modeling (He et al., 2021; Baevski et al., 2022a; Yao et al., 2022), which among other benefits reduces the computation time required for pre-training.",2,positive
"To study this, we fix the pre-training objective – MAE (He et al., 2021) – and then vary the composition of the pre-training dataset and the size of the visual backbone (ViTB with 86M parameters and ViT-L with 307M parameters).",2,positive
"As the final step in this paper, but as a first step towards this open problem, we study adapting VC-1 with either task-specific training losses or datasets (via MAE (He et al., 2021)) to specialize VC-1 for each domain.",2,positive
"In this work, we refer to a certain attribute of an element as a field and formulate the various design tasks as a unified masked field prediction, which is inspired by the recent masked autoencoders [9,15] and multi-task models [19,36].",1,neutral
CV community [66] and has been successfully applied to,2,positive
"Results for the Masked Autoencoder (He et al., 2022) (Appendix B.",0,negative
MAE [23] proposed a simple yet effective asymmetric framework for masked image modeling.,1,neutral
Self-supervised learning has achieved remarkable results on large-scale image datasets [23].,1,neutral
"MAE [21] encoder, with reduced embedding dimension to 480, depth to 8, and heads to 8 for efficient computation.",1,neutral
"The structural configuration of the dual-attention Transformer follows the design of the MAE [21] encoder, with reduced embedding dimension to 480, depth to 8, and heads to 8 for efficient computation.",2,positive
"Masked prediction has been successful in unimodal areas such as language (BERT [81], GPT [82]), and vision (MAE [19]), and its popularity has been increasing in multimodal areas due to its ability to unify different modalities.",1,neutral
", as famously realized by instance discrimination [18] or masked prediction objectives [19].",1,neutral
"As shown in Table 8, the CSformerT pre-trained for 30 epochs significantly performs better than without pre-training, but worse than pre-trained for 60 epochs, which shows that the performance improves steadily with longer pre-training [29].",0,negative
"Specifically, we apply a linear layer to project the latent features Fl to patch pixels [86], and compute the mean squared error (MSE) between the reconstructed and original images on the masked pixels [29].",1,neutral
"MAE [29] finds that masking a high proportion of the input image can contribute to meaningful self-supervised learning, and proposes an asymmetric encoder-decoder structure to reduce pre-training time.",1,neutral
"Among them, masked autoencoders (MAE) [7, 110, 29, 86, 28], which pre-train image models by predicting masked tokens from seen tokens, have demonstrated superior learning ability and scalability on various high-level vision tasks.",1,neutral
"Similar to MAE [29] and SIMMIM [86], our MAEIP is a simple autoencoding approach, which masks a portion of image signals and learns to reconstruct them.",2,positive
Learning image representation is a more difficult task [29].,1,neutral
"Masked autoencoders such as BEiT [7], MAE [29], and SimMIM [86], first embed image patches to tokens, and then adopt a random mask on input tokens.",1,neutral
"We follow the conventions in [29, 86] and mask random patches with 16× 16 pixels, and adopt a high masking ratio i.",2,positive
"Besides the improvement of architectural design, recent self-supervised learning frameworks, such as DINO [10], MOCO-V3 [17], MAE [29], have further unleashed the potential of ViT and achieved high performance on various high-level vision tasks [32, 30].",2,positive
"Following MAE [27], ẑ is then “unmixed” to recover the input batch before mixing by inserting a special [MASK] token with M j .",1,neutral
"Specifically, adding color jittering, an essential augmentation technique of contrastive learning [9], with MAE [27] even degrades transfer results, suggesting that MIM might possess a different preference for data augmentations, and the effective data augmentation strategies for MIM are still an open question.",1,neutral
", random [3, 27], attention-guide [31] and sample-dependent [50]).",1,neutral
"In this paper, we explore the usage of image mixing, a commonly used technique in both supervised [60, 61] and contrastive learning [49, 59], with MAE [27].",1,neutral
"and the reconstruction target due to the redundancy of image signals [27], naı̈ve mixing will instead increase the MI, and thus, ease the reconstruction pretext task.",1,neutral
"In this section, we start by adopting mixing in MAE [27] with a simple baseline in Sec.",1,neutral
"Specifically, MixedAE surpasses MAE [27] consistently with only 3% extra overhead, while outperforms the strong iBOT [63] with only 53.",0,negative
MAE [27] proposes an asymmetric encoder-decoder architecture for better efficiency.,2,positive
", visual tokenizers [3, 17], pixels [27,58], graphical features [54] and instance discrimination [2, 19, 63]) and masking strategies (e.",1,neutral
"Instead of random masking [3, 27], AttMask [31] proposes a novel attention-guided masking strategy by masking according to the attention map of the final Transformer layer, while ADIOS [50] introduces an adversarial objective between masking and reconstruction to generate learnable masks for MIM pre-training.",2,positive
"Recently, breakthrough frameworks have been developed based on masked image modeling (MIM) (He et al., 2022; Bao et al., 2021; Tong et al., 2022).",2,positive
", 2021), MAE (He et al., 2022), and CAE (Chen et al.",2,positive
"By reconstructing pixels, MIM methods produce visual features that are more generalizable.",1,neutral
"Meanwhile, the feature representations via SSL are more generalizable to benefit downstream recognition scenarios (Grill et al., 2020; Chen et al., 2021; Xie et al., 2021; He et al., 2022; Wang et al., 2021).",2,positive
"Implementation Details: We follow most of the practices of [1, 8].",2,positive
"To this end, we closely follow the approach of MAE [8], and employ the Masked Image Modeling strategy.",2,positive
"With the advent of Vision Transformers (ViT) [7], similar strategies such as Masked Image Modeling have been developed for computer vision [2, 8, 24], showing equally high benefit in complex computer vision tasks.",1,neutral
We tailor the MAE approach for the endoscopic setting with three modifications: Layer Wise Learning Rate Decay: The MAE encoder and decoder consist of several layers.,2,positive
"Among the self-supervised learning tasks, masked image modeling (MIM) [3, 23, 58, 62, 64, 68] achieves SoTA finetuning performance on ImageNet [14].",1,neutral
"While iGPT [10], ViT [17], and BEiT [3] adopt sophisticated paradigm in modeling, MAE [23] and SimMIM [63] show that directly regressing the masked continuous RGB pixels can achieve competitive results.",1,neutral
"Paired Masked Image Modeling (MIM) MIM is extensively adopted in image classification task [23, 63].",1,neutral
"Recently, researchers use semantic masks to facilitate representation learning [4, 8, 37], where a mask predictor is required.",1,neutral
"Recently, masking strategy has been widely utilized in various language [9, 23, 2] and visual applications [1, 12, 41, 38, 43, 28] to learn meaningful representation.",1,neutral
"Especially, the image masking strategy is used to pre-train a large capacity backbone model to learn general representation for various downstream tasks, such as recognition [12, 41], video applications [38], 3D application [28].",1,neutral
"This unsupervised learning style normally requires numerous data and computation resources [41], [42], so we put the training of it on the resourceful cloud which can collect a lot of data from multiple edges.",2,positive
"Inspired by the progress of Masked Image Modeling (MIM) in image classification [12,39,40], P-STMO [32] applies Masked Joint Modeling to 3D HPE with self-supervised learning.",1,neutral
"Kaiming He recently proposed a patch-level occlusion and reconstruction model called MAE [18], which is based on the ViT [13] autoencoder, dif-",2,positive
"Nevertheless, since the introduction of the MAE[18] method, autoencoders based on pure Transformer[34] have gained attention and have been applied to a variety of downstream tasks[9, 15, 37].",1,neutral
"In this section, we will first revisit the original image-version Masked Autoencoders (MAE) (He et al., 2022) and the video extension of Masked Autoencoders (Feichtenhofer et al., 2022), then detail the tailored MAE for the specific tasks.",2,positive
"In detail, (a) depicts the original MAE proposed by He et al (He et al., 2022), (b) represents the customized version of MAE pre-trained on task-specific data, and (c) is tailored by replacing the transformer architecture of the original MAE with the pure convolution neural network.",2,positive
"To implement our belief in a regressive manner where the learned loss function can be treated as image prior, the first choice is the masked autoencoders (MAE) proposed by He et al (He et al., 2022).",2,positive
"In this section, we will first revisit the original image-version Masked Autoencoders (MAE) (He et al., 2022) and the video extension of Masked Autoencoders (Feichtenhofer et al.",2,positive
"To solve this issue, we adopt an approach inspired by MAE [22], where we only feed the non-masked embeddings to the student .",2,positive
"Due to the sucess of pre-trained Transformer architectures in various domains [2, 9, 16, 22], we recently see a shift towards pre-training Transformer-based approaches for point clouds [28, 31, 49, 51].",1,neutral
[22] show that moving masked embeddings to a deferred shallow decoder reduces memory requirements and training time significantly.,1,neutral
"At the same time, self-supervised training has shown impressive results in natural language processing [16, 47], speech [3, 25], and 2D vision [2, 9, 12, 21, 22], enabling learning of meaningful representations from massive unla-",1,neutral
"This is an important difference to other masked-prediction methods such as BERT [16] and MAE [22], where the targets only comprise local information, e.",1,neutral
"The success of self-supervised learning in 2D vision [2, 4, 5, 9, 12, 21, 22, 42], natural language processing [2, 16], and speech [2, 3] has inspired a number of recent works proposing self-supervised learning frameworks for point cloud understanding tasks.",1,neutral
"Only recently, we have seen self-supervised methods being successfully applied to Transformer architectures for 2D vision [2, 9, 22] and 3D point clouds [32, 49, 51].",1,neutral
"With the emergence of MAE [16], some works [53, 12, 5] combine multi-modality learning with MAE-based pre-training paradigm and achieve great representation capabilities.",1,neutral
"Following the framework of [49], I2P-MAE [53] utilizes projected multi-view 2D depth maps to guide 3D point cloud pre-training.",2,positive
"For instance, if BERT4ETH follows the original masking ratio (15%) and uses 85% unmasked addresses to predict 15% masked addresses, the masked addresses have a high likelihood of being present in the unmasked addresses, leading to an overly easy prediction task [12].",1,neutral
", 2022f) for example, supports a video masked encoder for MAE (He et al., 2022) losses in addition to a module similar to ALBEF.",1,neutral
"InternVideo [52] for example, supports a video masked encoder for MAE style losses in addition to a module similar to ALBEF.",1,neutral
"We train the tokenizers of the columns C, the encoder, and the decoder by following the approach of the masked autoencoder (He et al., 2022); that is, we randomly select the columns with a masking ratio and replace the corresponding embeddings with a special embedding called a mask token.",2,positive
"While the original masked autoencoder in He et al. (2022) uses its decoder only in the pretext task and removes it in the downstream task, our model uses the decoder trained on the pretext task in the downstream tasks.",2,positive
"Furthermore, we note that our model does not use the [cls] token, unlike the approach by He et al. (2022).",2,positive
"First, TabRet is pre-trained based on the reconstruction loss with masking augmentation (Devlin et al., 2019; He et al., 2022).",2,positive
"In He et al. (2022), whose modality of interest is images, models have to solve qualitatively different problems as the pretext and downstream tasks – for example, image inpainting for the pretext task object classification for the downstream task.",1,neutral
"Recent works on generative modeling have also learned efficient representations for both global and dense prediction tasks like classification [28, 33, 13, 8, 19] and segmentation [46, 82, 10, 3, 9].",1,neutral
"Given the limited data scale, we leverage mask modeling [32] to make good use of the video data.",2,positive
"The progress in video understanding is currently driven by the Image Foundation Models (IFMs) [23, 32, 6, 62, 37], which are trained from massive datasets and adapted for different downstream tasks [18, 90, 99, 61].",2,positive
"BeiT [7] is the first to propose a BERT-like mask-then-predict framework to recover the discrete tokens [63], while MAE [32] designs masked autoencoders to reconstruct normalized pixel values, which reduces memory consumption by processing only unmasked tokens in the encoder.",1,neutral
This asymmetric encoder–decoder structure ensures the encoder learns rich semantic features and reduces the pretraining time significantly [32].,1,neutral
Masked Autoencoders (MAEs) [32] are self-supervised pretraining models based on an encoder–decoder structure that enable the encoder to learn visual representations by reconstructing the masked image.,1,neutral
"For plain architectures, we adopt the MAE [31] pre-training strategy to provide a good initialization for QFormerp, which has been proven effective in mitigating over-fitting in plain ViTs.",2,positive
"The official MAE pre-trained weights for the backbone are utilized, and the entire model is finetuned for 100 epochs on the MS COCO dataset.",0,negative
"For large models, we use plain ViT as the reference architecture and adopt the MAE pretrained [31] weights for initialization.",2,positive
"The hyperparameter for the proposed regularization term is set to 1, and all other parameters follow the same settings as in [31].",1,neutral
"The MAE pre-trained weights are used to initialize the backbone, and the whole model is fine-tuned for 160k iterations.",2,positive
"We use the official MAE pre-trained model to initialize the ViT-B backbone and the default training settings in MMPose, i.e., an input image size of 256×192 and a learning rate of 5e-4.",2,positive
"The UPerNet [38] is adopted as the segmentation head, following the common practice [11], [31], and the default training setting in MMSegmentation [39] is adopted.",1,neutral
"For the plain models, we adopt the fine-tuning practice proposed in [31], which involves pre-training the model on the ImageNet-1K dataset for 1,600 epochs using the ImageNet-1K training set, followed by supervised fine-tuning.",2,positive
Masked Autoencoders [19] are scalable self-supervised learners.,1,neutral
We overcome the above-mentioned issues by leveraging generic Masked AutoEncoder (MAE) [19].,2,positive
We use the publicly available Masked Autoencoders [19] pretrained on ImageNet to assist blind defense.,2,positive
Masked Autoencoders (MAE) [19] are scalable self-supervised learners based on Vision Transformer [12].,1,neutral
A novel framework of Blind Defense with Masked Autoencoders (BDMAE) is devised to detect possible triggers and restore images on the fly.,2,positive
"Masked Autoencoders (He et al., 2022) are scalable self-supervised learners.",1,neutral
"We use two pretrained Masked Autoencoders (He et al., 2022) that are available from their official repository.",2,positive
"In terms of supervision, in addition to the standard supervised pre-training on ImageNet-1k, we also consider self-supervised methods MoCo-V3 [17], SwAV [11], DINO [12], MAE [32] trained on ImageNet-1k, the weakly supervised SWAG [76] trained on 3.6B uncurated instagram images and CLIP [59] trained on 400M image-language pairs [71].",2,positive
"In terms of supervision, in addition to the standard supervised pre-training on ImageNet-1k, we also consider self-supervised methods MoCo-V3 [17], SwAV [11], DINO [12], MAE [32] trained on ImageNet-1k, the weakly supervised SWAG [76] trained on 3.",2,positive
"It is common to use large scale self-supervised [11, 12, 15, 17, 32, 33] and weakly-supervised [37, 53, 76] pre-trained models as starting points in various downstream applications.",1,neutral
"IN 1k MOCO-IN 1k
SWAV-IN 1k DINO-IN 1k MAE-IN 1k
SWAG-IG 3.7B CLIP-LAION 400M
Figure 7.",0,negative
6B (ViT-B-swag-3B) shows significant improvements over self-supervised training methods like MAE (ViT-B-mae-IN1k) and DINO (ViTB-dino-IN1k).,2,positive
"Another interesting direction is the observation of stepwise behavior in masked-image modeling frameworks, which currently constitute a large fraction of the SSL literature (Baevski et al., 2022; He et al., 2022; Assran et al., 2023).",1,neutral
"Masked auto-encoders assign mask tokens to an encoded latent space [16, 44], and adopt them to masked sequences at the decoder level.",1,neutral
"Previous approaches have employed transformer decoder-level mask tokens and linear interpolation (LERP)-based tokens have been explored to work around this issue [10, 16, 31].",1,neutral
"the performance of our architecture against the BERTbased motion in-painting transformer [10], the encoderdecoder-based ∆-interpolator [31], the RNN-based approach TGcomplete [15], and the masked auto-encoder (MAE) architecture [16].",2,positive
"Among them, TTT-MAE [399] is a recent extension of TTT that utilizes the transformer backbone and replaces the self-supervision with masked autoencoders [401].",2,positive
"Moreover, inspired by MIM [2, 10], we propose a simple yet effective strategy named Masked Pose and Shape Estimation, which helps the global transformer further mine the inter-frame correlation on human motion.",2,positive
", masked Language modeling (MLM) [6] in NLP, masked image modeling (MIM) [2, 10] in CV, enhance the representation of transformers.",1,neutral
"Except for the design of the structure, some strategies for pretraining transformers, e.g., masked Language modeling (MLM) [6] in NLP, masked image modeling (MIM) [2, 10] in CV, enhance the representation of transformers.",1,neutral
"During the global decoder phase, following [10], we pad the SMPL tokens onto the masked location and send the whole sequence into the global decoder to obtain a long-term representation.",2,positive
"the multiple discriminative features and spatial information [9, 21, 29, 34, 47, 48, 59], so it is more helpful to use SSL pre-trained encoder than label-supervised encoder for robustly performing on diverse downstream tasks.",1,neutral
"Discriminative SSL methods [11, 22, 27, 79] learn the embeddings by enforcing closeness and/or distantness on the pairwise distance structure among the augmented training samples.",1,neutral
"Beyond the label supervision [31, 66], selfsupervised learning (SSL) [12,25,29,30,88,92] approaches that do not rely on human-annotated labels hit the machine learning community.",1,neutral
"Beyond the label supervision [28, 55], selfsupervised learning (SSL) [11,22,26,27,75,79] approaches that do not rely on human-annotated labels hit the machine learning community.",1,neutral
We conjecture that an SSL pre-trained encoder is desirable to capture the demanding diverse semantics instead of a supervised one learned from pre-defined labels.,1,neutral
"Given a frozen prediction model Pθ(y|x), and perturbed image x̃ with prompt corresponding to the label y, the training objective is formulated as:
argmin ϕ
− logPθ;ϕ(y|x̃)
While VP and AR optimize the input space visual prompt directly, we reparameterize the visual prompt to the prompt generation network hϕ(·) parameterized by ϕ = {ϕd, ϕt} ∈ Rd. Specifically, we build a novel autoencoderstyle network named Coordinator composed of a frozen encoder f(·) which is pre-trained on ImageNet [14] by selfsupervised learning (SSL) objective and followed by an extremely light-weight learnable decoder gϕd(·).",2,positive
"Therefore, for Coordinator, BlackVIP adopts an SSL encoder (i.e., Masked Auto-Encoder [26]).",1,neutral
"Though the encoder can also be a supervised counterpart or lightweight learnable network, we adopt the SSL pre-trained encoder for the following three reasons: 1) It has been widely substantiated that self-supervised representation contains
the multiple discriminative features and spatial information [8, 19, 26, 31, 42, 43, 50], so it is more helpful to use SSL pre-trained encoder than label-supervised encoder for robustly performing on diverse downstream tasks.",1,neutral
SSL approaches can roughly be categorized into discriminative and generative approaches.,1,neutral
"Meanwhile, the recently emerging generative SSL methods [4, 29, 88] are based on masked image modeling, which supervises the model by encoding and reconstructing the partially masked individual images.",1,neutral
"Consequently, the image equipped with a prompt (prompted image) is constructed as follows:
x̃ = clip(x+ ϵhϕ(x)) hϕ(x) = gϕd(zx, ϕt)
where zx = f(x) is the feature vector of x from the frozen SSL encoder f(·), and ϵ ∈ [0, 1] is a hyperparameter that controls the intensity of visual prompt.",1,neutral
"SSL approaches are appealing not only due to their label-free training regime but also produce a more transferable representation [9, 29, 48] getting over the pre-defined label category.",0,negative
"SSL approaches are appealing not only due to their label-free training regime but also produce a more transferable representation [8, 26, 43] getting over the pre-defined label category.",0,negative
"Meanwhile, the recently emerging generative SSL methods [4, 26, 75] are based on masked image modeling, which supervises the model by encoding and reconstructing the partially masked individual images.",1,neutral
6 confirms that the SSL encoder outperforms the supervised pre-trained or randomly initialized encoder (scratch).,2,positive
We exploit an SSL pre-trained encoder while we plug the randomly initialized extremely lightweight decoder.,2,positive
"Concretely, the computation of (2) can directly benefit from just dropping all the masked patches before forwarding them, in a similar manner to masked autoencoder [12].",1,neutral
"MaskAHand can be viewed as an extension of the “masked image modeling” paradigm [20,54,56] to “masked hand grounding”.",1,neutral
"Recent works [27, 9, 6, 21, 8, 28] attempted to decode fMRI signals based on pre-trained generative models like Instance-Conditional GAN [3], diffusion models [17], masked autoencoders [14], CLIP [31], to name a few.",1,neutral
"(4) Inspired by the principles of self-supervised training [41, 15, 14], we leverage this methodology to train our encoder-decoder architecture.",2,positive
"Inspired by the advantage of long-range receptive fields from transformer layers, we follow MinD-Vis [6] to adopt the architecture of masked autoencoder [14] as the encoder-decoder model for fMRI signals.",2,positive
The only difference from MAE is that we finetune on iNaturalist21 rather than iNaturalist17.,2,positive
"As a base model, we consider the ViT-Base model using the Masked AutoEncoder (MAE) pretraining setup [16], which leads to state-of-the-art results for this general task.",2,positive
"For iNaturalist, we consider a Masked AutoEncoder (MAE) [16] pre-trained model and finetune it following the same procedure and hyperparameters as the original paper.",2,positive
"Mask modeling has been proven to be effective in both recognition learning [9, 14, 16] and generative modeling [7, 32].",1,neutral
"To meet the high-quality image generation requirements of the diffusion model, the sideinterpolater is placed in the middle of the network instead of the end of the network in recognition models [3, 16].",1,neutral
"In comparison, mask modeling for recognition models commonly calculates loss on masked tokens [3,16].",1,neutral
"In vision recognition, pretraining schemes that utilize mask modeling enable good representation quality [54], scalability [16] and faster convergence [14].",1,neutral
"Existing literature pays significant attention to both the unsupervised pretraining [7,13,14,18] and supervised finetuning [26].",0,negative
"Some recent research [2,14,19,45] explores generative methods that predict the missing content inside input samples, also achieving promising performance over vision transformers.",1,neutral
"In the most recent work, STEP [247] proposed a pretraining model combined with the Mask Auto-Encoder (MAE) [248] architecture to efficiently learn temporal patterns from very long-term history spatio-temporal graph data.",1,neutral
"In the predictive branch, the STG decoder directly outputs the prediction results and traditional data point errors, such as mean absolute error (MAE), can be used as the loss function.",1,neutral
"design a unified and efficient architecture to process crossmodal data since Transformer [36] has shown the flexibility and superiority in vision [37], [38], [24] and language [39], [40], [41] modeling.",1,neutral
work and has shown improvements in vision [24] and language processing [25].,1,neutral
"tasks [39], [40], [41] then entered vision [46], [37], [38], [24] and 3D field [47], [48], [49], [50], [42].",1,neutral
"Self-supervised learning aims to learn indicative feature representations from unlabeled data, which are then used to assist downstream supervised learning tasks [28, 29, 30].",1,neutral
"Though the scRNA-seq data is distinct from images, our results show that the performance gains of xTrimoGene are comparable to those of MAE, with more efficient training and better downstream task performance.",2,positive
"Unlike MAE, xTrimoGene utilizes the biased masking strategy to avoid the learning process being dominated by zero tokens.",2,positive
"Similarly, the principle of asymmetric encoder-decoder design has been proven powerful in masked autoencoders (MAE) (He et al., 2021), which is tailored for CV data pre-training.",1,neutral
"To further tackle the mode collapse challenge in unsupervised learning and scale up the optimization iterations, inspired by recent masked autoencoders [23, 57], we construct a masked point modeling paradigm where both point color reconstruction objective and surfel normal reconstruction objective are proposed to recover the masked color and geometric information of the point cloud respectively.",2,positive
"Based on a denoising autoencoder [48]-style architecture, the task is to reconstruct the RGB value [23,57], discrete token [3,60], or feature [50] of masked pixels.",2,positive
"This varies from the conclusion of MAE [23], in which a higher masking ratio of 75% achieves top performance.",0,negative
"In 2D unsupervised learning, there is also a recent trend of switching the pretext task from instance discrimination [4, 6, 7, 20, 24] to masked image modeling [3,23,50,57,60].",1,neutral
"Motivated by the success of masked image modeling [23, 57] in 2D representations, we propose masked point modeling, which can be naturally integrated into our contrastive learning framework.",2,positive
"To this end, we turn to use MAE [19], a SOTA image encoder pre-trained in an unsupervised manner, similar to MLM.",2,positive
"To this end, we turn to use MAE [18], a SOTA image encoder pre-trained in an unsupervised manner, similar to MLM.",2,positive
"We find MAEbase-MLM clearly improves the standard MAEbase on HM with the TS model, but obtains marginal gains with the E2E model.",2,positive
"We extract the ViT features from the same ViT architecture training in different ways: (b) supervised ViT [17], (c) self-supervised DINO [43], and (d) MAE [44].",2,positive
"Fortunately, the difference can be significantly revealed by the ViT features, especially the MAE [44] shown in Fig.",1,neutral
"The second and third types, called DINO [43] and MAE [44], respectively, are based on selfsupervised learning frameworks.",1,neutral
using transformer feature representations extracted from pretrained masked autoencoder (MAE) [44].,1,neutral
"Note that other popular image feature extractors [3, 9] can also be applied in our framework.",2,positive
"[26] pre-trained fMRI data using a method similar to MAE [27], and fine-tuned the LDM [28] using the extracted characterizations from the 2D fMRI structure to obtain reconstructed images.",1,neutral
"Chen et al. [26] pre-trained fMRI data using a method similar to MAE [27], and fine-tuned the LDM [28] using the extracted characterizations from the 2D fMRI structure to obtain reconstructed images.",1,neutral
"MAE was initially used in images [7], dividing a picture",1,neutral
"Given the simplicity and efficiency of Masked AutoEncoding (MAE) [33], we leverage it as the self-supervised prepretraining approach.",1,neutral
We follow [33] to train MAE models on IG-3B without using any labels.,0,negative
"In particular, we show that (i) MAE not only scales with model size as shown in [33], but also with the size of the training data (Figure 2).",1,neutral
"Of particular interest to us is MAE [33] for its state of the art performance on many transfer tasks [25, 28, 33, 46, 74] and its computational efficiency.",2,positive
"With the advent of Vision Transformers [23], approaches based on reconstructions such as [5, 33, 78] got renewed interest for their simplicity and state of the art performance.",1,neutral
We first begin with the Masked Autoencoder (MAE) [33] self-supervised learning technique to pre-pretrain vision models without using any labels.,2,positive
We compare the performance of MAE pretraining on the large scale IG-3B with the original MAE [33] models trained on IN1k for 1600 epochs.,2,positive
Pre-pretraining (MAE) [33] learns visual representations from image datasets without using any labels.,1,neutral
We follow the same hyperparameters used in [33] for pretraining on IN1k.,2,positive
"We follow SimpleClick [26] to build the interactive segmentation model, which consists of two patch embedding modules for image and click map respectively, a ViT [10] backbone initialized with MAE [16], a simple feature pyramid [21], and an MLP segmentation head.",2,positive
"As shown in Table 3, the model initialized with MIS pre-training the MAE [16] counterpart especially on the relatively hard dataset (i.e., SBD and DAVIS).",2,positive
"As shown in Table 3, the model initialized with MIS pre-training the MAE [16] counterpart especially on the relatively hard dataset (i.",2,positive
"Following the principle of MAE [27], we randomly mask out some tokens, and utilize the EAT encoder to extract high-level latent features.",2,positive
Masked auto-encoding [27] masks a portion of input data and adopts an auto-encoder to reconstruct explicit features (e.,1,neutral
"Inspired by the successful applications of transformers in NLP [16], [26] and image region [27], [28], a lot of 3D vision backbones have been proposed.",1,neutral
"Finally, MAE [18] relies on a masked autoencoder pipeline and a reconstruction objective to learn dense representations.",1,neutral
"As MAE does not rely on a cross-view consistency objective, this approach is well-suited for scenecentric datasets and of particular interest to us.",2,positive
"…contrastive learning (He et al., 2020; Chen et al., 2020c;a; Oord et al., 2018), and non-contrastive learning (Grill et al., 2020; Zbontar et al., 2021; Chen & He, 2021; Bardes et al., 2022; Wei et al., 2022; Gidaris et al., 2021; Asano et al., 2019; Caron et al., 2020; He et al., 2022).",2,positive
", 2018), and non-contrastive learning (Grill et al., 2020; Zbontar et al., 2021; Chen & He, 2021; Bardes et al., 2022; Wei et al., 2022; Gidaris et al., 2021; Asano et al., 2019; Caron et al., 2020; He et al., 2022).",2,positive
"Compared with previous MIM works [2, 22, 68], using the same 300 epochs of pre-training, our CIM can achieve better performances with both ViT-S/16 and ViT-B/16 models.",2,positive
"Compared with previous MIM works [2, 26, 79], using the same 300 epochs of pre-training, our CIM can achieve better performances with both ViT-S/16 and ViT-B/16 models.",2,positive
"In terms of how to mask, most MIM approaches, such as BEiT [2], MAE [26] and SimMIM [79], extend the mask-word recipe in MLM to randomly mask image patches in the spatial domain.",1,neutral
"Beyond augment-and-compare or mask-and-predict pretext tasks in MV-SSL and MIM, in this paper, we endeavor to investigate another simple yet effective paradigm for selfsupervised visual representation learning.",1,neutral
"Among existing pretraining paradigms, Multi-View Self-Supervised Learning (MV-SSL) [9, 11–14, 25, 27] and Masked Image Modeling (MIM) [2, 26, 65, 79] are two leading methods in the selfsupervised learning racetrack, thanks to their nontrivial and meaningful self-supervisory pretext tasks.",1,neutral
"Overall, our CIM is a simple yet effective approach that can perform on par or better than existing MV-SSL and MIM methods with both ViT and ResNet models.",2,positive
"As to what to predict, beyond default raw pixels [26, 79], several other reconstruction targets are proposed, e.",1,neutral
"Due to the architectural difference between ViT and CNN models, we observe performance degeneration of some MIM and MV-SSL pretraining methods, such as SimMIM [68], MoCo v2 [10], and SimSiam [11].",2,positive
"On the contrary, following the success of Masked Language Modeling (MLM) [16], MIM conducts a mask-and-predict pretext task within a single view (Figure 1(b)) – removing a proportion of random image patches and then learning to predict the missing information.",1,neutral
"Following previous works [2, 26], we fine-tune ViT-S/16 for 200 epochs, and ViT-B/16 for 100 epochs.",2,positive
"Unlike existing MV-SSL and MIM approaches, CIM considers correlation modeling in visual tracking as a useful pre-training paradigm.",1,neutral
"Among existing pretraining paradigms, Multi-View Self-Supervised Learning (MV-SSL) [8–12, 21, 23] and Masked Image Modeling (MIM) [2, 22, 54, 68] are two leading methods in the selfsupervised learning racetrack, thanks to their nontrivial and meaningful self-supervisory pretext tasks.",1,neutral
"Our CIM is a general framework that can learn meaningful representations for both ViT and CNN architectures, unlike state-of-the-art methods such as MAE [26].",2,positive
"Our CIM is a general framework that can learn meaningful representations for both ViT and CNN architectures, unlike state-of-the-art methods such as MAE [22].",2,positive
"Two key steps can be identified in a typical MIM pipeline: i) how to mask, ii) what to predict.",1,neutral
All these initiatives are proven less effective than the state-of-the-art MIM and MV-SSL approaches in large-scale visual pre-training.,2,positive
2) We demonstrate the advantages of our CIM in learning transferable representations for both ViT and ResNet models that can perform on par or better than the current state-of-the-art MIM and MV-SSL learners while improving model robustness and training efficiency.,2,positive
"In terms of how to mask, most MIM approaches, such as BEiT [2], MAE [22] and SimMIM [68], extend the mask-word recipe in MLM to randomly mask image patches in the spatial domain.",1,neutral
"For all ablation experiments, we choose ViT-Base (ViT-B/16) as the default backbone and follow a common setting used in existing works [2, 26]: 300-epoch self-supervised pre-training without labels and 100-epoch supervised end-to-end fine-tuning, to evaluate the quality of learned representations.",2,positive
"We omit the results in other metrics (NDCG, MAE MAPE) and on other data as their trends are similar.",2,positive
"In recent years, masked autoencoding has found versatile applications thanks to several groundbreaking practices, such as BERT [13] in natural language processing (NLP) and the very recent MAE [24] in computer vision (CV).",1,neutral
"Q2: for typical prediction tasks, how to design a principled family of loss functions and training scheme for dynamic graphs? In recent years, masked autoencoding has found versatile applications thanks to several groundbreaking practices, such as BERT [13] in natural language processing (NLP) and the very recent MAE [24] in computer vision (CV).",1,neutral
"We also adopt the mean absolute error (MAE), rooted mean squared error (RMSE) as well as mean absolute percentage error (MAPE) to evaluate the model accuracies.",2,positive
"The concatenation of unmasked patches’ embedding and masked tokens are processed by the decoder to predict the unmasked patches [17, 43].",1,neutral
Prior work [17] showed that MAE is both efficient at reducing redundancy in feature representations and capturing detailed information from holistic image statistics.,1,neutral
"This paper presents DRAM, a test-time defense using masked autoencoder (MAE) [17], one of the strongest SSL tasks, to detect and repair adversarial attacks, including unforeseen ones.",2,positive
Masked autoencoder (MAE) [141] (see Figure 8) simplifies it to an end-to-end denoising framework by predicting the masked patches from the unmasked patches.,1,neutral
The second class of AI techniques mainly consists of backbone architecture (like Transformer [443]) and self-supervised pretraining (like BERT [87] or MAE [141]).,1,neutral
MAE structure (figure obtained from [141]).,1,neutral
"Outperforming contrastive learning and negative-free joint-embedding methods, MAE has become a new variant of the visual SSL framework.",2,positive
Masked Autoencoders (MAE) [30].,1,neutral
"One of the key observations of the MAE work [30] is that the decoder does not need to be very good for the encoder to achieve good performance: by using only a small decoder, MAE successfully trains a ViT in an auto-encoder fashion.",2,positive
Masked Autoencoders (MAE) [30] perform a random masking of the input token and give the task to reconstruct the original image to a decoder.,1,neutral
"Then, rapidly, a wide variety of other tasks have been conquered by Transformer-based architectures, such as object detection [27], image segmentation [28], self-supervised learning [29, 30] and image generation [31, 32].",1,neutral
"Self-supervised visual representation learning has led to great success in image benchmarks [10, 28, 8, 27].",1,neutral
"ViC-MAE pre-training follows previously used configurations [27, 21].",0,negative
"Compared to a model that uses masked image modeling, the original MAE [27] and to the MaskFeat model [55], our model underperforms by 0.",2,positive
"More recently, self-supervised learning approaches based on masked auto encoders (MAE) [27] rely on masked image modeling adapted to video data to pre-train models.",1,neutral
"These methods work by randomly masking out parts of the input and forcing a model to predict the masked parts [3, 27, 21, 55].",1,neutral
"First, inspired by [28, 97], we randomly mask some joints and require the model to reconstruct them.",2,positive
"Generative modeling trains the model to reconstruct the entire original image, or some target regions within it [36], from its corrupted [37] or masked [38] variants.",1,neutral
"Inspired by the spirits of these works, this paper provides a thorough evaluation of MIM visual representation learning [5, 148, 133, 51] that significantly bridge the gap between large-scale visual representations that achieve stateof-the-art performance and models that are affordable and accessible for the wider research community.",2,positive
"Following the evaluation procedure in [51, 45], all these models are first fine-tuned on the original IN-1K training set, and then directly evaluated on different val sets using the same fine-tuned model without further hyperparameter selection and specialized fine-tuning.",0,negative
"weight initialization JAX random initialization [14, 51] MIM teacher EVA-CLIP vision encoder [45] image data source IN-21K / IN-21K / IN-21K / Merged-38M peak learning rate 3e-3 / 3e-3 / 1.",0,negative
"Starting with the baseline ViT configurations used in the original BEiT series pre-training [5, 92, 123] (∗ in Table 2), we progressively refine the model design and make the following observations: (i) The performance of SwiGLU FFN is mediocre with the random weight initialization method used in BEiT, but works quite well with JAX weight initialization [14, 51] (+1.",2,positive
") [121], as well as ObjectNet (ObjNet) [6], following the settings in [51, 45].",1,neutral
"Still, we note that this efficient decoder retains much of the modeling capacity of standard masked decoders [5, 8, 14] that employ full self-attention on context and mask {zL, zM}.",1,neutral
"Since the standard masked modeling [5, 8, 14] of applying a bidirectional transformer on mask and context {zM , zL} leads to a complexity of O((NM + NL)(2)), we opt into an efficient decoder that reduces the cost to linear to mask sizeO(NM ) while preserving as much modeling capacity as possible.",2,positive
"Various pretext tasks have been proposed to learn these representations, such as image inpainting [22], colorization [64], or prediction of the rotation [18] or position of patches [14, 4].",1,neutral
"The current state-of-the-art SSL methods integrate contrastive learning with Siamese networks to increase the similarity between two augmented versions of images [26, 2, 12, 10, 3], or use autoencoder to reconstruct the input images [8, 1, 11], while all these solutions assume training images are centrally available in cloud servers.",2,positive
"Recent advancement of Transformers has great achievements in computer vision [8, 11].",1,neutral
"The current most popular way of pretraining Transformer is MAE [11] and MaskFeat [25], which utilize the idea of self-supervised learning to predict features of the masked area.",1,neutral
"First of all, we pre-train a Masked Autoencoder (MAE) [13, 36] on a large-scale facial dataset in a selfsupervised manner.",2,positive
Our framework starts with pre-training a masked autoencoder (MAE) [9] from a set of face databases.,2,positive
Masked autoencoder (MAE) [9] is a self-supervised learned model which reconstructs original images from a set of masked images.,1,neutral
"The concise of both network and training scheme support its generalized representation of the real dataset [45,15].",1,neutral
Li et al. [23] also use MAE pre-trained weights combined with AffectNet supervised pre-trained weights and ranked 2nd in ABAW4.,0,negative
It also used MAE pre-trained weights to enhance its performance.,0,negative
"Differently, MAE [7] try to reconstruct a masked image to learn semantic features.",1,neutral
"To address this bottleneck, graph neural network (GNN) architectures [82], [114] were proposed and applied with success to model liquids and granular materials.",1,neutral
"Masked autoencoders [23, 3, 62] introduce BERT-like masked image modeling (MIM) pre-training for Vision Transformers [17], but they seem not natural and practical for convolutional networks.",1,neutral
"More recently, masked autoencoders (MAE) [23] have further highlighted the effectiveness of denoising pre-training, which can also be inherited by networks in diffusion models — resembling MAE’s de-masking, recovering images with large and multi-scale noise is a nontrivial task and may also require a high-level understanding of visual concepts.",1,neutral
"To verify transfer learning ability on scalable Vision Transformers beyond UNets, we compare ImageNet 2562 pre-trained DiT [43] to MAE pre-trained vanilla ViTs [17] on CIFAR-10
and Tiny-ImageNet.",2,positive
"Drawing from the connections between diffusion networks and denoising autoencoders (Section 3.1), we infer that DDAEs can produce linear-separable representations at some implicit encoderdecoder interfaces, resembling DAEs and MAEs.",1,neutral
"However, diffusion pre-trained DiTs may not be as efficient as MAE pre-trained ViTs on recognition tasks, since the former is specifically designed for advanced image generation without optimizing its representation learning ability.",1,neutral
"Evaluations on CIFAR-10 [34] and Tiny-ImageNet [35] show that our diffusion-based approach is comparable to supervised Wide ResNet [65], contrastive SimCLRs [12, 13] and MAE [23] for the first time.",2,positive
"The comparison between DDAE and MAE on transfer learning further suggests that de-masking may not be a necessary, essential, and optimal choice for vision.",1,neutral
"Computer Vision MAE [23] Encoder-only, MIM SemiMasked iGPT [11] Decoder-only, AR Full –",2,positive
Table 3 and Table 4 show that the scaled DiT-XL/2 outperforms the smaller MAE ViT-B/16 under all settings by large margins except for linear probing on CIFAR-10.,2,positive
"Following [10, 26], we randomly mask the features of joints after the embedding layer.",1,neutral
"Different from MAE [10] and some existing graph data augmentation methods [12, 23], we feed the topology of the original joints into the encoder.",2,positive
"However, MAE [10] uses the location information of the image patches for the decoder to assist in reconstructing.",2,positive
The great success of MAE [10] makes us rethink data augmentation.,2,positive
"Surprisingly, when we directly reconstruct masked joints using a method similar to that in MAE [10], the performance degrades instead.",1,neutral
We will discuss the relationship between MAE [10] and graph data augmentation in detail in Sec.,2,positive
"Inspired by MAE [10], we propose an augmentation framework named MGPose.",2,positive
MAE [10] uses an autoencoder architecture.,2,positive
"MAE [10] uses the encoder to extract features of the masked image patches, and then reconstructs the original image patches by the decoder.",2,positive
"Pre-training on self-supervised tasks such as masked image modeling and autoregressive text generation is effective for large language and vision models (Brown et al., 2020; He et al., 2022).",1,neutral
"Despite the empirical success in language and vision (Brown et al., 2020; He et al., 2022), their performance on graph data applications remains unsatisfactory because of the significant gap between the graph self-supervised task and the graph label prediction task.",2,positive
"Early work in fine-tuning focused on adapting pre-trained models through layer-wise fine-tuning [49, 48, 12], where only a subset of layers in the network were fine-tuned, while the rest of the network remained frozen.",1,neutral
"Basically, these techniques [3, 33, 11, 10, 44] pre-train a deep model on large-scale data and then adapt the pretrained model to novel tasks.",1,neutral
The second type of visual feature is MAE-based model [23].,1,neutral
"To address the burdens of collecting large-scale labeled datasets for supervised learning [16, 68, 84], self-supervised learning methods [12, 13, 25,28,29,79] have been introduced to learn general-purpose visual representations from unlabeled data.",1,neutral
"To overcome the data scarcity problem, SelfSupervised Learning (SSL) techniques are nowadays being widely used for computer vision tasks [9, 10, 16, 21, 23, 24]), and more precisely for text recognition [2, 52].",1,neutral
"However, the MAE original design does not take continual learning into consideration and thus can not generalize well both in the previous and current tasks.",2,positive
"As in MAE [23], the encoded tokens zT (Eqn.",1,neutral
"Thus, this work is based on Masked AutoEncoders (MAE) [23] for pre-training.",2,positive
"Specifically, MAE discards low-level information by masking a large portion of the image patches and enables the encoder to extract semantic information by reconstructing the pixels from a very small number of neighboring patches [6] with a lightweight decoder.",2,positive
"We implemented the 3D MAE using 3D-ResNetBlocks instead of Vision Transformer, different from the previous study [13], due to the constraint of GPU memory.",2,positive
"Following the previous study [13], we implement an asymmetric design with a lightweight decoder compared to the encoder.",2,positive
"Following the previous study [13], MSE calculated on masked regions is used as loss function.",1,neutral
", w/o MAE pretraining) in downstream tasks [13].",1,neutral
"To avoid information leakage, we use self-supervised MAE for model pre-training on split ImageNet-subset.",2,positive
One exception is fine-tuning MAE on ImageNet-subset where we found the transformations used for CPP generate inferior results and thus we follow the data transformations used in the original paper [20].,1,neutral
"DualPrompt [59] is originally designed using Transformer structures, so we only change the pre-trained weights to MAE to avoid information leakage.",2,positive
"To this end, we implement CPP with four up-to-date pre-training methods including ViT [14], Deit [53], Dino [5], and MAE [20] that sweep supervised and self/un-supervised learning as well as discriminative and generative models.",2,positive
"However, these reconstruction objectives used in works such as iBOT [40], BEiT [3] and MAE [17] are computationally expensive and rely on vision transformers exclusively.",1,neutral
"works such as iBOT [40], BEiT [3] and MAE [17] are computationally expensive and rely on vision transformers exclusively.",1,neutral
"In SSL, Self-attention has been widely used on generative frameworks [17, 3, 40], where they train the transformer backbone to reconstruct the given masked image.",1,neutral
"2) MIM: To handle unpaired images, we leverage the Masked Image Modeling (MIM) paradigm [59].",1,neutral
"Therefore, we use a multiway transformer to extract multi-modal features and two linear layers to solve PLM and MIM tasks, respectively [38], [59].",1,neutral
"Our VLP model achieved the best performance with a higher masking ratio of 85%, which is in contrast to the optimal masking ratio of 75% reported by MAE [59].",2,positive
"PosterV2-Vit feature: To acquire advanced visual features, we trained a Vision Transformer (ViT) [4] model on the AffectNet7 dataset [21] using unsupervised learning techniques [5].",2,positive
"Self-supervised Vision Transformers (ViT) [13], such as DINO [6], MAE [21], and BEiT [4], have demonstrated immense potential in unsupervised dense prediction tasks.",1,neutral
"Differently from reconstruction targets in natural language processing with rich semantics, reconstruction targets in computer vision are low-level pixels [15, 54].",1,neutral
"In this paper, we revisit deep supervision for masked image modeling (MIM) [15, 11, 48, 2], a self-supervised pretraining strategy for Vision Transformer [12] (ViT).",2,positive
"Recent MIM works have studied the question: what are appropriate reconstruction targets? Proposals have included discrete tokens [2], RGB pixels [15, 54], histograms of oriented gradients [48] and CLIP features [35, 23].",1,neutral
"In the same spirit as masked language modeling, MAE [15] and SimMIM [54] employ raw pixels as the targets for reconstruction.",1,neutral
"Then, similar to MAE [15], we randomly mask a high proportion of patches, yielding a masked image x̃.",1,neutral
"For concreteness, we use MAE [15] to illustrate our underlying approach.",2,positive
", RGB pixels [15, 54], discrete tokens [2], histograms of oriented gradients [48], CLIP features [23] and DINO features [4].",1,neutral
"Then ViT-B is finetuned on ImageNet-1K [9], following common practice [15].",1,neutral
"We follow the same training recipe of MAE [15], more details can be found in supplementary materials.",0,negative
"works [39, 15, 11] have explored more semantic reconstruction targets, e.",1,neutral
"In particular, the variance of ViT-B/16 pre-trained with MAE [20] is twice as large as that of the supervised pre-trained ViT-B/16.",1,neutral
"FULL 100% 79.1 86.2 59.7 75.0
Addition-based methods
MLP-3 1.60% 73.6 75.2 35.7 61.5 PROMPT-SHALLOW 0.04% 79.9 82.5 37.8 66.7 PROMPT-DEEP 0.23% 76.8 84.5 53.4 71.6 ADAPTER-8 1.18% 81.7 87.3 61.2 76.7 SPT-ADAPTER (ours) 0.33% 83.0 87.3 62.1 77.5
Reparameterization-based methods
baseline method on VTAB-1k benchmark with only 0.26% and 0.08% trainable parameters for MAE and MoCo v3 pretrained backbones, respectively.",0,negative
"We show more parameter sensitivity patterns for ViT-B/16 with various pre-training strategies (i.e., MAE [20] and MoCo V3 [11]) and datasets sampled from FGVC benchmark [24].",2,positive
"We conduct experiments on the plain vision Transformer backbone ViT-B/16 [13] that is pre-trained on ImageNet [27] with different pre-training strategies following [24], including supervised pre-training and self-supervised pre-training with MAE [20] and MoCo v3 [11] following [24].",2,positive
"Visualizations of sampled VTAB-1k datasets with MAE and MoCo V3 pre-trained ViTB/16 are shown in Figures 5, 6, 7.",1,neutral
Table 2: Comparisons on VTAB-1k [62] benchmark using self-supervised ViT-B/16 backbone pre-trained by MAE [20] and MoCo v3 [11].,2,positive
"As shown in Table 2, existing PEFT approaches exhibit inferior results than full fine-tuning with the self-supervised pre-trained
backbones MAE and MoCo v3.",2,positive
"Nearly all SSL methods [8, 11, 16, 17] proposed on natural image recognition utilize a small portion of annotations to get promising fine-tuning accuracy compared to full supervision, which is higher than Linear Probing [17] by a large margin.",1,neutral
"in [16,17], the vanilla FT method can improve about 15 percent of accuracy in IN-1K compared to vanilla Linear Probing [17].",1,neutral
"Self-supervised Learning (SSL) has shown to be a promising paradigm both in computer vision [4,8,11,16,17] and natural language processing [12,33,44].",1,neutral
"Similar to some visual attribution methods like CAM [37, 49], LRP [3] and patch masking [4, 16, 28] in ViT, an IB-based attribution method is proposed in [36] by adding noise to intermediate feature maps, restricting the flow of information, then how much information image regions provide can be quantified.",1,neutral
"The proposed framework only relies on annotations at WSI levels, which is similar to recent SSL approaches [8,11,16,17].",2,positive
"Recently, mask-based image augmentation has been proved an efficient way to extract global context information, especially combined with transformers [14, 17, 46].",1,neutral
"Masked image modeling (MIM) [2, 19, 42] trains models to predict masked regions of input images, a training mechanism we adopt for RFFR.",1,neutral
"For our RFFR model, we adopt a base version of Masked Autoencoder (MAE) [19] and train it on real faces with a batch size of 128.",2,positive
"Prediction can happen in the input space by, for instance, reconstructing one part of an image from another, as for autoencoders [19], or by predicting the next word in a sentence, as done in language models.",1,neutral
ing (MAE) [18] – an efficient self-supervised visual representation learning algorithm designed for pretraining vision transformers [13] (ViTs) – to improve the performance of our ViT-based agent.,2,positive
"A flurry of recent work on image and video understanding has found that visual transformers [13] (ViTs) powered by self-supervised representation learning can provide general-purpose visual representations for recognition [3, 11, 18] and generation [4, 6] tasks.",1,neutral
We find that Data2Vec [3] (row 3) attains similar performance to the MAE [18] (row 4) initialization we use in Sec.,2,positive
"Specifically, we find that visual representation learning (using masked autoencoding (MAE) [18]) not only improves performance, but also enables model scaling with ViTs.",1,neutral
Table 3: Visual pretraining using MAE [18] enables positive scaling of the ViT-BASE architectures on IMAGENAV.,2,positive
"Specifically, we propose novel FaceMAE module, a masked autoencoder [25] specialized for FER-W, to achieve the goal of FFL.",2,positive
"However, we designed FaceMAE, a masked autoencoder [25] specialized for FER-W, by making two major modifications to the original masked autoencoding scheme.",2,positive
"Masked autoencoding has strengths in context learning because a defined autoencoder infers the entire image with only limited information [50, 25].",1,neutral
"In the case of random masking, a certain level of context can be considered because the masked facial image must be reconstructed with extremely limited information due to the high masking ratio [50, 25].",1,neutral
The previously proposed MAE [25] with ViT-base [15] was used for the autoencoder architecture.,2,positive
"However, we notice that since MAE’s mask tokens only pass through the decoder [20], a shareddecoder design is critical in our scheme for mask tokens to learn mutual information before performing reconstructions in separate modal-specific decoders.",2,positive
"[20], which demonstrate a straightforward yet powerful pre-training framework for Vision Transformers [10] (ViTs) and show promising results for independent modalities of both 2D and 3D vision [2, 14, 17, 63, 64].",2,positive
We follow MAE [20] and Point-M2AE [64] to generate input tokens from images and point clouds.,2,positive
"Recently, inspired by advances in masked language modeling, masked image modeling (MIM) approaches [3,20,59] have shown superior performance, proposing a self-supervised training method based on masked image prediction.",1,neutral
"Consequently, we design a MAE [20]-structured multi-modal learning framework that incorporates projection alignment for more interactive multi-modal learning.",2,positive
"MAE [20], in particular, predicts pixels from highly masked images using a ViT decoder.",1,neutral
"For the image branch, we follow [20] to divide images into regular patches with a size of 16 × 16, before the ViT backbone.",1,neutral
"Towards this end, an interesting question may be raised: is there a principled way to automatically distill the important self-supervision signals for adaptive augmentation? Inspired by the emerging success of generative self-supervised learning in vision learner [12] with the reconstruction objectives, we propose an automated framework for self-supervised augmentation in graph-based CF paradigm via a masked graph auto-encoder architecture, to explore the following questions for model design.",2,positive
"This result is highly competitive and surpasses all other existing methods, even when compared with counterparts with vision transformer backbone [95, 67] or with advanced model pretraining mechanism [31, 67].",2,positive
"Due to the gap of information density [19] between languages and images, prompting for vision models is more challenging and complex.",1,neutral
"With the increasing scale of training data and model size, the pretraining-finetuning paradigm has shown remarkable achievement in many areas, including natural language processing (NLP) [4,13] and computer vision (CV) [2,7,8,19].",1,neutral
"Recently, equipped with a more aggressive masking strategy, SimMIM [62] and MAE [26] further demonstrate that simple pixel reconstruction can achieve competitive results from previous pre-training methods.",2,positive
"To examine the effectiveness of our method, we perform DPPMask on two representative MIM methods: MAE [26], iBOT [67], which represent two different MIM frameworks: pixel reconstruction and feature contrast.",2,positive
"Another key factor of successfully applying MIM is the masking ratio of input images [26, 62].",1,neutral
"Benefiting from the new network architectures like ViT [17], Masked Image Modeling (MIM) has become highly popular, and there is a series of more aggressive masking strategies like MAE [26], simMIM [62].",2,positive
"For the self-supervised learning models, DINO [5] and MAE [16], we additionally measure the endto-end performance of benchmark models pre-trained on our UnlabelledNAIP.",2,positive
"DINO [5] based on knowledge distillation and the generative model MAE [16] based on autoencoder, on our FireRisk.",2,positive
"For the self-supervised architectures, MAE [16] and DINO [5], we use ViT-B/16 [10] as the backbone and fine-tune on FireRisk using latent representa-",2,positive
"Using transfer learning, we fine-tune ResNet-50 [17], ViT-B/16 [10], as well as DINO [5] and MAE [16] with ViT-B/16 as the backbone, all of which were pre-trained on ImageNet [8], using our",2,positive
"• To investigate the performance of supervised and self-supervised learning on our FireRisk, we employ ResNet [17], ViT [10], DINO [5], and MAE [16] as benchmark models.",2,positive
"Inspired by the outstanding performance of ViT [10] for feature extraction, MAE [16] reconstructed randomly masked patches using the",2,positive
"On FireRisk, we present benchmark performance for supervised and self-supervised representations, with Masked Autoencoders (MAE) [16] pre-trained on ImageNet1k [8] achieving the highest classification accuracy, 65.",2,positive
"learning, we select two representative self-supervised models for their performance, namely DINO [5] and MAE [16].",2,positive
"In generating the latent representations, we use ViT-B/16 as the backbone architecture for MAE [16], pre-",1,neutral
"• We gather an unlabelled dataset, UnlabelledNAIP, from the NAIP remote sensing project and utilize it to pre-train novel latent representations of DINO [5] and MAE [16].",2,positive
"Among them, ViT-B/16-DINO and ViT-B/16-MAE are trained with self-supervised loss, and ViT-B/16-CLIP is trained on 400 million image-text pairs with contrastive loss.",1,neutral
"2 [23] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
", ResNet18/50/152 [24], ViT-B/16-IN1K/21K, ViT-L/16-IN1K, ViT-B/16-DINO [7], ViT-B/16-SAM [11], ViT-B/16-MAE [23], ViT-B/16CLIP [50] (image encoder) for a holistic evaluation, and report the results in Figure 6.",0,negative
"We choose publicly available PTMs, i.e., ResNet18/50/152 [24], ViT-B/16-IN1K/21K, ViT-L/16-IN1K, ViT-B/16-DINO [7], ViT-B/16-SAM [11], ViT-B/16-MAE [23], ViT-B/16CLIP [50] (image encoder) for a holistic evaluation, and report the results in Figure 6.",0,negative
"Additionally, we report the backbones adopted in the ablation study, including ViT-L/16-IN1K, ViTB/16-DINO [7], ViT-B/16-SAM [11], ViT-B/16-MAE [23], ViT-B/16-CLIP [50] (image encoder), in the table.",2,positive
"3M MoCo-v2 [7], MAE [24] TOTAL (47) 32M",0,negative
"(3) RefCOCO, RefCOCOg, RefCOCO+ [81] 60K MLM with PEVL text encoder [78] Phrase Grounding (1) Flickr30K [79] 32K MLM with PEVL text encoder [78]
Visual Relationship Detection (1) Visual Genome [41] 101K MLM with PEVL text encoder [78] Visual Commonsense Reasoning (1) VCR [84] 100K MLM with PEVL text encoder [78]
Self-Supervised Learning (2) ImageNet-1K [10] 1.3M MoCo-v2 [7], MAE [24] TOTAL (47) 32M
them for specific downstream task.",2,positive
", classification [26], contrastive learning [6, 25, 54] and masked image modeling [1, 24, 76].",1,neutral
An example of a pretext task is to retain part of the input data to be predicted by a model that is trained on the other part of the data [85].,1,neutral
"Similar to MAE [22] and PointMAE [37], we compute the loss only on masked parts.",1,neutral
"Following MAE [22] and VideoMAE [51], we adopt the asymmetric encoder-decoder design to reduce computation.",2,positive
"One of the most popular methods is MAE [22], which randomly masks input patches and trains the model to recover masked patches in pixel space.",1,neutral
"One of the most promising self-supervised methods is the masked autoencoders (MAE) [22] which achieve success in various tasks [37, 51].",1,neutral
The random masking approach in the original MAE [16] cannot adaptively perceive the variation of information density and applies a unified masking probability over the entire image.,1,neutral
MAE [16] and SimMIM [42] show that masking a high ratio of patches and directly predicting RGB values can achieve BEiT-level performance.,2,positive
"Inspired by the masked language modeling framework, masked autoencoder (MAE) [16, 42, 23], also known as masked image modeling (MIM), has been introduced to computer vision and become a promising self-supervised learning method based on Vision Transformer (ViT) [12].",1,neutral
(a) The original MAE [16] randomly masks 70% image patches with a uniform probability.,1,neutral
"We use 3 alternative models to initialize the feature extractor in the mask generator: (1) the ViT-B network pretrained with MAE [16] (termed as MAE-800), (2) the ViT-B pretrained with iBOT [48] (termed as iBOTB), and (3) the ViT-S pretrained with DINO [3] (termed as DINO-S).",2,positive
"However, based on some recent research, the MAE method may not have strong domain generalization capability compared to the second approach.",2,positive
"The first approach is to reconstruct the input image using masked auto-encoders (MAE) [11] directly, and the second is to introduce the pairing text descriptions of images as weak supervising labels.",1,neutral
"Also, since the methods
in this section refer to MAE, a comparison test is done between the methods in this section and MAE using the same training method.",1,neutral
"The self-supervised pretraining method in this paper takes reference from MAE, but differs from it in that MAE uses two identical structures of ViT as encoder and decoder, while our method uses a symmetric convolution-deconvolution structure for the autoencoder.",2,positive
"The resulting data are shown in Table III, from which it can be seen that in each of the four datasets, our method is higher than MAE by more than 3 points in each metric.",0,negative
"Meanwhile, to better prove the reliability of the method proposed in this chapter, we conducted peer-to-peer experiments using MAE, i.e., we first performed masked self-supervised
learning pre-training, and then selected the one with the lowest training loss model for the pedestrian re-identification task.",2,positive
"In 2022, Kaiming He proposed MAE [31], which enables the network to easily cope with various computer vision downstream tasks by reconstructing important regions in images for pre-training.",2,positive
"A straightforward solution can be obtained from Masked Image Modeling (MIM) [2,9].",1,neutral
"Considering ViT’s flexibility and great potential in masked image modeling [9,14], we explore acceleration algorithms based on the standard ViT.",2,positive
"We implement a baseline inspired by MIM [2,9].",2,positive
", EViT [15] and DynamicViT [21], to sparse encoding, and masked image modeling (MIM) [9,2] to token completion.",1,neutral
"A-TA requires model-specific adapters to adapt different pre-trained models, e.g., the residual adapter TSA [25] for ResNets [15], the self-attention adapter eTT [55] for ViTs [9].",1,neutral
"Over the past years, a community-wide enthusiasm has been ignited to narrow this gap, especially in fields such as computer vision [15,26,47], machine translation [5, 31, 54] and reinforcement learning [11, 17, 39].",1,neutral
"Various attempts have been made to adapt the pre-trained models to few-shot tasks by devising model-specific adapters, e.g., the residual adapter TSA [25] for ResNets [15], the self-attention adapter eTT [55] for ViTs [9].",1,neutral
", the residual adapter TSA [25] for ResNets [15], the self-attention adapter eTT [55] for ViTs [9].",1,neutral
"Additionally, traditional image augmentation methods often make limited adjustments at the feature level due to the vast amount of redundant and irrelevant information in digital images [28, 14].",1,neutral
"Instead of a random formulation [9, 30], we sample a fixed ratio γ of the tokens [M ] (X) to be masked out according to a multinomial distribution related to the patch hand saliency m(X):",1,neutral
"Then this prior is encoded as the token form [9], and a ViT [15] sketcher is trained to disentangle the corresponding structure tokens from partial image patches [30].",1,neutral
"Inspired by this, we implement an MAE by masking the outputs of our encoder, Fo, and then passing the masked encoded features along with the masked tokens to our decoder.",2,positive
"As illustrated in Figure 1 (b), we adopt a simple Regression head composed of two deconvolution layers and one 1×1 convolution layer on the reshaped Fo following the common setting of the previous works [23].",1,neutral
"Masked Auto-Encoders (MAE) for pre-training transformer networks has shown strong results on a variety of applications such as NLP [24], pose estimation [11], and image classification [23].",1,neutral
"We initialize the Temporal ViTPose weights from the pre-trained model of MAE [23], and perform masked region reconstruction with a masking rate of 75%.",2,positive
This 3D-ViT was then embedded in the MAE approach of [13].,1,neutral
"One leading self-supervised approach is the masked autoencoder (MAE) [13], which was developed on natural imaging data.",1,neutral
"Weights of the trained model of [13], developed on ImageNet, were used to initialize the transformer layers in the encoder, while weights of the encoding layer and the decoder were randomly initialized.",1,neutral
"Regarding selfsupervised models, the masked autoencoder (MAE [30]), DINO [7], MoCov3 [10], MSN [2] were selected, because they all include the base ViT (ViT-B/16) for comparison between pretraining schemes (Fig.",0,negative
"There has been a lot of recent work in self-supervised learning where the input is masked and the model is tasked at reconstructing the missing pixels (He et al., 2022; Vincent et al., 2010; Assran et al., 2022).",1,neutral
"The video features contain much redundant information, while the text features are more semantic and have higher information density [9].",0,negative
"Partial Fintuning is a setting between head finetuning and full finetuning [23], which finetunes the last several layers while freezing the others.",1,neutral
Side length of the random 3D patches is set to 16 voxels following He et al. (2022). xsub is initialized to Gaussian noise.,2,positive
"Self-Supervised Multimodal Representation Learning via M3AE: Masked autoencoders (MAEs) have been proven successful as scalable self-supervised vision learn-
ers (He et al. 2022), where the pretext task is to reconstruct the original signal given its partial observation.",1,neutral
"Here, we make a modification to the original MAE (He et al. 2022) by replacing the masked contents with x and processing them in the same way as non-masked ones, instead of discarding them.",2,positive
"Therefore, we empirically set an even higher combined masking rate of 87.5% (compared to 75% used by He et al. (2022)) in our M3AE to make the self-supervising task nontrivial.",2,positive
"ers (He et al. 2022), where the pretext task is to reconstruct the original signal given its partial observation.",1,neutral
"A notable difference between the original MAE for natural images and our M3AE is that, masked patches of the former can only be inferred from surrounding context, whereas those of the latter can be additionally inferred from other modalities and thus expected to be easier.",2,positive
"Here, we make a modification to the original MAE (He et al. 2022) by replacing the masked contents with xsub and processing them in the same way as non-masked ones, instead of discarding them.",2,positive
"Above all, witnessing the recent success of masked autoencoders in learning rich visual representations (He et al. 2022), we propose multimodal masked autoencoder (M3AE), where a random subset of the modalities and random patches of the remaining ones are masked simultaneously.",1,neutral
"Therefore, we sample a random subset of the modalities for masking to mimic the real situation, in addition to randomly masking 3D patches of the remaining modalities as in the original MAE for natural images.",1,neutral
"Evaluation of Representation: So, why is the Slow Learner such effective, and what accounts for the remaining performance gap? We perform a linear probing experiment [12] to evaluate the performance of the representation layer.",1,neutral
"In addition to supervised pre-training, we consider representative self-supervised paradigms that provide pre-trained checkpoints on ViT-B/16, i.e., MoCo v3 [4], BEiT [2] and
MAE [12].",2,positive
"exceeds that of the more recent MAE [12], although their joint training performance is comparable.",0,negative
"Considering architectural consistency with previous works of CLPM [43, 42], we select representative self-supervised methods (i.e., MoCo v3 [4], MAE [12] and BEiT [2]) that release checkpoints on ViT-B/16 in our comparisons.",2,positive
", MoCo v3 [4], MAE [12] and BEiT [2]) that release checkpoints on ViT-B/16 in our comparisons.",2,positive
"Interestingly, the performance of Seq FT w/ SL for MoCo v3 [4] far
exceeds that of the more recent MAE [12], although their joint training performance is comparable.",2,positive
"DeiT [33] is a strong supervised method for (pre-)training vision transformer, while MoCo v3 [4], MAE [12] and BEiT [2] are representative self-supervised methods.",1,neutral
We utilize the mask ratio 25% and 8 decoder blocks following the practices in MAE [4].,2,positive
"To address the above issues, we propose to Mimic before Reconstruct for Masked Autoencoders, termed as MR-MAE, which is a simple and effective strategy to enhance MAE [4] by regularizing the intermediate representations with pre-trained off-the-shelf feature encoders.",2,positive
Compared with the original MAE base model [4] (83.,0,negative
"BeiT [18], MAE [4], CAE [36] have validated Masked Image Modeling (MIM) paradigm to be effective approaches for pre-training vision transformers.",1,neutral
"Although DINO and CLIP exhibits strong objectness cues and open-world recognition ability, the fine-tuning performance on downstream tasks are inferior to representations learned through MAE [4] manner.",1,neutral
"Secondly, MAE [4] employs an asymmetric architecture with a heavy encoder and a light decoder, where the encoder is preserved after pre-training for downstream transfer learning.",2,positive
Masked Autoencoders (MAE) [4] employ an asymmetric encoder-decoder design for computationally efficient masked image modeling.,1,neutral
"Different from the high-level supervisions in language modeling, the low-level RGB signals of MAE [4] is too primitive and redundant, which fail to unleash the full understanding capacity of masked autoencoding on downstream vision tasks.",1,neutral
"Motivated by this, Masked Autoencoders (MAE) [4] explore how to adopt MLM paradigm into vision representation learning with a vision transformer [5] of asymmetric encoder-decoder architectures.",1,neutral
"Motivated by MLM, masked image modeling (MIM) was proposed to boost the visual pretrained models [20, 63, 7].",1,neutral
"In comparison, the pre-training tasks in vision area are various, e.g., supervised pre-training [15], masked image modeling (MIM) [20, 7], and masked visual token modeling (MVTM) [2, 44].",1,neutral
"The supervised pre-trained models are not equipped with generative task, and MIM pre-training task recovers each patch in pixel space, which lacks semanticrich representations.",1,neutral
", supervised pre-training [15], masked image modeling (MIM) [20, 7], and masked visual token modeling (MVTM) [2, 44].",1,neutral
"…learning successes across many domains follow a common recipe: rst pre-training large and expressive models on general-purpose, Internetscale data, followed by ne-tuning the pre-trained initialization on a limited amount of data for the task of interest (He et al., 2022; Devlin et al., 2018).",2,positive
"Modern machine learning successes across many domains follow a common recipe: rst pre-training large and expressive models on general-purpose, Internetscale data, followed by ne-tuning the pre-trained initialization on a limited amount of data for the task of interest (He et al., 2022; Devlin et al., 2018).",1,neutral
"High-quality representation learning has been a core topic in deep learning research, which is challenging for computer vision due to the low information density [19, 18].",1,neutral
"Inspired by the popular masked image modeling [12, 13], we build a universal masked autoencoder architecture for VAD by embedding random mask tokens to simulate anomalies, which is a simple yet efficient synthetic method that avoids extra data processing in the original normal data.",2,positive
"As shown in Figure 1, fine-tuning [10, 24] and linear probing [15, 30] are two commonly used methods for this adaptation.",1,neutral
"SimpleClick [15] greatly improved performance by adopting the Plain Vision Transformer (Plain ViT), which was pretrained with MAE [9], as the backbone of the RITM approach.",2,positive
"The image encoder is initialized with the first 12 layers of MAE-base (He et al. 2022) weight, which is pre-trained on the ImageNet-1k without any labels.",2,positive
"We also provide a new pipeline achieving data augmentation efficiently for imbalanced image datasets, using cGAN or diffusion models and ResNet or Masked Autoencoder (MAE) classifiers.",2,positive
Table 6 provides goodness-of-fit (R2) and Mean Absolute Error (MAE) measurements for the function f .,1,neutral
"To justify our proposed metric and pipeline work regardless of the classifier selection, we also provide the results with Masked Autoencoder (MAE) ViT-H128 [14] which shows state-of-the-art results in dataset such as [12].",2,positive
a ResNet [15] or Masked Autoencoder (MAE) [14] classifier also trained on the original set with sub-class labels are applied on the synthesized images to select qualified ones.,1,neutral
5% 1 State-of-the-art (SOTA) classification validation accuracy with Masked Autoencoder ViT-H448 [14],0,negative
The high R2 and low MAE values show that the formulation of f is highly effective on modeling the relationship between SSIM-supSubCls and accuracy improvement with our proposed data augmentation pipeline.,2,positive
"The ResNet18 classifiers are trained for 100 epochs and the MAE for 50 epochs when their validation accuracy converges, with their hyperparameters remaining the same throughout the whole procedure in each case.",2,positive
Table 7 compares the state-of-the-art (SOTA) classification validation accuracy from [14] and ours on the whole iNaturalist-2019 dataset.,0,negative
"To certify that this conclusion can be drawn regardless of the classifier selection, we also conduct the experiments with MAE classifier and the same results are obtained.",2,positive
"Then
a ResNet [15] or Masked Autoencoder (MAE) [14] classifier also trained on the original set with sub-class labels are applied on the synthesized images to select qualified ones.",1,neutral
"From this table, our results with Masked Autoencoder (MAE) is comparable with the SOTA one, even though the SOTA is with 448× 448 while ours is with 128×128 input image resolutions which largely reduce the need of running time and computational resources.",2,positive
"In our experiments, the deep generative models, ResNet18 and MAE classifiers are first trained on the original imbalanced set with sub-class instead of super-class labels. cGAN models are trained until the Frechet Inception Distance (FID) scores converge.",2,positive
", 2021), masked image models (He et al., 2022; Dosovitskiy et al., 2020; Doersch et al., 2015; Pathak et al., 2016), masked language models (Devlin et al.",1,neutral
"…in training foundational models with enormous computational power, vast amounts of data, and gigantic neural networks (Radford et al., 2021; Chen et al., 2020; Radford et al., 2019; Brown et al., 2020; Ramesh et al., 2021, 2022; Sohl-Dickstein et al., 2015; Rombach et al., 2022; He et al., 2022).",1,neutral
"…(Chen et al., 2020; He et al., 2020; Grill et al., 2020; Chen and He, 2021; Noroozi and Favaro, 2016; Zbontar et al., 2021), masked image models (He et al., 2022; Dosovitskiy et al., 2020; Doersch et al., 2015; Pathak et al., 2016), masked language models (Devlin et al., 2018; Raffel et al.,…",1,neutral
"In recent years, remarkable progress has been made in training foundational models with enormous computational power, vast amounts of data, and gigantic neural networks (Radford et al., 2021; Chen et al., 2020; Radford et al., 2019; Brown et al., 2020; Ramesh et al., 2021, 2022; Sohl-Dickstein et al., 2015; Rombach et al., 2022; He et al., 2022).",2,positive
"MIM, which is first proposed in BEiT, has been validated its remarkable results in recent works [2, 21, 35, 43, 48] and becomes the new paradigm in visual pre-training.",1,neutral
"Contrarily, MAE [21] reconstructs raw pixels of the image explicitly",1,neutral
"Specifically, our efficient centroid-based MIM outperforms the prior tokenbased MIM [2] and pixel-based MIM [21] in equivalent ViT size and epochs.",2,positive
"Pixel-based MIM with non-parametric tokenizer such as MAE [21] and SplitMask [16], considers vanilla pixels or patches as pre-training targets instead of tokens and need a redundant decoder.",1,neutral
"Actually, in contrast to primarily language tokens as the target in NLP, various reconstruction targets have emerged in previous works in computer vision, including visual tokens [2, 14, 31, 35, 48], high-level features [11] , vanilla pixels [21] and original image features [43], due to the different information density between vision and language.",1,neutral
"…of pretext tasks are tasks to recover an input image from the image with incomplete information [Pathak et al., 2016, Zhang et al., 2016, 2017, He et al., 2022], tasks to predict spatial relationships between subregions of an image, [Doersch et al., 2015, Noroozi and Favaro, 2016, Noroozi et…",1,neutral
"…recall (Equation 8), specificity (Equation 9), and F1 Score (Equation 10) (Labatut and Cherifi, 2012; Giraudo et al., 2018; Alamprese et al., 2021; Chen et al., 2022a; Saranya et al., 2022) in this paper.
accuracy = (TP + TN)
(TP + FP + FN + TN) × 100 (5)
Kappa =
∑n i=1 xii N − ∑n i=1 ( ∑n j=1…",1,neutral
Chen et al. (2022b) developed a high-performance classification model based on a 152-layer deep ResNet to identify different types of walnuts.,2,positive
"The recent MAE method (He et al., 2022) has achieved great success without explicit learning of augmentation invariances.",1,neutral
"2020] and masked autoencoding [He et al. 2022] can be directly applied to the agent’s image observations, providing state representations that can be further finetuned by BC or RL objectives [Sermanet et al.",2,positive
"For instance, when an agent’s perception is based on images, contrastive learning [Chen et al. 2020] and masked autoencoding [He et al. 2022] can be directly applied to the agent’s image observations, providing state representations that can be further finetuned by BC or RL objectives [Sermanet et…",1,neutral
"Self-supervised learning (SSL) is a widely adopted solution in Natural Language Processing (NLP) [1, 2] and Computer Vision (CV) [3, 4].",1,neutral
"of patches, which is adopted by recent pre-training methods like MAE [13].",1,neutral
"Besides, we also consider a baseline strategy (nofill) that drops the masked patches without refilling as in MAE [13].",2,positive
"Masked image modeling [4,13,42,46] has been proved to be an effective approach to vision model pre-training, where random sampling is a common strategy for masking.",1,neutral
"After choosing the patches to mask, simply dropping them following MAE [13] is an intuitive approach to constructing the masked image, i.",1,neutral
"Inspired by the work [24], we propose to use a masked autoencoder based on vision transformer.",2,positive
"Moreover, we have picked up some parameters that have been proven successful in [24].",2,positive
"Thus, a recent development in selfsupervised learning [17, 25, 24], especially with the rise of transformers architectures [15, 18], is now appearing as a solution.",1,neutral
"Motivated by this success in computer vision, such as image classification and object detection [24, 25], image retrieval [26] and speech recognition [27] tasks, we propose in this paper an end-to-end keyword spotting approach in handwritten documents which is based on a self-supervised technique and makes use of masked autoencoders with the self-attention mechanism.",1,neutral
"Different from conventional autoencoders, the used masked encoderdecoder operates in an asymmetric fashion [24] enabling the encoder to operate on only the partial observed signal (without the masked patches) and the decoder to rebuild the full image based on the representation given by the encoder and the masked patches.",2,positive
"This study is inspired by MAE [37], an image representation method that first masks random patches of the input image and then encourages the model to reconstruct those missing pixels.",1,neutral
"Similar with BERT [46] and original MAE [37], only the reconstruction loss of masked hyperedges was calculated.",1,neutral
"2, the partially masked set of hyperedges with the positional embedding was fed into a Transformer [45] based asymmetric autoencoder [37] to reconstruct the missing hyperedges according to the semantic of available hyperedges",1,neutral
"We use the standard ViT-B [14] as the encoder network and initialize it with the MAE pretrained [18] weights following [43, 88].",2,positive
"Language models have also demonstrated their ability to model high-level, long-term sequences for different content types, as shown by the recent advances in text [40, 41, 42, 43, 44] and image [45, 46, 47, 48, 49, 50].",1,neutral
"1 Some works address this issue by tailoring frameworks for dense prediction tasks [7], [25], [38], [59], [62] and a few studies examine segmentation tasks, however, with very large datasets [10].",1,neutral
"The core of the paper is a meticulous analysis based on the milestone algorithm – MAE [20], which discloses critical but neglected bottlenecks of most pixel-based MIM methods.",2,positive
75% in MAE [20] and 60% in SimMIM [56]).,0,negative
"Early MIM methods share a simple pipeline – a portion of non-overlapped image patches are randomly masked, and the model learns to extract discriminative representations by reconstructing the pixel or feature values of the masked patches [1, 20, 56].",1,neutral
"Instead of reconstructing these highlevel features, MAE [20] reconstructs these masked pixel values.",2,positive
BEiT [1] RRC+40% mask ViT+Linear DALLE SimMIM [56] RRC+60% mask ViT+Linear RGB MaskFeat [53] RRC+40% mask ViT+Linear HOG ConvMAE [15] RRC+75% mask ConvViT+MSA RGB MAE [20] RRC+75% mask ViT+MSA RGB,1,neutral
"In the following analysis, we investigate MAE [20] by diagnosing its reconstruction target and input image patches, identifying two important but overlooked bottlenecks that could have hurt the representation quality.",2,positive
"As in supervised learning, the random resized crop (RRC) is the de facto operation for A(·) in MIM [1,20,56].",1,neutral
"We then present a careful analysis with the milestone method, MAE [20], to disclose two important but overlooked bottlenecks of most pixel-based MIM approaches (subsection 3.",2,positive
"We thoroughly evaluate it with three well-established approaches, MAE [20], ConvMAE [15], and LSMAE [28].",2,positive
Pioneering works such as BEiT [1] and MAE [20] exploit Vision Transformers (ViT) to learn discriminative visual represen-,1,neutral
This design lends itself to unsupervised learning and is particularly useful for denoising [17] and reconstruction [18] applications.,1,neutral
"Inspired by MAE [25], our approach reconstructs the holistic features from the latent occluded features.",2,positive
"However, unlike the MAE, the occlusion positions of samples generated by OIA are randomly sampled, so we need to conduct completion on each instance.",2,positive
"It is worth noting that the position of MAE’s mask token is fixed within the batch, which means that each instance can use the same set of mask tokens for feature completion.",1,neutral
"As mentioned in III-C, our FCD adapts MAE’s notion [25] of restoring entire features using implicit unoccluded features.",2,positive
We customize the original masking strategy from MAE [22] and design a facial part masking strategy to ensure that the model can learn the consistencies of all facial parts.,2,positive
Our masked autoencoder is based on an asymmetric encoder-decoder architecture [22].,2,positive
"Unlike MAE [22], which randomly masks most areas of the image,Mover introduces a masking strategy conditioned on facial part consistencies to randomly mask the Regions of interest (ROIs).",1,neutral
"To realize this restoration concept, we pretrain a simple yet powerful masked autoencoder (MAE) model [22] solely on real images.",2,positive
"The original MAE [22] masks random patches of the input image, but the same strategy is not suitable for ourMover for the following reasons.",2,positive
MAE [22] is designed for pretraining Vision Transformers (ViT) rather than for Deepfake detection.,2,positive
"In this stage, we customize MAE[22] and its masking strategy with modifications to perform self-supervised learning of real faces, so that the model can learn generic facial part consistency features and can also prevent over-fitting to specific forgery patterns.",2,positive
"inal MAE [22], cheek & nose, and the strategy without divid-",1,neutral
"Inspired by [22], we propose a novel self-supervised pretraining method that is based on masking and recovering faces, named Mover.",2,positive
The emergence of the masked autoencoder (MAE) [22] has greatly influenced our community.,2,positive
self-supervised pre-training MAE-ViT-L/16 [17] 126K - - 53.,0,negative
"Conventional visual pre-training methods aim to encode the input image as latent representations and learn the representations with pretext tasks like contrastive learning [18, 10] and masked image modeling [2, 17] or massive annotations in classification and vision-language tasks.",1,neutral
"Besides, self-supervised learning such as contrastive learning [7, 18] and masked image modelling [38, 17] have also proved to be able to learn transferrable representations.",1,neutral
The compared methods include self-supervised pre-training (MAE [17]) and superTable 2.,1,neutral
"For 8K iterations, we find VPDA32 surpass all the baseline methods, including those pre-trained on mask image modelling [17, 38], contrastive learning [7] and supervised learning [27, 29].",1,neutral
"Recently, vision transformers [12], [13] have been ported to the audio domain [14]–[18] showing excellent audio classification and general-purpose audio extraction results.",2,positive
"Despite recent advances in deep learning [15, 24, 23, 22], deep neural networks often suffer from performance degradation when the source and target domains differ significantly [8, 43, 38].",1,neutral
"Recent work has shown tremendous improvements in vision community, which are mainly built on top of convolution or attention (e.g., ConvNeXt (Liu et al., 2022), MAE (He et al., 2022), and CLIP (Radford et al., 2021)).",2,positive
"♠ SpiderCNN (Xu et al., 2018) 69.8 73.7 ♠ DGCNN (Wang et al., 2019) 73.6 78.1 ♠ PointCNN (Li et al., 2018) 75.1 78.5 ♠ GBNet (Qiu et al., 2021) 77.8 80.5 q PointBert (Yu et al., 2022d) - 83.1 q Point-MAE (Pang et al., 2022) - 85.2 q Point-TnT (Berg et al., 2022) 81.0 83.5
♣ PointNet (Qi et al., 2017a) 63.4 68.2 ♣ PointNet++ (Qi et al., 2017b) 75.4 77.9 ♣ BGA-PN++ (Uy et al., 2019) 77.5 80.2 ♣ PointMLP (Ma et al., 2022) 83.9 85.4 ♣ PointMLP-elite (Ma et al., 2022) 81.8 83.8 r PointMLP-CoC (ours) 84.4↑0.5 86.2↑0.8
Context Clusters are a natural fit for point clouds Qi et al. (2017b); Lu et al. (2022).",0,negative
", 2022), MAE (He et al., 2022), and CLIP (Radford et al.",2,positive
"on masked image modeling [117], [118] are developing rapidly and is able to even surpass fully-supervised conterparts.",1,neutral
"1 on ImageNet and do not use dropout when pre-training on the much larger JFT-300M dataset; recent languagesupervised or self-supervised vision models (Radford et al., 2021; He et al., 2021) do not use dropout.",0,negative
", 2020) and algorithms for learning with unlabeled or weakly-labeled data (Brown et al., 2020; Radford et al., 2021; He et al., 2021) have provided even more data to train on than the model can fit to.",2,positive
"are exploited to learn image representations, for instance, [3, 21] via image tokens, [31] in pretraining, [36], [36, 81] in self-supervised segmentation, and [19] in detection.",1,neutral
"In particular, a CNN-based network with the RoI-Align He et al. (2017) operation produces visual features for the masked regions.",2,positive
"In MAE He et al. (2022), BEiT Bao et al. (2022), and SimMIM Xie et al. (2022), patch-level Masked Image Modeling has shown strong potential in representation learning.",1,neutral
"MAE He et al. (2022) and SimMIM Xie et al. (2022) take a simpler behavior, predicting RGB values of raw pixels by direct regression.",1,neutral
"…modeling (MIM) that inherits the concept of vision-based self-supervised learning such as BEiT Bao et al. (2022), SimMIM Xie et al. (2022), MAE He et al. (2022), CAE Chen et al. (2022b), and DiT Li et al. (2022), etc. MIM is a powerful image-only pre-training technique to learn the visual…",2,positive
"In particular, a CNN-based network with the RoI-Align He et al. (2017) operation produces visual features for the masked regions. Inspired by ViBERTGrid Lin et al. (2021), the backbone uses FPN Lin et al.",2,positive
"In particular, a CNN-based network with the RoI-Align He et al. (2017) operation produces visual features for the masked regions. Inspired by ViBERTGrid Lin et al. (2021), the backbone uses FPN Lin et al. (2017) to integrate features of CNN.",2,positive
"Unlike MAE He et al. (2022), BEiT Bao et al. (2022), and DIT Li et al. (2022), we do not use patch-level masking strategy for pre-training.",2,positive
"…2018; Brown et al., 2020; Radford et al., 2021; Jia et al., 2021), which demonstrate strong generalization ability across multiple downstream tasks in visual (He et al., 2022b; Bao et al., 2021), language (Liu et al., 2019; Raffel et al., 2020) and
1Alibaba Group 2National University of Singapore.",2,positive
"Another way to insert adapters is to add a scaling factor and design the adapter explicitly as a parallel module (He et al., 2022a; Chen et al., 2022), which can be similarly viewed as parallel structures.",1,neutral
"…8, we compare several tuning methods with our proposed U-Tuning based on different architectures and pre-trained models, i.e., IN-21K (Deng et al., 2009), MAE (He et al., 2022b) and CLIP (Radford et al., 2021) pre-trained ViT/B16 (Dosovitskiy et al., 2020), as well as CLIP pre-trained ViT/L-14.",2,positive
"In Table 8, we compare several tuning methods with our proposed U-Tuning based on different architectures and pre-trained models, i.e., IN-21K (Deng et al., 2009), MAE (He et al., 2022b) and CLIP (Radford et al., 2021) pre-trained ViT/B16 (Dosovitskiy et al., 2020), as well as CLIP pre-trained ViT/L-14.",2,positive
"For FFNs, the adaptation is generally made by adapter (Houlsby et al., 2019) and its generalized versions (Pfeiffer et al., 2020; Karimi Mahabadi et al., 2021b;a; He et al., 2022a), which usually insert a bottleneck layer into each FFN layer.",2,positive
"Due to the lack of labeled resources, to train such large models, some self-supervised methods have achieved great success, such as MAE [66] and masked language modeling [4].",1,neutral
"Masked autoencoders [20], [21], which can well overcome these aforementioned limitations, have been proposed before, whose main philosophy is to encode the maskingstyle corrupt input into latent space followed by a recovery of the raw inputs via the encoder and decoder.",1,neutral
"Currently, self-supervised pre-training paradigms have nearly become the default configuration in the domains of natural language (NLP) [12], [13], [20] and computer vision (CV) [14], [15], [21].",1,neutral
"Instead of using a fixed masking ratio in language and visual pre-training [6, 17], the generative transformer needs to generate tokens from scratch and applies a randomly sampled ratio γ(r) ∈ (0, 1] in training.",1,neutral
"They are reported to achieve results better than supervised models trained on fewer labeled samples and have found applications in computer vision (He et al., 2022), natural language processing (Devlin et al., 2018; Vaswani et al., 2017) and audio processing (Schneider et al., 2019).",1,neutral
"vised models trained on fewer labeled samples and have found applications in computer vision (He et al., 2022), natural language processing (Devlin et al.",1,neutral
", MAE [15]), we use this finding to further reduce the computational complexity of our model.",1,neutral
"Similar to recent works in the computer vision domain (e.g., MAE [15]), we use this finding to further reduce the computational complexity of our model.",1,neutral
"Recently, some general pre-trained models [8], [9] have been widely used for better representation and then they are fine-tuned for various downstream tasks, e.",1,neutral
"Motivated by the success of BERT [13] in NLP, many recent works show a variety of MIM schemes for pre-training vision models in a self-supervised way, using reconstruction targets such as pixels [9], [40] and discrete tokens [41].",1,neutral
"Masked language modeling (MLM) (Kenton & Toutanova, 2019) and masked image modeling (MIM) (Xie et al., 2022; He et al., 2022) have been dominant self-supervised approaches in NLP and CV domains.",1,neutral
", 2021), self-supervised pretraining (He et al., 2022), to name a few.",2,positive
"…of fast-evolving and well-studied computer vision techniques in the time series domain, such as better model architecture (Liu et al., 2022), data augmentation (Shorten & Khoshgoftaar, 2019), interpretability (Chefer et al., 2021), self-supervised pretraining (He et al., 2022), to name a few.",2,positive
"problem is easy to tune by changing the used time horizon or including masking during training [12,14].",1,neutral
"For model fine-tuning with MAE, we adopt the settings in (He et al., 2021).",1,neutral
"For example, we obtain better results by integrating our method into the fine-tuning process of the Masked AutoEncoder (MAE) in (He et al., 2021), achieving improvements on ImageNet by 0.3%, ImageNet-A by 1.8%, ImageNet-R by 1.4% and ImageNet-C by 3.7% on Vision Transformer Large (Dosovitskiy et…",2,positive
"The baseline models are taken directly from (He et al., 2021).",2,positive
"15 is that we take the results directly from (He et al., 2021), which have no detailed data of different corruptions.",0,negative
"For example, we obtain better results by integrating our method into the fine-tuning process of the Masked AutoEncoder (MAE) in (He et al., 2021), achieving improvements on ImageNet by 0.",2,positive
"C.15 is that we take the results directly from (He et al., 2021), which have no detailed data of different corruptions.",0,negative
"FT follows MAE (He et al., 2022), while Ours uses MoCo v2 contrastive loss and regularization coefficient λ = 0.",1,neutral
"FT follows MAE (He et al., 2022), while Ours uses MoCo v2 contrastive loss and regularization coefficient λ = 0.1.",2,positive
"FT follows the training in MAE (He et al., 2022) mostly, using SGD for 200 epochs with a cosine learning-rate scheduler, the base learning rate of 0.",2,positive
"FT follows the training in MAE (He et al., 2022) mostly, using SGD for 200 epochs with a cosine learning-rate scheduler, the base learning rate of 0.06, weight decay 5e-4, momentum 0.9 and batch size 256.",2,positive
"We adopted the masked auto-encoder (MAE) (He et al., 2022) to process the point cloud data (denoted as MAE-dense) for pre-training, which is close with Point-MAE (Pang et al., 2022).",2,positive
"To pre-train the MAE-dense and MAE-sampled, we first follow the standard train-test split of ShapeNet (Chang et al., 2015) adopted by Pang et al. (2022); Yu et al. (2022).",2,positive
Few Shot Learning The pre-trained MAE-dense and MAE-sampled models are finally evaluated on a few shot learning task.,1,neutral
It is denoted as MAE-sampled.,1,neutral
"In addition, the representation power of the transformer has been explored by the pre-training and fine-tuning models (Bao et al., 2021; Yu et al., 2022; He et al., 2022).",2,positive
"Specifically, we build the classifier by keeping the encoder structure and weights of the pre-trained MAE-dense and MAE-sampled models, followed by max-pooling as well as a fully connected layer of dimension [256, 256, 40] to map the global token of a dimension of 256 to the 40 categories.",2,positive
"Classification The pre-trained MAE-dense and MAEsampled models are first evaluated on the classification task in ModelNet40 (Wu et al., 2015).",1,neutral
Note that MAE-dense adopts dense-attention layers in its encoder and decoder network.,1,neutral
"3, our proposed MAE-sampled outperformed all state-of-the-art methods on 3 out of 4 settings, while MAE-sampled consistently outperformed MAE-dense.",2,positive
"We adopted the masked auto-encoder (MAE) (He et al., 2022) to process the point cloud data (denoted as MAE-dense) for pre-training, which is close with Point-MAE (Pang et al.",2,positive
"To evaluate the effectiveness of our claimed contribution, we replace the dense-attention layer in MAE-dense with our sampled-attention layer (Fig.",2,positive
"To maintain a fully self-supervised pre-training paradigm, we initialize with weights obtained by self-supervised ImageNet pre-training [19].",2,positive
MAE [19] is used for pre-training with randomly shuffled ScanNet images.,1,neutral
"MAE [19] then proposed an approach inspired by BERT [13], which randomly masks words in sentences and leveraged masked image reconstruction for self-supervised pre-training that achieved state-of-the-art results in ViT.",2,positive
"Similar to MAE [19], we normalize the output patches as well as the target patches prior to computing the loss, which we found to empirically improve the performance.",2,positive
2) MAE-unsupIN→SN [19] ViT ImageNet+ScanNet 54.,1,neutral
6) MAE-unsupIN→SN [19] ViT ImageNet+ScanNet 63.,1,neutral
"MOCOv3 achieves slightly better performance than normal training as shown in Table 4, and MAE improves the natural robustness significantly based on ViT, as shown in Table 5.",0,negative
"Second, MAE (He et al, 2022) improves adversarial robustness on ViTs, while MOCOv3 (Chen et al, 2021) benefits adversarial robustness.",2,positive
"Although similar to MAE, we choose SimMIM because it adopts the backbone of Swin Transformer, which performs better than ViT adopted in MAE, as shown in the experiment.",2,positive
"It is shown that the robust curves of the models with the same
Springer Nature 2021 LATEX template
ARES-Bench 3
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
VGGs VGG13_Normal VGG16_Normal VGG19_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
ResNets ResNet50_Normal ResNet50_MOCO ResNet101_Normal ResNet152_Normal Wide-ResNet50_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
DenseNets DenseNet121_Normal DenseNet161_Normal DenseNet201_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(d)
ConvNexts ConvNextS_Normal ConvNextS_21K ConvNextB_Normal ConvNextB_21K ConvNextL_Normal ConvNextL_21K
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(e)
ViTs ViTS_Normal ViTS_21K ViTB_Normal ViTB_21K ViTB_MAE ViTL_Normal ViTL_21K ViTL_MAE
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(f)
XciTs XciTS_Normal XciTM_Normal XciTL_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (g)
T2Ts T2T14_Normal T2T19_Normal T2T24_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(h)
Swins SwinS_Normal SwinS_21K SwinB_Normal SwinB_21K SwinL_21K
Fig.",0,negative
"Springer Nature 2021 LATEX template
ARES-Bench 13
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
Normal Models VGG19_Normal ResNet152_Normal DenseNet161_Normal ConvNextL_Normal ViTL_Normal XciTL_Normal T2T24_Normal SwinB_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
Pre-trained Models ResNet50_Normal ResNet50_MOCO ViTB_Normal ViTB_21K ViTB_MAE ConvNextL_Normal ConvNextL_21K SwinB_Normal SwinB_21K
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
AT Models ResNet152_Normal ResNet152_AT ConvNextL_Normal ConvNextL_AT ViTB_Normal ViTB_AT XciTL_Normal XciTL_RB SwinB_Normal SwinB_AT",0,negative
"For self-supervised learning methods, MAE has a negative impact on the adversarial robustness especially under large perturbation budgets, but MOCOv3 improves adversarial robustness compared with the normally trained models, as shown in Fig.",1,neutral
"IN-Val IN-V2 IN-Real ON IN-A IN-R IN-V IN-C SIN IN-Sketch
ViTS
Normal 74.4 61.6 80.0 13.1 8.8 30.4 11.2 32.0 9.1 19.9 34.0 Pre-train 81.4 70.3 86.8 22.7 27.3 45.7 16.6 47.1 15.8 32.5 44.6 AT 70.2 57.3 77.9 11.5 6.1 46.0 8.5 27.8 16.8 29.8 35.2
ViTB
Normal 75.8 61.6 80.9 13.2 11.4 32.8 13.3 34.3 10.9 23.7 35.8 Pre-train 84.6 73.9 88.8 27.4 44.5 56.8 19.4 57.5 22.6 43.0 51.9 MAE 83.6 73.1 88.1 24.9 37.7 49.8 18.2 49.4 20.2 36.4 48.1 AT 73.4 60.4 80.5 12.7 8.9 50.7 9.4 36.6 22.2 35.7 39.1
ViTL
Normal 75.2 60.7 79.8 11.2 11.3 33.3 13.4 35.4 9.3 25.0 35.4 Pre-train 85.8 76.0 89.2 30.5 56.1 64.2 25.5 65.3 30.1 51.8 57.4 MAE 85.1 75.6 89.0 27.3 50.6 60.0 21.5 56.2 24.1 46.4 53.6
XciTS Normal 82.4 71.5 86.8 23.7 31.3 45.0 17.0 50.1 19.5 32.9 46.0
RB 73.3 60.5 80.6 12.7 6.3 45.7 9.7 28.5 18.4 31.2 36.7
XciTM Normal 82.6 71.0 86.8 23.4 33.3 44.7 17.7 50.5 20.3 33.1 46.3
RB 74.1 61.7 81.3 13.6 7.0 47.1 9.5 30.2 19.7 32.6 37.7
XciTL Normal 83.0 72.0 86.9 23.7 36.2 46.2 17.9 50.2 20.4 34.4 47.1
RB 75.1 62.7 81.7 13.4 8.8 49.0 10.7 32.0 19.9 34.4 38.7
T2T14 Normal 81.6 70.9 86.8 22.3 24.1 44.7 16.7 46.8 17.7 32.2 44.4
T2T19 Normal 82.3 71.6 87.2 23.2 29.0 47.3 18.0 50.2 20.9 34.4 46.4
T2T24 Normal 82.4 71.7 87.2 22.9 29.7 47.9 18.0 52.0 20.8 35.1 46.8
SwinS
Normal 83.2 72.1 87.5 24.7 33.0 44.9 19.3 45.1 16.8 32.0 45.8 Pre-train 83.3 73.5 88.6 28.1 43.9 54.8 21.3 50.6 17.2 41.2 50.3 AT 75.8 63.3 82.6 15.3 10.6 52.5 10.8 37.1 21.1 37.1 40.6
SwinB
Normal 83.4 72.3 87.6 25.5 35.8 46.6 20.2 45.6 17.9 32.4 46.7 Pre-train 85.1 75.2 89.1 28.8 51.8 59.1 22.7 56.4 19.6 45.1 53.3 AT 76.8 64.5 83.4 15.5 13.1 53.5 11.8 39.3 22.7 39.3 42.0
SwinL Pre-train 86.3 77.0 89.6 31.6 61.0 63.6 26.4 61.3 23.4 48.8 56.9
AT 78.7 66.9 84.9 18.2 18.1 57.3 11.6 43.4 25.2 42.9 44.7
increases from 34.1% of ResNet50 to 36.8% of ResNet101, and finally to 38.0% of ResNet-152.",0,negative
"C V
] 2
8 Fe
b 20
23
2 ARES-Bench
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
VGGs VGG13_Normal VGG16_Normal VGG19_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
ResNets ResNet50_Normal ResNet50_MOCO ResNet50_AT ResNet50_RB ResNet50_RL ResNet101_Normal ResNet101_AT ResNet152_Normal ResNet152_AT ResNet152_FD Wide-ResNet50_Normal Wide-ResNet50_AT Wide-ResNet50_RB
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
DenseNets DenseNet121_Normal DenseNet161_Normal DenseNet201_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(d)
ConvNexts ConvNextS_Normal ConvNextS_21K ConvNextS_AT ConvNextB_Normal ConvNextB_21K ConvNextB_AT ConvNextL_Normal ConvNextL_21K ConvNextL_AT
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(e)
ViTs ViTS_Normal ViTS_21K ViTS_AT ViTB_Normal ViTB_21K ViTB_MAE ViTB_AT ViTL_Normal ViTL_21K ViTL_MAE
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(f)
XciTs XciTS_Normal XciTS_RB XciTM_Normal XciTM_RB XciTL_Normal XciTL_RB
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (g)
T2Ts T2T14_Normal T2T19_Normal T2T24_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(h)
Swins SwinS_Normal SwinS_21K SwinS_AT SwinB_Normal SwinB_21K SwinB_AT SwinL_21K SwinL_AT
Fig.",0,negative
"…including typical CNNs and Transformers, and different learning algorithms, including normal supervised training, pre-training on large-scale datasets (Dosovitskiy et al, 2021), selfsupervised learning (SSL) (Chen et al, 2021; He et al, 2022), and adversarial training (AT) (Madry et al, 2018).",2,positive
Different reconstruction results of MAE [19] correspond to different mask seeds.,1,neutral
"Besides, mask image modeling [1,4,14,19,30,48,53] is currently the focus of the research community.",1,neutral
MAE [19] adopts 75% mask ratio while BERT [12] uses 15% mask ratio).,1,neutral
"Is it possible to reduce the random mask ratio to increase pre-training efficiency and improve consistency? In fact, the prior work [19] already shows that reducing the mask ratio brings lower transfer ability for downstream tasks.",1,neutral
It is noted that MAE [19] proposes an asymmetric encoder-decoder architecture for the MIM task and shows excellent performance in a variety of visual downstream tasks.,2,positive
"Specifically, according to the pioneering work [19], given a natural image from an unlabeled dataset X, we divide it into N regular image patches, denoted as x ∈ RN×S where S denotes the patch size (e.",1,neutral
"Commonly, the random mask ratio of MIM is much higher than that of MLM due to the difference in the information density of image and language data [19] (e.",1,neutral
"Following the setting of MAE [19], EMAE is mainly evaluated on linear probing, finetuning classification, object detection, instance segmentation, and semantic segmentation tasks.",1,neutral
"The training settings are the same as MAE [19], we adopt the same encoder-decoder structure to perform the MIM task.",2,positive
"For a fair comparison, we fully follow the hyperparameters of MAE [19] in our image classification experiments.",2,positive
"Masked Convolution Meets Masked Autoencoders (ConvMAE) ConvMAE [21], a derivative of the popular MAE [19], is proposed to train scalable visual representation with hybrid convolution-transformer architectures and masking convolution.",2,positive
"Vision Image BEiT v1 [16], v2 [27], MAE [19], SimMIM [28], ADIOS [29], AMT [30], AttMask [31], Beyond-Masking [32], BootMAE [33], CAE [20], CAN [34], ConvMAE [21], Contrastive MAE [22], ContrastMask [35], dBOT [36], DMAE [37], Denoising MAE [38], GreenMAE [23], iBOT [39], LoMaR [40], LS-MAE [41], MaskAlign [42], MaskDistill [18], MaskFeat [43], MaskTune [44], MetaMask [45], MFM [46], MILAN [47], MixMask [48], MixMIM [24], MRA [49], MSN [50], MST [51], MultiMAE [52], MVP [53], RC-MAE [54], SDMAE [55], SemMAE [56], SdAE [57], SupMAE [58], U-MAE [59], UM-MAE [60]",2,positive
MAE’s ablation study also points out that a high masking ratio is good for fine-tuning and linear probing [19].,1,neutral
"The most famous one is Masked Autoencoder (MAE) [19], which owns a very simple learning architecture but has been proven to be a strong and scalable pre-training framework for visual representation learning.",2,positive
"With those meticulous designs, MAE is three times (or more) faster than BEiT [16] while achieving superior performance [19].",2,positive
"In detial, MAE[7] and SimMIM[23] replace a random subset of input tokens with a special MASK symbol and aim at reconstructing original image tokens from the corrupted image with Vision transformers[5, 14].",1,neutral
"Several works[7, 23, 19, 11, 12] focus on using valid information via the pre-text task to improve downstream vision tasks.",1,neutral
"Subsequently, VideoMAE[19] proves that an extremely high proportion of masking ratio still yields favorable performance on videos.",2,positive
"In detail, MAE[7] and SimMIM[23] replace a random subset of input tokens with a special MASK symbol designed to reconstruct the original image tokens from corrupted images using Vision transformers[5, 14].",1,neutral
"Several works[7, 23, 19, 11, 12] focus on improving downstream visual tasks by using effective information in pre-text tasks.",1,neutral
Dataset Images ViT[13] DeiT III[14] MAE[15] IN-LT 18.,1,neutral
"DeiT[14] proposes an effective receipt to train ViT with limited data, and MAE[15] adopts a masked autoencoder to pre-train the ViT.",0,negative
"We adopt the recipe in vanilla ViT[13], DeiT III[14], and MAE[15] to train ViTs.",2,positive
"supervised visual pre-training can be classified into three categories: contrastive learning based [15, 23, 40], distillation based [6, 20], and masked image modeling based [22, 52].",1,neutral
"But along with the progress of these efforts, the huge amount of data, in turn, becomes a barrier to both storage and training [56, 20].",2,positive
"Follow MAE (He et al., 2021), we evaluate the performance of the proposed Layer Grafted Pre-training with different number of fixing blocks.",2,positive
"…leads to different strengths: On the one hand, empirical results highlight that the
masked view can benefit the downstream fine-tuning task (Touvron et al., 2022; He et al., 2021), which may be because it helps to learn the correlation between sparse patches that cannot be built under full view.",2,positive
"…first step of Layer Grafted Pre-training, since it identically follows the original MIM pipeline, we directly adopt the pre-trained model of MAE (He et al., 2021) and conduct the second step, where we initialize the network with the MIM pre-trained model and train with Moco V3 (Chen et al.,…",2,positive
"For other settings such as data augmentation, LR scheduling or weight decay, we identically follow He et al. (2021).",1,neutral
"The performance of MIM and CL are from MAE (He et al., 2021) and MocoV3 (Chen et al.",2,positive
"…65.3 -
C-MAE (Huang et al., 2022) 73.9 65.3 77.3 MimCo (Zhou et al., 2022) 70.2 62.7 - Layer Grafted Pre-training (Ours) 77.7 65.5 77.8
ViT-L/16 MAE (He et al., 2021) 75.8 55.2 78.7
Moco V3 (Chen et al., 2021) 77.6 - - Layer Grafted Pre-training (Ours) 81.0 69.3 80.1
Our method also demonstrates…",0,negative
"MAE (He et al., 2021) and simMIM (Xie et al., 2022) further show the possibility of directly reconstructing the original pixels.",2,positive
"For fine-tuning, we train with AdamW (Loshchilov & Hutter, 2017) for 100 epochs following MAE (He et al., 2021).",2,positive
"We by default adopt MAE (He et al., 2021) and Moco V3 (Chen et al., 2021) as our MIM and CL frameworks, respectively.",2,positive
"Then, the image with minimal augmentation would be utilized for computing MIM loss following MAE He et al. (2021).",1,neutral
"…with two mainstream representative methods: Contrastive learning (CL) (Chen et al., 2020b; He et al., 2020; Chen et al., 2020d; 2021; Grill et al., 2020; Caron et al., 2021), and Mask Image Modeling (MIM) (Bao et al., 2021; He et al., 2021; Xie et al., 2022; Dong et al., 2021; 2022).",2,positive
"In contrast, we employ MAE (He et al., 2021), an image modeling framework that is free from strong augmentations.",2,positive
", 2021), and Mask Image Modeling (MIM) (Bao et al., 2021; He et al., 2021; Xie et al., 2022; Dong et al., 2021; 2022).",2,positive
"However, the masked autoencoders [23] that corrupt input and then attempt to recover it has shown great success in vision tasks, which might also be applicable to graphs.",1,neutral
The mask-and-reconstruct training paradigm has been proven to be effective in computer vision [23] and NLP [1].,1,neutral
"For example, BERT [1] adopts random dropping to generate partially observed word sequences for language modeling, and MAE [23] applies patch-aware random masking to yield masked image channels for visual representation.",1,neutral
"However, if the graph structure is corrupted, especially when the non-trivial perturbation is conducted [23], the GNN encoder would inevitably be affected, leading to noisy node representations.",1,neutral
"Similar to MAE [23], the intuitive solution is to treat nodes as pixels and then uniformly sample neighboring nodes for edge masking.",1,neutral
Masked autoencoding is a highly successful framework for pretraining in text [1] and image [23] domains.,1,neutral
A similar idea has been successfully explored in the text [1] and image [23] fields.,1,neutral
"In this paper, we propose to predict the masked edges in Emask in training, inspired by the success of masked autoencoding in computer vision [23].",2,positive
"Also, inspired by masked image modeling [He et al., 2021], a series of works for masked point modeling [Liu et al.",1,neutral
"In 2D modality, MAE [He et al., 2021] and its followup work efficiently conduct 2D masked autoencoding with multi-scale convolution stages [Gao et al.",1,neutral
"Unlike MAE [He et al., 2021] and Point-MAE [Pang et al.",1,neutral
"As shown in Figure 4, we adopt two reconstruct heads of simple linear layers to respectively recover the masked 2D pixels and 3D coordinates following MAE [He et al., 2021] and Point-MAE [Pang et al.",1,neutral
"Inspired by the success of MAE pre-training independently in 2D and 3D vision [He et al., 2021; Gao et al., 2022; Pang et al., 2022; Zhang et al., 2022a], we expect to fully incorporate MAE pre-training and multi-modality learning to unleash their potientials for 3D representation learning.",2,positive
"The information density of image is much lower than that of text, and more patches need to be masked for a better performance [11, 33].",1,neutral
"Since previous research [13] has shown that a 75% masking rate produces optimal representations in autoencoders, this paper will use a 75% masking rate by default.",1,neutral
"Additionally, some researchers [10] replaced the ViT [12] used by MAE [13] with Swin Transformer [14] to adapt to small medical datasets, while others [11] applied the masked autoencoder to medical multimodal data.",1,neutral
"Additionally, we compared our method with some recent self-supervised methods, including MoCoV2 [32], MAE [13], and ConvMAE [33].",2,positive
", 2021), masking-based (Bao et al., 2021; He et al., 2022), or multimodal (Radford et al.",2,positive
"Foundation models [57, 7, 3, 24], trained on broad data (with self-supervised learning [32, 68, 11, 51]), have shown great promise on a wide spectrum of downstream tasks [39, 49, 16, 1] with great generalisation ability.",1,neutral
"masked input, like MAE [9] and SimMIM [10].",1,neutral
"Besides, several state-of-theart self-supervised learning methods are chosen as the selfsupervised learning baselines in our experiments, including SimCLR [5], BYOL [6], SwAV [7], MAE [9], and SimMIM [10].",2,positive
"Generative approaches hypothesize that a model that can capture the image distribution will learn semantically relevant features [26, 31, 37, 70, 98, 115].",1,neutral
"Training models to predict masked out portions of the input data is an approach to self-supervised learning that has led to strong empirical results in the deep learning literature (Devlin et al., 2019; Yang et al., 2019; Brown et al., 2020; He et al., 2022).",1,neutral
"Masked Visual Pretraining [MVP; 65] proposes using masked autoencoding [29] to prioritize visual reconstruction from heavily masked video frames, encoding representations that facilitate per-pixel reconstruction.",1,neutral
Masked AutoEncoders (MAE) [29] masks random patches from the input image and learns a beneficial visual representation via reconstructing the missing patches in the pixel space.,1,neutral
"In this paper, we present VoxFormer, a strong camera-based 3D semantic scene completion (SSC) framework composed of (1) class-agnostic query proposal based on depth estimation and (2) class-specific segmentation with a sparse-to-dense MAE-like design.",2,positive
Stage-2 is based on a novel sparse-to-dense MAE-like architecture as shown in Fig.,2,positive
"Our framework is a two-stage cascade composed of class-agnostic proposals and class-specific segmentation similar to [68]: stage-1 generates class-agnostic query proposals, and stage-2 uses an MAE-like architecture to propagate information to all voxels.",2,positive
"After obtaining voxel query proposals based on depth, VoxFormer generates semantic voxels via an MAE-like architecture [3].",2,positive
"Motivated by reconstruction-before-hallucination and sparsity-in-3D-space , we build a two-stage framework: stage-1 based on CNN proposes a sparse set of voxel queries from image depth to attend to images since the image features correspond to visible and occupied voxels instead of non-visible and empty ones; stage-2 based on Transformer uses an MAE-like architecture to first strengthen the featurization of the proposed voxels by voxel-to-image cross-attention, and then process the full set of voxels with self-attention to enable the voxel interactions.",2,positive
We name such learnable parameter as mask token [3] for conciseness since unselected from Q is analogous to masked from Q.,1,neutral
• A novel Transformer similar to masked autoencoder (MAE) [3] that yields complete 3D scene representation.,1,neutral
"As a result, a modified variant of the MAE architecture named DropMAE was introduced, as depicted in Fig.",1,neutral
"Similar to DropMAE, MAT randomly masks patches of template and search image pairs, which are then jointly processed by the encoder to capture their visual representations.",1,neutral
OSTrack utilizes a self-supervised learning-based Masked Auto Encoder (MAE) [108] pre-trained model to initialize the ViT backbone.,2,positive
"Closely related to DropMAE, another masked encoderbased pre-trained model has been specifically designed and trained for the tracking task, referred to as MAT [102].",2,positive
"Track [66], have demonstrated that initializing the backbone with a self-supervised learning based Masked Autoencoder (MAE) [108] pre-trained model can achieve higher tracking accuracy compared to models based on supervised learning.",1,neutral
"This improvement can be attributed to the MAE’s ability to capture fine-grained local structures within an image, which are essential for accurate target localization.",1,neutral
The OSTrack [62] approach showed better performance while fine-tuning the DropMAE as the backbone compared to the MAE [108] backbone.,2,positive
"Due to the great success of OSTrack in the tracking community, several recent follow-up approaches [63], [65], [66], [67] have been proposed, utilizing self-supervised learning based MAE pre-trained model to initialize the backbone network.",2,positive
"encoding, in contrast to the single decoder of other models [65], [108], MAT employs two identical decoders to separately reconstruct the search image and the target region in the search image.",1,neutral
"Additionally, DropMAE follows an attention dropout mechanism that restricts the interaction between tokens within the",1,neutral
DropMAE captures spatial cues within individual images and also captures the correlated spatial cues between two frames by randomly masking the input frames and processing them through the encoder-decoder architecture.,2,positive
The experimental results demonstrated that the OSTrack [62] approach achieved superior performance when utilizing DropMAE to initialize the backbone compared to initializing with MAE backbone.,2,positive
enhanced their tracking accuracy by utilizing self-supervised learning based masked autoencoder pre-trained models [108] to initialize the tracker encoder.,2,positive
"Recently, Wu et al. [65] discovered that the MAE architecture exhibits a lack of robustness when applied to feature matching tasks between two images.",1,neutral
"[65] Q. Wu, T. Yang, Z. Liu, B. Wu, Y. Shan, and A. B. Chan, ‘‘DropMAE: Masked autoencoders with spatial-attention dropout for tracking tasks,’’ in Proc.",1,neutral
Auto Encoder (MAE) [108] pre-trained model to initialize the ViT backbone.,2,positive
"However, recent fully Transformer-based trackers, such as OSTrack [62], ProContEXT [63], GRM [67], and VideoTrack [66], have demonstrated that initializing the backbone with a self-supervised learning based Masked Autoencoder (MAE) [108] pre-trained model can achieve higher tracking accuracy compared to models based on supervised learning.",1,neutral
"Algorithmically, the Figure 2: Illustration of representative SSL methods: SimCLR [9], MoCo V3 [9], BYOL [15], and the Masked Auto-Encoder [10].",1,neutral
"We further evaluate our defense under other popular SSL training algorithms and different model structures and datasets, e.g., ResNet-18 and ViT-Small/16 trained using SimCLR, MoCO V3, BYOL, MAE over CIFAR-10 or the ImageNet (Appendix 6.3).",2,positive
"By contrast, the recently proposed SSL method, MAE [10], trains the encoder f (·|θ) by masking a portion of pixels in an image x (the masked image is denoted by x′) and then using f (x′|θ) with a decoder d(·) to restore x.",1,neutral
"With the thriving development of SSL, especially contrastive learning (e.g., SimCLR [9, 13], MoCo [14, 41, 42], BYOL [15]) and the MAE [10], backdoor attacks targeting SSL have also been explored.",1,neutral
", SimCLR [9], MoCo V3 [14], BYOL [15], and the MAE [10], for evaluation.",2,positive
", through contrastive learning [13–15] or masked autoencoder (MAE) [10]) and then fine-tunes only the last layer using labeled data from a specific downstream task.",1,neutral
"Here we use self-supervised learning methods consistent with those in Section 4.3, including the contrastive learning method SimCLR, MoCO V3, BYOL, and the masked-model training method MAE.",2,positive
", [10, 11, 24]); (2) FT-last (or linear adaptation): only the last fully-connected layer is updated (e.",1,neutral
"In SSL adaptation, one pre-trains a model on large unlabeled data (e.g., through contrastive learning [13–15] or masked autoencoder (MAE) [10]) and then fine-tunes only the last layer using labeled data from a specific downstream task.",1,neutral
", SimCLR [9, 13], MoCo [14, 41, 42], BYOL [15]) and the MAE [10], backdoor attacks targeting SSL have also been explored.",2,positive
"For Case-1, we incorporate four state-of-the-art SSL training methods, i.e., SimCLR [9], MoCo V3 [14], BYOL [15], and the MAE [10], for evaluation.",2,positive
This paper focuses primarily on two of the most recent SSL schemes: contrastive learning and masked auto-encoder (MAE).,1,neutral
"For example, the accuracies of our model pre-trained with MAE were 8.68% and 5.16% higher than those of ResNet101 pre-trained with SimSiam in the five-class and binary classification tasks.",0,negative
"By leveraging a simple SSL framework, MAE, we alleviated the problem of training classification models without sufficient high-quality labeled OCT images.",2,positive
"He et al. [35] have demonstrated that MAE has the advantages of scalability and simplicity compared with contrastive self-supervised learning approaches, such as DINO and MoCo.",1,neutral
"Third, MAE can help Transformer-based models achieve better classification performance than DINO, one of the SOTA contrastive learning frameworks.",2,positive
"The first stage is that the proposed classification model is pre-trained with MAE [35], a non-contrastive SSL framework, using label-free cervical OCT images.",2,positive
"Besides, it is worth noting that Transformer-based models pretrained with MAE outperformed those supervised ViT models pre-trained on the ImageNet-1 K dataset.",2,positive
[35] as the initial weights of our model.,2,positive
"The purpose of such operations is to reduce the image resolution as much as possible, thereby reducing the amount of graphics processing unit (GPU) memory used and speeding up model pre-training with MAE.",2,positive
Section III presents the image classification model built based on ViT and MAE for cervical OCT images.,0,negative
"The first stage is that the proposed classification model is pre-trained with MAE [35], a non-contrastive
SSL framework, using label-free cervical OCT images.",2,positive
"Due to the popularity and advantage of non-contrastive SSL frameworks (e.g., masked auto-encoders (MAE) [35]), this study aims to design a ViT-based classification model for cervical OCT images and pre-train it in a non-contrastive self-supervised manner.",1,neutral
"In the model pre-training stage, we transfer the weights of the MAE encoder and decoder pre-trained on the ImageNet-1 K dataset [35] (see the upper part of Fig.",2,positive
"For the classification task of cervical OCT images, we are
the first to propose a ViT-based image classification model pre-trained with a non-contrastive SSL framework, MAE, which can help ease the burden of insufficient labeled image data on the model’s prediction performance.",2,positive
A few essential parameters were configured in the selfsupervised model pre-training with MAE.,2,positive
", masked auto-encoders (MAE) [35]), this study aims to design a ViT-based classification model for cervical OCT images and pre-train it in a non-contrastive self-supervised manner.",1,neutral
"The decoder of MAE, more specifically, a lightweight Transformer decoder, needs to process the complete token set of the input image.",1,neutral
"For example, when selecting ViT-B as the backbone, the five-class accuracy, binary accuracy, sensitivity, specificity, and AUC of MAE were increased by 2.39%, 1.52%, -0.30%, 2.98%, and -0.08%, respectively.",1,neutral
"1(a) presents the self-supervised pre-training of our model with MAE, which includes a ViT encoder and a lightweight Transformer decoder.",2,positive
"Meanwhile, the weights of three ViT models are transferred from the weights obtained in the self-supervised model pre-training with MAE on the image resolution of 224×224 pixels.",2,positive
"For example, compared with the supervised MViT-B (the SOTA model) with the weights transferred from the ImageNet-1 K dataset, the five-class and binary classification accuracies of ViT-B (224) pre-trained with MAE were increased by 2.65% and 1.52%, respectively.",0,negative
"[35] have demonstrated that MAE has the advantages of scalability and simplicity compared with contrastive self-supervised learning approaches, such as DINO and MoCo.",1,neutral
"…2018), clustering (Van Gansbeke et al., 2020; Caron et al., 2020), contrastive learning (Chen et al., 2020; He et al., 2020), mask and reconstruct (He et al., 2022), etc. are adopted to extract transferable representations from the
Algorithm 1 Effective bias-Conflicting Scoring (ECS)
Input:…",2,positive
"This provides a strong regularization effect, and improves generalization consistently, across architectures, modalities and loss functions (Hernández-Garcı́a & König, 2018; Steiner et al., 2021; Hou et al., 2018; Shen et al., 2020; Chen et al., 2020; Caron et al., 2020; He et al., 2021).",2,positive
"However, note that our approach is general and easily extends to self-supervised settings. e.g. (Chen et al., 2020; He et al., 2021; Chen et al., 2021).",1,neutral
"This provides a strong regularization effect, and improves generalization consistently, across architectures, modalities and loss functions (Hernández-Garcı́a & König, 2018; Steiner et al., 2021; Hou et al., 2018; Shen et al., 2020; Chen et al., 2020; Caron et al., 2020; He et al., 2021).",2,positive
We only take the unmasked patches as the input of the encoder similar to MAE [9].,2,positive
"• Vanilla MAE (MAE) [9], which uses an autoencoder to reconstruct the images from masked images.",1,neutral
"(Yang et al., 2022)
Oriented RCNN ViTAE 81.24 Use MAE (He et al., 2022) to pretrain the plain ViTAE transformer.",2,positive
"24 Use MAE (He et al., 2022) to pretrain the plain ViTAE",2,positive
"Furthermore, to explore the effectiveness of unsupervised pretraining, MAE (He et al., 2022) is employed to pretrain the model on the MillionAID dataset (Long et al.",2,positive
"With the pretraining method MAE andRVSA, the detector outperformed all previousmethods, achieving 81.24% and 71.05%mAP onDOTA-V1.",0,negative
"Furthermore, to explore the effectiveness of unsupervised pretraining, MAE (He et al., 2022) is employed to pretrain the model on the MillionAID dataset (Long et al., 2021).",2,positive
"In the first stage, Semi-ViT uses the pre-training model of MAE.",0,negative
"First, it trains a ViT-based encoder fθ(·) on all images in X via self-supervised methods such as MAE [28].",2,positive
"Recent advances in self-supervised learning3 [90, 91, 92, 27, 93, 28] and diffusion probabilistic models [1, 2, 3, 4, 5, 6] achieve excellent performance in the two tasks respectively.",1,neutral
"For feature extraction, we use MAE [7].",2,positive
"In the seminal work of [39], the authors propose a simple framework where an autoencoder is fed with partially masked images, and the accompanying decoder is tasked with reconstructing the original images.",1,neutral
"While the core underlying idea of the MaskedKD can be applied to the case of selfsupervised learning, it is unclear how one can combine it with self-supervision strategies that utilize masking, e.g., masked autoencoders (He et al., 2022).",1,neutral
", 2019), the masked pre-training of ViTs gives significant performance boosts to various downstream supervised tasks (Dosovitskiy et al., 2020; He et al., 2022).",2,positive
"Similar to the masked language modeling methods (Devlin et al., 2019), the masked pre-training of ViTs gives significant performance boosts to various downstream supervised tasks (Dosovitskiy et al., 2020; He et al., 2022).",2,positive
"For instance, many advanced frameworks
firstly were developed for CV tasks, like CPC[118], momentum contrast (MoCo)[119], SimCLR[113], BYOL[114], SwAV[120], MAE[121], Siamese[122], etc. Additionally, BERT[8] still performs poorly in processing NLP-related tasks.",1,neutral
"For instance, many advanced frameworks firstly were developed for CV tasks, like CPC[118], momentum contrast (MoCo)[119], SimCLR[113], BYOL[114], SwAV[120], MAE[121], Siamese[122], etc.",1,neutral
"Z = z1, z2, · · · , zB Q = q1, q2, · · · , qB C = c1, c2, · · · , cK Swapping assignments between multiple views (SwAV)[120] is a cluster assignment-based contrastive learning paradigm.",1,neutral
The ViT models are further improved by pre-training masked auto-encoders on unlabeled images [7].,2,positive
"For pre-training Edge MAE, we tuned the learning rate as 1.5e-4, weight decay as 0.05, the mask ratio as 1/3, and batch size as 2048.",0,negative
"From the two tables, we can see that, Edge Transformer outperforms almost all baseline models, and the results can be further improved by pre-training Edge MAE on unlabeled edges.",2,positive
"Furthermore, we propose a novel Edge Transformer model and pre-train the model via Masked Auto-Encoders (MAE).",2,positive
The decoder layer of Edge MAE is tuned as 1.,1,neutral
The encoder of an Edge MAE is an Edge Transformer but only applied on unmasked tokens.,1,neutral
All four edge tokens are the input to the Edge MAE decoder.,1,neutral
"In this section, we evaluate our Edge Transformer and Edge MAE in two settings.",2,positive
"Following MAE, Edge MAE randomly masks a proportion of input tokens.",1,neutral
"We can see the proposed Edge MAE achieves the best results, and link prediction models perform better as they model the whole return process entirely.",2,positive
"4.2 Parameter Settings For Edge Transformer and Edge MAE, we set the latent vector size 𝐷 as 256, the dropout rate as 0.0, and the number of attention heads as 3.",1,neutral
The reconstruction target in MAE allows the model to learn the prior distribution of node and edge features.,1,neutral
"Inspired by MAE [7] in CV which pre-trains masked auto-encoders on unlabeled images, we propose to improve Edge Transformer with an Edge MAEmodel.",2,positive
"For the MAEViT-Base (He et al., 2022) baseline, we report additional results for λ values 1, 10, and 50.",2,positive
"When η = 1.0, it means the number of features in the baseline is the same as the number of features learned using PISCO and as a result, similar for CIFAR-10 results in §D.2, we observe the in-distribution performance of PISCO in this case being almost the same as that of baseline methods even for higher values of λ.
MAE-ViT-Base results for λ values 1, 10, and 50: Results for additional values of λ when MAE-ViT-Base is the baseline are in Table 19.",2,positive
"We also report analogous results for another popular feature extractor, MAE-ViT-Base (He et al., 2022), in Table 5.",2,positive
"We train two types of auto-encoders (AEs) to reconstruct the images in the collection D, namely denoising AEs [73] and masked AEs [28].",2,positive
We employ Adam [36] and the mean squared error (MSE) to train both AEs.,2,positive
"However, as soon as an image from a different distribution is given as input, AEs exhibit poor reconstruction capabilities.",1,neutral
"First of all, we propose four novel pre-retrieval predictors, namely (i) the magnitude of the reconstruction error of denoising [73] or masked [28] auto-encoders trained on the database, (ii) the density of the k-means cluster to which the query image embedding is assigned, (iii) the confidence distribution of a classification head attached to the embedding layer of the retrieval model, and (iv) the score predicted by a fine-tuned ViT model [20].",2,positive
"Many new SOTA performances are achieved on several downstream CV tasks, including object detection[61], semantic segmentation[62], image processing[63], video understanding[63].",1,neutral
"We compare the soups to a nominal model, the l∞-robust classifier used in the soups, their ensemble, the Masked Autoencoders of [18], AdvProp [54], PyramidAT [24], and the ensemble obtained by averaging the output (after softmax) of the four models included in the soups.",2,positive
"Finally, while the capability of selecting a model specific to each image distribution is a main point of our model soups, we also show that it is possible to jointly select a soup for average performance across several IMAGENET variants to achieve better accuracy than adversarial and self-supervised baselines [18,24].",2,positive
Other alternatives carefully tweak R per dataset and architectures e.g. to only compute the reconstruction loss on parts of the data as with BERT Devlin et al. (2018) or MAEs He et al. (2022).,2,positive
"Transformers have been used for robot control and planning [25, 26, 27], object recognition [28], and robot navigation [29].",1,neutral
"|D<t) where k ∈ [1, . . . ,K] on ImageNet for 4 different architecture and pre-training methods: ResNet50 + SimCLR, ResNet50 + BYOL, ViT/B16 + DINO and ViT/B16 + MAE.",2,positive
"…and self-supervised learning, the network is trained on a surrogate task, such as reconstruction (Hinton & Salakhutdinov, 2006; Kingma & Welling, 2013; He et al., 2021) and contrastive prediction (van den Oord et al., 2018; Chen et al., 2020), which is supposed to capture generic prior of the data.",2,positive
"MAE generalizes well with dataset shift, but the performance on natural datasets suffers compared to other methods.",2,positive
"MAE (He et al., 2021) uses masked reconstruction tasks to learn representation.",1,neutral
"In unsupervised and self-supervised learning, the network is trained on a surrogate task, such as reconstruction (Hinton & Salakhutdinov, 2006; Kingma & Welling, 2013; He et al., 2021) and contrastive prediction (van den Oord et al.",1,neutral
"For ViT/B16 backbone, DINO features outperform both supervised and MAE, while supervised and MAE have no significant difference.",2,positive
"Natural Special Structured
A rc
hi te
ct ur
e
O bj
ec tiv
e
C al
te ch
10 1
C ifa
r1 00
D T D Fl
ow er
s1 02
Pe ts SV H N Su
n3 97
E ur
oS A T Pa tc hC
am el
yo n
R es
is c4 5 R et in
op at hy C le vr C ou nt C le vr D is
ta nc e D M L ab D Sp ri te sL oc at
io n
D Sp
ri te
sO ri
en ta
tio n
K itt
iD is
ta nc e Sm al lN O R B A
zi m
ut h
Sm al
lN O
R B
E le
va tio
n
ResNet50 SUP 1 1 1 1 1 1 1 1 3 1 3 4 3 3 4 2 4 3 2 ResNet101 SUP 1 1 3 1 0 1 1 1 2 1 2 3 3 2 2 2 4 3 2 ResNet152 SUP 1 1 1 2 0 1 1 1 2 1 2 3 4 2 1 2 5 3 7 ResNet50 SimCLR 1 1 1 1 3 2 1 1 4 1 2 7 3 5 3 3 4 7 7 ResNet50 BYOL 1 1 1 1 1 2 1 1 6 1 3 4 5 5 4 3 7 6 7 ResNet101 BYOL 1 1 1 1 1 2 1 1 3 1 3 4 6 5 3 3 6 6 7 ResNet152 BYOL 1 1 1 1 0 2 1 1 3 1 3 5 4 4 2 3 4 6 3 ViT/S16 SUP 1 1 1 1 1 1 0 1 4 1 3 3 2 2 4 5 5 7 4 ViT/B16 SUP 0 1 2 0 1 1 1 1 3 1 3 7 6 2 3 3 4 3 7 ViT/L16 SUP 1 1 2 1 2 0 1 1 3 2 2 7 7 3 1 3 5 4 7 ViT/B16 DINO 0 0 1 0 1 1 0 1 3 1 4 5 5 6 3 3 3 3 7 ViT/S16 MAE 1 1 1 1 2 2 1 1 3 1 1 2 2 6 7 4 7 6 5 ViT/B16 MAE 0 1 1 1 2 1 0 1 2 1 1 3 4 4 4 6 5 7 4 ViT/L16 MAE 1 1 1 1 2 1 1 1 2 1 1 4 4 4 4 5 6 5 6
Readout Models.",0,negative
"This is consistent with previous observations (Resnick et al., 2019; He et al., 2021) and demonstrates the importance of incorporating a multitude of readout methods into the evaluation framework.",2,positive
"Interestingly, the reconstruction-based MAE objective scales well to larger architectures and has not saturated at ViT/L16.",2,positive
"For ResNet-50, we pre-train on supervised SimCLR and BYOL losses; for ViT/B16, we pre-train on supervised (Touvron et al., 2020; He et al., 2021) DINO and MAE losses.",0,negative
"Natural # Special # Structured #
ViT/B16 DINO 1.57 1 1.00 1 2.63 3 ResNet50 BYOL 2.71 2 3.00 2 2.13 1 ViT/B16 MAE 5.71 6 3.25 3 2.38 2 ViT/B16 SUP 2.86 3 3.25 4 5.75 6 ResNet50 SimCLR 4.29 5 5.75 6 3.38 4 ResNet50 SUP 3.86 4 4.75 5 4.75 5
ranks in Table 1b.",0,negative
He et al. (2021) also argue that the use of linear probes limits the development of methods that induce non-linear representations.,1,neutral
"Linear probing, using a linear layer for readout, is the most prevalent evaluation protocol for representations, the performance of which is reported and compared in almost all of the recent representation learning works (van den Oord et al., 2018; Chen et al., 2020; Grill et al., 2020; Li et al., 2021; Caron et al., 2021; He et al., 2021).",1,neutral
"…is the most prevalent evaluation protocol for representations, the performance of which is reported and compared in almost all of the recent representation learning works (van den Oord et al., 2018; Chen et al., 2020; Grill et al., 2020; Li et al., 2021; Caron et al., 2021; He et al., 2021).",2,positive
"Following the command in Masked AutoEncoder [9], we use a lightweight decoder, which has only 10% computation per token compared with the encoder.",2,positive
"After the pre-training & fine-tune method [18] was proposed, more andmore experiments demonstrated its superior performance, so here, we also used the Masked AutoEncoder pre-trained ViT-base model.",2,positive
"As for the structure of our decoder, inspired by Masked AutoEncoder [9], Our decoder is also based on the attention algorithm, which has the same structure as the encoder(ViT-base) but is only 10% of its size.",2,positive
Proposed Masked AutoEncoder.,2,positive
"(2022), Z is obtained by clustering the latent features produced by a Masked Autoencoder (He et al., 2022).",1,neutral
"For example, in Liu et al. (2022), Z is obtained by clustering the latent features produced by a Masked Autoencoder (He et al., 2022).",1,neutral
MAEDAY [68] uses the reconstruction error of a pre-trained masked autoencoder [27] to generate anomaly segmentation masks.,1,neutral
"[27] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Inspired by [11], a Self-supervised Masking Reconstruction branch (SMR) is designed to impose implicit regularization by providing extra noise-free supervised signal.",1,neutral
"To further improve the robustness of SGLDM to different sketches, we adopt the arbitrarily masking conditional training strategy in training inspired by Masked AutoeEncoder [7], which masks random patches of the input and let the model reconstruct it automatically.",2,positive
"2021; Tomasev et al., 2022; Caron et al., 2021; Chen et al., 2021; Li et al., 2022a; Zhou et al., 2022a;b; HaoChen et al., 2021; He et al., 2022; Bardes et al., 2022), catching up to supervised baselines in tasks requiring high-level information such as classification.",0,negative
Currently two types of popular pretext tasks have been designed and applied for self-supervised learning: masked image modeling (Bao et al. 2022; He et al. 2022) and contrastive learning (Chen et al. 2020; He et al. 2020; Grill et al. 2020).,1,neutral
"2022), we aim to adapt the data-efficient masked image autoencoder (MAE) (He et al. 2022) to multi-modal and small-scale RGB-D datasets for better representation learning.",2,positive
"Driven by the good self-supervised learning performance of MAE (He et al. 2022) in images, we design a multi-modal masked autoencoder (MM-MAE) in RGBD data.",2,positive
"Unlike the original MAE (He et al. 2022), our CoMAE presents a shared encoder and decoder among RGB and depth modalities and acts as a kind of regularizer to guide pre-training.",2,positive
Currently two types of popular pretext tasks have been designed and applied for self-supervised learning: masked image modeling (Bao et al. 2022; He et al. 2022) and contrastive learning (Chen et al.,1,neutral
"competitive and even better results of the same ViT-B model pre-trained by ImageMAE (He et al. 2022), this reveals the data efficiency advantage of our CoMAE pre-training.",2,positive
"Inspired by the results in VideoMAE (Tong et al. 2022), we aim to adapt the data-efficient masked image autoencoder (MAE) (He et al. 2022) to multi-modal and small-scale RGB-D datasets for better representation learning.",2,positive
"In addition, our CoMAE instantiated with ViT-B also achieves
competitive and even better results of the same ViT-B model pre-trained by ImageMAE (He et al. 2022), this reveals the data efficiency advantage of our CoMAE pre-training.",2,positive
", 2021), MAE (He et al., 2022), and ResNet (He et al.",2,positive
"Vision models We include 14 computer vision models (VMs) in our experiments, representing three families of models: SegFormer (Xie et al., 2021), MAE (He et al., 2022), and ResNet (He et al., 2016).",2,positive
"With the rapid growth of the number of large-scale pretrained models [15, 42], we believe our work paves a new way for efficient model development and deployment, yielding a significant step towards Green AI.",2,positive
"Most recently, large-scale self-supervised pretraining has helped ViTs achieve promising results on ImageNet, including contrastive learning [6, 9] and masked image modeling [3, 15, 19, 67].",1,neutral
", BEiT [4] and MAE [3,18]) benefit the excellent representation learning, which improve the finetuning performance in downstream tasks.",1,neutral
We also compare M2A2E with the symmetric multimodal MAE [3] when finetuning on all downstream modality settings.,2,positive
Then random sampling strategy [18] is used to mask out p percentage of the visual tokens in Xi.,1,neutral
"On the other hand, self-supervised masked image modeling (MIM) methods (e.g., BEiT [4] and MAE [3,18]) benefit the excellent representation learning, which improve the finetuning performance in downstream tasks.",1,neutral
"Relation to modality-symmetric autoencoders [3, 18].",1,neutral
"Compared with the vanilla MAE [18], M2A2E adopts the same masked strategy in unimodal ViT encoder but targeting at multimodal reconstruction with multiple unshared ViT decoders.",2,positive
"Different from the conclusions from [3, 18] using very large mask ratio (e.",1,neutral
"Compared with the vanilla MAE [18], M(2)A(2)E adopts the same masked strategy in unimodal ViT encoder but targeting at multimodal reconstruction with multiple unshared ViT decoders.",2,positive
"Besides, M2A2E is similar to the multimodal MAE [3] only when partial tokens from a single modality are visible while masking all tokens from other modalities.",1,neutral
"Despite mature exploration and finds [3,18,44] of ViT on other computer vision communities (e.",1,neutral
"We find that 1) leveraging local feature descriptors benefits the ViT on IR modality; 2) partially finetuning or using adapters can achieve reasonable performance for ViT-based multimodal FAS but still far from satisfaction; and 3) mask autoencoder [3, 18] pre-training cannot provide better finetuning performance compared with ImageNet pre-trained models.",2,positive
"Compared with modality-symmetric autoencoders [3,18], the proposed M(2)A(2)E is able to learn more intrinsic task-aware representation and compatible with modality-agnostic downstream settings.",2,positive
"9 that with more challenging reconstruction target (from masked unimodal inputs to multimodal prediction), M2A2E is outperforms the best settings of multimodal MAE [3] on most modalities (‘RGB’, ‘IR’, ‘RGB+IR’, ‘RGB+Depth’, ‘RGB+IR+Depth’), indicating its excellent downstream modality-agnostic capacity.",2,positive
Comparison between multimodal MAE [3] and M2A2E.,1,neutral
"Following MAE [21], we use the normalized pixel values as targets in the reconstruction task.",2,positive
"Following MAE [21], we only feed the visible, unmasked patches to the online encoder Es.",0,negative
"Here, u and r are random masking, which is similar to the “random sampling” adopted in MAE [21].",1,neutral
"For MIM tasks, we apply augmentations and masking strategy following MAE [21] to the input of the online encoder.",2,positive
"Existing work mainly differ in their regression objectives [2, 14, 21, 48, 50, 53] or masking strategies [27, 30, 43].",1,neutral
"Nowadays, contrastive learning (CL) [3, 11, 40, 57] and masked image modeling (MIM) [2, 21, 53], together with Vision Transformers (ViTs) [16, 32], have revolutionized the field of SSL in computer vision and medical imaging, which achieve the state-of-the-art (SOTA) performance for a variety of tasks [3, 21, 45, 52, 63].",1,neutral
"However, when pretrain and downstream tasks are very different, adapting the features is important and FT outperforms HP (Chen et al., 2020b; Zhai et al., 2019; He et al., 2022).",2,positive
"Recently, self-supervised Masked Autoencoder (MAE) [13] has achieved great success in feature representation and assisted many downstream tasks.",1,neutral
"Since MAE [47] only applied masks in 2D images, while video anomalies are related to the temporal information, TMAE first located video foregrounds and constructed temporal cubes to be masked objects.",1,neutral
Liu et al. [87] proposed an Appearance-Motion united Auto-Encoder (AMAE) framework using two independent auto-encoders to perform denoising and optical flow generation tasks separately.,1,neutral
"Inspired by the Masked Auto-Encoder (MAE) [47], their proposed TMAE learned representations using a visual transformer performing a complementary task.",2,positive
"Meanwhile, several works [16, 38] have demonstrated that pretraining networks to predict masked patches from unmasked patches on a large-scale dataset can enhance the fully-supervised training on another small-scale dataset significantly.",1,neutral
"annotation-free pretext tasks and learning to predict them [25], [26].",1,neutral
"5, which is lower than the best configuration reported in (He et al., 2022).",0,negative
"Interestingly, fixed-region representations, namely CNNFeat and MAEPatch, failed for the comparison tasks while also utilizing a transformer pooling layer.",1,neutral
"MAE
The Multi-modal Auto-Encoder (MAE) encoder architecture is based on the Vision Transformer Base (ViT-Base) architecture (Dosovitskiy et al., 2020).",2,positive
", 2018) for end-to-end learned fixedregion representations and the mini-patch representations from pre-trained Masked AutoEncoder (MAE) (He et al., 2022) for pre-trained fixed-region representations.",1,neutral
"The masking ratio used during pre-training was set to 0.5, which is lower than the best configuration reported in (He et al., 2022).",0,negative
"For our experiments, we use the CNN feature map (Santoro et al., 2017; Zambaldi et al., 2018) for end-to-end learned fixedregion representations and the mini-patch representations from pre-trained Masked AutoEncoder (MAE) (He et al., 2022) for pre-trained fixed-region representations.",2,positive
"The pre-training of the MAE encoder was conducted using a batch size of 128, a learning rate of 0.001, and a weight decay of 0.05.",0,negative
"Recently, [41] and [18] show that a masked image modeling task of simply regressing directly on the image pixels is sufficient and effective.",1,neutral
"Masked image modeling (MIM) has been proposed in various forms in recent years, and has recently been found to be particularly effective in the natural image domain, surpassing many contrastive works and being shown to be friendlier to downstream optimization [41, 18, 44, 3, 40] In general, the goal is to learn from data in a self-supervised manner by asking the model to generate pixel values for intentionally-withheld regions in an image.",1,neutral
"On the other hand, MIM objectives like [41, 18] rely only on simple spatial augmentations such as flipping and cropping.",1,neutral
"In this work, we develop our pretraining objective based on a masked image modeling approach like [41, 18].",2,positive
MIM was first proposed in natural language processing [2].,1,neutral
"However, MIM on a single ViT significantly improves its AUROC to 98.30% (2.07% higher), Model Fine-tuned AUROC(",2,positive
We perform OOD detection with MIM pretext task with each metric – the results are shown in Tab.,2,positive
"For one-class OOD detection, we pre-train the MIM model and finely tune it on ImageNet-21k [49], as recommended by BEiT [2].",2,positive
"Specifically, we adopt the masked image modeling (MIM) [2, 11, 20] as our self-supervised pretext task, which has been demonstrated to have great potential in both natural language processing [11] and computer vision [2,20].",2,positive
"In our method, we pre-train the model with Masked Image Modeling (MIM) pretext [11] on a large dataset and fine-tune it on the ID dataset.",2,positive
"Specifically, we adopt the masked image modeling (MIM) [2,11,20] as our self-supervised pretext task, which has been demonstrated to have great potential in both natu-ral language processing [11] and computer vision [2,20].",2,positive
We compare the performance of MIM and contrastive learning pretext task MoCov3 [8] in Tab.,2,positive
"In the MIM task, we split images into patches and randomly mask a proportion of image patches before feeding the corrupted input to the vision transformer.",2,positive
"For multi-class OOD detection, we pre-train the MIM model, intermediately use fine-tuning on ImageNet-21k [49], and apply fine-tuning again on the ID dataset.",2,positive
"Transformer has achieved promising performance in computer vision [2, 20] and natural language processing [11].",2,positive
"Follow-up research [11, 20] transfers the similar idea from natural language processing to computer vision, masking different proportions of the image patches to recover results.",1,neutral
The self-supervised pretext task in our framework is Masked Image Modeling (MIM).,2,positive
It shows that the performance of MIM is much increased by 13.3% to 98.66%.,0,negative
"Specifically, masked autoencoders are a form of more general denoising autoencoders [32, 79], which adopt a simple concept to remove a proportion of the data and then learn to recover the removed parts.",1,neutral
"Inspired by the tremendous success of the masked autoencoding paradigm in NLP [19] and Computer Vision [32], we propose to incorporate a new masked autoencoder model into our generalized framework.",2,positive
"paradigm in NLP [19] and Computer Vision [32], we propose to incorporate a new masked autoencoder model into our generalized framework.",2,positive
"Notably, whenγ = 1, our masked autoencoder is equivalent to MAE for vision [32].",1,neutral
"Parameter-efficient finetuning techniques (Houlsby et al., 2019; Hu et al., 2022; Lester et al., 2021; Li & Liang, 2021; He et al., 2022a; Ben Zaken et al., 2022; Sung et al., 2021; Qing et al., 2022) are first proposed in NLP since full finetuning the increasingly larger language models for…",2,positive
"To alleviate the labeling cost, self-supervised learning methods (Chen et al., 2021; Bao et al., 2021; Zhou et al., 2021; He et al., 2022b; Xie et al., 2022) are introduced to learn effective representations from unlabeled data.",2,positive
"We also hope to incorporate selfsupervised methods that learn representations by training on a union of unlabeled data from source and target via proxy tasks like reconstruction (Gidaris et al., 2018; He et al., 2022) and contrastive learning (Caron et al.",2,positive
"We also hope to incorporate self-supervised methods that learn representations by training on a union of unlabeled data from source and target via proxy tasks like reconstruction (Gidaris et al., 2018; He et al., 2022) and contrastive learning (Caron et al., 2020; Chen et al., 2020).",2,positive
"The lack of usability explains why generative encoders such as MAE do not give a good linear probing performance, despite their strong fine-tuning performance (He et al., 2022).",2,positive
"SSL pipelines differ in many design choices, such as the objective (Chen et al., 2020a; He et al., 2022), architecture (Caron et al.",2,positive
"Pretrained MAE and CLIP with world model (MAE+WM, CLIP+WM)
Masked autoencoder (MAE; He et al. 2021) learns visual representation in a self-supervised manner by training a vision Transformer (ViT; Dosovitskiy et al. 2021) to reconstruct masked patches.",2,positive
", 2021) and MAE (He et al., 2021) representations as they have recently shown to be effective for robotic manipulation (Shridhar et al.",1,neutral
"Convolutional feature embedding Unlike prior work that masks random pixel patches (He et al., 2021), we embed camera observations into convolutional feature maps and mask these features following the design of Seo et al. (2022a).",2,positive
", 2022a) that masks convolutional features instead of pixel patches (He et al., 2021) and predicts rewards to learn representations capturing fine-grained details required for visual control.",2,positive
"Specifically, we consider CLIP (Radford et al., 2021) and MAE (He et al., 2021) representations as they have recently shown to be effective for robotic manipulation (Shridhar et al., 2021; Radosavovic et al., 2022).",2,positive
"We hypothesize this is because spatial information redundancy (He et al., 2021) is more significant in visual observations from
manipulation tasks than natural images.",1,neutral
"We hypothesize this is because spatial information redundancy (He et al., 2021) is more significant in visual observations from manipulation tasks than natural images.",1,neutral
"We also incorporate the idea of a prior work (Seo et al., 2022a) that masks convolutional features instead of pixel patches (He et al., 2021) and predicts rewards to learn representations capturing fine-grained details required for visual control.",2,positive
"Convolutional feature embedding Unlike prior work that masks random pixel patches (He et al., 2021), we embed camera observations into convolutional feature maps and mask these features following the design of Seo et al.",2,positive
"R O
] 3
1 M
ay 2
02 3
experiments on RLBench (James et al., 2020) show that our method outperforms single-view representation learning baselines (Radford et al., 2021; He et al., 2021; Seo et al., 2022a) and a multi-view representation learning baseline (Sermanet et al., 2018).",2,positive
", 2020) show that our method outperforms single-view representation learning baselines (Radford et al., 2021; He et al., 2021; Seo et al., 2022a) and a multi-view representation learning baseline (Sermanet et al.",2,positive
"…as computer vision (Ren et al., 2015; He et al., 2017; Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; Chen & He, 2021; Grill et al., 2020; He et al., 2022), natural language processing (Devlin et al., 2019; Brown et al., 2020; Raffel et al., 2020) and cross-modal learning (Radford et…",1,neutral
"…compare logistic regression and naïve Bayes on the CIFAR10 and CIFAR100 datasets in various models, which are trained on image-label pairs (Dosovitskiy et al., 2021; He et al., 2016), image-text pairs (Radford et al., 2021), or pure images (Chen et al., 2020d;c; He et al., 2022; Xie et al., 2022).",1,neutral
"We then perform extensive experiments on CIFAR10 and CIFAR100 datasets with various representative pre-trained vision models (He et al., 2016; Dosovitskiy et al., 2021; Chen et al., 2020d;c; Radford et al., 2021; Xie et al., 2022; He et al., 2022), which are trained in supervised or self-supervised manners.",2,positive
"Deep representation learning has achieved great success in many fields such as computer vision (Ren et al., 2015; He et al., 2017; Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; Chen & He, 2021; Grill et al., 2020; He et al., 2022), natural language processing (Devlin et al.",1,neutral
"1, we empirically analyze the main assumptions of our theory in various deep vision models (Dosovitskiy et al., 2021; He et al., 2016; Radford et al., 2021; Chen et al., 2020d;c; He et al., 2022; Xie et al., 2022).",2,positive
"It has made remarkable progress in various machine learning fields (Ren et al., 2015; He et al., 2017; Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; Chen & He, 2021; Grill et al., 2020; He et al., 2022; Xie et al., 2022; Devlin et al., 2019; Brown et al., 2020; Raffel et al., 2020).",2,positive
"com/facebookresearch/mae (He et al., 2022) License https://github.",2,positive
"We adopt pre-trained checkpoint in (He et al., 2022).",2,positive
"…extensive experiments on CIFAR10 and CIFAR100 datasets with various representative pre-trained vision models (He et al., 2016; Dosovitskiy et al., 2021; Chen et al., 2020d;c; Radford et al., 2021; Xie et al., 2022; He et al., 2022), which are trained in supervised or self-supervised manners.",2,positive
", 2022) extends MAE (He et al., 2022) by reconstructing masked point clouds.",2,positive
"In 3D, PointMAE (Pang et al., 2022) extends MAE (He et al., 2022) by reconstructing masked point clouds.",2,positive
"(3) (4)Note that for methods like MAE (He et al., 2022; Pang et al., 2022), the teacher network is an identity mapping with no parameters, and therefore φ is an empty set.",1,neutral
"Ouyang et al., 2022), 2D machine vision (He et al., 2020; 2022), and both (vision-language, VL) (Radford et al., 2021; Rombach et al., 2022; Alayrac et al., 2022).",2,positive
"(3) 4Note that for methods like MAE (He et al., 2022; Pang et al., 2022), the teacher network is an identity mapping with no parameters, and therefore φ is an empty set.",1,neutral
"Generative Masked Representation Learning has emerged as another paradigm of self-supervised learning from NLP (Devlin et al., 2019) to Vision (He et al., 2022).",1,neutral
"It can be `2 distance (He et al., 2022), crossentropy (Devlin et al.",1,neutral
"It can be `2 distance (He et al., 2022), crossentropy (Devlin et al., 2019; Bao et al., 2022), or Chamfer-Distance (Fan et al., 2017; Pang et al., 2022).",1,neutral
He et al. (2022) propose masked autoencoder (MAE) to reconstruct RGB pixels.,2,positive
"The masked ratio can be even higher than the original configuration described in MAE (He et al., 2022) (e.",1,neutral
"Masked AutoEncoder Masked AutoEncoder (He et al., 2022) is the dominant approach in visual pre-training, surpassing the performance of contrastive learning with less computational requirements.",1,neutral
", 2022), (He et al., 2022)) has become another main paradigm for learning self-supervised vision representations.",1,neutral
"MAE (He et al., 2022) (see Figure 2) and SimMIM (Xie et al.",1,neutral
", 2021) and MAE (He et al., 2022), we adopted AdamW (Loshchilov & Hutter, 2017) for pre-training, fine-tuning.",2,positive
", 2020)(He et al., 2022) has shown impressive potential in various vision tasks and applications, owing to increasingly available data and advancing hardware.",2,positive
"According to the setup in MoCo v3 and MAE (He et al., 2022), both adopted a batch size of 4,096.",0,negative
"To address the representation deficiency issue in MLM, we propose a simple framework MAE-LM, which pretrains bidirectional Transformer encoders using the MLM objective, but based on the Masked Autoencoder (He et al., 2022) structure.",2,positive
"To address the representation deficiency issue, we propose a simple text encoder pretraining method, MAE-LM, which conducts MLM pretraining based on the Masked Autoencoder architecture (He et al., 2022).",2,positive
"(2021a); Ho et al. (2020) with ↓ (x) = √γx + √ 1− γε, with ε ∼ N (0, I) and γ uniformly sampled as γ ∼ U(0, 1). As shown in the quantitative results in Table 2 and visualizations in Figure 3, with proper degrees of corruption, restoring the original images may require the network to infer the general content given the corrupted pixels, and recover the details using the knowledge learned from the true samples and stored in the network weights. For example, in the image colorization experiments, the pretrained vision model learns the common colors of different objects from the massive unlabeled data in a self-supervised way. As visualized in Figure 3, the vision model learns common knowledge such as stop signs are usually red, and the background of a horse is usually green while manatees are marine mammals therefore the background is usually blue. Summarizing such knowledge requires the vision models to learn identifying objects first, therefore transferable network weights and feature representations can be obtained from pretraining. And as shown in the ‘Denoising’ row of Figure 3, with strong noise injected to the input, the model is able to recover objects that are almost invisible. This finding potentially connects our pretraining method with genitive models Song et al. (2021a); Ho et al. (2020); Song & Ermon (2019); Song et al. (2021b). We will further investigate the connections in future efforts.",0,negative
"AE + SR 16 denotes a baseline experiment with a auto-encoder architecture as in He
et al. (2022).",2,positive
The remarkable success of exploiting context information resides in the massive unlabeled data in natural language processing (NLP) stimulates the recent progress of self-supervised vision model through masked image modeling (MIM) He et al. (2022); Wei et al. (2021); Xie et al. (2022).,1,neutral
"In Table 3, we compare our method against DINO Caron et al. (2021), MoCo-V3 Chen et al. (2021), MaskFeat Wei et al. (2021), BEiT Bao et al. (2021), iBOT Zhou et al. (2022), and MAE He et al. (2022).",2,positive
"Note that in the Gridded (16) experiments, the patch partition in the image masking matches exactly with the patch partition in the ViT networks, therefore it is a fair comparison against MAE He et al. (2022).",2,positive
Recent self-supervised vision model pretraining methods Xie et al. (2022); He et al. (2022); Wei et al. (2021) invariably adopt masked image modeling as the pretext task.,1,neutral
The proposed method can achieve better results with fewer epochs of training compared to MAE He et al. (2022).,2,positive
"This phenomena is also observed and discussed in He et al. (2022), and may be attributed to the fact that both He et al. (2022) and our method do not explicitly encourage linear separation of features in the pretraining stage as the contrastive learning based method do.",1,neutral
"Unlike recent methods Zhou et al. (2022); He et al. (2022), we do not perform exhaustive searches for the optimal hyperparameters such as learning rates.",1,neutral
Most of the experimental settings follow He et al. (2022).,2,positive
"Different from models like MAE He et al. (2022) and SimMIM Xie et al. (2022) that are specifically tailored for particular network architectures, our framework can be seamlessly applied to any deep vision models without any customization or auxiliary network components beside the simple linear head h.",2,positive
"Different from models like MAE He et al. (2022) and SimMIM Xie et al. (2022) that are specifically tailored for particular network architectures, our framework can be seamlessly applied to any deep vision models without any customization or auxiliary network components beside the simple linear head…",2,positive
The result of MAE He et al. (2022) with 400 epochs is based on our reimplementation.,2,positive
"(2021a); Ho et al. (2020) with ↓ (x) = √γx + √ 1− γε, with ε ∼ N (0, I) and γ uniformly sampled as γ ∼ U(0, 1).",1,neutral
"We follow the details presented in MAE He et al. (2022) and implement an asymmetric
Methods GPUs × H Acc.",2,positive
Masked auto-encoder He et al. (2022) adopts an asymmetric encoder-decoder architecture and shows that scalable vision learners can be obtained simply by reconstructing the missing pixels.,2,positive
"(2021a); Ho et al. (2020) with ↓ (x) = √γx + √ 1− γε, with ε ∼ N (0, I) and γ uniformly sampled as γ ∼ U(0, 1). As shown in the quantitative results in Table 2 and visualizations in Figure 3, with proper degrees of corruption, restoring the original images may require the network to infer the general content given the corrupted pixels, and recover the details using the knowledge learned from the true samples and stored in the network weights. For example, in the image colorization experiments, the pretrained vision model learns the common colors of different objects from the massive unlabeled data in a self-supervised way. As visualized in Figure 3, the vision model learns common knowledge such as stop signs are usually red, and the background of a horse is usually green while manatees are marine mammals therefore the background is usually blue. Summarizing such knowledge requires the vision models to learn identifying objects first, therefore transferable network weights and feature representations can be obtained from pretraining. And as shown in the ‘Denoising’ row of Figure 3, with strong noise injected to the input, the model is able to recover objects that are almost invisible. This finding potentially connects our pretraining method with genitive models Song et al. (2021a); Ho et al. (2020); Song & Ermon (2019); Song et al.",2,positive
"For MIM, representative work [He et al., 2022] shows that in vision, removing the masked image patches before the encoder demonstrates stronger performance and 3× or more lower overall pre-training time and memory consumption than keeping the masked tokens.",2,positive
"For MIM, representative work [32] shows that in vision, removing the masked image patches before the encoder demonstrates stronger performance and 3× or more lower overall pre-training time and memory consumption than keeping the masked tokens.",1,neutral
"Token masking is a dominant approach in self-supervised pre-training tasks, such as masked language modeling (MLM) [43, 9] and masked image modeling (MIM) [6, 32].",1,neutral
"With the prevailing of large pretrained Transformer-based models, such as BERT [9] and MAE [32], the pretraining and fine-tuning paradigm has yielded strong empirical performance on various downstream tasks in NLP and CV.",1,neutral
"Token masking is a dominant approach in self-supervised pre-training tasks, such as masked language modeling (MLM) [Kenton and Toutanova, 2019; Brown et al., 2020] and masked image modeling (MIM) [Bao et al., 2022; He et al., 2022].",1,neutral
"Since squeezing the sequence length reduces both the computational and memory complexity quadratically, skipping processing the masked tokens brings considerable training efficiency gain for MLM and MIM.",1,neutral
"While BEiT proposes to predict discrete tokens based on a pretrained image tokenizer, MAE [10] and SimMIM [24] show that simple target like `1 or `2 loss on pixels is effective enough.",1,neutral
"supervised learning in the last few years, but recently, with the introduction of strong and scalable Transformer-based vision models [9, 14, 15], masked image modeling (MIM) has been developed rapidly and became the new dominant paradigm for visual feature pretraining [6, 10, 17, 24].",1,neutral
", contrastive learning [5] is shown to be effective for training ViTs without label, now it is a common belief that the generative method MIM is the most promising framework for ViTs’ self-supervised pretraining [10, 14, 17, 24].",1,neutral
"To show the effectiveness of noisy image modeling in visual feature learning and adversarial defense, we adopt two simple, representative MIM methods, SimMIM [24] and MAE [10] for comparison.",1,neutral
"For example, for masking strategy, [10] has shown that up to 75% of patches can be masked in order to learn rich representations, while [24] mask 60% of patches with a larger mask patch size — 32 pixels instead of 16 pixels for model’s patch size, and an earlier work BEiT [2] adopts a less mask ratio.",1,neutral
"It is also notable that TST (2021) outperforms all the contrastive-based baselines, where TST directly adopts the
vanilla masking protocol presented by He et al. (2022) into time series.",2,positive
"Elaborative manually-designed self-supervised tasks are presented, which can be roughly categorized into contrastive learning (He et al., 2020; Chen et al., 2020) and masked modeling (Devlin et al., 2018; He et al., 2022).",1,neutral
"…progress in natural language processing (NLP) (Brown et al., 2020; Devlin et al., 2018; Gao et al., 2020; Radford et al., 2019) and computer vision (CV) (He et al., 2022; Li et al., 2022; Radford et al., 2021) to alleviate the problem of overreliance on large-scale labeled data in deep learning.",1,neutral
", 2019) and computer vision (CV) (He et al., 2022; Li et al., 2022; Radford et al., 2021) to alleviate the problem of overreliance on large-scale labeled data in deep learning.",2,positive
", 2020) and masked image modeling (MIM) (He et al., 2022; Xie et al., 2022b; Li et al., 2022).",2,positive
"Especially, as a well-recognized pre-training paradigm, masked modeling has achieved great successes in many areas, such as masked language modeling (MLM) (Devlin et al., 2018; Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Gao et al., 2020) and masked image modeling (MIM) (He et al., 2022; Xie et al., 2022b; Li et al., 2022).",1,neutral
"…paradigm, masked modeling has achieved great successes in many areas, such as masked language modeling (MLM) (Devlin et al., 2018; Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Gao et al., 2020) and masked image modeling (MIM) (He et al., 2022; Xie et al., 2022b; Li et al., 2022).",1,neutral
", 2018) and masked patches of an image (He et al., 2022; Xie et al., 2022b) respectively.",0,negative
"This paradigm has been widely explored in computer vision and natural language processing, which is to predict the masked words of a sentence (Devlin et al., 2018) and masked patches of an image (He et al., 2022; Xie et al., 2022b) respectively.",1,neutral
"Bert-type models [1,10,21,27,29,38] and denoising auto-encoding based approaches [13, 32, 45] are good examples for this strategy.",1,neutral
"In our study, the random sampling masking ratio is 75% [37].",2,positive
The powerful global modeling capability of Transformer [37] enables it to utilize a small set of patches to repair the image.,2,positive
"Inspired by the powerful global modeling ability of Transformer [37], our network can utilize the information in a small number of image patches to repair an image.",2,positive
"Referring to the research about the decoder in pretraining in MAE [37], combined with the task of our work, we design an extremely lightweight encoder.",2,positive
Masked Auto-encoders (MAE) [31] are self-supervised learning approaches based on an asymmetric encoder-decoder architecture.,1,neutral
"Going into details, the solution proposed in [31] is based on an asymmetric encoder-decoder design where the encoder takes in input a subset of the image patches ignoring the masked ones.",2,positive
"On the other hand, two transformer-based architectures have been implemented: self-attention learners called Masked AutoEncoders (MAE) [31], which are able to automatically highlight relevant regions in brain images, and data-efficient image transformers (DeiT) [32,33], which use a renewed training procedure and require far fewer data and computational resources to build a powerful image classification model.",1,neutral
"Beyond ViT base implementation, some improvements have been recently proposed and two of the most promising approaches are the Masked Auto-Encoders (MAE) [31] and Data-efficient image Transformers (DeiT) [32].",2,positive
"The ViT architectures used the training hyperparameters and image augmentation strategies based on the cutmix and mix up approaches as described in the original paper implementation of [31,32].",2,positive
"Inspired by the success of the Transformer-based encoder presented in [31] for image restoration in terms of masked autoencoders, to improve anomaly detection performance, this article uses an inpainting subnetwork based on the Swin Transformer to restructure the masked anomaly image to an anomaly-free image.",2,positive
Recent dominant masked autoencoders for CV [12] and NLP [11] frameworks suggest that self-supervised learning should consider two tasks of the optimization process: reconstructing the masked input and forecasting whether a given sequence is the next sequence.,1,neutral
"Application-wise, masked autoencoder, a more general form, has achieved tremendous successes in both natural language processing (NLP) [11] and computer vision (CV) [12] domains.",1,neutral
"However, recent studies like MAE show promise of reconstruction-based objectives over contrastive-based counterparts, which we believe is worthy of exploration [He et al., 2022].",2,positive
"Because the reconstruction part appears only in pretraining and is not used in the sequence-based inference, we follow He et al. (2022) and use a lightweight decoder with fewer channels in Revolution.",2,positive
Mask mechanism has widely used in computer vision [26] and natural language processing [27].,1,neutral
"Later, the MAE algorithm based on the scalable self supervised learning device [19] proposed by he Kaiming and other large model designs in the CV field have been successively launched, and the architectural rudiments of the application of large models in the CV field have gradually emerged.",1,neutral
"At present, the design of self-supervised learning task is mainly image enhancement and restoration, that is, input the image processed by rotation and cutting, and then restore it [8]; Suppression or mask prediction task similar to natural language processing, randomly mask the image, and then make the model reconstruction realize self-supervised learning [9].",1,neutral
"On the other hand, visual self-supervision (Caron et al. (2021); He et al. (2022); Chen et al. (2020a); Zhou et al. (2021b)) has been widely used for visual pre-training.",1,neutral
Visual self-supervision is mainly divided into generative (He et al. (2022); Bao et al. (2021)) and contrastive (He et al. (2020a); Caron et al. (2021); Chen et al. (2020a)).,2,positive
"(2021b); Kim et al. (2021)) tasks, including textsupervised semantic segmentation (Xu et al.",1,neutral
It includes two categories: reconstructing masked images (He et al. (2022); Zhou et al. (2021b)) and multicrop image contrast (Caron et al. (2021); Chen et al. (2020a)).,2,positive
"Inspired by the work of Masked Autoencoder [20], our transformer can be divided into an encoder part and a decoder
1 3
part.",2,positive
"As mentioned in [20], a wellperformance transformer requires huge amounts of labeled training data.",1,neutral
"Inspired by the work of Masked Autoencoder [20], our transformer can be divided into an encoder part and a decoder Fig.",2,positive
"Recently, spectrograms (akin to 1-channel 2D images) autoencoder (Gong et al., 2022; He et al., 2022) with reconstruction objective as self-supervision have demonstrated the effectiveness of heterogeneous image-to-audio transfer, advancing the field of speech and audio processing on a variety of…",1,neutral
"(2022) study the Masked Autoencoders (MAE) (He et al., 2022) to self-supervised representation learning from audio spectrograms.",1,neutral
"Different from modeling fine-grain details of the signal, the usage of high-level self-supervised learning (SSL) (Baevski et al., 2020; Hsu et al., 2021; He et al., 2022) has been shown to effectively reduce the sampling space of generative algorithms.",1,neutral
"Among these approaches, Xu et al. (2022) study the Masked Autoencoders (MAE) (He et al., 2022) to self-supervised representation learning from audio spectrograms.",1,neutral
"Recently, spectrograms (akin to 1-channel 2D images) autoencoder (Gong et al., 2022; He et al., 2022) with reconstruction objective as self-supervision have demonstrated the effectiveness of heterogeneous image-to-audio transfer, advancing the field of speech and audio processing on
a variety of…",1,neutral
"Inspired by these, we inherit the recent success of spectrogram SSL in the frequency domain, which guarantees efficient compression and high-level semantic understanding.",2,positive
"Recently, spectrograms (akin to 1-channel 2D images) autoencoder (Gong et al., 2022; He et al., 2022) with reconstruction objective as self-supervision have demonstrated the effectiveness of heterogeneous image-to-audio transfer, advancing the field of speech and audio processing on a variety of downstream tasks.",1,neutral
"Following He et al. (2022), we split low-resolution radiograph into nonoverlapping image patches, where 75% patches are randomly masked.",2,positive
"In Figure 2, we provide relation maps of three samples from ImageNet, using 10 checkpoints of MAE-Large [13].",2,positive
Figure 2: Data samples and their labels from ImageNet and the corresponding relation maps by an MAE-Large model [13].,1,neutral
"When training the MAE-Large model [13] on ImageNet with 8% label noise, the validation top1-accuracy decreases by 1.",2,positive
2 Results and analysis ImageNet We measure the label error detection performance on ImageNet with the synthetic label noise by training an MAE-Large model [13].,2,positive
Figure 3: A relation graph with samples from ImageNet and the MAE-Large model [13].,1,neutral
"We measure the detection performance with MAE-Large [13], BEIT-Large [1], and ConvNeXt-Large [29] models.",2,positive
"1 Robustness to unseen augmentations The augmentation invariance guides the semantic features extraction in SSL [28, 29, 11], and demonstrated in the successes of non-contrastive models [3, 4, 5].",1,neutral
"It has been shown that the projection head can significantly improve the performance of SSL methods [1]; thus, the projection head design has been widely adopted in diverse SSL models [1, 2, 13, 14, 3, 15, 4, 5].",1,neutral
"With the rapid progress in the SSL models [1, 2, 3, 4, 5], a simple classifier learned from the pre-trained representations can achieve comparable performance to direct supervised learning.",1,neutral
"This projection head design is widely adopted by the later proposed methods, including MoCo-V2 [2, 13], BYOL [3], SwAV [14], Barlow Twins [15], SimSiam [4], MAE [5] where the architectures are displayed in Figure 3.",2,positive
"The algorithms include MoCo-v2 (He et al., 2020), MoCo-v3 (Chen et al., 2021a), InstDisc (Wu et al., 2018), BYOL (Grill et al., 2020), SwAV (Caron et al., 2020), OBoW (Gidaris et al., 2021), SimSiam (Chen & He, 2021), Barlow Twins (Zbontar et al., 2021), DINO (Caron et al., 2021), MAE (He et al., 2022), iBOT (Zhou et al., 2022) and EsViT (Li et al., 2022a).",2,positive
"This observation is in contrast to the finetuning procedure in transfer learning literature (Kornblith et al., 2019; He et al., 2022) where the learning rate is shared across the whole network; (2) the optimal hyperparameters are quite different for different ways, shots, and test datasets.",1,neutral
"…et al., 2021a), InstDisc (Wu et al., 2018), BYOL (Grill et al., 2020), SwAV (Caron et al., 2020), OBoW (Gidaris et al., 2021), SimSiam (Chen & He, 2021), Barlow Twins (Zbontar et al., 2021), DINO (Caron et al., 2021), MAE (He et al., 2022), iBOT (Zhou et al., 2022) and EsViT (Li et al., 2022a).",2,positive
"…for analyzing SSL models, contrastive learning methods MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021); masked image modeling (MIM) methods BEiT (Bao et al., 2021), MAE (He et al., 2021), and CAE (Chen et al., 2022a); and iBOT (Zhou et al., 2021) that combines contrastive learning and MIM.",2,positive
"For ADE20K, the input size is set to 512×512 following previous works (Bao et al., 2021; He et al., 2021; Chen et al., 2022a; Zhou et al., 2021).",2,positive
"For all the models involved in the experiments including DeiT (Touvron et al., 2020), MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021), BEiT (Bao et al., 2021), MAE (He et al., 2021), CAE (Chen et al., 2022a), and iBOT (Zhou et al., 2021), we use their official code to implement the encoders.",2,positive
", 2021) and MAE (Masked Autoencoder) (He et al., 2021) do not have an explicit process to predict the encoded representations of masked patches, instead, directly reconstruct the targets.",2,positive
"MAE (He et al., 2021) learns rich hidden information by directly performing masked image reconstruction in RGB color space using ViT while SimMIM (Xie et al., 2021b) uses Swin-transformer (Liu et al., 2021).",1,neutral
", 2021), or normalized RGB values used in MAE (He et al., 2021).",1,neutral
"…self-supervised representation pretraining methods, including MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021), CAE (Chen et al., 2022a), MAE (He et al., 2021), BEiT (Bao et al., 2021), and iBOT (Zhou et al., 2021), on object-level recognition (image classification and object segmentation)…",2,positive
"…reconstruct the targets.
methods:
`MIM(Decoder(Regressor(Encoder(Rv))),Target(Rm)), (3) where Target(Rm) is a function to map the masked patches to the targets, e.g., d-VAE (Ramesh et al., 2021) token used in CAE and BeiT (Bao et al., 2021), or normalized RGB values used in MAE (He et al., 2021).",2,positive
"…patches, which is commonly used in masked image modeling (MIM)
4Some MIM methods, such as BEiT (Bao et al., 2021) and MAE (Masked Autoencoder) (He et al., 2021) do not have an explicit process to predict the encoded representations of masked patches, instead, directly reconstruct the…",1,neutral
", 2021), MAE (He et al., 2021), and CAE (Chen et al.",1,neutral
"MAE (He et al., 2021) learns rich hidden information by directly performing masked image reconstruction in RGB color space using ViT while SimMIM (Xie et al.",1,neutral
"Recently, there has been a surge in reconstruction-based self-supervised pretraining methods with the introduction of MSN (Assran et al., 2022b), and MAE (He et al., 2021).",2,positive
"Similar to the conclusion obtained by the MAE(He et al. 2022), the optimal ratios are relatively high, and the accuracy increases steadily with the masking ratio growing until reaching 75%, which produces the best tracking results.",2,positive
"MAE (He et al. 2022) develops an asymmetric encoder-decoder architecture, the encoder operates on a small proportion of the visible patches, and the decoder reconstructs the original pixels.",2,positive
"The main tracker only consists of a ViT backbone and a box estimation head, we test both ViTBase and ViT-Large, and the ViT parameters are initialized with MAE (He et al. 2022) pre-trained model.",2,positive
"Our approach is inspired by the previous MIM method (Xie et al. 2022; He et al. 2022), but we have to deal with two fundamental problems in the tracking framework: (1) Visual tracking is a downstream vision task that generally does not have the pre-train process to apply the MIM strategy.",2,positive
"Recently, MIM approaches((Chen et al. 2020; He et al. 2022; Xie et al. 2022; Wei et al. 2022; Bao, Dong, and Wei 2021)) are extended to the modern vision transformers (Dosovitskiy et al. 2021; Liu et al. 2021). iGPT (Chen et al. 2020) first proposes a transformer to predict unknown pixels from a…",2,positive
"The correlative masked decoder, which is inspired by Masked Image Modeling (He et al. 2022; Xie et al. 2022), reconstructs the both original template and search pixels from the corresponding masked tokens, to guide the encoder to capture the invariant feature for tracking.",2,positive
"Recently, MIM approaches((Chen et al. 2020; He et al. 2022; Xie et al. 2022; Wei et al. 2022; Bao, Dong, and Wei 2021)) are extended to the modern vision transformers (Dosovitskiy et al.",2,positive
"Sparse training is critical and widely used in many fields of deep learning, such as MAE [36], pruning [41] and supernet training [18, 19].",1,neutral
"Dynamic sparsity is also widely used in the training strategies(sparse training), such as MAE [36] in vision, UniLMv2 [14] in NLP, CogView2 [27] and FLIP [43] in multi-modal, DropConnect [57] in robustness training, Oncefor-All [18] and Autoformer [19] in supernet training.",2,positive
Sparse training algorithms dynamically drop tokens/image pixels for better accuracy and efficiency [36].,1,neutral
"When a co-registered histology image is available, POLARIS first employs MAE [32] to extract features from the image tile of each spot and the image tile of its neighborhood.",2,positive
We only use the pretrained encoder part to extract the image features [32].,2,positive
"Several popular pre-trained models, such as convolutional neural networks, stacked sparse autoencoders, and masked autoencoders (MAE), have been employed as a first step to reduce image dimensions and demonstrate advantages in many applications [9, 10, 12, 13, 15].",1,neutral
"Also, it proposes a self-supervised pretraining strategy for point data inspired by the masked token modeling approaches in the RGB image [22] and text [12] domains.",1,neutral
"It means that our RangeViT approach, by being able to use off-the-shelf pre-trained ViT models, can directly benefit from current and future advances on the training ViT models with natural RGB images, a very active and rapidly growing research field [22, 39, 46, 47].",2,positive
"Note, we have not demonstrated it here, but Zorro can also be trained using unimodal self-supervised methods such as MAE [25] and DINO [12] separately on the audio and visual streams.",0,negative
"While approaches based on reconstruction [3, 20] have had a resurgence in the last years, the current state of the art in transfer linear probe is still held by joint-embedding approaches such as DINO [9] or iBOT [46].",1,neutral
"Recently, self-supervised learning using autoencoders for computer vision tasks has also achieved great success [8].",1,neutral
"As a result, researchers have divided images into patches and treated each patch as the smallest unit, as seen in works such as [7, 8].",1,neutral
"As a benchmark model often used in natural language, BERT (Devlin et al., 2019) uses a masking ratio of 15% while MAE uses a ratio of 75% for images (He et al., 2021) and 90% for videos (Feichtenhofer et al., 2022).",2,positive
"The optimal ratios are around 75%, which is in contrast to BERT (Devlin et al., 2019) and video-MAE (Feichtenhofer et al., 2022) but similar to MAE for images (He et al., 2021).",2,positive
"Following (He et al., 2021), the decoder is designed to be smaller than the encoder.",2,positive
"Motivated by the great success of other MAE-style approaches (He et al., 2021; Feichtenhofer et al., 2022; Hou et al., 2022), we also adopt an asymmetric design that the encoder only operates visible tokens after applying masking on input embedding, and a lighter decoder processes encoded tokens…",2,positive
"MAE (He et al., 2021) proposed to mask a high portion of patches and retain a small set of visible patches received by encoder in pre-training on image data.",2,positive
"It is hypothesized and summarized in (He et al., 2021; Feichtenhofer et al., 2022) that the masking ratio is related to the information density and redundancy of the data, which has an immense impact on the performance of the autoencoders.",2,positive
[22] proposed a pre-training method based on masked autoencoders [43] to perform SSL on the C-MAPSS datasets.,1,neutral
"For an image, even if more than 70% of the image is masked, the model can still produce a reliable restoration [18] , and this phenomenon can be explained from a high redundancy of the image information.",1,neutral
Our encoder/predictor architecture is reminiscent of the generative masked autoencoders (MAE) [35] method.,2,positive
"ods, such as MAE and data2vec.",2,positive
"I-JEPA outperforms pixel-reconstruction methods such as MAE [35] on ImageNet-1K linear probing, semi-supervised 1% ImageNet-1K, and semantic transfer tasks.",1,neutral
"I-JEPA outperforms pixel-reconstruction methods such as MAE [35] on ImageNet-1K linear probing, semi-supervised 1% ImageNet-1K, and se-
mantic transfer tasks.",1,neutral
"By reconstructing missing patches in pixels space, MAE achieves strong performance when fine-tuned end-to-end on large labeled datasets and exhibits good scaling properties.",2,positive
"Compared to reconstructionbased methods, such as MAE, which directly use pixels as targets, I-JEPA introduces extra overhead by computing targets in representation space (about 7% slower time per iteration).",1,neutral
"This idea is at the core of self-supervised generative methods, which remove or corrupt portions of the input and learn to predict the corrupted content [8, 35, 56, 65, 66, 69].",1,neutral
"The idea of image denoising has recently been revisited in the context of masked image modelling [8, 35, 69], where a Vision Transformer [28] is used to reconstruct missing input patches.",1,neutral
"Methods without view data augmentations data2vec [7] ViT-L/16 1600 73.3
MAE [35] ViT-L/16 1600 67.1 ViT-H/14 1600 71.5
trained with I-JEPA requires less computational effort than a ViT-Small/16 trained with iBOT.",0,negative
"Pretraining a ViT-H/14 on ImageNet requires less than 1200 GPU hours, which is over 2.5⇥ faster than a ViTS/16 pretrained with iBOT [75] and over 10⇥ more efficient than a ViT-H/14 pretrained with MAE.",0,negative
"In computer vision, there are two common families of approaches for self-supervised learning from images: invariance-based methods [9,16,17,23,34,36,71] and generative methods [7, 27, 35, 56].",1,neutral
I-JEPA outperforms MAE while requiring less pretraining epochs when using a similar encoder architecture.,2,positive
"I-JEPA significantly outperforms previous methods that do not use augmentations (MAE and data2vec), and decreases the gap with the best view-invariance-based methods, which leverage hand-crafted data augmentations during pretraining, even surpassing the popular DINO [17] on CIFAR100 and Place205 with a linear probe.",2,positive
"Methods without view data augmentations data2vec [7] ViT-L/16 1600 77.3
MAE [35] ViT-B/16 1600 68.0 ViT-L/16 1600 76.0 ViT-H/14 1600 77.2
CAE [21] ViT-B/16 1600 70.4 ViT-L/16 1600 78.1
Closest to our work is data2vec [7] and Context Autoencoders [24].",2,positive
The work on Masked Autoencoders (MAE) [35] proposed an efficient architecture that only requires the encoder to process visible image patches.,1,neutral
"Compared to popular methods such as Masked Autoencoders (MAE) [35], Context Autoencoders (CAE) [21], and data2vec [7], which also do not rely on extensive hand-crafted data-augmentations during pretraining, we see that I-JEPA significantly improves linear probing performance, while using less computational effort (see section 7).",2,positive
"Yet, pixel-level pre-training has been shown to outperform BEiT for fine-tuning [35].",1,neutral
"Our decoder is a lighter-weight 8-layer 512-dimensional transformer, following MAE [20].",2,positive
"Image-based recognition is entering a new era thanks to domain-agnostic architectures, like transformers [11, 65], and large-scale category-agnostic learning [20] .",1,neutral
"For images, masked autoencoders [20] paired with transformers and large-scale category-agnostic training learn general representations for 2D recognition.",1,neutral
This finding directly connects to recent advances in image-based self-supervised learning and masked autoencoders (MAE) [20] which learn powerful image representations by predicting masked (unseen) image patches.,1,neutral
We draw inspiration from MAE [19] for this design.,2,positive
We draw inspiration from MAE [20] for this design.,2,positive
"Our decoder is a lighter-weight 8-layer 512-dimensional transformer, following MAE [19].",2,positive
"LayerNorm [1] is used in all self-attention and MLP layers following standard practice [11, 20, 65].",1,neutral
This finding directly connects to recent advances in image-based self-supervised learning and masked autoencoders (MAE) [19] which learn powerful image representations by predicting masked (unseen) image patches.,1,neutral
"Self-supervised learning has advanced image [2, 20, 46] and language [3, 10] understanding.",1,neutral
Our decoder follows the decoder design from MAE [20].,2,positive
"For the image reconstruction loss, we normalize the ground-truth per-patch, following [38].",1,neutral
"MAE [38] drops the discretization, instead predicting raw pixel loss for a subset of the encoded patches in a manner strongly reminiscent of BART [51].",1,neutral
"During finetuning, we use a layer-wise learning rate decay, following [17, 38, 3].",1,neutral
"Positional encodings are once again added to all elements, following [38].",1,neutral
"We find that a combination of two state of the art approaches: masked auto-encoders, MAE [38] and contrastive language image pre-training, CLIP [69] provides a benefit over CLIP when trained on a corpus of 11.",2,positive
"For all models which rely on masking, we use a 75% masking ratio, consistent with [38, 31], as we did not find an alternative that improved downstream results.",2,positive
In MAE [38] the authors demonstrate a simple technique for self-supervised image-encoder pre-training that—to our knowledge—is still considered state-of-the-art.,2,positive
"[85, 23, 2, 98, 52, 62, 82, 16, 42, 48] apply the masked patch prediction problem from [21, 51, 38] to a joint image-text data space.",1,neutral
"As in [38] we experiment with predicting both normalized and un-normalized patch values, finding that predicting the normalized patch value slightly improves the performance of our MAE implementation.",2,positive
"For evaluation on CIFAR-10, CIFAR-100, Tiny Imagenet-100 and CBIS-DDSM datasets, we use the linear probing strategy (He et al., 2021).",2,positive
"As opposed to contrastive learning, the recent vision transformer autoencoder (ViT-AE ) approach (He et al., 2021) is different from the above methods in principle.",1,neutral
"Vision transformer-based autoencoder (ViT-AE ) (He et al., 2021) is a recent self-supervised learning technique that employs a patch-masking strategy to learn a meaningful latent space.",1,neutral
"For evaluation on CIFAR-10, CIFAR-100, Tiny Imagenet-100, and chest X-ray datasets, we use the linear probing strategy (He et al., 2021).",2,positive
"models pretrained solely on visual data, such as MAE [23], vision-language models are also supervised by the correar X iv :2 30 1.",1,neutral
"We explore several powerful encoders, which can be divided into two categories:
• Vision-based models such as ViT [19], MAE [23] and DiNO [15], which are pretrained solely on images and encompass the image visual content, including the class and location of its objects.",2,positive
"• Vision-based models such as ViT [19], MAE [23] and DiNO [15], which are pretrained solely on images and encompass the image visual content, including the class and location of its objects.",2,positive
"3 exhibits the performance of PARSeq with CLIPTER, when leveraging the vision-based image encoders of DiNO, ViT-MAE and OWL-ViT, and when using the visionlanguage models of CLIP, BLIP and GIT.",2,positive
"†Work done during an Amazon internship.
models pretrained solely on visual data, such as MAE [23], vision-language models are also supervised by the corre-
ar X
iv :2
30 1.",0,negative
"In the case of visual data, discriminative contrastive learning [6], [11], [12], [13], [14], [25], [48] and generative masked image modeling [4], [5], [24], [42], [58] have been demonstrated to learn transferable representations from images by attempting to solve pre-defined pretext objectives that aim to indirectly optimize I(X;Z), achieving state-of-the-art results on popular computer vision benchmarks.",1,neutral
"Inspired by the success of masked language modeling (MLM) using transformers [55] for natural language processing [8], [20], masked image modeling [5], [10], [24], [66] has been explored in the context of learning representations using vision transformers [21], [52].",1,neutral
"MIM has been shown to be more effective at learning transferable representations compared to contrastive learning [24], [66], indicating the effectiveness of generative pre-training.",1,neutral
", MAE [24]), and (iii) multi-modal discriminative (e.",1,neutral
", many existing methods are only evaluated on the ImageNet1K dataset [24], [25], [63].",1,neutral
", MAE [24]), (iii) multi-modal discriminative (e.",1,neutral
"When transferring the representation to ImageNet-1K [18], we follow the widely used fine-tuning recipe introduced by [5], [24].",1,neutral
single-modal pre-training MAE [24] gen.,1,neutral
"The training task was further simplified by MAE [24] and SimMIM [66], which only enforce the model to reconstruct the masked pixels of the input image using a `2 loss and do not require the use of discrete token encoders.",2,positive
"For single-modal SSL methods, we choose MoCoV3 [14] and SimCLR [11] as representative discriminative methods, and MAE [24] as a representative generative method.",1,neutral
MAE [28] and SimMIM [66] demonstrate directly reconstruct masked patches in raw pixel space can also lead to favorable transferability as well as scalability.,2,positive
"(MIM) [3, 28] open a new era of self-supervised visual representation learning, and show unprecedented transferability on various tasks, especially on fine-grained tasks such as object detection [22, 39].",1,neutral
"During pre-training, input images are resized to 224× 224 and we set random mask ratio to 75% following [28].",2,positive
6% better than the MAE [28] and CLIP [47] counterparts.,0,negative
"Among numerous architecture designing spaces, without loss of generalization, we adopt an asymmetric encoderdecoder architecture following MAE [28] and a dualencoder architecture following CLIP [47] for their flexibility.",2,positive
We follow most of setups in [28] for fine-tuning.,1,neutral
", raw pixel [28,66], low-level features [3,62] or high-level perceptions [12, 34, 63, 75]), we map patch features to a probabilistic distribution over a batch of text features as the reconstruction target, which is enabled by ITC that progressively aligns the image and text spaces.",2,positive
"Architecture comparisons between MAE [28], CLIP [47], MAE+CLIP and RILS.",2,positive
"Meanwhile, following MAE [28], we randomly mask a large portion of image patches and leave the remaining patches to be",2,positive
We choose MAE [28] and CLIP [47] as representative methods of masked image modeling and vision-language contrastive learning.,2,positive
"In computer vision, simple self-supervised methods have been employed in various models by randomly masking significant patches [70].",1,neutral
"Recently, due to its strong modeling capability, it has quickly provided leading methods for various vision tasks, including image classification [12, 16], object detection [37, 39], semantic segmentation [70,76], image generation [29,34], and self-supervised learning [2, 21]—see the surveys in [19, 30].",1,neutral
"Along the way, recent efforts have been applied to the popular image self-supervised learning [6, 16, 17] to reduce the demand for labeled data.",1,neutral
"In contrast, in the recognition field, the long-standing paradigm has been to build recognition models [29, 37, 76] by starting from a foundation model pretrained on large-scale image data [4,15,16] or image-text pairs [30, 44, 68].",1,neutral
"Recent Masked Image Modeling (MIM) methods [2, 8, 13], in the “mask-and-predict” style with Vision Transformer [4], are simple yet capable of achieving promising performance in various downstream tasks.",1,neutral
"Following [13], CMAE also introduces the pixel decoder Gp for mapping the latent features z s and MASK tokes z m s (shared in contrastive loss) to the feature space of the target encoder and the original images, i.",2,positive
3 [68] 4096 CL Based on Clustering: SwAV [66] 75.,1,neutral
"Beyond ViTs, a separate early investigation adopted context encoders [115], employing a concept akin to MAE, i.e. , image inpainting.",1,neutral
"Furthermore, MAE has been extended to other modalities beyond images [124]–[126].",1,neutral
"7)—namely, bidirectional encoder representation from image transformers (BEiT) [112], masked AE (MAE) [68], context AE (CAE) [113], and a simple framework for MIM (SimMIM) [114]—have gained significant popularity and pose a considerable challenge to the prevailing dominance of CL. MIM leverages co-occurrence relationships among image patches as supervision signals.",2,positive
"For a Low-Level Targets High-Level Targets Self-Distillation Contrastive / Multi-modal Teacher Algorithm ViT [5] MAE [68] SimMIM [114] Maskfeat [118] BEiT [112] CAE [113] PeCo [119] data2vec [120] SdAE [121] MimCo [122] BEiT v2 [ model of a certain size, when the dataset reaches a certain magnitude, further scaling of the data does not lead to significant performance gains in generative self-supervised methods.",1,neutral
"In contrast to the classic paradigm, during training, the main task head utilizes features acquired from the MAE encoder rather than the original examples.",1,neutral
"A notable distinction between the NLP and CV communities is their use of different that the actual differences their pipelines primary models, with transformers being prevalent in NLP and CNNs being widely adopted in CV. between On the other hand, MAE is a one-stage end-to-end approach, incorporating a decoder to decode the encoder-derived representation into the original pixels. limited to what is shown The landscape changed significantly with the introduction of the original ViT [5], which marked a pivotal moment.",2,positive
"7)—namely, bidirectional encoder representation from image transformers (BEiT) [112], masked AE (MAE) [68], context AE (CAE) [113], and a simple framework for MIM",1,neutral
Low-Level Targets High-Level Targets Self-Distillation Contrastive / Multi-modal Teacher Algorithm ViT [5] MAE [68] SimMIM [114] Maskfeat [118] BEiT [112] CAE [113] PeCo [119] data2vec [120] SdAE [121] MimCo [122] BEiT v2 [123] Target Raw Pixel HOG VQ-VAE VQ-GAN self MoCo v3 CLIP,2,positive
"further extended to various vision-related applications, as evidenced by [52], [68], [84], [112], [249], [250].",1,neutral
"Notably, it is pre-trained using the generative SSL method MAE [68] on the large-scale MillionAID dataset.",2,positive
"Recently, Gandelsman et al. [162] combined TTT with MAE for improved performance.",1,neutral
MAE’s simplicity and effectiveness have established it as a crucial baseline within the MIM domain.,2,positive
"They argued that by treating TTT as a one-sample learning problem, optimizing a model for each test input could be addressed using the MAE as Here, f and g refer to the encoder and decoder of MAE, and h denotes the main task head, respectively.",1,neutral
"In contrast to BEiT, MAE does not utilize special mask tokens and treats the task as a regression problem.",1,neutral
"Following the introduction of BEiT and MAE, several variants have been proposed. iBOT [111] is an “online tokenizer” adaptation of BEiT, aiming to address the limitation of dVAE in capturing only low-level semantics within local details.",2,positive
"Despite their structural alignment, MAE did not find significant application in vision research until the emergence of BEiT.",1,neutral
"While BEiT employs the token output from the pre-trained tokenizer as its target, MAE directly uses the original pixels as its target.",2,positive
The primary distinction between BEiT and MAE lies in their choice of T .,1,neutral
"However, to be consistent with [68], this paper considers BYOL and SimSiam to belong",1,neutral
"The two representative MIM approaches BEiT and MAE, showcase different architectural designs, with subsequent MIM methods often following one of these techniques.",2,positive
"Contrastive approaches are not always used in self-supervised methods [He et al., 2021; Ermolov et al., 2021; Chen et al., 2022].",1,neutral
"The learning targets vary from pixels (He et al., 2022) to image tokens (Bao et al.",1,neutral
"…processing (NLP), computer vision (CV), and other fields (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020; Raffel et al., 2020; Brown et al., 2020; Dosovitskiy et al., 2021; He et al., 2022; Bao et al., 2021; Lu
1ByteDance AI Lab 2The Hong Kong University of Science and Technology.",2,positive
"The learning targets vary from pixels (He et al., 2022) to image tokens (Bao et al., 2021; Peng et al., 2022).",2,positive
"Inspired by MAE [12], we propose a naive background reconstruction method (NBR).",2,positive
"MAE [12] developed an asymmetric encoder-decoder architecture, which masks random patches of the input image and reconstructs the missing pixels.",2,positive
"Therefore, we utilize a mature but large Vision Transformer (MAE-ViT-large [12]) for background reconstruction.",2,positive
MAE [12] removes random patches to reconstruct pixels under a high masking ratio (75%) and works well.,2,positive
"1: Pad the left and top regions of Xori with a width of 16 pixels to the right and bottom: Xori → X∗ ori ∈ R224×224; 2: Horizontally shift the padded image X∗ ori by ∆x pixels, and vertically shift by ∆y pixels; 3: Embed X∗ ori to the feature: X ∗ ori → Fori ∈ RB×P×C ; 4: Complementarily grid-mask Fori twice with a masking ratio of 50%: Fori → (F1, F2), Fi ∈ RB×P× C 2 , F1 ∪ F2 = Fori; 5: for Fi in (F1, F2) do 6: Reconstruct Fi by MAE [12]: Fi → F i ∈ RB×P× C 2 ;",1,neutral
"3) Comparison With Feature Extraction: We use pre-trained MAE10 [72] and SimCLR11 [73] models to extract features for the NUS-WIDE dataset and obtain 768- and 2,048dimensional feature representations respectively.",2,positive
"3) Comparison With Feature Extraction: We use pre-trained MAE10 [72] and SimCLR11 [73] models to extract features for the NUS-WIDE dataset and obtain 768- and 2,048- dimensional feature representations respectively.",2,positive
"advances, such as self-supervised learning for pre-training, as used by [54] and [55], or vision-and-language pre-training for",1,neutral
", 2019) are conceptually simple: they remove a portion of the data and learn to predict the removed parts (He et al., 2021).",2,positive
"We compute the loss only on masked parts, similar to BERT (Devlin et al., 2019) and MEA (He et al., 2021), so the model output on unmasked parts is qualitatively worse.",2,positive
"After being Inspired by NLP, researchers utilized masked autoencoder with the idea of target reconstruction (He et al., 2021).",1,neutral
"Our research is based on masked autoencoding, which is a form of more general denoising autoencoding (He et al., 2021).",2,positive
", 2019) and MEA (He et al., 2021), so the model output on unmasked parts is qualitatively worse.",2,positive
"%) is largely credited with this success (He et al., 2021; Xie et al., 2021).",1,neutral
"Both He et al. (2021) and Xie et al. (2021) regress raw RGBs to simplify the pre-training, while Wei et al. (2022) selects HOG (Dalal & Triggs, 2005) as targets due to their rich semantics.",0,negative
"We also observe calculating loss values only on masked patches gives higher accuracy (row 6), which is consistent with He et al. (2021).",1,neutral
"A bold move that increases the mask ratio to a staggering level (60~75%) is largely credited with this success (He et al., 2021; Xie et al., 2021).",1,neutral
"We try to conclude that in essence, the difficulty is rooted in the gap between language and vision in terms of data processing (Bateman, 2014; He et al., 2021).",1,neutral
"It is the emerging masked image modeling (Bao et al., 2021; He et al., 2021; Xie et al., 2021; Chen et al., 2022) initially extends the success of BERT from language transformers to vision transformers (ViTs).",2,positive
"Similar to MAE (He et al., 2021), SparK has the advantage of encoding efficiency, especially compared to contrastive learning that encodes two or more images in a forward pass.",1,neutral
He et al. (2021) ingeniously takes advantage of transformer’s ability to handle variable-length inputs and implements an efficient and scalable method.,2,positive
"The computer vision community has recently paid more attention to vision transformers, while convnets no longer appear in the spotlight (Liu et al., 2021; He et al., 2021).",2,positive
"One may therefore ask: what exactly is preventing the application of BERT to convnets? We try to conclude that in essence, the difficulty is rooted in the gap between language and vision in terms of data processing (Bateman, 2014; He et al., 2021).",1,neutral
"We then remove the hierarchical design (row 4), which results in a single-scale masked modeling that is commonly used
for transformers (Devlin et al., 2018; Bao et al., 2021; He et al., 2021; Xie et al., 2021).",2,positive
"(a) illustrates MAE (He et al., 2021) that has no such side effect thanks to the transformer’s ability to process variable-length input.",1,neutral
"These designs have been proven to facilitate models to learn more informative features in He et al. (2021), and are also verified by the ablation study later in section 4.5.",2,positive
"Many self-supervised methods [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12] are closing the performance gap with supervised pretraining in computer vision.",1,neutral
"As vision encoder, we consider (1) ViT-B/16 [7] (patch size of 16×16 pixels) with pre-trained weights from self-supervised MoCo-v3 [5], DINO [2] and MAE [10], all trained on IN-1K but without any labels.",2,positive
"%) pretraining are close, while MAE is worse (42.4%), presumably because the representations learned by instance discrimination (MoCo-v3 and DINO), which learns different embeddings for different images, is closer to zero-shot classification than MAE’s training objective.",0,negative
"For the downstream task of OAR segmentation, we employed the ViT backbone and UperNet [20] decoder as the encoder and decoder parts of the segmentation model, following the implementation described in a previous work [10].",2,positive
"[10], on the other hand, proposed a simpler masked autoencoder (MAE) strategy that employs an efficient encoder-decoder design to directly predict pixels within the masked patches.",1,neutral
"Specifically, we replace the multi-crop strategy with a random masked sampling strategy, consistent with pioneering SSL approaches that use masked image modeling with ViT in a patch-wise manner [9], [10].",1,neutral
"Furthermore, various self-supervised learning (SSL) methods have been proposed for ViT, including knowledge distillation-based semantic meaning learning [8] and masked image modeling [9], [10].",1,neutral
"Then for enhancing the learning of visual representations, we introduce a masked autoencoder (MAE)[8] branch, which is parallel to the SSL framework and they share the same encoder.",2,positive
"Therefore, we investigate other MIM methods besides MAE[8] and observe that LoMaR[18] can further boost the model performance by 0.",2,positive
"Other masked image modeling methods Several masked image modeling methods[18, 7, 8] have demonstrated their effectiveness to learn visual representations from images.",1,neutral
"Following the asymmetric design in [8], a small and independent decoder is used to reconstruct the corrupted image from the latent representation and mask tokens.",1,neutral
Therefore we introduce a masked autoencoder (MAE)[8] branch to enforce the visual representation learning and help ViT generate more accurate pseudo,2,positive
"As for the MAE branch, we follow the default settings of [8].",2,positive
"To date, Vision Transformers (ViT)[1] have achieved significant progress in supervised learning[1, 2, 3], self-supervised learning[4, 5, 6, 7, 8], and various other computer vision tasks[9, 10, 11, 12].",1,neutral
"Recently, masked autoencoder (MAE)[8] proposed an autoencoding approach, whose objective was simply to reconstruct missing original patches in the pixel space given a partial observation.",1,neutral
"has achieved the best results in the natural language processing (NLP) [10, 36] and computer vision (CV) [12, 15] fields, since its advanced self-attention mechanism.",1,neutral
"However, as shown in Table 2, MIM pre-training [18] mainly effects for relatively large models.",0,negative
"For data augmentation, we follow the settings in MAE [18].",2,positive
"Comparison among TinyMIM (ours), MAE [18] and training from scratch by using ViT-T, -S and -B on ImageNet-1K.",2,positive
"In this work, we adopt MAE [18] as the MIM model due to its popularity and simplicity.",2,positive
SimMIM [53] and MAE [18] find that reconstructing RGB pixels results in favorable representations.,1,neutral
"Masked image modeling (MIM), which masks a large portion of the image area and trains a network to recover the original signals for the masked area, has proven to be a very effective self-supervised method for pre-training vision Transformers [2, 12, 18, 53].",1,neutral
"Recently, masked autoencoders [4, 83, 33, 71, 28] have shown training efficiency [33], model scalability [33], data efficiency [63], and effectiveness on videos [71, 63, 28].",1,neutral
"Therefore, instead of following the common transferring assumption, we revisit the old good idea of training with indomain egocentric data only, but this time in light of the development of recent data-efficient training methods, such as masked autoencoders [33, 63, 28] as well as the scale growth of egocentric data collections (e.",2,positive
"MAE [33, 28] reconstruction results on Ego4D [31] MQ val set.",1,neutral
Our method applies the original MAE [33] and video MAE [28] algorithms.,2,positive
"4, we visualize the MAE [33, 28] reconstruction results on a few Ego4D [31] examples with a ViT-B [25] trained for 200 epochs without per-patch normalization.",2,positive
"…including ImageNet (Deng et al., 2009) classification, contrastive learning (He et al., 2020; Chen et al., 2020c), masked image modeling (MIM) (He et al., 2022; Xie et al., 2022; Xue et al., 2022), and language-vision pre-training (Radford et al., 2021), could guarantee superior…",2,positive
MIM.,2,positive
"SimMIM (Xie et al., 2022) is adopted as it is suitable for convolutional networks.",2,positive
", 2020c;b) and masked image modeling (Bao et al., 2021; He et al., 2022; Xie et al., 2022; Peng et al., 2022; Xue et al., 2022) have gained impressive improvement over ImageNet pre-training on various vision benchmarks.",2,positive
", 2020c), masked image modeling (MIM) (He et al., 2022; Xie et al., 2022; Xue et al., 2022), and language-vision pre-training (Radford et al.",2,positive
"C V
] 1
5 M
ar 2
02 3
et al., 2022; Xiao et al., 2022; Radosavovic et al., 2022; Shah et al., 2022) have demonstrated that applying popular visual pre-training approaches, including ImageNet (Deng et al., 2009) classification, contrastive learning (He et al., 2020; Chen et al., 2020c), masked image modeling (MIM) (He et al., 2022; Xie et al., 2022; Xue et al., 2022), and language-vision pre-training (Radford et al., 2021), could guarantee superior representation for robotic policy learning tasks, e.g., dexterous manipulation, motor control skills and visual navigation.",2,positive
Xiao et al. (2022); Radosavovic et al. (2022); Seo et al. (2022); Gupta et al. (2022) inherit the MIM spirit to realize visual pre-training for control tasks.,2,positive
"…self-supervised learning methods such as contrastive learning (He et al., 2020; Chen et al., 2020c;b) and masked image modeling (Bao et al., 2021; He et al., 2022; Xie et al., 2022; Peng et al., 2022; Xue et al., 2022) have gained impressive improvement over ImageNet pre-training on various…",2,positive
"In image representation, MAE [24] and SimMIM [25] use the random masking strategy to discard or replace the selected patches and in this paper, we adopt the former approach for selected frames.",1,neutral
"It is worth noting that, unlike MAE [24] or SimMIM [25], we do notmake predictions for themasked sequences at the pixel level, choosing to reconstruct the input at the representation level in an implicit way and ensuring that the pair of masked sequences can be as close as possible in the representation.",2,positive
"Inspired by the masked auto-encoders [24], we consider the masking mechanism as a way of data augmentation and combine it with contrastive learning framework; the frame-",2,positive
"On the other hand, in addition to contrastive learning methods, the development of masked auto-encoders [24,25], which aim to mask out patches from an input and attempt to reconstruct the masked parts by combining global and local information, shows the model’s understanding of data distribution.",1,neutral
"MAE [24] and SimMIM [25] model a simple reconstruction loss on the masked patches to achieve pixel-level restoration, while BEiT [33] is a token-level repair.",1,neutral
"Another prominent line of work in self-supervised learning is Masked Image Modeling, the core idea of which is to pre-train a encoder by masking part of the input patches and then reconstructing it [24,25,32,33].",1,neutral
"Moreover, recent work [63] also adopts ViTs for self-supervised learning via masked images, achieving stronger results than supervised learning.",1,neutral
"Masked image modeling, represented by masked autoencoders [31], is one of the latest self-supervised learning strategies.",1,neutral
This behavior is similar to that of the MAE pre-trained ViT model [31].,1,neutral
"Compared to the plain ViT pre-trained with MAE [31], our approach performs similarly up to the Large model regime, despite using much fewer parameters (198M vs 307M).",2,positive
"We compare our approach to previous masked auto-encoder methods [3, 31, 77], which were all designed for transformer-based models.",2,positive
"This is in contrast to the recent success of masked image modeling using transformer-based models [3, 31, 77], where the pre-trained models significantly outperform the supervised counterparts.",1,neutral
"To perform this analysis, we randomly select 1,000 images from different classes in the ImageNet-1K validation set and extract the high-dimensional features from each layer of different models, including the FCMAE models, the ConvNeXt supervised model [52] and the MAE pretrained ViT model [31].",2,positive
"Similar to MAE [31], the target is a patch-wise normalized image of the original input, and the loss is applied only on the masked patches.",1,neutral
"We also considered more complex decoders such as hierarchical decoders [48, 59] or transformers [21, 31], but the simpler single ConvNeXt block decoder performed well in terms of fine-tuning accuracy and reduced pre-training time considerably, demonstrated in Table 1.",1,neutral
"Among many different self-supervised algorithms, masked autoencoders (MAE) [31] have recently brought success in masked language modeling to the vision domain and quickly become a popular approach for visual representation learning.",1,neutral
"While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders (MAE) [31].",1,neutral
We retrained the CSWin transformer without redesign and original transformer [9] separately and compared them with our redesigned,2,positive
", 2020) and vision tasks (He et al., 2022; Chang et al., 2022); new families of generative models such as diffusion (Ho et al.",1,neutral
"In this work, we tackle an active visual exploration problem using a vision transformer model [15] as the architectural backbone.",2,positive
"Table 3: MAE Attention as glimpse: We use CLS attention of pre-trained MAE[15] as the glimpse map and use locations corresponding to the max value, min value, median value, and random value as the next glimpse location.",2,positive
We study the use of CLS attention map as an alternative for the learned glimpse map through a set of heuristics to use the base MAE model for active visual exploration.,2,positive
"Since the pretext task for training MAE, i.e. random masking and pixel value prediction for masked regions, resembles the partial observability constraint that we are trying to solve, we find MAE encoder to be best suited for our context extractor module and the pre-trained weights to be transferable for our use case.",2,positive
"We build our context encoder on the mask-auto-encoder (MAE) [15] model, pre-trained as a masked-image-model for the image reconstruction pretext task.",2,positive
"In this work, we evaluate our method on the widely studied task of image reconstruction,
We first compare against a baseline where the base MAE model with random glimpse selection is finetuned on the SUN360 and ADE20K datasets, denoted by ‘Random glimpse’ in Table 1.",2,positive
"Next, in addition to finetuning the task module and the context extractor, initialized with MAE weights, we train the glimpse selection module to predict the loss of the task module (i.e reconstruction loss).",2,positive
"While the random selection outperforms other heuristics for base MAE, we see that random selection of glimpse performs inferior to a learned glimpse selection policy, Table 1.",1,neutral
Analysis for the importance of self-supervised feature: Our encoder and decoder are initialized with pretrained self-supervised weights [15].,1,neutral
"As we intend to evaluate the use of base MAE for active vision, we do not finetune it on SUN360 dataset.",2,positive
9% on Imagenet 1000 class classification [15].,0,negative
"We show that vision transformer mod-
els, and in particular MAE, trained on large unlabelled data can replace contemporary CNN-based counterparts.",2,positive
We therefore use MAE’s decoder as our task module.,2,positive
"As the MAE’s pretext task is image reconstruction, it provides a good initialization for the task module’s decoder.",2,positive
Our context extractor is a ViT [12] initialized with MAE’s encoder weights.,2,positive
"mask Transformer but are usually determined by experimental results [8], [10], [38].",1,neutral
"With the development of self-supervised or unsupervised learning methods in ViT [38], recent works [6], [8], [10], [11], [12] using self-supervised pre-training to boost the point cloud understanding and improve the performance of the fine-tuning in the downstream tasks.",1,neutral
"Following [8], [38], the width (feature dimension) of the Transformer encoder is set to 384 with six heads.",1,neutral
"Masking in Transformers is a widely used scheme in both languages [39], [40] and images [38], [41], [42], which usually",1,neutral
"With the development of self-supervised or unsupervised learning methods in ViT [38], recent works [6], [8], [10], [11], [12] using self-supervised pre-training to boost the point cloud understanding and improve the performance of the fine-tuning in",1,neutral
"2) Generating Input Tokens: According to ViT [52], a trainable and lightweight projection should be applied to embedding patches, denoted as F .",1,neutral
"1) Transformer Block: Following ViT [52], the Transformer encoder consists of serial Transformer blocks with the same or similar structure.",1,neutral
"Baselines We benchmark the performance of our two baselines - Average Pairwise Feature Distance (Distance) and Cluster Assignment Distribution (Cluster) using five different feature extractors for each: pixels (φ ), three intermediate ResNext-101 [32] convolutional feature layers (φ, φ, and φ), and features from the encoder output of a Masked Autoencoder [13] (φ ).",2,positive
"Finally, φ extracts the per-patch features produced by the encoder of a large Transformer model (we used Masked AutoEncoder [13]).",1,neutral
"Finally, ϕT extracts the per-patch features produced by the encoder of a large Transformer model (we used Masked AutoEncoder [13]).",2,positive
"Baselines We benchmark the performance of our two baselines - Average Pairwise Feature Distance (Distance) and Cluster Assignment Distribution (Cluster) using five different feature extractors for each: pixels (ϕP ), three intermediate ResNext-101 [32] convolutional feature layers (ϕR(1), ϕR(2), and ϕR(3)), and features from the encoder output of a Masked Autoencoder [13] (ϕT ).",2,positive
"In recent years, global representation-based MAE [20] and ViT [21] have achieved promising classification performance in popular data sets.",2,positive
"For example, in MAE [20], even though the input image is randomly masked, the well-designed encoder can still construct the invisible pixels for recognition tasks.",1,neutral
", transformer [166]–[168], unsupervised representation learning [158], [169]–[171]), and the lack of a concise and easily extensible code base for researchers.",1,neutral
MAE [14] and SimMIM [15] reveal that raw pixels as reconstruction targets are adequate for effective visual learning.,1,neutral
"It has been justified that discrete visual tokens [10]–[13], raw pixels [14], [15], and hand-crafted features [16] are suitable targets to learn versatile models for a broad spectrum of downstream tasks.",1,neutral
"Specifically, we pre-train models based on MAE [14] for 100 epochs on ImageNet-1k [17] with uniform masking and plot the corresponding convergence curve on the validation set during fine-tuning.",2,positive
"enough context to correctly recover the masked patches [14], making the model more intricate and unpredictable.",1,neutral
"LN, which enhances the patch-level local contrast for better performance [14]; Z i and Ti ·(1−M i ) are the encoder outputs of visible tokens and the corresponding targets, respectively; β is experimentally set to 2.",1,neutral
"We pre-train models on ImageNet1K [17] following the settings of MAE [14], where the decoder TABLE I ABLATION STUDY ON THE ADAPTIVE LEARNING RATE SCALE RULE.",2,positive
1) Disjoint Masking: We evaluate DM based on MAE [14] with normalized pixels as reconstruction targets and only pretrain 100 epochs for fast ablation study on ImageNet-1K.,2,positive
"Following prior arts [10], [14], [15], we use ViT [2] with different scales as backbones, i.",1,neutral
"Following a standard SSL evaluation recipe [14], [21], we conduct end-to-end fine-tuning or linear probing for classification on ImageNet-1K and transfer learning for object detection/instance segmentation on COCO [56] and semantic segmentation on ADE20k [57].",2,positive
"In Table III, we evaluate the training efficiency of DM based on several MIM methods with their original pre-training recipes, including MAE [14], BEiT [10], SimMIM [15], and MaskFeat [16].",2,positive
"Although the masked autoencoders have been successfully applied for SSL in images [14] and videos [12,46], it remains challenging and still in exploration due to the inherent irregularity and sparsity of the point cloud data.",2,positive
"Another line of work is completionbased [25, 31, 36, 55, 58, 60] methods, which get inspiration from Masked Autoencoders [14].",1,neutral
"(c) How does a pretraining procedure learn the desirable representation through the backward pass? Empirically, the pretraining procedure that minimizes empirical risks for predicting masked tokens (Devlin et al., 2018; Dosovitskiy et al., 2020; He et al., 2022) appears to succeed in the presence of long sequences.",1,neutral
"Particularly, in the self-supervised setting, e.g., as in MAE (He et al., 2022), we identify a condition number that is pivotal to solving downstream tasks.",1,neutral
"See, e.g., Devlin et al. (2018); Radford et al. (2018, 2019); Dai et al. (2019); Brown et al. (2020); Dosovitskiy et al. (2020); He et al. (2022) and the references therein.",2,positive
", as in MAE (He et al., 2022), we identify a condition number that is pivotal to solving downstream tasks.",1,neutral
"Empirically, the pretraining procedure that minimizes empirical risks for predicting masked tokens (Devlin et al., 2018; Dosovitskiy et al., 2020; He et al., 2022) appears to succeed in the presence of long sequences.",1,neutral
"The Masked Autoencoder (MAE) is a recent state-of-the-art self-supervised representation learning method in computer vision that pretrains a ViT encoder by masking an image, feeding the unmasked portion into a transformer-based encoder, and then tasking the decoder with reconstructing the input image [25].",1,neutral
Scale-MAE is a self-supervised pretraining framework based on the Masked Autoencoder (MAE) [25].,2,positive
Scale-MAE is a selfsupervised pretraining framework based on the Masked Autoencoder (MAE) [25].,2,positive
"As mentioned earlier, MAE-like pre-training frameworks [66, 34, 27, 68] that are specialized for transformer-style 3D point cloud backbones also attract much attention for their impressive performances, despite the inapplicability to many other widelyused (non-transformer) deep set architectures.",2,positive
"4.6 we also particularly included comparisons with the very recent MAE-like approaches [34, 27, 68], which are specialized for transformer-style backbones and thus not applicable to generic types of deep set architectures [36, 53].",2,positive
"More recently, inspired by the success of masked autoencoders (MAE) [17] in the 2D vision community, some researchers are devoted to adapting MAE-like pre-training pipelines [66, 34, 27, 68], which achieve impressive performances.",2,positive
"As a result, we develop an asymmetric encoder-decoder architecture similar to MAE [15].",2,positive
"The masked transformers turn out to be scalable vision learners via valid self-supervised learning [25], [35].",1,neutral
"In addition, an interesting discovery is that plain ViTs with masked pre-training [25], [35] achieve better segmentation results of small objects compared to [15], [18].",1,neutral
"Therefore, in this framework, we construct a semi-supervised cross-algorithm ensemble method for lesion identification in UW-OCTA DR images based on MAE, ConvNeXt, and SegFormer algorithms[10,24,13,5,6].",1,neutral
This stage mainly relies on the MAE algorithm [10] to pre-train on the diabetic retinopathy grade classification data set (Task 3) in the DRAC2022 challenge.,0,negative
"Finally, in order to form a good complement with MAE [10] and SegFormer algorithm [24] based on the self-attention mechanism.",1,neutral
"In this framework, first, we pre-train the MAE algorithm [10] on the DR grade assessment dataset in the diabetic retinopathy analysis challenge (DRAC) 2022.",2,positive
"MAE algorithm [10] as a common structure in self-supervised masked image modeling visual representation learning [1,10].",1,neutral
"After the pre-training, we would extract the backbone of the MAE encoder (also considered as pre-trained MAE-ViT) [9,10].",2,positive
"For example, the masked image modeling algorithm [1,10] in the domain of self-supervised learning.",1,neutral
"Then, the MAE encoder [10] performs feature extraction on the visible patches subset of the image.",1,neutral
"Although the MAE [10] algorithm designed based on the vision transformer (ViT) [9] architecture has good performance, the algorithm of this architecture outputs singlescale low-resolution features instead of multi-scale features.",2,positive
"Several approaches [6, 7, 14, 15] treat the augmented version of the original sample as a positive sample, and the rest of the samples as negative samples.",1,neutral
"We conduct experiments using MVTN with ResNet-50 [37] and ViT [21] as backbone networks, starting from scratch or using weights from ImageNet [73], Dino [9], and Masked Autoencoders (MAE) [36] as initial weights.",2,positive
"The pretraining methods include using ImageNet [73] weights and using SSL weights from Dino [9], and MAE [36].",2,positive
Trajectory analysis [Ayhan and Samet 2016; Hamed et al. 2013; Kanneganti et al. 2018] performs a key role in the design of aircraft by modeling performance for a mission.,2,positive
PRMAE is a simple and effective derivative of the popular MAE [8] with minimal but effective modifications on the masking strategy.,2,positive
"Figure 1: Architecture of the Masked Autoencoder (MAE) in [8], with Vision Transformer (ViT) backbone.",2,positive
"In [8], they compared different mask sampling strategies including the block-wise sampling, random sampling and grid-wise sampling.",1,neutral
"Masked Autoencoder (MAE) [8] is essentially a denoising autoencoder, which has a straight forward motivation that randomly masks patches of the input image and reconstruct the missing pixels.",1,neutral
"Inspired by the success of BERT, a series of works [5-12] has been proposed recently in the vision community for image understanding.",2,positive
"Among them, Masked AutoEncoder (MAE) [8] is the most representative method which significantly optimizes both the pre-training efficiency and fine-tuning accuracy and it is leading the new trend of SSL across computer vision tasks.",1,neutral
"linear probing) over a series of downstream task including image classification using ImageNet-1K(IN1K) [13], object detection and segmentation using COCO [26] and semantic segmentation using ADE20K [27] as suggested in MAE [8].",1,neutral
"As indicated in [8], MAE reconstructs pixels, which are not semantic entities and they observed that MAE infers complex, holistic reconstructions, suggesting it has learned numerous visual con-",1,neutral
Its great success is attributed to “a rich hidden representation inside the MAE” [8].,0,negative
We divide an image into regular non-overlapping patches as in [8] and then calculate the gradient sum of each patch and rank them by its gradient sum.,1,neutral
"Inspired by He et al[7], we propose a full convolutional neural network[16] based cross-modal text feature extractor(TFE).",2,positive
"via self-supervised pre-training(Oord et al., 2018; Devlin et al., 2018; He et al., 2022; Xie et al., 2022) or multi-task learning (Ruder, 2017; Caruana, 1997; Sener & Koltun, 2018).",2,positive
"Researchers have devoted great efforts to make the learned features to be more universally applicable, e.g. via self-supervised pre-training(Oord et al., 2018; Devlin et al., 2018; He et al., 2022; Xie et al., 2022) or multi-task learning (Ruder, 2017; Caruana, 1997; Sener & Koltun, 2018).",1,neutral
"One is a decoder (He et al., 2022) which reconstructs the input image, the other is a linear classifier.",1,neutral
"Secondly, we compared the classical MIM-based methods such as MAE (ViT-B) (He et al., 2022) and SimMIM (Swin-B) (Xie et al.",2,positive
", 2020), classical MIM-based methods (△) (He et al., 2022; Xie et al., 2022), self-distillation method ( ) (Caron et al.",1,neutral
", 2020), has achieved competitive results in many image interpretation tasks (Bao et al., 2021; He et al., 2022; Xie et al., 2022).",2,positive
"Notably, concurrent works on self-supervised learning with MAE [87] and BEiT [88] validates the potential of reconstruction objectives.",1,neutral
"Furthermore, some works like MAE (He et al., 2022), SiT (Atito et al., 2021) and BEiT (Bao et al., 2021) exhibit promising performance with the powerful random mask strategy exclusively.",2,positive
"MAE (He et al., 2022) and BEiT (Bao et al., 2021) achieve highly competitive results through inpainting the images occluded by random masks.",2,positive
MAE [13] and BEiT [14] achieve highly competitive results through inpainting the images occluded by random masks.,1,neutral
"Furthermore, some works like MAE [13], SiT [31] and BEiT [14] exhibit promising performance with the powerful random mask strategy exclusively.",1,neutral
"Deep learning models have witnessed pre-training on increasingly large-scale datasets as a general and effective pathway to succeed in both computer vision [2, 21, 40] and natural language processing [7, 13, 33].",1,neutral
"Borrowing the idea from MAE [16], Pang et al. [39] designs a masked auto-encoder to recover the masked parts of objects.",1,neutral
"We denote the methods with * when they are adapted to Point Transformer backbone, e.g., “PointMAE [39]* + PointTrans”.",1,neutral
"Compared to the conventional random masking method (i.e., PointMAE [39]) or contrastive-based method (i.e., PointContrast [56]), our MM-3DScene performs best on the linear probing task.",2,positive
"To solve this, self-supervised learning (SSL) becomes a favorable choice since it can extract rich representations without any annotation [10, 16, 17].",1,neutral
"With the same consistency loss, PointMAE gets 71.0%, which is 0.9% lower than ours.",0,negative
"Masked Modeling (MM) [16, 57], as one of the representative methods in SSL, recently draws significant attention in the vision community.",1,neutral
The table shows that the random masking strategy of PointMAE does not help the downstream segmentation task.,1,neutral
"Unsupervised Scene Pre-training Self-supervised learning (SSL) has recently achieved great success in 2D vision [2, 5, 6, 14, 16, 17, 57] and NLP tasks [3, 10, 11].",1,neutral
"Borrowing the idea from MAE [16], Pang et al.",1,neutral
"While BEIT and related methods [15, 18, 26, 61, 65, 71] essentially apply BERT-style pretraining onto images, several extensions to different data modalities have recently been proposed, e.",1,neutral
"Examples of pretext tasks in vision are image reconstruction from masked or transformed input patches [1, 26], re-ordering of image patches [43], or predicting parameters of image rotations [22].",1,neutral
"[30], whereas recently, a plethora of methods have been proposed for ViT architectures [3, 10, 26, 31].",1,neutral
"It has recently shown promising results in the NLP domain with BERT [14] as well as in the frame-based vision community [3,4,8,9,23,26,30].",2,positive
"As masked image modelingbased self-supervised learning methods using vision transformer [68] have recently become a new trend, we also use two SOTA methods RGMIM [69] and MAE [70] for comparison.",2,positive
"In our experiments, RGMIM and MAE used the ViT-Base model.",2,positive
"Compared with the vision transformer-based methods RGMIM [57] and MAE [58], although we use the traditional and straightfor-",2,positive
"As masked image modeling-based self-supervised learning methods using vision transformer [56] have recently become a new trend, we also use two SOTA methods RGMIM [57] and MAE [58] for comparison.",2,positive
"Compared with the vision transformer-based methods RGMIM [69] and MAE [70], although we use the traditional and straightforward ResNet model, our method outperformed them, especially when the amount of annotated data was significantly reduced.",2,positive
"…signals from unlabeled data itself and thus leverages underlying structure and common representation in data, which has achieved great success in natural language processing (Devlin et al., 2018; Brown et al., 2020) and computer vision (Chen et al., 2020a; Caron et al., 2021; He et al., 2022).",1,neutral
"During BERT pretraining, this percentage is typically 15%, while (He et al., 2022; Wettig et al., 2022) show that a large mask rate is beneficial for pre-training.",0,negative
"He et al., 2022; Wang, Cai, Gao, & Vasconcelos, 2019; X. Zhou, Koltun, & Krähenbühl, 2022).",2,positive
"He et al., 2022; K.",1,neutral
"The motivation of using DETR architecture [9] for background image encoding and understanding stems from its state-of-the-art performance for object detection using the state-of-the-art visual transformer (ViT) encoder architecture [22, 78].",2,positive
"Another important line of work generalizes masked modeling [23], which was initially proposed for language modeling, to other data modalities and domains [38, 67, 79].",1,neutral
"As a result, models learned from the masking supervision demonstrate strong capability for capturing correlations between sequential tokens [38].",1,neutral
This observation is similar to spatial masking in MAE [38] where an optimal masking ratio is found.,1,neutral
"Until now, the MAE-style reconstruction pre-training methods (Baade et al., 2022; Niizumi et al., 2022; Chong et al., 2022; Xu et al., 2022) show the best audio understanding performance on various audio classification tasks.",0,negative
"We employ the vanilla ViT model (Dosovitskiy et al., 2021) as the backbone of our audio SSL models without heavy
structure engineering, and apply the speed-up technique proposed in He et al. (2022).",2,positive
"Inspired by the success of the recent visual pre-training method MAE [He et al., 2022], MSM-MAE [Niizumi et al.",2,positive
"More importantly, it is a zero-cost backdoor removal solution, as conventional fine-tuning is a necessary step for users to adapt the pre-trained encoders to downstream tasks [8, 17, 26, 28].",2,positive
"In particular, for computer vision related tasks, self-supervised pre-training methods - including contrastive [46], reconstruction [47] and self-distillation-based ([48], [49]) methods - have achieved substantial success in transfer learning without the need for ground truth labels at the pretraining stage.",1,neutral
"A learnable corruption embedding e[M] is used to replace the masked position, with which the corrupted representation ZM = 1(M) e[M] + 1(1−M) T is input to encoder (Devlin et al., 2019) or decoder (He et al., 2022b)2.",1,neutral
"Since the rapid development of Transformer (Vaswani et al., 2017) in vision (Dosovitskiy et al., 2021; Liu et al., 2021b), various efforts have been made to spread this trend from NLP towards foundational 2D visual understanding (Bao et al., 2022; He et al., 2022b; Wang et al., 2022a).",2,positive
"We find that this leads to an inferior result, consistent with the observation in 2D that data of low semantics requires a non-trivial decoder for modeling purpose (He et al., 2022b).",1,neutral
"Pioneering architectures like PointNet (Qi et al., 2017a;b) can only encode 3D coordinates and it is not applicable for masked denoising autoencoding (DAE) (Vincent et al., 2008; 2010; Devlin et al., 2019) which is proved successful in NLP and 2D vision (He et al., 2022b).",2,positive
"…Liu et al., 2021b), abundant works have been proposed to generalize DAE to masked modeling of RGB pixel (Zhang et al., 2016; Chen et al., 2020a; He et al., 2022b), pretrained DALL-E token (Ramesh et al., 2021; Bao et al., 2022), online teacher token feature (Zhou et al., 2022), and HOG feature…",2,positive
"The objective is to train the a student encoder fS to predict/reconstruct the output from a teacher encoder fT , where the teacher could be a discrete variational autoencoder (dVAE) (Bao et al., 2022) or simply identity mapping (He et al., 2022b).",2,positive
"Masked signal modeling can be viewed as an extension of the classical denoising autoencoders (DAE) with masked corruption (He et al., 2022b), which has been recently explored for language models (Devlin et al., 2019) and vision (Bao et al., 2022).",1,neutral
"D (7)
With this parameter-efficient prompt tuning strategy, we are able to tune the pretrained foundational Transformer while preserving as much pretrained knowledge as possible (He et al., 2022a).",2,positive
Masked autoencoder [13] randomly masks 75% tokens which significantly speeds up the self-supervised visual representation pre-training based on masked image modeling.,1,neutral
"MaskCLIP [7] combines CLIP with the masked image modeling [2,13,24, 37].",1,neutral
The random masking method has been shown to be effective for masked image modeling [13].,1,neutral
"It has shown to perform also effectively on masked image modeling based pre-training approaches [13,15,26,31,36] when combined with the vision Transformer architectures [8].",2,positive
"Unlike ours, it uses a random masking strategy for CLIP training, like MAE [13].",2,positive
The mean absolute error (MAE) metric is also used in our evaluation to measure the foreground-background segmentation error.,1,neutral
"Specifically, compared with the currently second best result, Fωβ increased by 7.6% on average, Sm increased by 4.6%, Em increased by 2.6%, and MAE lowered by 1.4%.",0,negative
"Firstly, a vanilla Vision Transformer(ViT) is introduced in DQnet, which is pretrained in a self-supervised manner [12], to generate representations with long-range correlations.",1,neutral
"The design of image as patches facilitates the learning of object-centric representations, as evidenced by recent works, e.g. DINO, EsViT, MAE [2, 12, 15].",1,neutral
"Second, the design of image as patches facilitates the learning of object-centric representations, as evidenced by recent works, e.g. DINO, EsViT, MAE [2,12,15], that demonstrate ViTs trained with self-supervised learning (SSL) capture objects in the image without label annotations.",1,neutral
"To comprehensively compare our proposed model with other state-of-the-art methods, We use four widely used metrics to evaluate our method: structuremeasure (Sm) [6], E-measure (Em) [7], weighted F-measure (Fωβ ) [19], and mean absolute error (MAE).",2,positive
"DINO, EsViT, MAE [2,12,15], that demonstrate ViTs trained with self-supervised learning (SSL) capture objects in the image without label annotations.",1,neutral
"Recent works, such as [21, 2, 10, 1], have applied masking-based pretext tasks to computer vision tasks and achieved comparable performance to contrastive approaches.",1,neutral
"We note that this budget constraint is similar to MAE [10] and other random masking schemes, which choose to randomly mask out a fixed 75% of the image.",1,neutral
"This is supported by the boosted performance of MAE [10] when removing larger amounts of the image, forcing the network to use some notion of global reasoning.",1,neutral
"pre-trained by MAE [16], which inputs the template image and the search area image into the network together to realize the integration of feature extraction and matching.",2,positive
"The backbone network of OSTrack adopts the vanilla ViT-Base [14] model pre-trained by MAE [16], which inputs the template image and the search area image into the network together to realize the integration of feature extraction and matching.",2,positive
"Patchification has unlocked new capabilities, such as (random) dropping of image patch tokens [10, 20, 44, 53, 61], adding specialized tokens for new tasks [54, 56] or mixing image tokens with tokens from other modalities [1, 38, 64].",2,positive
"SuperViT [34] is most related to FlexiViT as it patchifies an image at multiple scales, passes all these patches to ViT, while dropping random tokens [20] to reduce the sequence length.",1,neutral
"6, 15 [20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B.",0,negative
"Some suggest removing tokens, either in randomized [20] or structured [10] fashion throughout training.",1,neutral
"[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B.",0,negative
"Visual text and tokenization in NLP The most closely related method to CLIPPO from the NLP domain is PIXEL [54], which is an MAE [24] trained on rendered text.",1,neutral
"Visual text and tokenization in NLP The most closely related method to CLIPPO from the NLP domain is PIXEL [60], which is a masked autoencoder (MAE) [26] trained on rendered text.",1,neutral
"Following the design choices in MAE [2], MAViL employs 12-layer Transformers (ViT-B) with 12 attention heads as the encoders for each modality .",2,positive
"To learn robust audio-video representations, we go beyond raw input reconstruction in uni-model MAEs [2, 4, 55], multimodal MAE [56], and CAV-MAE [41].",1,neutral
"Masked autoencoders (MAEs) have recently emerged as powerful tools for learning uni-modal representations in various modalities such as image [2], video [3], and audio [4].",1,neutral
"Following MAE [2], we only keep the pre-trained encoders and use the average-pooled top-layer outputs for classification.",2,positive
We notice a concurrent and independent study CAV-MAE [41] uses inter-modal contrastive objective and MAE [2] to reconstruct raw inputs.,1,neutral
"It aims to reconstruct audio and video simultaneously as self-supervision, which sets it apart from uni-modal MAE approaches such as MAE [2], Audio-MAE [4], or Video-MAE [3].",2,positive
"For the visual branch, we either randomly initialize it or initialize it with the self-supervised MAE [2] pre-trained on ImageNet (compared in Table 6).",1,neutral
"NLIP contains a visual encoder-decoder inspired by MAE (He et al. 2021) for reducing the computation cost and maintaining the high quality of visual feature representation, a text encoder encoding the texts enriched by extra auxiliary visual concepts and a concept-conditioned cross-modal decoder…",2,positive
The text encoder and concept-conditioned cross-modal decoder are initialized from BARTbase (Lewis et al. 2020) and the MAE decoder only has 4 transformer blocks with 64-d head.,2,positive
"To enhance visual feature representation via self-supervised regularization, an MAE decoder is adopted to restore masked patches by Image Reconstruction (IR) loss LIR:
LIR = N∑ i=1 ( Ve(x′i) ∥Ve(x′i)∥ − xi ∥xi∥ )2.",1,neutral
"NLIP contains a visual encoder-decoder inspired by MAE (He et al. 2021) for reducing the computation cost and maintaining the high quality of visual feature representation, a text encoder encoding the texts enriched by extra auxiliary visual concepts and a concept-conditioned cross-modal decoder learning to synthesize semantic-consistent captions to complete noisy ones.",2,positive
"Furthermore, inspired by He et al. (2021), we explore enhancing the visual encoder via randomly masking the input image tokens and then reconstructing them, which can help reduce the computation cost during training and boost visual embedding by maintaining low-level visual information.",2,positive
"Patch-based masking methods for self-supervised learning (Bao et al., 2022; He et al., 2021; El-Nouby et al., 2021a) have recently demonstrated their potential for image generation (Chang et al.",1,neutral
MAE means MAE unsupervised pretraining [30] on the MillionAID [54].,0,negative
"Especially the reconstruction of a masked input gained huge traction across various domains of application, among others like natural language processing (NLP) [11] and 2D imagery [15] also on point clouds.",1,neutral
", image-image contrast [5, 9, 10, 21, 28], language-image contrast [12, 53], and masked image modeling [3, 4, 18, 19, 27, 31, 42, 70].",1,neutral
"Mirroring the idea of masked language modeling (MLM), MAE (He et al., 2022), BEiT (Bao et al.",1,neutral
"Mirroring the idea of masked language modeling (MLM), MAE (He et al., 2022), BEiT (Bao et al., 2021), and SimMIM (Xie et al., 2022) use masked image modeling (MIM) for self-supervised vision pretraining.",1,neutral
"Recent years have seen the rapid proliferation of vision transformers (ViTs) across a diverse range of tasks from image classification to semantic segmentation to object detection (Dosovitskiy et al., 2020; He et al., 2021; Dong et al., 2021; Liu et al., 2021; Zhai et al., 2021; Dai et al., 2021).",1,neutral
"1 INTRODUCTION Recent years have seen the rapid proliferation of vision transformers (ViTs) across a diverse range of tasks from image classification to semantic segmentation to object detection (Dosovitskiy et al., 2020; He et al., 2021; Dong et al., 2021; Liu et al., 2021; Zhai et al., 2021; Dai et al., 2021).",2,positive
"com/alinlab/OAMixer including out-of-distribution generalization [4, 37, 39], a natural extension to video domains [3, 5], integration with other domains like language or speech [2, 40], and easily combined with state-of-the-art visual self-supervised learning [22].",2,positive
"In works such as [21, 31], a masked autoencoder approach [18] is applied in the point cloud domain.",1,neutral
"Some works (He et al., 2021; Xie et al., 2022) use pixels as targets; others use pre-trained tokenisers (Bao et al., 2021) or modalityspecific handcrafted features (Wei et al., 2021; Hsu et al., 2021; Shi et al., 2022).",1,neutral
"…successful in various domains, such as natural language processing (Devlin et al., 2019; Radford et al., 2018; 2019; Brown et al., 2020), image recognition (He et al., 2021; Xie et al., 2022; Bao et al., 2021), and speech recognition (Baevski et al., 2020; Hsu et al., 2021; Shi et al., 2022).",1,neutral
"Note that mask tokens are applied to the predictors rather than the encoders (He et al., 2021).",1,neutral
"It is common to apply the loss only on masked inputs for within-modal losses (Hsu et al., 2021; He et al., 2021), since predicting targets corresponding to unmasked inputs may be trivial.",1,neutral
"Some works (He et al., 2021; Xie et al., 2022) use pixels as targets; others use pre-trained tokenisers (Bao et al.",1,neutral
", 2020), image recognition (He et al., 2021; Xie et al., 2022; Bao et al., 2021), and speech recognition (Baevski et al.",1,neutral
"Our masking is less aggressive than what was found to be optimal in related self-supervised image and action recognition literature (where 75% or even 90% of the input is masked) (He et al., 2021; Tong et al., 2022).",2,positive
"successful in 2D (images), even reaching the level of supervised pre-training [12, 16, 33, 37].",0,negative
"Another work, related to MAE [37] (images) or Point-MAE [67] (part segmentation): Voxel-MAE [59] reconstruct the complete voxel grid, given a partially masked input.",1,neutral
"The latter can operate the reconstruction in the feature space [12, 16, 29, 30, 33] trying to reconstruct the features issued from a teacher signal or in the images domain [4, 37], a partially masked input image being reconstructed.",1,neutral
"Recent advances in deep learning [6, 7, 13, 14, 53, 54] rely on massive amounts of training data that not only consume a lot of computational resources, but it is also timeconsuming to train these models on large data.",1,neutral
"We followed the data augmentation scheme of the MAE[21] fine-tuning phase, and the rest of the settings were kept consistent with the pre-training phase.",0,negative
Recently MAE[21] has effectively improved the performance of ViT downstream tasks by recovering the mask tokens pixel information.,2,positive
"proposed MAE[21], which greatly improves the model efficiency while enhancing the performance of ViT.",2,positive
"In Table 11, we compare our results with previous supervised pretraining methods[18, 29], self-supervised MIM methods [2, 10, 3, 7, 5] and CLIP-based MIM methods [25, 31, 13, 24] (i.",2,positive
"The prediction target is a vital component and has attracted a lot of research efforts exploiting different targets such as discrete dVAE code [2], pixels [10, 26], perceptual codebook [5], HOG features [23], online features [6, 1, 8] and so on.",2,positive
Recent advances of SSL methods (Caron et al. 2021; He et al. 2022; Zhou et al. 2021) can achieve comparable or even superior performance to their supervised pre-training version in solving the downstream task.,2,positive
"More recent works can be categorized in discriminative (Dosovitskiy et al. 2014; Bachman, Hjelm, and Buchwalter 2019; He et al. 2020; Chen et al. 2020a) or generative (Kingma and Welling 2013; Xie et al. 2021b; Bao et al. 2021; He et al. 2022) fashion.",2,positive
"Among the possible solutions, selfsupervised representation learning has shown its effectiveness in a wide range of fields including images [2, 12, 13], videos [6, 9, 15, 22, 28] and point clouds [16, 24, 31, 33, 34].",1,neutral
"Benefiting from the rapid development of 2D representational learning [12, 13, 20], 3D representational learning [23, 24, 32, 34] has also been widely explored.",1,neutral
", 2022) investigates the use of the masked autoencoder loss (He et al., 2021a) as a vision loss.",2,positive
"Finally, recent work (Singh et al., 2022) investigates the use of the masked autoencoder loss (He et al., 2021a) as a vision loss.",1,neutral
He et al. (2021b) discovered conducting self-supervised learning on images requires increasing the masking rate to 80%.,1,neutral
"This method was adopted in multiple recent papers and in particular for fine-tuning of self-supervised approaches [5, 22], yet the contribution of this fine-tuning ingredient was not quantitatively measured.",1,neutral
"For instance, recently there has been renewed interest in (masked) auto-encoders [5, 22, 16], which were popular in the early deep learning literature [7, 19, 27].",1,neutral
"More recently, similar methods have been adopted in the vision community as well [10, 41, 82].",1,neutral
We extend the masked-autoencoding framework [41] to learn audiovisual feature representations that can be leveraged for both multimodal and unimodal downstream tasks.,2,positive
"In the masked autoencoding framework [41], the input, x, is tokenised following previous supervised learning setups [6, 26, 33].",1,neutral
"Despite recent advances in selfsupervised image- [10, 41] and video-representation learning [29,76,82], these works still ignore the additional auditory information that is already present in their pretraining",1,neutral
"In the masked autoencoding framework [10, 29, 41], the decoder is another transformer that reconstructs the masked tokens given the encoded tokens as context.",1,neutral
"Additional standardisation may also be applied to x̃ [41, 76].",1,neutral
"Our approach is inspired by the masked autoencoding framework [10,41], which itself is based on similar masked data modelling approaches in NLP [24] and earlier works ar X iv :2 21 2.",2,positive
We begin with an overview of masked autoencoders [41] and transformers in vision in Sec.,2,positive
"Masked Autoencoders (MAE) [41] further showed that simply regressing to the original inputs in pixel-space was just as effective, and by only processing unmasked tokens in the encoder, training could be significantly accelerated.",1,neutral
"More recently, approaches such as masked autoencoders [41] have demonstrated how vision transformers can be pretrained with only self-supervision on smaller datasets.",1,neutral
"Following MAE [31] and BEiT [2], existing masked video modeling methods [21, 57, 63] pretrain video transformers through reconstructing low-level features, e.",1,neutral
We follow the training strategy in MAE [31] and VideoMAE [21] for image teachers and video teachers respectively.,2,positive
"In this paper, we follow the decoupled encoder-decoder transformer architecture in MAE [31] due to its effectiveness and simplicity.",2,positive
MAE [31] proposes an asymmetric encoderdecoder framework for the reconstruction of pixels.,2,positive
"For obtaining spatial targets, we adopt the vanilla image ViT pretrained by masked image modeling [31] on the image dataset (e.",1,neutral
"For pixel regression in MAE [31] and VideoMAE [57], the L2 distance is used as the distance metric.",1,neutral
"This is achieved by a two-stage framework, MVD, optimized to predict high-level features that derived from off-the-shelf MIM pretrained image models [31] and MVM pretrained video models [57] which are readily available.",2,positive
"For self-supervised visual representation learning, recent masked image modeling (MIM) methods like MAE [31] and BEiT [2] achieve promising results with vision transformers [17] on various vision downstream tasks.",1,neutral
MAE [21] adopts an asymmetric encoder-decoder architecture to produce informative latent representation by training image reconstruction tasks on 75% masked images.,2,positive
"To show the feasibility of the concept, we explore the adoption of masked autoencoder (MAE) structure [21] for image reconstruction, which achieves state-of-the-art performance in the self-supervised learning regime.",2,positive
"1, we compare the pooling strategies based on the scratch training and fine-tuning on pre-trained models by Self-Supervised Learning (SSL), including MAE [27], BeiT [3], SimMIM [77] and Data2Vec [2].",2,positive
"For ViT, recent works exploit the average pooling for achieving better performances than the class token [12, 52, 53], or preserving such per-token information [2, 3, 27, 77].",1,neutral
"Moreover, ViT has been actively used for Self-Supervised Learning (SSL) task [2, 3, 7, 27, 72, 77, 78].",1,neutral
"The average pooling [1] has been a standard pooling strategy for CNNs and also has been actively exploited in ViT [2, 3, 27, 77].",1,neutral
"Every SSL model are pre-trained with ImageNet-1K, except BeiT [3] with † used ImageNet-22K.",1,neutral
"Recent ViT based SSL approaches, such as MAE [27], SimMIM [77], BeiT [3] and Data2Vec [2], feed direct loss to patch tokens for each objective (i.",1,neutral
"As a result, Vision Transformer (ViT) [21] was introduced, and its variants have shown great success in image recognition [29, 47, 66, 71], self-supervised learning [2, 3, 7, 27, 77, 78], object detection [6, 24, 44, 61, 62], segmentation [9, 64], image compression [35], image retrieval [22,23], and multi-modal representation learning [36, 49, 56].",1,neutral
We ablate the type of visual representation and prior use by trying an initialization using the VGG16 network [68] (VideoDex-VGG) and the MVP network [7] [69] (VideoDex-MVP) based representation trained for robot learning.,2,positive
"Inspired by the representation bottleneck and the mask-based method MAE [45], we propose a training pipeline based on the mask to boost data augmentation.",2,positive
"Inspired by the representation bottleneck and the mask-based method MAE [45], we propose a training",2,positive
model image size FLOPs #param E2E-ViT [13] E2E [15] E2E-DeiT [38] DeiT + ours ∆,1,neutral
"efficiency-accuracy trade-off than all E2E baselines, including the recently proposed improved E2E baselines [15].",2,positive
"We also compare Deep Incubation with the recently proposed improved E2E baselines in [15], where a systematically hyper-parameter search is performed on training configurations.",2,positive
"A natural basis for image retrieval methods are self-supervised models that inherently learn strong feature descriptors, matching similar images to similar representations [11,13,15,29,31].",1,neutral
We follow the design choice of MAE [23] and VideoMAE [50] that skips the video mask token [M] in the encoder and then insert it in the decoder.,2,positive
15% in BERT [13]) since languages are highly semantic and information-dense [23].,1,neutral
"Following [23,50], we only apply the encoder on visible video tokens, a small subset (e.",1,neutral
", 75% in MAE [23] and 90% in VideoMAE [50]) to construct a challenging self-supervisory task for good feature learning.",1,neutral
"Although not included in our MEDIAR, we expect that self-supervised learning approaches [6, 7, 12, 26, 34, 39] can be a promising alternative direction for using the unlabeled images.",2,positive
"Although MAE-based methods have shown effectiveness in 2D image [18] and video [52], how to apply it in large-scale point clouds remains an open problem.",1,neutral
"Different from MAE [18], the multi-scale structure requires the backtracing [12, 73] to make the masked regions consistent across scales to avoid information leakage from previous stages.",2,positive
"Mask Autoencoder (MAE) [18], serving as one of the effective ways for pre-training, has demonstrated great potential in learning holistic representations.",1,neutral
"Previous MAE-style pre-training architectures of (a) single-scale [18, 19, 38] and (b) multi-scale [12, 73] take as inputs the visible tokens and learnable tokens for decoders.",1,neutral
"Inspired by the promising results achieved by MAE [18] in 2D vision, some works extend it into point clouds.",2,positive
"Inspired by the success of MAE [18] in 2D images, we develop the masked autoencoder for self-supervised learning on LiDAR point clouds, as shown in Figure 4.",2,positive
"To set a baseline with the transformer decoder, we follow MAE [18] and some existing works [12,18,19,38,73] to design the pipeline, as shown in Figure 5(a).",2,positive
"Directly applying the original masking strategy [12, 18, 38, 73] to the last stage of the multi-scale encoder would make the pretext task too difficult, especially for small objects.",1,neutral
"Then, we preserve the encoder part of MAE as our shared face encoder and fix it all the time during training.",2,positive
"Thus, we opt to employ a pre-trained masked autoencoder (MAE) (He et al. 2022) to extract facial features that better capture facial appearances and identity information.",2,positive
"In contrast to the ViT decoder in MAE, we find the convolutional decoder achieves more realistic results.",2,positive
"Compared to the compact latent code of StyleGAN2 (Karras et al. 2020) and the identity embedding, the latent space of MAE can better capture facial appearances and identity information, because masked training requires reconstructing masked image patches from visible neighboring patches, thus ensuring each patch embedding contains rich topology and semantic information.",2,positive
The encoder is designed following MAE (He et al. 2022) and pre-trained on a large-scale face dataset using the masked training strategy.,2,positive
"As for F swa, we first pre-trained the face encoder following the training strategy of MAE on our face dataset.",2,positive
"To verify the superiority of using the latent representation of MAE, we train a new model which adopts the identity embedding as the identity representation and employs AdaIN as the injection method.",2,positive
"We first train our face masked autoencoder, following the same settings in MAE4 (He et al. 2022).",2,positive
The improvement is more salient in the MAE backbone.,0,negative
"To demonstrate the robustness of VQT to different pretraining setups, we also evaluate VQT on self-supervised (MAE) [20] and image-language (CLIP) pre-trained [39]",2,positive
"As shown in Figure 2, Adapt-
2Table 3 shows the transfer learning results by each method alone, using the MAE backbone.",2,positive
"Moreover, in order to demonstrate the robustness of the compatibility, we evaluate performance on three different pre-trained backbones: self-supervised pre-trained (MAE with ImageNet1K) [20], image-language pre-trained (CLIP) [39] and supervised pre-trained (ImageNet-21K).",2,positive
"To demonstrate the robustness of VQT to different pretraining setups, we also evaluate VQT on self-supervised (MAE) [20] and image-language (CLIP) pre-trained [39]
backbones.",2,positive
"Since the MAE pre-training uses the reconstruction objective instead of the classification or contrastive one, we hypothesize that some useful intermediate features for classification may not be propagated to the final layer2.",0,negative
"Recent years have witnessed rapid development in self-supervised learning and it has achieved success in both computer vision [12, 13, 14, 15, 16] and speech processing domain [17, 18, 19, 20].",1,neutral
"BeiT [20] propose visual token prediction with the pretrained tokenizer [44], MaskFeat [6] predicts the hand-crafted image descriptor, and MAE [25] directly reconstructs the raw pixels.",2,positive
"To establish a feasible and effective spatiotemporal representation, we study both popular video masked modeling [23, 25] and multimodal contrastive learning [13, 26].",1,neutral
"Instead of directly performing mask image modeling on pixels [21, 56], the pioneer BEiT [2] reconstructs masked patches quantized by a discrete VAE [45].",1,neutral
"To overcome these limitations, non-autoregressive (NAR) transformers are introduced based on different theories, like mask image modeling [2, 21] (i.",1,neutral
"In previous studies, the random masked strategy has been proven the most effective in MAE [16] and SimMIM [17], rather than that of square and block-wise.",1,neutral
"studies, the random masked strategy has been proven the most effective in MAE [16] and SimMIM [17], rather than that of square and block-wise.",1,neutral
"Recently, adopting the MIM in the ViT-based model has been popular as investigated in a variety of frameworks, including iGPT [14], BEiT [15], MAE [16], and SimMIM [17], which have demonstrated strong representation learning ability in computer vision tasks.",1,neutral
We make the following modifications to the MAE decoding process to customize it for document image generation and our task unification framework: (4.a) Cross-Attention with Character Embeddings.,2,positive
"For UDOP-Dual, the text-layout encoder-decoder follows T5-large, and the vision encoderdecoder has the same configuration as MAE-large.",2,positive
MAE demonstrations with 75% masking.,0,negative
The vision decoder is MAE-large decoder [14].,2,positive
"Next, we describe the MAE decoding process.",2,positive
"Besides sequential generation, UDOP can also generate vision documents by leveraging masked autoencoders (MAE) [14] by reconstructing the document image from text and layout modalities.",2,positive
"Originally, MAE masks a percentage of the image patches and feed non-masked patches into a vision encoder.",2,positive
"For the vision decoder, we adopt the decoder of MAE [14] and directly generate the image pixels with text and layout information.",2,positive
We adopt the MAE objective [14] for vision selfsupervised learning.,2,positive
MAE uses mean squared error and apply loss only on masked patches.,1,neutral
"Thus, here we directly apply a standard masked image modeling (MIM) pipeline [5, 19, 48] for training, illustrated in Figure 2.",1,neutral
"community [32, 38] and previous works [5, 19, 48] to use a learnable token vector to replace each masked patch.",1,neutral
"We show that LOCA yields improved performance over state-of-the-art supervised [53,60,65] and unsupervised [13,17,34,84] representation learning methods for ViTs when transferred to 11 diverse and challenging semantic segmentation benchmarks.",2,positive
"Specifically, the task is to reconstruct masked [6] or dropped [34] patches from the input sequence tokens, either directly in pixel space [34] or in feature space [6,69,84].",1,neutral
"This is +4.8 points above the best self-supervised competitor, MAE, and +3.1 points above supervised pretraining with DeiT-3.",0,negative
"For example, dense contrastive approaches adapt the popular contrastive SSL paradigm to the patch level [52, 55, 68, 72, 73] while masked autoencoders propose to reconstruct masked patches [6, 34].",1,neutral
"Following the standard protocol of the Vision Transformers [26], query and reference views are divided into non overlapping patches of resolution P × P .",2,positive
"In terms of training efficiency, based on our implementation, one LOCA epoch takes 17.4 minutes while one MAE epoch takes 5.7 minutes.",0,negative
"Hence, LOCA achieves an improvement of +4.3 points over MAE while being only 1.1× longer to pretrain.",0,negative
"By contrast, models trained with a spatially-aware objective such as MAE or LOCA produce more spatially accurate predictions.",1,neutral
"Indeed, we have observed that freezing the backbone and training a linear classifier on top of MAE features perform very poorly [34].",1,neutral
"However, LOCA reaches 82.1% average relative improvement over random initialization in 600 epochs while MAE reaches 77.8% in 2.6× more epochs (1600).",0,negative
"In this section, we compare LOCA to popular state-ofthe-art SSL models for ViTs: DINO [13], MoCo-v3 [17], MAE [34] and iBOT [84].",2,positive
"Of particular interest, MAE representations achieve the second best SSL performance.",2,positive
"Interestingly, Caron et al. [13] have shown that segmentation masks emerge from the attention maps of Vision Transformers (ViT) [26] trained with these contrastive methods and several works have built on this observation to generate completely unsupervised segmentations [33, 58, 85].",1,neutral
"Recently, patchlevel SSL pretrainings have attracted more and more attention in the community [5, 6, 34, 70, 74, 80].",0,negative
"Recently, masked auto-encoders revisit this “inpainting” approach to pretraining Vision Transformers [6, 34, 69].",1,neutral
"In particular, compared to human-generated sources such as natural languages, which are highly semantic and information-dense [10], images typically require larger transmission bandwidth, and are generally natural signals with a lot of spatial redundant information, thus efficient image semantic communications deserve investigations.",1,neutral
"Under the longer training schedule (800 epochs), our model reaches 83.9% accuracy, 0.4% higher than MAE (He et al. 2021) and
0.9% higher than RandSAC (Hua et al. 2022) (a concurrent autoregressive work of ours).",2,positive
"For linear probing, we use the feature of the last block and adopt an extra BatchNorm (Ioffe and Szegedy 2015) layer before the linear classifier following MAE.",2,positive
"2020), recent works (He et al. 2021; Bao, Dong, and Wei 2021; Wei et al. 2021; Xie et al. 2021) introduce BERT-style pretraining by reconstructing the masked patches, which achieve an overall improvement in downstream tasks and greatly narrow the gap between vision and language.",2,positive
"We followed the pre-training parameters in MAE (He et al. 2021) and MaskFeat (Wei et al. 2021) without using color jittering, drop path, and gradient clip.",2,positive
"Our model pretrained with 300 epochs achieves the same accuracy as MAE (He et al. 2021) pretrained with 1600 epochs, which indicates that our method converges faster in the pretraining stage.",2,positive
"According to the MAE, reducing the number of image masks can improve reconstruction results, but it will decrease model representation due to image local dependency.",2,positive
"We observe that our method achieves 47.8 mIoU, which is slightly lower than MAE by 0.3, but higher than all others.",2,positive
"3, we discovered that MAE masked out 75% of the image tokens, resulting in blurred reconstruction results in large masked regions.",1,neutral
MAE (He et al. 2021) masks a high proportion of the input image and just predicts raw pixels.,1,neutral
"We note that BEIT and MAE are pretrained with 1600 epochs and use grid-search to find the best hyperparameters, while we only pretrain 800 epochs and don’t tune any parameters in the fine-tune stage due to limited access to computation.",2,positive
"3, shows that the baby in the image is ignored by MAE because it’s totally masked, but SAIM can efficiently generate the baby in the image.",1,neutral
"The patch-based iGPT achieves 82.70 Top1 accuracy, which is higher than DeiT-base (Touvron et al. 2021)(81.8) but still lags behind the MIM method (e.g. 83.6 for MAE).",2,positive
"While, our proposed stochastic autoregressive image modeling, SAIM, utilizes all the information of the image to generate clear images, and achieve better fine-tuning accuracy than MAE on ImageNet-1K.",2,positive
"Given a partly masked image, the network is trained to reconstruct properties of the masked areas such as VAE features [2, 17, 51], HOG features [93], or color information [29, 99].",1,neutral
"Method Plane Bcycl Bus Car Horse Knife Mcyle Persn Plant Sktb Train Truck Mean
CDAN [48]
R es
N et 85.2 66.9 83.0 50.8 84.2 74.9 88.1 74.5 83.4 76.0 81.9 38.0 73.9 MCC [36] 88.1 80.3 80.5 71.5 90.1 93.2 85.0 71.6 89.4 73.8 85.0 36.9 78.8 SDAT [57] 95.8 85.5 76.9 69.0 93.5 97.4 88.5 78.2 93.1 91.6 86.3 55.3 84.3 MIC (SDAT) 96.7 88.5 84.2 74.3 96.0 96.3 90.2 81.2 94.3 95.4 88.9 56.6 86.9 TVT [87]
V iT 92.9 85.6 77.5 60.5 93.6 98.2 89.3 76.4 93.6 92.0 91.7 55.7 83.9 CDTrans [85] 97.1 90.5 82.4 77.5 96.6 96.1 93.6 88.6 97.9 86.9 90.3 62.8 88.4 SDAT [57] 98.4 90.9 85.4 82.1 98.5 97.6 96.3 86.1 96.2 96.7 92.9 56.8 89.8 SDAT w/ MAE [25] 97.1 88.4 80.9 75.3 95.4 97.9 94.3 85.5 95.8 91.0 93.0 65.4 88.4 MIC (SDAT) 99.0 93.3 86.5 87.6 98.9 99.0 97.2 89.8 98.9 98.9 96.5 68.0 92.8",0,negative
"3 further provides a baseline of SDAT with MAE [25] pretraining, which includes masked image modeling (MIM) and ImageNet supervision.",2,positive
"3 further provides a baseline of SDAT with MAE [29] pretraining, which includes masked image modeling (MIM) and ImageNet supervision.",2,positive
"To sample the mask, block-wise masking [2], random patch masking [29,99], and attentionguided masking [43, 54] have been explored.",1,neutral
"Moreover, we compare the image imputation performance of our Edge-MAE with vanilla MAE [24].",2,positive
"Furthermore, the results of “With data augmentation” indicate that Edge-MAE works effectively without data augmentation, which is consistent with the findings of [24].",2,positive
"We adopt the partial fine-tuning strategy inspired by [24], i.",2,positive
"More recently, Masked Autoencoder (MAE) [24] is proposed, in which a transformerbased encoder learns the latent representation of a small subset of visible patches, while a lightweight decoder imputes the original input from mask tokens and latent representation.",1,neutral
", image segmentation), the network is initialized by performing a pretext task, such as solving jigsaw puzzles [36], masked pixel prediction [37], or image imputation for randomly masked image patches [24].",1,neutral
"Inspired by Masked Autoencoders [24], an Edge-preserving Masked Autoencoder (EdgeMAE) is presented, which is pre-trained using both paired and unpaired multimodal MR images in a self-supervised learning manner.",1,neutral
"To obtain α for each patch, a binary mask is generated based on the random masking strategy [24].",1,neutral
The original MAE [24] utilizes a transformer-based decoder to impute patches in the masked position.,2,positive
"Inspired by [16,23,46], we perform mask sampling on video and text to achieve end-to-end video-text alignment.",2,positive
"Different from [4, 16, 23, 46], we do not apply masked prediction on video and text but directly apply the cross-modal contrastive loss to pull video and text together.",1,neutral
"This is consistent with the conclusions of MAE [23] and BERT [4]: the information density of image is low, and the information density of text is high.",1,neutral
"Instead of blindly applying the mask-thenprediction paradigm from MAE, we propose a maskedthen-alignment paradigm, namely Masked Contrastive Pretraining, for efficient video-text alignment.",2,positive
"This is consistent with the conclusions of MAE [23] and BERT [4]: the information density of image is low, and the
information density of text is high.",1,neutral
"Benefiting from the success of the transformer architecture in the computer vision domain, recent methods [4,23] draw inspiration from MLM and propose a masked visual modeling (MVM) technique to randomly mask a high portion of spatial patches and encode with visible patches, which greatly reduces spatial redundancy.",1,neutral
This suggests that the masked-thenprediction paradigm in MAE [23] is inconsistent with the goal of retrieval tasks.,1,neutral
"The final masking ratio is 60% for video and 15% for text, which is consistent with [12] and [23, 46].",0,negative
"Different from [4, 16, 23, 46], we do not apply the masked prediction on video and text but directly apply the contrastive objective to pull the paired video and text together while pushing the unpaired video and text apart.",1,neutral
"From VideoMAE [46] and ST-MAE [16] perspectives, the high masking ratio also prevents the model from simply copying neighborhood pixels for low-level reconstruction and use an extreme masking ratio of 90%.",2,positive
"Without blindly applying mask-then-prediction paradigm from MAE, we explore the masking contrastive mechanism based on
the video language domain, and propose a mask-thenalignment paradigm to efficiently learn a multimodal alignment.",2,positive
"MAE [23] and follow-up works [16, 46, 55] utilize the autoencoder to directly reconstruct RGB pixels in an end-to-end manner.",2,positive
"Recent mask sampling techniques in the visual domain [16, 23, 46] propose to randomly mask a high-ratio of spatial regions and adopt the unmasked regions to pretrain the encoder, which brings a new idea to reduce spatial redundancy.",1,neutral
"Another common tactic is fulfilled with sinusoidal mapping Fsine [20,54], through which p is generated on-the-fly by a fixed function dependent on NH , NW and D. Due to space limitation, we provide explicit expression in Appendix B.1.",1,neutral
"All backbones are pre-trained on ImageNet-1k, among which MAE [21] uses unsupervised pre-training.",2,positive
We demonstrate the explicit mapping function Fsine for sine-cosine positional embedding p as follows.,1,neutral
"Another common tactic is fulfilled with sinusoidal mapping Fsine [21,53], through which p is generated on-the-fly by a fixed function dependent on NH , NW and D.",1,neutral
"Particularly, d-th dimension of pm,n can be mapped with Fsine(m,n, d) as below,
Fsin(m,n, d) = { fsin(m, d,NH , D) if d   D/2 fsin(n, d,NW , D) otherwise ,
fsin(pos, d,N,D) =
{ sin( posN+ /T
2d/D) if d%2 = 0 cos( posN+ /T 2(d−1)/D) otherwise ,
where the temperature T and is set to 10000 and 1e−6 respectively, and a normalization is also used to ensure better continuity among varying resolutions.",1,neutral
The Masked Autoencoder (MAE) method [29] further takes advantage of masking to reduce training time and memory.,1,neutral
"In computer vision, explorations along this direction include predicting large missing regions [50], sequence of pixels [10], patches [20, 29, 71], or pre-computed features [6, 66].",1,neutral
"Our fine-tuning implementation follows MAE [29], with the learning rate tuned for each entry.",2,positive
This is in line with the observation [29] that language data has higher information-density than images and thus the text masking ratio should be lower.,1,neutral
"The MAE design has been applied to videos [61, 22], point clouds [49], graphs [59, 9, 32], audio [4, 47, 13, 35], visual control [70, 57], vision-language [23, 41, 31, 19], and other modalities [5].",1,neutral
", 50% or 75%) of patches; the ViT encoder is only applied to the visible patches, following [29].",0,negative
MAE sparsely applies the ViT encoder [20] to visible content.,2,positive
"Our work is related to MAE and its vision-language extensions [23, 41, 31, 19].",2,positive
The reconstruction head follows the design in MAE [29]: it has a small decoder and reconstructs normalized image pixels.,2,positive
"We do not use a reconstruction loss, unlike MAE [29].",1,neutral
"While the encoder is pre-trained on masked images, it can be directly applied on intact images without changes, as is done in [29].",1,neutral
"Inspired by the sparse computation of Masked Autoencoders (MAE) [29], we randomly remove a large portion of image patches during training.",2,positive
"By default, we apply our models on intact images at inference-time, similar to [29].",2,positive
"Self-supervised learning (SSL) has recently provided a promising paradigm toward human-level intelligence and achieved great success in the domains of natural language processing and computer vision, such as BERT,(31) SimCLR,(32) and MAE.(33) SSL firstly pre-trains a model on a well-designed pretext task, then fine-tunes it on a specific downstream task of interest.",1,neutral
"Self-supervised learning (SSL) has recently provided a promising paradigm toward human-level intelligence and achieved great success in the domains of natural language processing and computer vision, such as BERT,31 SimCLR,32 and MAE.33 SSL firstly pre-trains a model on a well-designed pretext task, then fine-tunes it on a specific downstream task of interest.",1,neutral
We use MAE pre-training for most experiments by default unless otherwise specified.,2,positive
"MAE pretraining is to recover the masked patches in the image, which may exhibit a stronger localization capability helping object detection task.",1,neutral
"Thus, for the model initialized from MAE pre-training, we increase finetuning iterations to 180k and batch size to 64.",2,positive
"As shown in Table 3, the performance improves notably as the model size increases, and MAE pre-training shows better performance than GIT pre-training.",0,negative
"In experiments, we explore two pre-training schemes: 1) MAE pre-training: The ViT backbone is initialized from the self-supervised MAE [13] trained on ImageNet-1K [7], while the rest of the model parameters are randomly set; 2) GIT pre-training: The ViT backbone and text decoder are initialized from the pre-trained image VL model GIT [33] and the rest are randomly set.",2,positive
MAE [13] Image Reconstruction (ImageNet-1K) backbone ViT-B 53.,2,positive
We find MAE pre-training can alleviate overfitting and benefit from more training epochs as discussed in [20].,2,positive
Masked Traffic Reconstruction: The mask reconstruction model used by MAE [14] is applicable to What sets METCMVAE apart is its use of traffic tokens with ambiguous semantics to discover associations between traffic window lengths.,1,neutral
The architecture of METC-MVAE is made up of encoder and decoder blocks [14].,2,positive
"In recent years, self-supervised pre-training technique benefits from utilizing unlabeled data, have been widely used, for example, in NLP [12], [13], computer vision [14], [15], etc.",1,neutral
Masked Traffic Reconstruction: The mask reconstruction model used by MAE [14] is applicable to What sets METCMVAE apart is its use of traffic tokens with ambiguous,2,positive
"Number of Transformer laye [6,8,10,12,14,16] 12 Activation Functions [tanh,relu,elu,leakyrelu] tanh Batch Size [4,.",1,neutral
"Two other methods will be tested as well, the first being an adapted version of the Masked Language Modelling (MLM) of BERT [4], and the other being the one of Masked AutoEncoder (MAE) [7].",2,positive
These are heavily inspired by the choices of MAE [7].,1,neutral
"The MAE [7], like BEiT, is a variant of a denoising autoencoder [20] and is based on the pre-training of BERT’s MLM.",2,positive
"The self-supervised pre-training methods explored are mainly based on three methods, either Autoencoder based methods [4], [7], where the aim is to reconstruct the original data point, often after the noise has been added.",1,neutral
"Additionally, a special Classification (CLS) token is appended before adding the sinusoidal positional embedding as per convention [4], [5], [7] such that the final dimension of the input data after patchification is (S+1)×D.",1,neutral
"The target for BERT and MAE pre-training methods were normalized as proposed in MAE [7], and the outputs of the Transformer encoder/decoder are sent through a linear projection before the masked patches are compared with the target using L2loss.",2,positive
"In situations where attaining large labelled data sets on relevant tasks is not feasible, self-supervised learning has proven to be a powerful substitution, being able to leverage unlabelled data to increase performance on smaller data sets [3], [4], [7], [8], by learning a representation of the data for the task at hand.",1,neutral
"The pre-training methods evaluated are BERT style MDM self-supervised learning [4], MAE self-supervised learning [7], and the contrastive learning approach of BYOL.",2,positive
"MDM approaches, however, seem promising, as not only have MDM-based methods shown excellent results in two very different domains [4], [7], the methods used are conceptually simple and appear",1,neutral
"self-supervised learning on an unlabelled data set [4], [7].",1,neutral
"In the inference stage, the RMR module keeps the decoder for reconstruction, which is different from MAE.",2,positive
"The modified MAE is trained by adding the data augmentation methods, including random horizontal flips and color jitters.",2,positive
"Inspired by the masked auto-encoder (MAE) [21], we modified and applied it to the adversarial environment.",2,positive
"Another prominent line of works aims to use large unlabelled datasets to learn a strong visual representation, which can then be utilized for labelling downstream datasets with fewer supervised samples [4, 12, 13].",1,neutral
"In computer vision, linear probing performance is used as a fast on-the-fly metric for model evaluation [8, 13, 18], which is complementary to fine-tuning considering the computational cost.",1,neutral
"With these probing tasks, we compare advanced vision-only models including MAE [18] and MOCOv3 [8], with vision-andlanguage pretrained models including OFA [54], FLAVA [43] and CLIP [39].",2,positive
"Recently, thanks to the invention of vision transformer ViT [13] and the follow-up improvements [4, 18, 35, 49], end-to-end pretraining from raw images pixels is made possible.",2,positive
weights of a ViT that has been pre-trained on ImageNet-21k in a self-supervised manner using the Masked Autoencoders (MAE) technique [33].,1,neutral
"For instance, SCALE would achieve a performance improvement of 0.3% over the best VideoMAE [54] (1600 epochs checkpoint – finetuning) and of 0.6% in the case of the best ρBYOL [18] (800 epochs checkpoint – linear probe) on Kinetics400 with 64 V100 GPUs in about 5 minutes.",2,positive
", as the initialization parameters of the trained model) [25, 35, 54] on a downstream task, where only a small labeled dataset is available.",1,neutral
"Input Sparsity: Sparsity in the input to the model [1, 3, 19, 25, 54] is an effective way to drastically reduce the computational load and memory requirements, while taking advantage of the information redundancy in images and videos [16].",1,neutral
"In contrast, as a reference and with the same computational resources, VideoMAE requires about 27.7 hours to improve its performance of 0.5% through fine-tuning from its 800 to 1600 epochs checkpoint, and ρBYOL needs at least 48 hours to improve of 0.4% its linear probing performance from its 200 to 400 epochs checkpoint.",2,positive
"Similar to MAE [25], we found that applying a batch normalization layer [30] without affine transformations is beneficial for VideoMAE models.",1,neutral
"This is in contrast to MAE-based SSL methods for vision, where the proposed pseudo-tasks are based on the reconstruction of the whole input [16, 19, 25, 54], and even if the loss uses a subset of the tokens (the masked ones) for the loss calculation, the remaining tokens are still part of the decoder’s output and computation graph.",1,neutral
This task is similar to that of a masked autoencoder [25] and gives you an enhanced per-clip representation.,1,neutral
"Masked input reconstruction methods have recently become popular on images [25] and successfully translated to video [16, 54].",1,neutral
"Unless stated otherwise, we train our models for 500 epochs (for example, training with VMAEB on SSV2 takes 137 minutes with one 3090 GPU) with a batch size of 512 and use all 16 clips.",0,negative
"Our SCALE linear with SVT backbone beats the previous state of the art (71.8% vs. 71.5% [18]) and SCALE ft can even improve the accuracy of VMAEBft , which is a strong supervised model, from 81.5% to 81.84%.",2,positive
"We choose ρBYOL for their excellent linear performance, SVT for the usage of ViT [14], and VMAE for showing 1) the applicability of our proposed method to MAE models, 2) the scalability of our method to larger models, and 3) possibility of using supervisedly fine-tuned models as our backbone.",2,positive
We also used a backbone pretrained and fine-tuned on SSv2 (VMAEBSSv2) for the SSv2 experiment to show the universality of SCALE with respect to the pretraining dataset.,2,positive
"Since our clip representations are somewhat abstract representations of the video, we expect the optimal masking ratio to be close to NLP models rather than video MAEs.",2,positive
"Pretrained backbones: We use the pretrained checkpoints of ρBYOL [18], SVT [44], and three variants of VideoMAE [54] (base(B), large(L), and fine-tuned base(FT)).",2,positive
"With SCALE k-NN, we see a consistent improvement over the baseline and find that pre-trained MAE-based models greatly benefit from our training.",2,positive
"All the models are self-supervisedly pretrained on Kinetics-400, except the fine-tuned VMAE base that was also supervisedly finetuned on Kinetics-400.",2,positive
"Masking Ratio: Masking ratio is an important hyperparameter and depends on the data modality, for example, BERT [12] uses 15%, MSN [3] uses 30% (for ViT-Base), MAE [25] uses 75%, and VideoMAE [54] uses 90 to 95% masking.",1,neutral
We even improve the supervised model trained on SSv2 (VMAEBSSv2).,2,positive
"…(Chen et al.,
2020; He et al., 2020; Grill et al., 2020; Chen & He, 2021; Noroozi & Favaro, 2016; Zbontar et al., 2021), masked image models (He et al., 2022; Dosovitskiy et al., 2020; Doersch et al., 2015; Pathak et al., 2016), masked language models (Devlin et al., 2018; Raffel et al.,…",1,neutral
"…remarkable proficiency in addressing a myriad of complex downstream tasks that were very difficult or impossible for the previous methods (Ramesh et al., 2021; Rombach et al., 2022; Ramesh et al., 2022; Sohl-Dickstein et al., 2015; Brown et al., 2020; Radford et al., 2018; 2019; He et al., 2022).",2,positive
", 2021), masked image models (He et al., 2022; Dosovitskiy et al., 2020; Doersch et al., 2015; Pathak et al., 2016), masked language models (Devlin et al.",1,neutral
"Foundation models have recently exhibited remarkable proficiency in addressing a myriad of complex downstream tasks that were very difficult or impossible for the previous methods (Ramesh et al., 2021; Rombach et al., 2022; Ramesh et al., 2022; Sohl-Dickstein et al., 2015; Brown et al., 2020; Radford et al., 2018; 2019; He et al., 2022).",2,positive
"…representation learning have significantly improved downstream performance on virtually every modality, from images Aghajanyan et al. (2022); Bao et al. (2021); He et al. (2022) to text Liu et al. (2019); Lewis et al. (2020); Aghajanyan et al. (2021) to speech Conneau et al. (2020); Radford et al..",2,positive
The Transformer architecture [Vaswani et al. 2017] has received growing interest from various tasks in computer vision [Bao et al. 2021; Chang et al. 2022; Dosovitskiy et al. 2021; Esser et al. 2021a; He et al. 2021; Li et al. 2022; Liu et al. 2021].,2,positive
"Then, with this frozen visual encoder, we used the same feed forward architecture, Q-function parameterization, and training objective (CQL with C51) as scaled QL to
finetune the MAE network.",2,positive
"As before, we also evaluate fine-tuning performance using the representations obtained via masked auto-encoder pre-training (He et al., 2021; Xiao et al.).",2,positive
"In Figure 7, we present our results for offline fine-tuning on 5 games from Lee et al. (2022), ALIEN, MSPACMAN, SPACE INVADERS, STARGUNNER and PONG, alongside the prior approach based on decision transformers (“DT (pre-trained)”), and fine-tuning using pre-trained representations learned from state-of-the-art self-supervised representation learning methods such as contrastive predictive coding (CPC) (Oord et al., 2018) and masked autoencoders (MAE) (He et al., 2021).",2,positive
"• Representations learned by offline Q-learning give rise to more than 80% better performance when fine-tuning on new games compared to representations from state-of-the-art returnconditioned supervised (Lee et al., 2022) and self-supervised methods (He et al., 2021; Oord et al., 2018).",1,neutral
"For MAE, we first pretrained a vision transformer (ViT-Base) (Dosovitskiy et al., 2020) encoder with 80M parameters trained via a reconstruction loss on observations from multi-game Atari dataset and freeze the encoder weights as done in prior work (Xiao et al.).",2,positive
"…based on decision transformers (“DT (pre-trained)”), and fine-tuning using pre-trained representations learned from state-of-the-art self-supervised representation learning methods such as contrastive predictive coding (CPC) (Oord et al., 2018) and masked autoencoders (MAE) (He et al., 2021).",1,neutral
MAE is a more recent self-supervised approach that we find generally outperformed CPC in this comparison.,2,positive
"For the MAE implementation, we used the Scenic library (Dehghani et al., 2022) with the typical configuration used for ImageNet pretraining, except using 84× 84× 4 sized Atari observations, instead of images of size 224× 224× 3.",2,positive
"1 INTRODUCTION High-capacity neural networks trained on large, diverse datasets have led to remarkable models that can solve numerous tasks, rapidly adapt to new tasks, and produce general-purpose representations in NLP and vision (Brown et al., 2020; He et al., 2021).",1,neutral
"RA discussed the experiment design and project direction, helped set up and debug the training pipeline, took the lead on setting up and running the MAE baseline and the online fine-tuning experiments.",2,positive
"High-capacity neural networks trained on large, diverse datasets have led to remarkable models that can solve numerous tasks, rapidly adapt to new tasks, and produce general-purpose representations in NLP and vision (Brown et al., 2020; He et al., 2021).",1,neutral
"Fine-tuning with the frozen representations learned by MAE performs poorly, which we hypothesize is due to differences in game dynamics and subtle changes in observations, which must be accurately accounted for in order to learn optimal behavior (Dean et al., 2022).",1,neutral
", 2022) and self-supervised methods (He et al., 2021; Oord et al., 2018).",1,neutral
", 2018) and masked autoencoders (MAE) (He et al., 2021).",2,positive
We train the MAE for 2 epochs on the entire multi-task offline Atari dataset and we observe that the reconstruction loss plateaus to a low value.,2,positive
"Increasingly powerful architectures [3,13,24], learning methods [4, 12] and a large body of other techniques [15, 27] are constantly introduced.",1,neutral
"Recently, self-supervised learning [2, 3] has emerged as a possible solution to alleviate the dependency on large-scale ar X iv :2 21 1.",1,neutral
"robust visual features from discriminative [9–12, 27] and generative [13] self-supervised learning and vision-language alignment learning [28].",1,neutral
"[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"We train the completion network on the 11,426 complete skeletons using a masked auto-encoder strategy [32] where the missing keypoints are masked at the input and will be predicted using the unmasked keypoints.",2,positive
"More recently, generative-based methods, e.g., MAE [116], BEiT [117], and MST [118], has become the most successful self-supervised methods in the vision community and they have surpassed the promising performance achieved by contrastive learning methods.",1,neutral
", MAE [116], BEiT [117], and MST [118], has become the most successful self-supervised methods in the vision community and they have surpassed the promising performance achieved by contrastive learning methods.",2,positive
"Among popular methods to learn representation via self-supervise pre-training, masked autoencoder has been applied successfully to NLP (Devlin et al., 2018) and CV (He et al., 2021) domains.",1,neutral
", 2022) and masked autoencoders (He et al., 2021) are all using patches as input.",2,positive
"The following influential works such as BEiT (Bao et al., 2022) and masked autoencoders (He et al., 2021) are all using patches as input.",1,neutral
"Similar to [7], we utilized ADAMW [13] with learning of 1.",1,neutral
"1, our framework consists of three asymmetric vision transformer inspired by the work of [7].",2,positive
"Several pretext tasks such as jigsaw [8], inpainting [7], [9], contrastive learning [6], [10] are being widely studied.",1,neutral
"To address this problem, there has been increasing interest in self-supervision method [6], [7] that can learn the visual representation without additional data annotation.",1,neutral
"Inspired by these two papers, we add a mask-based random reconstruction module to the input images: x ← g(x), where g is a masked auto-encoder [16], defined by:",1,neutral
"Fortunately, [16] and [56] showcase that mask-based stochastic reconstruction models are semantically aware.",1,neutral
"Here, we examine whether an additional reconstruction module can regularize the partial derivatives to make them semantic-aware to produce human-meaningful images [16, 41, 56].",1,neutral
"The pre-training loss is computed on the masked tokens similar to MAE [29], and the final pre-training loss Lpre-train is defined as: Lpre-train = Lnode + Ledge, Lnode = ∑",1,neutral
"We investigated works in NLP [17] and computer vision (CV) [29] where the pre-trained models have been dominantly used, and we find that the key to most successful pre-training models is to design simple but effective tasks that can scale well.",2,positive
"Inspired by MAE [29], we propose a bi-branch graph masking and reconstruction task for molecular pre-training.",2,positive
"Based on [29], a narrower or shallower decoder would not impact the overall performance of the MAE.",0,negative
"Similar to MAE [29], we propose a transformer-style asymmetric encoder-decoder architecture for each branch.",2,positive
"[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"To enable effective expansion, we explore three prior models for guided imagination: DALL-E2 [56] and Stable Diffusion [59] are advanced image generative methods, while MAE [24] is skilled at reconstructing images.",2,positive
"Thanks to strong image reconstruction abilities, our GIF-MAE applies the MAE-trained model [24] as its prior model.",2,positive
"As DALL-E2 [56] and SD are powerful in generating images, and MAE [24] excels at reconstructing images, we explore their use as prior models of GIF for data imagination.",2,positive
"We choose ViT-Base (ViT-B) [24] as the backbone of our framework for both audio and video modalities, due to its stability in performance across different data streams [34, 30, 8].",2,positive
"This simple approach shows promise in different domains including image [13, 34, 11], video [65, 64, 25], and audio [50, 30, 20] among others.",1,neutral
"Inspired by the recent success of Transformers in different domains [23, 30, 34, 25], we use ViT [24] as the backbone of our framework for both audio and visual modalities.",2,positive
"It should be noted that, we use frame resolution of 112(2) and less, whereas, spatial resolutions of 224(2) and 384(2) are mostly used during pretraining with ViT amongst the earlier works [24, 8, 34, 64, 25, 3].",2,positive
"Further, we drop the masked tokens x before feeding the input to θae for computational efficiency [3, 34].",1,neutral
"Inspired by the recent success and scalability of pretraining with masked reconstruction in different domains [23, 13, 65, 30, 12, 34, 50, 64], we adopt masked data mod-",2,positive
"Self-supervised learning aims to learn meaningful representations from unlabelled data with no human supervision [16, 17, 46, 32, 34].",1,neutral
"Many self-supervised learning tasks have been explored [14, 21, 27, 34, 35, 45, 55], of which contrastive learning [23] currently most prevalent.",1,neutral
MAE [9] is trained on the self-supervised task of predicting an image from a partial observation.,1,neutral
"Our suggested method, MAEDAY, addresses FSAD by using Masked AutoEncoder (MAE) [9], a model trained for general image completion based on partial observations.",2,positive
[8] argued that the information density of NLP and CV are very different.,1,neutral
": To investigate the impact of neighbors on network performance, we set the number of neighbors [2, 8, 16, 32, 64] and compared them in the table II.",1,neutral
"Similar to the Masked Autoencoder (MAE) [16], an 8-layer transformer is used as an image decoder with the weights pre-trained on ImageNet dataset [6].",1,neutral
"To learn visual representations of images in a self-supervised manner, we apply Masked Autoencoder (MAE) which is
trained to reconstruct the randomly masked image patches.",2,positive
"Similar to the Masked Autoencoder (MAE)[17], a 8-layer transformer is used as an image decoder with the weights pre-trained on ImageNet dataset[7].",1,neutral
[11] use the self-supervised reconstruction task through masked autoencoders [15] for test-time adaptation.,1,neutral
"Considering the vigorous development of transformer [10–14] and computer vision technology in recent years, to reduce the computational cost and to ensure that the lane detection task can be efficiently completed, we propose a hybrid depth network composed of Swin Transformer and Predictive Recurrent Neural Network (PredRNN) [15] based on the MAE [16] network architecture for lane detection of continuous multi-frame image sequences.",2,positive
"Hence, the tokens can be arbitrarily masked (discarded from the input data) and the learning objective is to recover the masked contents at the pixel level [25, 62], the feature level [2, 55], or in the frequency space [39].",1,neutral
", BEiT [2] and MAE [25]) were built upon plain vision transformers.",1,neutral
", from the base level to the large or huge level) can boost the downstream performance [10, 25], which aligns with the observations in language modeling [3, 14].",1,neutral
"1,600-epoch MAE [25] by a significant margin of 3.",1,neutral
"1% accuracy with only 400 pre-training epochs, surpassing MAE [25] and HiViT [67] with 1,600 epochs.",0,negative
"Recent years have witnessed two major progresses in visual recognition, namely, the vision transformer architecture [18] as network backbone and masked image modeling (MIM) [2, 25, 62] for visual pre-training.",1,neutral
"The situation was changed when new pretext tasks were introduced, in particular, contrastive learning [5, 6, 9, 24, 24, 26, 61] and masked image modeling (MIM) [2, 25, 62], where the latter is yet another type of generation-based learning objective.",1,neutral
"A comparison between the attention maps generated by iTPN, the variant without integral pre-training (w/o iPT), and the MIM baseline (MAE [25]).",2,positive
"Following MAE [25], a random subset of 75% patches are masked from input, and the normalized pixels are preserved for reconstruction.",1,neutral
"Similar to prediction, VideoMAE cannot have feasible rewind results.",2,positive
"VideoMAE [73] is built upon MAE [30] and reconstructs the missing video cubes, which performs TVC by masking all video frames except the first or the last (or both).",2,positive
"VideoMAE attempts to produce all frames simultaneously, which is difficult to maintain video temporal consistency, resulting in a high 328.9 FVD on Kitchen.",2,positive
"The original work employed rotation prediction [15] as the auxiliary task, but subsequent works [12, 31] replaced it with the Masked Autoencoder reconstruction task [18] or contrastive learning [4].",1,neutral
"Accuracy, a common evaluation metric for image classification (Dosovitskiy et al., 2020; Xu et al., 2022b; He et al., 2022) was leveraged to assess different methods in a specific dataset.",2,positive
"To be more specific, training a ViT model 800 epochs in PlantCLEF2022 as MAE (He et al., 2022) requires more than five months with four RTX 3090 GPUs.",0,negative
"As shown in Figure 1, it is understood that three key factors essentially lead to a positive transfer learning performance, a desired source dataset, powerful model, and suitable loss function to pre-train the model (Wu et al., 2018; Kornblith et al., 2019; Kolesnikov et al., 2020; Tripuraneni et al., 2020; He et al., 2022).",2,positive
"To assess the generality, testing accuracy and mean testing accuracy was employed, instead of validation accuracy and mean validation accuracy as used in MAE (He et al., 2022).",2,positive
"Unfortunately, this setting may entail a long training epoch in PlantCLEF2022 to have a better performance, such as 800 epochs in MAE (He et al., 2022).",2,positive
"Specifically, MAE (He et al., 2022) uses reconstruction loss to learn better performance with a high occlusion.",1,neutral
"We emphasize here that we directly adopted the pre-trained model from masked autoencoder (MAE) (He et al., 2022), instead of training the model ourselves.",2,positive
"For a fair comparison, all models or transfer learning strategies were executed with the same settings with most of them following the fine-tuning schemes in MAE (He et al., 2022).",2,positive
"On the contrary, MAE (He et al., 2022) achieves a 85.",1,neutral
"Therefore, our dual transfer learning reduces training time via utilizing the public model from MAE (He et al., 2022).",2,positive
"These methods are simple to implement and scalable to large Internet-scale datasets and deep neural networks, leading to excellent flexibility and generalization for downstream tasks [9, 12, 1].",1,neutral
"Different from other masked prediction variants [12, 9], we found mask loss is not useful in our setting, as our goal is to obtain an scalable decision making model but not only for representation learning.",1,neutral
"• multi-goal reaching: For every trajectory in the validation set, we randomly sample a start state and 5 goal states at random future timesteps from [12, 60).",2,positive
Our first key observation is that masked token prediction with random masking similar to BERT [9] and MAE [12] provides a general and flexible way for learning from unsupervised data.,2,positive
"Unlike in MAE [12] and BERT [9] where the goal is learning representations, we want to directly apply MaskDP to various downstream tasks, and different mask ratios induce different pre-train and downstream gaps.",1,neutral
MAE [12] proposes to randomly mask patches of the input image and reconstruct the missing pixels.,1,neutral
"Architecture Our encoder is a Transformer [33] but applied only on visible, unmasked states and actions, similar to MAE [12].",2,positive
"Self-supervised pretraining has made tremendous successes for unsupervised representation learning in natural language processing (NLP) and vision [12, 9, 3, 4].",1,neutral
"methods have been proposed to model images [7, 10, 3, 12].",1,neutral
", Transformer [33], GPT [4], BERT [9], and MAE [12].",1,neutral
"The quality of the extracted features is then evaluated on the downstream application using a simple model, such as linear adaptation [1, 2, 10, 11], or a two hidden layer multilayer perceptron (MLP) [13, 32, 33] among others.",2,positive
"Hence, there has been growing interest in self-supervised methods [1, 2, 10, 11, 13, 32].",1,neutral
"The objective of unsupervised learning models [1, 2, 10, 11,13,32] is to pre-train neural networks or extract distilled information from input images without relying on labels.",1,neutral
"As in recent unsupervised papers [1, 10, 11, 13], we compare the quality of our unsupervised keypoints to the features obtained by a fullysupervised ImageNet model.",2,positive
"An exponential moving average (EMA)[20] with α = 0.998 is implemented, and the shadow weight is updated after each training step.",2,positive
An exponential moving average (EMA)[20] with α = 0.,1,neutral
"of reconstructing images from randomly masked image patches [12,27].",1,neutral
"In the self-supervised learning (SSL) literature, most works [2, 8, 9, 11, 13, 26, 27, 47, 59] focus on learning image-level representations by pre-training neural networks on natural images, such as ImageNet [16], where objects of interest are monotonously large, salient, and centered.",1,neutral
ST-MAE is proposed in [23] that is based on the MAE [19] utilizing the Siamese encoder structure and paradigm of deep feature transition for uniform VAD in multiple application scenarios.,2,positive
The structural configuration of SIVT follows the design of the MAE-base but we reduce the embedding dimension to 240 for efficient computation.,2,positive
ST-MAE is proposed in [23] that is based on the MAE [19] utilizing the Siamese encoder,2,positive
the vision Transformer has been explored and proven to be efficient for a wide range of visual tasks [19].,1,neutral
"Therefore, after adding fine training tricks and several key components, C-ResNet can also be a competitive model compared with SOTA method, such as Vision Transformer [4], Swin Transformer [5], ConvNeXt [18] and MAE[5].",2,positive
"It was competitive compared with Transformer [4, 5] in terms of accuracy and scalability and achieved 87.8% top-1 accuracy on ImageNet.",2,positive
"It was competitive compared with Transformer [4, 5] in terms of accuracy and scalability and achieved 87.",2,positive
"Co-DETR with Swin-L yields 56.9% and 62.3% AP on LVIS val and minival, surpassing ViTDet with MAE-pretrained [13] ViT-H and GLIPv2 [40] by
+3.5% and +2.5% AP, respectively.",0,negative
"3% AP on LVIS val and minival, surpassing ViTDet with MAE-pretrained [13] ViT-H and GLIPv2 [40] by Method Backbone enc.",0,negative
"Some researchers also turn their attention to masked self-supervised learning [66], [67], [68], [69], [70], [71], which predicts the masked patches from the visible ones.",1,neutral
", 2020), computer vision (He et al., 2022), and learning protein representations (Jumper et al.",1,neutral
"…transformers (Vaswani et al., 2017) has shown major success in several machine learning fields, including language (Devlin et al., 2018; Brown et al., 2020), computer vision (He et al., 2022), and learning protein representations (Jumper et al., 2021; Rao et al., 2021; Baek et al., 2021).",1,neutral
"Since images have heavy spatial redundancy, some degraded pixels can be recovered from contextual information of neighbor pixels [25].",1,neutral
"However, our network’s encoder and decoder are asymmetric, which indicates a significantly smaller decoder [25, 57].",2,positive
"zscdp is fed into a single decoder stage, which is asymmetrically smaller than the encoder [25, 57].",2,positive
"Similarly, while layer-wise lr decay improved the performance of high-level vision tasks when fine-tuning Transformer models [6, 25], our models could not learn the representations well with that regularization.",1,neutral
"Method Better Worse ×3, ×4 training warm-start [40] scratch std in normalization from data [25] 1.",1,neutral
Implementation Details We adopt the model architecture of Masked AutoEncoder (He et al. 2022) with BERT (Devlin et al. 2018) word embeddings and language modeling head.,2,positive
Implementation Details We adopt the model architecture of Masked AutoEncoder (He et al. 2022) with BERT (Devlin et al.,2,positive
"For instance, MAE [20] claims that having an image reconstruction pretraining stage using masked inputs can produce an effective image encoder for image classification tasks.",1,neutral
"Additionally, the most recent advancement [20,50] in the field of representation learning also indicates that autoencoding is a meaningful step for learning visual features.",1,neutral
"For test-time training we do not use any augmentation, instead we construct a batch from the single point cloud sample and for Masked Autoencoder reconstruction, we randomly mask 90% of the tokens.",2,positive
"Recently, He et al. [10] revisited autoencoders and proposed Masked Autoencoders (MAE) for self-supervised representation learning in the image domain.",1,neutral
[10] revisited autoencoders and proposed Masked Autoencoders (MAE) for self-supervised representation learning in the image domain.,1,neutral
"A concurrent work, TTT-MAE [6] substitutes the self-supervised objective with Masked Autoencoder [10] reconstruction task for TTT in the image domain.",1,neutral
"Source Tasks There is rich literature [8, 27, 28, 30] to use ImageNet1K to pretrain vision backbones for various downstream vision tasks (object detection [24], semantic segmentation [29]).",2,positive
"Recent transformer-based models can generate human-like texts (Brown et al. 2020), autocomplete codes (Chen et al. 2021), reconstruct images (He et al. 2022), or compose music (Dhariwal et al. 2020).",1,neutral
"2021), reconstruct images (He et al. 2022), or compose music (Dhariwal et al.",1,neutral
"Masked pretraining is a preeminent technique in image/text representation learning [3, 6, 10, 14, 61] and visual-language modeling [32,51,63].",1,neutral
"Recently, a lot of progress has been made towards representation learning with large-scale unsupervised data [Chen et al., 2020; Grill et al., 2020; He et al., 2022; Zbontar et al., 2021].",2,positive
"We test this approach on a diverse set of chromosome aberrations (an intra-chromosomal unbalanced abnormality: del(5q); intra-chromosomal balanced rearrangements: inv(3) and inv(16), and inter-chromosomal translocations: t(9;22), t(9;11), and t(11:19)) commonly seen in",2,positive
"In particular, del(5q) and t(9;22) returned perfect accuracy, while two of the other abnormalities (inv(3) and t(11;19)) showed 100% precision with >90% recall.",0,negative
"Moreover, when aggregating across multiple cells from the same specimen, mimicking the clinical practice in a diagnostic cytogenetics laboratory in that an abnormality is defined as clonal in nature when seen in at least 2 cells, the methods achieved perfect accuracy in multiple aberrations even in low-data regimes with only 39 abnormal examples of t(11;19) (Figure 4C).",0,negative
"(C) Precision-recall curves for t(9;11), t(11;19), del(5q), and t(9;22), at the individual chromosome image level (orange) or aggregated at the cell `(purple) or specimen levels.",1,neutral
"For example, the normal chromosome 9s from the held out pre-training folds were added to the t(9;11) aberration training set for normal vs aberrant chr9 identification and likewise for the remaining aberration datasets.",0,negative
"Similarly, (D) shows precision-recall for de novo aberration detection based on distance to N-nearest point (here 50th) for t(9;11), t(11;19), del(5q), and t(9;22), respectively.",1,neutral
"The great successes of transformer models [41] in other domains such as NLP [11, 33, 3] and computer vision [13, 19] motivates our work.",1,neutral
"Work in both NLP [11] and vision [5, 19] have explored how masked prediction is useful as a self-supervision task.",1,neutral
"MAE (He et al. 2021) eliminates the dVAE pre-training process by reconstructing pixels, in contrast to predicting tokens.",1,neutral
"In this section, we first briefly introduce Masked Image Modeling (MIM) for image representation learning and then extend it to our video representation learning scheme, Masked Video Modeling (MVM) (§3.1 and §3.2).",2,positive
"They find that spatiotemporal inductive bias in video clips helps a decoder predict input pixels in masked regions, allowing a higher masking ratio (∼ 90%) than MIM (∼ 60% [53] and ∼ 75% [24]) on image self-supervised learning.",1,neutral
"In vision tasks, Masked Image Modeling [24, 53] aims to learn representations of the input images by solving the regression problem in which a model predicts RGB pixel values in randomly masked patch regions of images.",1,neutral
"Masked video modeling Inspired by self-supervised learning with Masked Image Modeling [24, 53, 26], several recent works on video representation learning [57, 49] suggest spatiotemporal masking strategies given video streams.",1,neutral
"The initial condition follows a uniform distribution centered at [0, 0, 25] with width [36, 48, 41] respectively.",1,neutral
"To resolve this problem, we involve masked autoencoder [36], which applies 50% random masking to the input data.",1,neutral
"For example, one may consider generalization to BYOL (Grill et al., 2020) or SimSiam (Chen & He, 2021) that ensembles projection and/or prediction heads, or MAE (He et al., 2022) that ensembles the decoders (which introduces more training cost though).",2,positive
", 2020) or SimSiam (Chen & He, 2021) that ensembles projection and/or prediction heads, or MAE (He et al., 2022) that ensembles the decoders (which introduces more training cost though).",2,positive
"These successes have encouraged increasingly advanced SSL techniques
(e.g., Grill et al., 2020; Zbontar et al., 2021; He et al., 2022).",2,positive
"These successes have encouraged increasingly advanced SSL techniques (e.g., Grill et al., 2020; Zbontar et al., 2021; He et al., 2022).",2,positive
"Recent generative approaches that use masked image modeling as the pretraining task (Dosovitskiy et al., 2020; Bao et al., 2021; He et al., 2022; Zhou et al., 2022; Xie et al., 2022) have achieved competitive finetuning performance.",1,neutral
"Exploring spatiotemporal information by jointly using RNNs and convolutional neural networks (CNNs) [55, 8, 10] or using graph convolution networks [24, 40] also delivered decent performance.",1,neutral
"Nevertheless, to compare to other pre-training strategies, we consider MAE [29] pre-trained on ImageNet [63], thus with a cosine positional embedding, a ViT-Base encoder, and with a Small decoder that is randomly initialized.",2,positive
"In fact, pairs with high overlap make the task trivial, whereas pairs with negligible overlap reduce it to standard MIM [84].",1,neutral
"They have obtained some success on denser tasks such as object detection [29] or human pose estimation [91], and have been applied to robotic vision [59] when pre-trained on related datasets.",1,neutral
"We observe that CroCo pre-training obtains the lowest errors, significantly outperforming the MAE pre-training and the random initialization.",2,positive
"C V
] 1
8 A
ug 2
stance discrimination and MIM methods have achieved excellent performance on semantic tasks such as image classification, in particular with limited amounts of annotated data [2, 17, 71], but have not led to breakthroughs in more geometric tasks like stereo matching and optical flow.",1,neutral
"More recently, CroCo [84] introduces the pretext task of cross-view completion, where a second view of the same scene is added to MIM.",1,neutral
CroCo outperforms MIM pre-training on an array of geometric tasks.,2,positive
"Note that these methods, as MAE [29], regress pixel values that are normalized according to the mean and standard deviation inside each patch, we thus apply the inverse transform for
display: this means that the overall color of each patch will be correct, as it comes from the ground-truth values.",1,neutral
It extends MIM to pairs of images.,1,neutral
"Recently, [84] proposed the pretext task of cross-view completion (CroCo), a variant of MIM where a partially masked input image is reconstructed given visible patches and an additional view of the same scene.",1,neutral
"Overall, MIM models perform well on classification tasks.",2,positive
"Another recently successful pretext task is masked image modeling (MIM) [2, 22, 29, 83, 86, 102], where part of the input data is masked and an auto-encoder is trained to restore the full signal from the remaining visible parts.",1,neutral
"This is higher than the 75% masking ratio of MAE [29], as the unmasked reference view of the same scene adds redundancy.",0,negative
"Following MAE [29], CroCo [84] uses a small decoder of 8 blocks consisting of self-attention, cross-attention and an MLP, with 512 dimensions and 16 attention heads.",1,neutral
"MIM pre-training aims at reconstructing masked information from an input image either in the pixel space [3, 4, 15, 22, 29, 86], or in the feature space [2, 5, 83], and sometimes after quantization [7, 102].",1,neutral
The masking implementation follows [19]:,1,neutral
"Different from [19], we reshape xmask into a masked images as input xinput ∈ RH×W×C .",1,neutral
"Current vision Transformer based methods exploit the representation learning ability of MIM by predicting the clustering of colors [4], mean color [14], the color of raw pixels [19, 36] and patch tokens [1].",1,neutral
"MIM [19, 28] task is a recent promising self-supervised learning method that reconstructs the masked patches of the image.",1,neutral
"Specifically, apart from the original optimization target, we exploit the Masked Image Modeling (MIM) [1, 19] task to mask and recover random patches during the training.",2,positive
"Although we did not achieve the best performance on OSCC and temporal localization tasks in Ego4d Challenge 2022, we believe that, by paying much more
attention to downstream task formulation and optimization, models that are pretrained on egocentric datasets under the settings of VideoMAE will further improve state-of-the-art performance on various Ego4d tasks.",2,positive
"In this report, we demonstrate that merely pretraining the model on 3rd-person view datasets (e.g. Kinetics 400[7]) under the settings of VideoMAE can achieve state-of-the-art performance on egocentric video understanding tasks including Ego4d object state change classification and Ego4d PNR temporal localization.",2,positive
"We will show that even with weights obtained on 3rd-person view datasets, VideoMAE shows great generalization ability on egocentric downstream tasks and surpass most existing methods both on OSCC and temporal localization tasks.",2,positive
"MAE[5], proposed by Kaiming, etc, has now drawn the wide interest of researchers in self-supervised learning due to its pretraining efficiency and generalization ability in various downstream tasks.",2,positive
This demonstrates the great representation learning and generalization ability of VideoMAE in self-supervised video pretraining.,1,neutral
"As shown in Table 1 and Table 2, by simply pretraining on Kinetics 400 under the settings of VideoMAE, we ranked 2nd place in both tasks.",0,negative
"Recently, two parallel works[9][3] called VideoMAE are proposed to extend MAE from image to video domain.",2,positive
"In our experiments, we use ViT as our backbone of which weights are initialized from VideoMAE[9] pretrained on Kinetics-400.",2,positive
"Notably, MIM [5, 30] demonstrates great dense localization ability, while SiameseIM [67] can exhibit semantic alignment and spatial sensitivity at the same time.",1,neutral
MIM has also been proven to work well with large-scale networks [30].,1,neutral
"In recent years, large-scale pre-trained models [5,13,27, 30, 37, 55, 65, 89] have swept a variety of computer vision",1,neutral
"In order to make this mix strategy compatible with existing pretraining tasks like Masked Image Modeling (MIM) and Image Classification (IC), we split the mask m into patches with p× p size.",2,positive
"Some SSP methods have displayed great potential by surpassing SP on downstream tasks by a large margin [13,30,31].",1,neutral
"The comparison for intra-view pre-training is consistent with previous studies [30], implying that masking operation can greatly boost the model’s performance.",0,negative
Note that the widely used Mixup [94] and CutMix [90] are generally incompatible with MIM.,1,neutral
"Some SSP methods can already surpass SP on downstream tasks [5, 30, 31].",1,neutral
"Restrictions apply.
auto-encoder [34, 76], global/dense distillation [33, 81] and masked image modeling (MIM) [4, 5, 14, 30, 87].",1,neutral
", LAION-400M [56]), and self-supervised learning [5, 13, 27, 30, 89] on unlabeled images.",1,neutral
"Self-supervised Pre-training (intra-view) : Auto-Encoder view1 view1 dense feature dense pixels Gaussian (1)Dense Distillation FD [80],BEiT v2 tokenizer [54] view1 view1 dense feature dense feature stop gradient Gaussian Global Distillation view1 view1 dense feature global feature stop gradient Boltzmann Masked Image Modelingpixel MAE [30] masked view1 view1 dense feature dense pixels Gaussian",2,positive
"For example, p = 16 is by default used for MIM [5,30].",1,neutral
"create input and target from the same view, which includes auto-encoder [34, 76], global/dense distillation [33, 80] and masked image modeling (MIM) [4, 5, 14, 30, 85].",1,neutral
"Given that the supervision on masked patches is a regularization for the learning of CAE v2, we suppose that it may not be appropriate to adopt a high mask ratio (75% in MAE [27], 40%-50% in BEiT [3], CAE [10], and MVP [49]) for all scales of ViTs.",1,neutral
"Existing MIM methods explore different supervision targets on their frameworks, including RGB pixels [24, 27], HOG descriptors [48], discrete visual tokens [3,10,18,22,40], and feature representation from momentum models [13,44,51].",2,positive
"Unlike most MIM methods [3, 10, 27, 53] applying the reconstruction supervision on the masked patches, MVP supervises both masked and unmasked patches.",1,neutral
"Most previous MIM methods [3, 10, 27] apply the reconstruction supervision on the predictions of masked patches.",1,neutral
"The encoder F only receives the visible patches Xv following [10, 27].",1,neutral
"Our findings are different from the common sense in the current MIM methods [3,10,27] that only compute the loss on the masked patches, which is inherited from BERT [15] in the NLP areal and has been verified by most current works.",2,positive
"Specifically, in most MIM methods [3, 10, 27, 53], the supervision positions are only associated with the masked patches, i.",1,neutral
"It raises a question: how will the supervision position influence the CLIPtargeted MIM? On the other hand, the mask ratio performs differently for different supervision targets [3, 27].",1,neutral
"Following [10, 27], the encoder F maps the visible patches Xv to the latent representations Zv .",1,neutral
"For example, MAE [27] utilizes a mask ratio of 75%, BEiT [3], CAE [10], and MVP [49] empirically set the mask ratio as 40% and 50%.",1,neutral
Recall that MAE [27] points out a high mask ratio (75,1,neutral
"Following previous MIM methods [3, 10, 27, 48], CAE v2 first embeds x into a total length of N patches, which are then randomly masked by a specific proportion γ.",1,neutral
"With linear probing, CAE v2 shows significant improvements over previous methods with other targets, e.g., BEiT [3], MAE [27], CAE [10], and MaskFeat [48].",1,neutral
"The reconstruction loss of MIM can be applied in different domains or targets, such as RGB [27, 53], HOG [48], discrete visual tokens [3, 10, 18, 22, 40], momentum encoders [13,44,51], and pretrained models [48,49].",1,neutral
Recall that MAE [27] points out a high mask ratio (75%) is good for the balance of efficiency and effectiveness.,1,neutral
55× Table 10: Self-supervised learning results with MAE [22].,1,neutral
Pre-training Top-1 Accuracy (fine-tuning) Pre-training Speedup Epochs Baseline EfficientTrain Computation Wall-time MAE (ViT-B) [22] 86M 1600 83.,0,negative
Results with Masked Autoencoders (MAE).,0,negative
"Importantly, our method is also effective for self-supervised learning (e.g., MAE [22]).",1,neutral
"As a representative example, we deploy EfficientTrain on top of MAE [22] in Table 10.",2,positive
"[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Following [22], the models are pre-trained on ImageNet-1K w/ or w/o EfficientTrain, and evaluated by end-to-end fine-tuning.",2,positive
"Additionally, to test transferability, we use the adversarially generated samples using PRIME-ResNet50 and FANVIT as classifiers, and test them on other architectures: (1) ResNet50 [17], (2) MAE [16], a generalizable and scalable asymmetric encoder-decoder architecture, (3) RegNet-
Y [14], a ResNet-type model with a regulatory model to extract complementary features, and (4) data2vec [6], a selfsupervised transformer that predicts contextualized latent representations in a self-distillation setup for any modality.",2,positive
"Additionally, to test transferability, we use the adversarially generated samples using PRIME-ResNet50 and FANVIT as classifiers, and test them on other architectures: (1) ResNet50 [17], (2) MAE [16], a generalizable and scalable asymmetric encoder-decoder architecture, (3) RegNet-",2,positive
"…we notice that there are various open-sourced image ViTs (Wightman, 2019; Touvron et al., 2021), which have been well-pretrained on huge web datasets under rich supervision such as image-text contrastive learning (Radford et al., 2021) and mask image modeling (He et al., 2022; Bao et al., 2021).",2,positive
"…of our UniFormerV2 design, we apply it on the ViTs with different pertaining methods, including supervised learning (Dosovitskiy et al., 2021; Touvron et al., 2022), contrastive learning(Caron et al., 2021; Radford et al., 2021) and mask image modeling (He et al., 2022; Bao et al., 2021).",2,positive
"The availability of large weakly labeled web data combined with self-supervision methods [5, 10, 13, 18, 19] has made it easier to train such models.",1,neutral
MAE [4] has been proved to be a strong competitor of pre-training methods widely used in computer vision.,1,neutral
"1, existing MIM methods are mainly divided into two types: (a) inpaintingstyle [3, 43, 46] and (b) decoder-style [5, 11, 13].",1,neutral
"We only use standard random cropping and horizontal flipping for data augmentation, following MAE [13], CAE [5], etc.",2,positive
"For MAE [13] and MILAN [17], the encoder and decoder respectively process 25% and 100% patches.",2,positive
"Decoder-style models like MAE [13], CAE [5] and MCMAE [11] only take the partial image as the input.",1,neutral
"(MIM) has demonstrated a great ability of self-supervised learning [3, 13], while alleviating the data-hungry issue of Transformer architectures.",1,neutral
"masked modeling methods adopt the CLIP feature as the reconstruction target [17, 33, 44], outperforming counterparts using low-level features [6, 13].",1,neutral
"To restrain the feature magnitudes of teacher features, we generate the alignment target ỹ by normalizing each level of teacher features as MAE [13] does on pixel values:",1,neutral
"To generate a mask view V from an intact image I, one straightforward sampling strategy is random masking, which samples patches without replacement, following a uniform distribution [13].",1,neutral
"(b) Decoderstyle: MAE [13], CAE [5], MCMAE [11], etc.",1,neutral
MAE [13] and SimMIM [46] find that RGB values can act as a simple yet good enough reconstruction target for masked modeling.,1,neutral
"more than 75% of the image [88], thus stimulating the",1,neutral
Table 11: Validation accuracy of linear probing of MAE [21] on the ImageNet validation set.,1,neutral
"Self-supervised Learning
We pre-trained MAE following the official implementation9 on eight NVIDIA A-100 GPUs and fine-tuned the last layer of its encoder on 16 NVIDIA V-100 GPUs.",2,positive
"We pretrain a Vision Transformer model, specifically ViT-B [13],
as MAE’s encoder for 200 epochs with a mask ratio of 0.75.",2,positive
"To verify this idea, we adopt a self-supervised learning method called masked autoencoder (MAE) [21].",1,neutral
"Following MAE, we adopt the ViT as the backbone of the decoder g(·).",2,positive
3 improvement on AP bbox than MAE [20] (53.,0,negative
"In addition, α ≥ λ means that the pixel prediction task contributes more than reconstruction, i.e., prediction can provide more information, which is also demonstrated in MAE [20] and SimMIM [44].",1,neutral
"In the objective detection, MR SimCLR achieves the best results with 1.3 improvement on AP bbox than MAE [20] (53.7 vs. 52.4).",0,negative
"However, due to the low information density of image data [20, 44], there is still much noisy and redundant information after their proposed augmentation, which would affect the extraction of desired information and thus limit the performance of CL in practice.",2,positive
", prediction can provide more information, which is also demonstrated in MAE [20] and SimMIM [44].",1,neutral
"Especially this masking operation has been verified to be effective in Masked Image Modeling (MIM) and Masked Language Modeling (MLM) [6, 15, 20, 44].",1,neutral
"Recently, masked autoencoder (MAE) [20] and SimMIM [44] developed the MIM methods [4, 40, 50], using vision transformers (ViT) [16] backbone to narrow the data distinction between computer vision and natural language.",1,neutral
"In the objective segmentation, MR SimCLR significantly improves APmask over MAE by 0.4 points (46.9 vs. 46.5).",0,negative
MAE pre-trained the ViT model through mask and reconstruction tasks [17].,2,positive
"Latest works [322], [323] have demonstrated the capability of deep learning models to form learned priors and experiences from massive data.",2,positive
In this manuscript we use a ViT-B8 with the modifications proposed by He et al. (2022) where the classifier is applied after global average pooling over the vision tokens.,2,positive
"Among these methods, Masked Signal Modeling (MSM) has achieved promising results in both vision [18,62] and language understanding [8, 37] recently.",1,neutral
"Following [18], we divide the vectorized voxels into patches which will be subsequently transformed into embeddings using a 1D convolutional layer with a stride equal to the patch size.",2,positive
"We also adopt an asymmetric architecture as in [18]: the encoder is optimized to learn effective fMRI representations, while the decoder tries to predict the masked patches.",2,positive
"The portion of data to mask is different across data modalities, with an extremely high mask ratio (75%) usually used for visual signals [18].",1,neutral
"Different from [18], we use a much larger representation-to-data-space ratio to boost the information capacity of learned representations.",1,neutral
"Commonly, the encoder’s output is averaged, or a cls token is appended to produce a pooled 1D feature vector for downstream tasks [8,18].",1,neutral
"Masked Image Modeling (MIM) uses the embedding-topatch-size ratio around one [18], leading to a representation size similar to the original data size.",1,neutral
"Masked Signal Modeling The power of MSM in learning representations from a large-scale dataset was first exploited in [8], which was later adapted to computer vision [18,60,62].",1,neutral
"Recent work [4,21,30,32,38, 100, 110, 121] (re-)explore pixel / feature regression in MIM, but only in a relatively small model and data scales.",1,neutral
"Recently, masked image modeling (MIM) [5, 38, 110] has boomed as a viable approach for vision model pretraining and scaling.",1,neutral
"Following the evaluation procedure in [38], all these models are first fine-tuned on the original ImageNet1K training set and then evaluated on different validation sets using the same fine-tuned model without further hyperparameter selection and specialized fine-tuning.",0,negative
"The detailed model configurations are (arch-model size-img resolution-data): ConvNeXt-XL-384px21K [62], SwinV2-L-384px-21K [61], MAE-H-448px-1K [38], DeiT3-L384px-21K [93], EfficientNet-L2&NS-800px-JFT300M [107], BEiTv2-L224px-21K [66], BEiT-L-512px-21K [5], EVA-g-336px-merged30M&21k.",2,positive
"Self-supervised learning (SSL) has achieved remarkable success in the field of representation learning, applied in computer vision [1, 2], natural language processing [3, 4], as well as speech processing [5, 6].",1,neutral
"However, in general MAE is performing worse than other SSL methods on linear probing (see Table III), which has also been reported in recent works [41, 42].",1,neutral
"In this work, we benchmark four representative methods MoCo, DINO, MAE, and data2vec on the proposed dataset.",2,positive
"For MAE and data2vec, one random season is assigned for each patch in every epoch.",2,positive
"We find 70% to be the best masking ratio, which is similar to natural images as reported in MAE paper, where 75% is the best.",2,positive
"This way, we cover a reasonably diverse set of representative methods from each model category: MoCo utilizes contrastive representative, DINO represents a distillation method, MAE is based on masked reconstruction, and data2Vec combines the masking mechanism with a joint-embedding architecture.",2,positive
We pre-train the MAE models using its default settings following the publicly available repository (https: //github.com/facebookresearch/mae).,2,positive
"These methods reconstruct the masked parts of an input either at pixel-level [16], feature-level [17], or exploit visual tokens [22].",1,neutral
"Specifically, we evaluate four representative self-supervised learning algorithms—namely: MoCo [14], DINO [15], MAE [16], and data2vec [17]—on three different downstream tasks: scene classification, semantic segmentation and change detection.",2,positive
• MAE [16] is a masked autoencoding design by reconstructing missing patches in images.,1,neutral
3) Masking ratios of MAE: Table XIX shows the influence of masking ratios in MAE during pre-training.,0,negative
MAE.,0,negative
"For EO applications we demonstrate SSL4EOS12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec.",2,positive
"According to previous studies [8, 10], the key elements hidden in patches are different semantic information they contain, i.",1,neutral
"first extend the masked autoencodeing approach to the pre-training of Vision Transformer (ViT) model, which has gained great success on both model pre-training and inference [10].",2,positive
"The existing model pre-training of MAE encoder is based on a random mask mechanism [10], which only randomly samples a patch subset to pre-train ViT model.",2,positive
"data themselves via carefully designed pretext tasks, such as jigsaw puzzles [29, 42], contrastive learning [5, 15, 41], and masked modeling [14, 62, 30].",1,neutral
"Inspired by this, masked image modeling [3, 14] and masked video modeling [46, 11] are actively investigated and present considerable successes.",1,neutral
"Among these pretext tasks, the masked modeling has validated its effectiveness on various data modalities [14, 30, 46], including the point cloud [50, 62, 30].",1,neutral
"Over the past decade, by leveraging AI techniques such as convolutional neural network [1], deep learning [2], [3], residual [4] and dense [5] block, knowledge distillation [6], network architecture searching [7], [8], attention mechanism [9], [10], selfsupervised learning [11], [12], etc.",1,neutral
"representations [48]- [49], confirming vanilla ViT’s promising potential and capacity in object-level recognition.",1,neutral
"The encoder is adapted from MAE [20], which is a vision transformer (ViT) [18] without prediction head (norm layer is included).",2,positive
Masked AutoEncoder (MAE) [20] is one of the most influential works in masked image modelling for its simple design and excellent efficiency.,1,neutral
"The weight decay is set to zero, following the setups in MAE [20].",1,neutral
(a) Example of Masked Image Modelling [20] (b) Example of Contrastive Learning [21],1,neutral
MAE [20] (see Figure 1a) is another very influential work in masked modelling.,1,neutral
"Discriminative and generative self-supervised methods received growing attentions in recent years (Chen et al. 2020a,b; Donahue and Simonyan 2019; Gidaris, Singh, and Komodakis 2018; Jaiswal et al. 2020; Zhang, Isola, and Efros 2016; He et al. 2022).",1,neutral
", masked autoencoders [115]) to learn general feature representations for matching.",1,neutral
"This inspired many researchers to study Transformers as direct competitors to CNNs in different settings [10, 39, 40] and across different vision tasks [12, 26].",1,neutral
"MAE splits an image into several patches, randomly masks a percentage of them, and learns to reconstruct the masked ones.",1,neutral
"In the first stage, we pretrain the model by borrowing the idea from masked autoencoders (MAE) [11].",2,positive
Our encoder uses the same settings as ViT-B in MAE [11].,2,positive
"In the pretraining stage, we apply RandomResizedCrop to augment data, which is similar to MAE.",2,positive
Masked autoencoders (MAE).,1,neutral
MAE [11] randomly divides N image patches into Nu unmasked ones and Nm masked ones.,1,neutral
The Mean Squared Error (MSE) loss is used to optimize MAE model.,1,neutral
"To better use the pretrained knowledge, different from MAE or ViTSTR [2], which only fine-tune on the pretrained encoder, we fine-tune on both the encoder and the decoder.",2,positive
"To this end, we adopt a two-stage training strategy to train the model as follows:
In the first stage, we pretrain the model by borrowing the idea from masked autoencoders (MAE) [11].",2,positive
"However, recognizing texts is beyond the scope of MAE, and we propose a novel language-aware model to deal with it, as shown in Figure 1.",2,positive
"Different from MAE, our MVLT recognizes scene text in addition to reconstructing the masked patches.",0,negative
"The encoder of MAE is a ViT, which only operates on xu to learn the visual feature embeddings:
vu = encoder(xu), (1)
where vu ∈ RNu×D1 and D1 is the feature dimension in the encoder.",2,positive
"Like MAE, the reconstruction of image patches helps our model to learn an effective visual representation.",2,positive
MAE [27] tries different masking methods to train the autoencoder which can be adopted to serve as the pre-training model.,2,positive
", locality and translation equivariance), the pure ViTs are more over-parameterized and rely on large-scale pre-training [6, 31, 40, 64] to a great extent.",1,neutral
"When finetuning the detector on COCO, we find that applying learning rate decay [1,4,5,9] for the components of the detector (encoder and decoder) gives a ∼0.",1,neutral
"With encoder pretraining on large-scale data [7,19], the models [1,4,9,18,22,24]are able",2,positive
This MAE-liked [13] strategy has two benefits.,1,neutral
"works have sought to obviate this problem through the use of MixUp [72], masking [6, 34], and k-NN [24, 42, 71], the latter of which is directly relevant to our work.",1,neutral
"To produce informative self-supervision signals, the design of handcrafted pretext tasks has flourished for a long time, including jigsaw puzzle completion [47], relative position prediction [15, 16], rotation perception [21], inpainting [48], colorization [41, 67], masked image modeling [27, 60], etc.",1,neutral
"It is simple and intuitive but sub-optimal, and tends to miss opportunities to learn task-specific features [12].",1,neutral
"Among them, contrastive learning [15, 20, 9, 5, 18] and masked image modeling [8, 1, 19, 56] have been particularly successful.",1,neutral
"Then, REGCLR is instantiated by integrating masked autoencoders [12] as a representative example of a contrastive method and enhanced Barlow Twins as a representative example of a regularized method with configurable input image augmentations in both branches.",1,neutral
MAE branch uses weak augmentation and masking to obtain X1 and then follows procedure introduced in MAE [12] to compute reconstructed loss for unmasked patches as LMAE to obtain X ′ 1.,1,neutral
", MAE [12]), and even Generative Adversarial Networks.",1,neutral
"Using the ViT architecture, MAE [12], in particular, generalizes masked language modeling (MLM) popular in natural language processing to MIM in computer vision.",1,neutral
"Our implementation of pretraining is built on the repo provided in MAE [12], and the projector is implemented as a 2 layer multilayer perceptron with 1024 dimensional output.",2,positive
"Many self-supervised learning (SSL) methods for learning visual representation without supervision (or labels) have been proposed in recent years [28, 19, 9, 6, 10, 27, 12].",1,neutral
"This simple yet highly-scalable strategy of masked-based unsupervised pre-training has yielded promising transfer learning results on visionbased downstream tasks such as object detection and segmentation, image classification, and action detection, even outperforming supervised pre-training [16, 24].",1,neutral
"The success of MLM-based techniques has similarly inspired recent work re-examining the classical formulation of Denoising Autoencoders (DAEs) [51], but for ViTs [3, 13, 28], introducing tasks such as Masked Image Encoding [16] and Masked Feature Prediction [24] for image and video modelling, respectively.",1,neutral
"The observed difference in convergence speed between RGMIM and MAE can be attributed to the fact that RGMIM incorporates the spatial information of lung X-ray im-
ages through the region-guided masking strategy, which helps it to focus on the most informative regions of the images during training.",2,positive
"In line with this objective, MAE [10] conducts experiments involving end-to-end training of a masked autoencoder.",2,positive
Table 3 reveals the lung disease detection accuracy of RGMIM and MAE when employing different masking ratios.,1,neutral
"The proposed method was contrasted with five self-supervised learning techniques (MAE, SKD, Cross, BYOL, and, SimSiam).",2,positive
"For RGMIM and MAE, we employed the same settings in all experiments, except for the masking strategy.",2,positive
"In their proposed method, MAE directly predicts masked patches from the unmasked ones, employing a simple mean squared error (MSE) loss.",1,neutral
The results indicate that RGMIM exhibits superior accuracy and faster convergence speed compared to MAE.,0,negative
"Specifically, after only ten epochs of learning, the detection accuracy of RGMIM has already begun to converge, while MAE is still in the process of convergence.",2,positive
We also studied the masking ratio for RGMIM and MAE using hyperparameters.,1,neutral
"Concurrently, a similar architecture called Simple Masked Image Modeling (SimMIM) is introduced in [11], which corroborates the findings of MAE.",1,neutral
"As comparison methods, we used five SOTA selfsupervised learning methods as comparative methods, including masked autoencoder (MAE) [10], self-knowledge distillation based self-supervised learning (SKD) [47], cross-view self-supervised learning (Cross) [48], bootstrap your own latent (BYOL) [20], and simple siamese self-supervised learning (SimSiam) [21].",2,positive
Traditional MIM techniques frequently employ a random masking strategy for ordinary images [10].,1,neutral
"Specifically, SimMIM demonstrates that directly predicting the pixels, as done in MAE, performs no worse than other methods with more complex designs involving tokenization, clustering, or discretization.",1,neutral
"As comparison methods, we used five SOTA selfsupervised learning methods as comparative methods, including masked autoencoder (MAE) [10], self-knowledge distillation based self-supervised learning (SKD) [47], cross-view",2,positive
"In addition, we observe that RGMIM outperforms MAE in terms of robustness, especially when the masking ratio is relatively low, demonstrating the superiority of our proposed method in handling incomplete lung X-ray images.",2,positive
"Method Architecture 1% 5% 10% 50% 100% RGMIM ViT-Base 0.771 0.893 0.919 0.957 0.962 MAE [10] ViT-Base 0.754 0.875 0.903 0.948 0.956 SKD [47] ResNet-50 0.742 0.812 0.896 0.947 0.957 Cross [48] ResNet-50 0.747 0.795 0.817 0.934 0.953 BYOL [20] ResNet-50 0.683 0.754 0.790 0.933 0.954 SimSiam [21] ResNet-50 0.623 0.700 0.781 0.929 0.949 Transfer ViT-Base 0.689 0.861 0.893 0.940 0.953 Transfer ResNet-50 0.539 0.619 0.665 0.913 0.936 From Scratch ViT-Base 0.413 0.580 0.645 0.810 0.848 From Scratch ResNet-50 0.284 0.496 0.532 0.619 0.774
has layer L = 8, latent vector size D = 512, and the number of heads is 16.",0,negative
"Although existing Transformer models like ViT [32] and MAE [10] are usually trained on the large-scale dataset, We trained the ST-MAE model on the limited samples from scratch by an AdamW optimizer with a learning rate of 1e-4 and batch size of 8 for 400 training epochs, while the weights",2,positive
"Note that our ST-MAE is a feature vision Transformer that operates on the deep features of DCNN, which is slightly different from the base ViT [10], its consecutive computational process is roughly demonstrated.",2,positive
"like the MAE [10] and Intr [36], we adopted the feature-level measurement, the relaxed version of the pixel-level constraints, for better robustness.",2,positive
"Like MAE [10], each encoder in our approach maps the partially observed signal to the latent representation, which has been approved to be effective to learn object semantics",2,positive
"The Masked Autoencoder(MAE) proposed in [10] shows that the ViT can learn meaningful visual representations from the small proportion of visible patches subset, which yields promising performance in the downstream tasks.",2,positive
"1, in analogy to MAE [10], our ST-MAE has an asymmetric encoder-decoder design that reconstructs the input in feature space, yet our encoder applies a Siamese architecture.",2,positive
"In our proposed method, MobileNetV3([16]) is adopted as the backbone network.",2,positive
"In our proposed method, MobileNetV3[16] is adopted as the backbone network.",2,positive
"[18] exploited such a property to propose a self-learning framework called Masked AutoEncoder (MAE), by which masking 15% of the patches would still maintain the stateof-the-art accuracy.",2,positive
"[17], [18], to derive a patch shuffling scheme to protect the training data privacy.",1,neutral
"1) Black-Box Attack: For the attack model, we adopt a similar structure to the edge model: an MAE decoder [18] is used, and is pretrained on ImageNet.",2,positive
"Self-supervised learning (SSL) has emerged as a promising pre-training method due to its remarkable progress on various computer vision tasks [24, 9, 21, 6, 23, 59].",1,neutral
"We also report results for our MAE implementation, which approximately matches the original numbers reported by He et al. (2022), validating our MAE results on JFT-300M.",0,negative
"First, masked image models such as the masked autoencoder (MAE) (He et al., 2022) are a nascent set of methods based on a mask-and-reconstruct training mechanism.",1,neutral
"Notably MAE (He et al., 2022) showed that classical masked autoencoding approaches could be used to pre-train ViTs without passing masked tokens through the encoder.",1,neutral
For linear probing we list the hyperparameters in Table 10 for which we followed the settings in He et al. (2022).,1,neutral
"For MAE pre-training, we use the same hyperparameters as listed in He et al. (2022), except for the use of Glorot uniform initialization instead of LeCun initialization as done in He et al. (2022).",2,positive
"Following He et al. (2022), only the T ′ unmasked patches are passed to the ViT encoder, which processes the two views in parallel.",2,positive
"Our approach is a minimal synthesis of contrastive learning, the masked autoencoder (He et al., 2022), and the denoising loss used in the training of diffusion models.",2,positive
"Corroborating the findings of He et al. (2022), we find it best to only compute the reconstruction loss on masked patches:
Lrec = 1
2n ∑ v=1,2 n∑ i=1 ‖Mvi ◦ (xvi − x̂vi )‖22
where ◦ multiplies all pixels in the tth patch of the residual image xvi − x̂vi by (Mvi )t ∈ {0, 1}.",1,neutral
Decoder architecture: Our decoder architecture is the same as He et al. (2022).,2,positive
"BERT (Kenton and Toutanova, 2019) and MAE (He et al., 2022), to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information for classification (Wallat et al.",2,positive
"However, when we apply generative pretrained backbones, e.g. BERT (Kenton and Toutanova, 2019) and MAE (He et al., 2022), to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information for classification (Wallat et al., 2020; Yosinski et al., 2014).",2,positive
"However, when we apply generative pretrained backbones, e.g. BERT (Kenton and Toutanova, 2019) and MAE (He et al., 2022), to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information…",2,positive
"Recently, works like MAE (He et al., 2021) and CAE (Chen et al.",1,neutral
"Recently, works like MAE (He et al., 2021) and CAE (Chen et al., 2022) have learned representations by solving masked reconstruction tasks using vision transformers.",1,neutral
"[18] proposed a masked autoencoder (MAE), which is an asymmetric encoder–decoder structure, to build unknown masked pixels from known pixels, which can also be seen as an extension of BERT [19].",1,neutral
It is worth noting that the decoder was only used in the pre-training phase and can be replaced with any architecture if transferred to downstream tasks [18].,0,negative
Our model-based image augmentation method was implemented based on the official codes of the MAE [18] and ViTPose [21].,2,positive
"To solve the problem of loss and deformation of data acquired by clients, we used a masked image-encoding method that learns image representations corrupted by masking [7, 8].",1,neutral
Random sampling prevented bias in the unmasked area [8].,1,neutral
"Self-supervised learning has shown great success in both NLP [10], [11] and computer vision [12], [13].",1,neutral
"Following [13], we apply an end-to-end fine-tuning mechanism instead of linear probing to obtain the highest performance.",1,neutral
"Instead of random initialization, we initialize the ViT encoder using the ImageNet-1k pre-trained encoder released by [13].",2,positive
"As an alternative, MAE [13] proposes to directly reconstruct pixels of the masked patches, which is a more straightforward method natively designed for image modeling.",2,positive
"Among the various self-supervised vision learning methods, masked autoencoding [12], [13], [59] is used as our pre-training paradigm.",1,neutral
"(ii) MAE-IN1k refers to fine-tuned from the ImageNet-1k [14] pre-trained MAE [13], where we use the same fine-tuning settings as that of MAE-Face.",2,positive
"Among the various techniques proposed for self-supervised visual representation learning, we opt to adopt masked autoencoder (MAE) [13] as our baseline.",2,positive
"Specifically, the pre-training follows a masking-then-reconstruct procedure [13].",1,neutral
"Instead of the L2 loss proposed by [13], the model pre-trained using L1 loss exhibits better performance both in reconstruction and downstream tasks (see Section 4.",1,neutral
"We also test the impact of patch-wise normalization (w/ norm), which has also been studied in [13].",1,neutral
"Similar to the self-supervised pre-training tasks of NLP, some self-supervised tasks based on image transformers have also been proposed recently, such as MAE[18], BEiT[19], etc.",1,neutral
"CONCLUSIONS In this paper, based on MAE[16] and combined with convolution, we propose a transformer image inpainting model to solve the image inpainting problem of irregular missing areas.",2,positive
"In this paper, based on MAE, we propose a transformerbased inpainting model for irregular missing images.",2,positive
"As transformers[15] have made great breakthroughs in the field of NLP, some models with attention[16], [17] have also proved their strong performance in the field of cv. Similar to the self-supervised pre-training tasks of NLP, some self-supervised tasks based on image transformers have also been proposed recently, such as MAE[18], BEiT[19], etc.",1,neutral
"In [12], masked autoencoder was proposed using vision transformer to recover the original images even if some patches are masked.",1,neutral
"In the existing vision transformer [8][12], uniformly partitioned patches are used for an image, as illustrated in Fig.",1,neutral
1) peak signal-tonoise ratio (PSNR); 2) mean absolute error (MAE) [19].,1,neutral
"By utilizing MAE pretraining, ProContEXT outperforms the recent SOTA method, OStrack [5], by 0.9%, 1.5%, and 2.1% for AO, SR0.",2,positive
We implement the proposed ProContEXT using PyTorch and utilize the ViT-base backbone that is pre-trained by MAE [28].,2,positive
"Table 1: Comparison of State-of-the-Art (SOTA) Methods on TrackingNet [20] and GOT-10k [21] Datasets: The evaluated methods include ""DT"" (Dynamic Template) and ""EB"" (Extra Branch to update Dynamic Templates), along with different initialization methods, such as ""RandomNone"" and pre-training with additional datasets, such as ""CLIP-WIT [29]"", ""CLS-ImageNet-1k [30]"", ""CLS-ImageNet-22k [31]"", or ""MAE-ImageNet1k [28]"".",2,positive
Implementation Details We implement the proposed ProContEXT using PyTorch and utilize the ViT-base backbone that is pre-trained by MAE [28].,2,positive
"As mentioned in Section 1, we follow a similar approach to the work put forth in [7], where masked autoencoders reconstructed images with large amounts of masking.",2,positive
"Unlike [7], we don’t remove the masked patches from the input to the encoder and provide positional information to the input of the decoder.",2,positive
"Recently, there has been some major breakthroughs in image reconstruction using masked autoencoders [7], where an image can be realistically reconstructed with 90% of it being masked in patches.",1,neutral
"We denote the former as “speech branch” and the latter as “text branch”, where the former can be viewed as a masked autoencoder [28] and the latter is TTS.",1,neutral
"Recently, BEiT[33] and MAE[34] utilized a BERT-style pre-training as part of the visual learner, and they discovered that models are effective at learning semantics with such a scheme.",1,neutral
"Recently, BEiT [33] and MAE [34] utilize a BERT-style pre-training as part of the visual learner, and they discover that models are effective at learning semantics with such a scheme.",1,neutral
"It is worth noting that both DistilHuBERT and FitHuBERT are investigated under the contrained track on the SUPERB benchmark, which might not be able to reflect the potential effect of the distilled model as it does not fully explore the powerful modeling capacity lied in the Transformer encoder and merely treats it as a frozen feature extractor during the whole fine-tuning stage, missing the opportunity to pursue strong but non-linear features [19].",2,positive
"Specifically, we propose an SSL model, MAEEG, inspired by both BENDR and the MAE model proposed in computer vision [He et al., 2022], to compare with BENDR and to broaden our understanding of reconstruction-based SSL for EEG.",2,positive
"While this result may not be intuitive, the same trend was observed in computer vision where masking more of the signal during SSL pretraining could result in better downstream performance [He et al., 2022].",1,neutral
This study was inspired by MAE [1] for an MIM and Bootstrap Your Own Latent [12] (BYOL) as a framework for directly learning la-,2,positive
"Among them, Masked Autoencoders [1] (MAE) have inspired numerous subsequent studies and influenced not only the image domain [2–5] but also the audio domain [6–9].",1,neutral
"While we use the same positional encoding as MAE [1], we tested various masking ratios as discussed in Section 4.",2,positive
"Similar with MAE [15], the decoder of CAAE is only used in pre-training CAAE to perform image reconstruction task.",1,neutral
"As MAE [15] indicates, the design of the decoder architecture can be flexible and independent of the encoder design.",2,positive
"Unlike in [2], the model is trained to reconstruct the full image as a mixture of individual component reconstructions.",2,positive
"In this work, we capitalize in particular on the masked autoencoding of [2] and investigate using it for object-centric learning.",1,neutral
"Then, exactly as in MAE [2], we add a learnable mask token at the positions of the masked tokens and the sine-cosine position embeddings.",1,neutral
"We therefore choose the random masking strategy, exactly as in MAE [2].",1,neutral
"Another example is the object-reasoning-like results seen in the predictions of masked image-modelling frameworks [2, 3].",1,neutral
"Just as in MAE [2], we add fixed sine-cosine postional encodings from [23] to the embeddings of the patches.",1,neutral
Our model has a narrower bottleneck in comparison to MAE [2].,2,positive
"We hypothesize that the object-reasoning-like behaviour demonstrated to appear in the self-supervision task of masked image modelling [2, 3] can be utilized also for explicit object-centric representation learning.",1,neutral
Our broadcasting module takes inspiration both from the MAE decoder [2] and from the spatial broadcast decoder [25] used in several object-learning models [9–14].,2,positive
Our model is inspired by the transformer-based masked autoencoder [2] with modifications to enable end-to-end unsupervised multi-object segmentation and representation learning.,2,positive
"More specifically, we adapt the Masked Autoencoder (MAE) [2] design and modify it to for explicit object-centric representation learning and segmentation.",2,positive
This is in line with recent findings on training transformers that show the effectiveness of supervising multiple output tokens instead of just a single [CLS] token [73].,1,neutral
"PowerBERT is adopted from BERT model [16, 19, 20] to extract the high-dimensional representations from the massive unlabeled data.",1,neutral
[17] used Vision Transformers [7] for masked image inpainting and reconstruction.,1,neutral
"Denoising autoencoders [15] are a recent deep-learning method demonstrated to reconstruct original images from their corrupted versions across a variety of perturbations, from salt-and-pepper noise [16] to image masking [17].",1,neutral
"Similarly, while ViT has been shown to outperform CNNs on large datasets [17], we observe that ViT outputs contain uneven pixelation when decoding the 16 × 16 patch embeddings, resulting in its low SSIM score.",2,positive
"While [55, 8, 23, 5, 20, 10] have adopted CNN as pretraining backbones, recent works [6, 50, 11, 22, 29, 28] have explored Transformers [46] for self-supervised visual learning, demonstrating their superiority over traditional CNN.",1,neutral
"Despite no labels, models trained with self-supervision have outperformed their supervised counterparts on several downstream tasks, such as image classification [19, 55, 8, 23, 9, 5, 20, 10, 6, 50, 11, 22] and object detection [53, 7].",0,negative
"For MAE He et al. (2022), a method based on a reconstruction objective, we select an attention-based ViT encoder.",2,positive
"0 0.5 1
−0.6
−0.4
−0.2
0
0 0.5 1 0 0.5 1
0 0.5 1
−0.6
−0.4
−0.2
0 0.5 1
CLIPPretrained MAEPretrained MLPMixerPretrained1k MLPMixerPretrained21k ResNet50Pretrained1k ResNet50Pretrained21k SimCLRPretrained ViTPretrained1k ViTPretrained21k iBotPretrained1k iBotPretrained21k
Generalization when Lighting varies in training
percent of diverse Lighting training samples percent of diverse Lighting training samples percent of diverse Lighting training samples
percent of diverse Lighting training samples percent of diverse Lighting training samples
t o p - 1
a c c u r a c y d
r o p
t o p - 1
a c c u r a c y d
r o p",0,negative
"While all other models use either supervised or InfoNCE based objectives, MAE uses a reconstruction objective.",1,neutral
"In addition to the finetning results, we include here linear evaluation results for class generalization gaps A11.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Translation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 92.24% 91.37% 92.25% 94.01% 96.33% 77.78% 75.93% 68.52% 68.52% 58.89% 39.82% 57.70% 64.51% 65.45% 66.98% MAEPretrained 52.54% 55.93% 60.44% 67.91% 83.79% 20.37% 27.78% 33.33% 38.89% 27.78% 9.67% 12.91% 14.26% 15.56% 15.18% MLPMixerPretrained1k 94.66% 93.98% 94.58% 95.75% 97.25% 77.78% 74.07% 72.22% 66.67% 48.15% 36.86% 53.70% 59.35% 63.05% 63.46% MLPMixerPretrained21k 94.95% 95.09% 95.19% 96.02% 97.13% 75.93% 75.93% 74.07% 77.78% 70.37% 43.89% 67.36% 71.25% 73.69% 76.18% ResNet50Pretrained1k 95.00% 94.58% 94.83% 95.79% 97.23% 88.89% 90.74% 87.04% 83.33% 70.37% 44.35% 63.26% 68.99% 70.04% 70.01% ResNet50Pretrained21k 95.51% 95.30% 95.90% 96.27% 97.39% 77.78% 72.22% 74.07% 74.07% 70.37% 46.61% 68.04% 73.02% 75.90% 77.13% SimCLRPretrained 96.13% 95.74% 96.22% 96.82% 97.50% 81.48% 79.63% 81.48% 72.22% 70.00% 43.73% 62.96% 69.10% 70.63% 72.02% ViTPretrained1k 95.79% 96.18% 96.37% 96.80% 97.75% 88.89% 83.33% 77.78% 77.41% 75.93% 49.34% 67.92% 72.74% 75.65% 77.22% ViTPretrained21k 95.43% 95.01% 95.62% 96.39% 97.50% 83.33% 83.33% 83.33% 77.78% 72.22% 46.59% 67.21% 71.71% 74.02% 75.17% iBotPretrained1k 96.67% 96.43% 96.49% 97.01% 97.66% 81.48% 81.48% 79.63% 79.63% 72.22% 40.27% 65.23% 73.10% 76.06% 77.33% iBotPretrained21k 96.84% 96.30% 96.44% 96.92% 97.61% 90.74% 85.19% 87.04% 81.48% 72.22% 47.69% 70.55% 76.57% 78.53% 79.04%
Table A1: Position varying linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Rotation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 91.70% 91.43% 92.42% 93.98% 96.28% 81.48% 85.19% 85.19% 79.63% 55.56% 28.64% 41.16% 44.72% 46.17% 48.85% MAEPretrained 53.90% 57.24% 61.28% 68.44% 83.90% 25.93% 44.44% 38.89% 42.59% 29.63% 6.68% 7.93% 8.58% 8.63% 8.36% MLPMixerPretrained1k 94.24% 93.76% 94.74% 95.72% 97.28% 74.07% 74.07% 72.22% 70.37% 46.30% 26.84% 39.47% 43.04% 46.51% 48.73% MLPMixerPretrained21k 94.49% 94.81% 95.03% 95.88% 97.13% 79.63% 77.78% 77.78% 79.63% 72.22% 36.90% 55.46% 61.57% 63.67% 65.50% ResNet50Pretrained1k 94.72% 94.56% 94.89% 95.74% 97.18% 85.19% 87.04% 85.19% 81.48% 68.52% 34.44% 46.22% 49.90% 51.73% 53.24% ResNet50Pretrained21k 95.51% 94.96% 95.69% 96.12% 97.30% 77.78% 75.93% 75.93% 72.22% 66.67% 40.29% 57.68% 61.83% 63.91% 65.04% SimCLRPretrained 95.74% 95.63% 96.16% 96.80% 97.51% 81.48% 83.33% 83.33% 83.33% 62.96% 32.94% 47.35% 52.88% 55.97% 56.91% ViTPretrained1k 95.71% 95.76% 96.09% 96.79% 97.67% 87.04% 79.63% 81.48% 79.63% 59.26% 39.13% 55.09% 60.14% 64.14% 65.42% ViTPretrained21k 95.23% 94.89% 95.68% 96.42% 97.45% 85.19% 83.33% 83.33% 83.33% 66.67% 38.25% 54.50% 58.34% 60.94% 62.28% iBotPretrained1k 96.39% 96.22% 96.41% 96.96% 97.56% 83.33% 81.48% 81.48% 79.63% 72.22% 34.62% 51.88% 58.88% 62.19% 63.11% iBotPretrained21k 96.44% 96.09% 96.42% 96.92% 97.61% 90.74% 88.89% 90.74% 87.04% 72.22% 41.03% 57.48% 63.69% 65.49% 67.34%
Table A2: Pose linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Spot hue_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 92.49% 91.60% 92.46% 94.06% 96.32% 77.78% 75.93% 70.37% 70.37% 59.26% 39.61% 60.78% 66.72% 68.27% 68.64% MAEPretrained 52.26% 55.43% 60.36% 67.78% 83.62% 22.22% 31.48% 37.04% 37.04% 20.37% 11.51% 15.09% 16.10% 18.91% 15.58% MLPMixerPretrained1k 95.17% 94.20% 94.98% 95.86% 97.30% 79.63% 77.78% 70.37% 66.67% 57.04% 40.09% 56.90% 64.43% 67.35% 68.93% MLPMixerPretrained21k 94.89% 95.29% 95.44% 96.13% 97.20% 81.48% 79.63% 72.22% 74.07% 76.30% 44.68% 69.30% 73.32% 76.12% 77.75% ResNet50Pretrained1k 95.26% 94.81% 95.03% 95.80% 97.23% 87.04% 88.89% 87.04% 83.33% 77.78% 44.08% 62.96% 68.67% 71.81% 72.54% ResNet50Pretrained21k 95.91% 95.45% 96.00% 96.28% 97.41% 77.78% 75.93% 75.93% 72.22% 71.11% 44.22% 67.22% 70.95% 72.38% 74.39% SimCLRPretrained 96.30% 95.91% 96.27% 96.84% 97.59% 79.63% 75.93% 74.07% 74.07% 66.67% 43.36% 63.22% 71.32% 73.72% 72.26% ViTPretrained1k 95.99% 96.36% 96.54% 96.95% 97.80% 90.74% 87.04% 83.33% 81.48% 74.07% 52.53% 69.53% 71.81% 76.01% 77.28% ViTPretrained21k 95.57% 95.39% 95.84% 96.51% 97.59% 85.19% 81.48% 83.33% 77.78% 75.93% 46.01% 67.50% 71.97% 75.75% 77.06% iBotPretrained1k 96.50% 96.55% 96.74% 97.12% 97.70% 79.63% 81.48% 81.48% 77.78% 74.07% 42.32% 68.61% 72.75% 76.28% 78.04% iBotPretrained21k 97.01% 96.42% 96.65% 97.04% 97.64% 88.89% 87.04% 83.33% 83.33% 75.93% 48.83% 73.37% 78.09% 80.39% 81.55%
Table A3: Spot hue linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Scale_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 91.81% 91.51% 92.26% 93.90% 96.25% 79.63% 72.22% 74.07% 70.37% 61.11% 36.80% 51.29% 57.05% 56.99% 58.80% MAEPretrained 51.98% 56.15% 60.87% 68.07% 84.12% 25.93% 35.19% 33.33% 37.04% 35.19% 7.01% 10.74% 11.05% 11.29% 11.44% MLPMixerPretrained1k 94.27% 93.55% 94.47% 95.59% 97.17% 79.63% 77.78% 70.37% 68.52% 53.70% 32.50% 46.15% 51.66% 55.30% 56.62% MLPMixerPretrained21k 94.86% 94.98% 95.08% 95.93% 97.09% 79.63% 79.63% 75.93% 75.93% 74.07% 40.55% 60.87% 66.63% 67.52% 70.03% ResNet50Pretrained1k 94.89% 94.57% 94.84% 95.66% 97.16% 87.04% 90.74% 90.74% 83.33% 72.22% 39.22% 54.23% 59.10% 61.27% 60.89% ResNet50Pretrained21k 95.51% 95.28% 95.77% 96.08% 97.35% 81.48% 77.78% 77.78% 74.07% 74.07% 41.11% 60.06% 66.41% 69.69% 70.33% SimCLRPretrained 95.91% 95.53% 96.09% 96.66% 97.48% 83.33% 77.41% 77.78% 72.22% 62.96% 39.79% 54.93% 61.81% 62.93% 62.96% ViTPretrained1k 95.65% 96.01% 96.18% 96.69% 97.69% 87.04% 83.33% 79.63% 75.93% 72.22% 44.10% 58.13% 63.91% 67.85% 68.82% ViTPretrained21k 95.06% 94.87% 95.47% 96.30% 97.38% 83.33% 83.33% 83.33% 81.48% 72.22% 41.40% 58.53% 63.25% 65.43% 65.14% iBotPretrained1k 96.58% 96.37% 96.48% 96.98% 97.60% 84.07% 77.78% 79.63% 77.78% 66.67% 41.34% 58.93% 67.24% 68.66% 69.08% iBotPretrained21k 96.92% 96.18% 96.39% 96.86% 97.53% 85.19% 77.78% 81.48% 81.48% 75.93% 44.59% 63.11% 68.90% 71.45% 70.82%
Table A4: Scale linear eval top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Background path_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 90.37% 91.88% 92.73% 94.36% 96.70% 85.19% 77.78% 77.78% 77.78% 66.67% 41.30% 54.70% 59.96% 61.75% 63.41% MAEPretrained 51.51% 56.16% 62.31% 67.08% 84.32% 27.78% 31.48% 37.04% 37.04% 29.63% 9.47% 12.21% 11.84% 13.30% 12.67% MLPMixerPretrained1k 92.84% 93.82% 94.81% 95.41% 97.38% 75.93% 77.78% 72.22% 74.07% 61.11% 37.39% 49.19% 52.41% 55.75% 56.74% MLPMixerPretrained21k 95.42% 95.33% 95.62% 96.52% 97.55% 83.33% 81.48% 75.93% 75.93% 77.78% 48.30% 64.37% 66.79% 70.15% 70.84% ResNet50Pretrained1k 94.35% 95.12% 94.83% 95.93% 97.46% 90.74% 92.59% 88.89% 88.89% 83.33% 44.89% 59.87% 64.15% 66.70% 67.29% ResNet50Pretrained21k 95.20% 96.40% 95.89% 96.65% 97.67% 81.48% 79.63% 77.78% 77.78% 75.93% 51.21% 65.75% 69.47% 72.01% 73.90% SimCLRPretrained 95.50% 95.98% 96.14% 96.45% 97.52% 83.33% 83.33% 81.48% 77.78% 72.22% 44.33% 59.82% 65.39% 67.93% 68.93% ViTPretrained1k 95.72% 96.42% 96.27% 96.57% 97.95% 94.44% 88.89% 88.89% 87.04% 77.78% 50.62% 64.09% 67.14% 72.33% 73.19% ViTPretrained21k 94.91% 95.22% 96.09% 96.45% 97.71% 83.33% 83.33% 83.33% 83.33% 77.78% 49.36% 64.21% 67.42% 70.77% 72.13% iBotPretrained1k 96.42% 96.58% 96.60% 97.40% 97.28% 88.89% 83.33% 87.04% 81.48% 79.63% 44.58% 62.59% 68.10% 71.20% 73.36% iBotPretrained21k 96.16% 96.14% 96.38% 97.37% 97.27% 88.89% 90.74% 90.74% 83.33% 81.48% 51.20% 68.58% 71.57% 74.62% 75.94%
Table A5: Background path linear eval top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Translation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.49% 97.00% 97.25% 97.55% 96.80% 87.04% 90.74% 77.78% 81.48% 75.93% 45.33% 69.74% 74.53% 77.72% 78.19% MAEPretrained 96.75% 96.63% 97.20% 97.46% 97.91% 83.33% 74.07% 66.67% 64.81% 72.22% 29.17% 51.35% 61.05% 65.16% 70.07% MLPMixerPretrained1k 96.95% 96.73% 97.17% 97.24% 97.88% 88.89% 77.78% 77.78% 74.07% 64.81% 44.65% 64.56% 70.67% 74.34% 75.95% MLPMixerPretrained21k 97.71% 97.69% 97.90% 97.98% 98.35% 85.19% 88.89% 85.19% 81.48% 77.78% 46.53% 70.54% 77.27% 79.78% 81.68% ResNet50Pretrained1k 97.97% 97.92% 97.91% 97.86% 98.27% 87.04% 87.04% 72.22% 81.48% 81.48% 45.05% 64.83% 71.87% 76.56% 79.63% ResNet50Pretrained21k 97.54% 97.62% 97.74% 97.69% 98.20% 88.89% 83.33% 83.33% 83.33% 77.78% 52.22% 70.96% 75.78% 81.10% 82.45% SimCLRPretrained 97.40% 97.57% 97.68% 97.81% 98.12% 90.74% 77.78% 87.04% 83.33% 77.78% 45.21% 68.73% 74.59% 76.55% 79.86% ViTPretrained1k 97.80% 97.88% 98.00% 97.92% 98.28% 90.74% 90.74% 85.19% 81.48% 81.48% 49.30% 71.82% 76.95% 80.39% 82.58% ViTPretrained21k 97.80% 97.59% 97.89% 97.91% 98.25% 87.04% 88.89% 81.48% 81.48% 87.04% 45.83% 71.80% 75.51% 79.96% 81.86% iBotPretrained1k 97.77% 97.55% 97.64% 97.77% 98.06% 88.89% 85.19% 79.63% 75.93% 79.63% 45.69% 68.94% 74.76% 76.63% 79.83% iBotPretrained21k 97.97% 97.83% 97.88% 97.92% 98.13% 88.89% 87.04% 88.89% 81.48% 79.63% 49.84% 70.97% 78.13% 82.53% 84.07%
Table A6: Position finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Rotation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.60% 97.01% 97.30% 97.58% 97.97% 88.89% 88.89% 85.19% 83.33% 72.22% 34.29% 57.01% 62.67% 65.58% 68.45% MAEPretrained 96.87% 96.75% 97.25% 97.52% 97.95% 75.93% 68.52% 62.96% 68.52% 62.96% 22.64% 45.24% 50.87% 54.18% 53.82% MLPMixerPretrained1k 96.75% 96.58% 97.08% 97.25% 97.86% 83.33% 85.19% 83.33% 77.78% 64.81% 33.29% 51.43% 55.55% 58.78% 59.35% MLPMixerPretrained21k 97.68% 97.65% 97.90% 97.90% 98.35% 87.04% 87.04% 77.78% 77.78% 75.93% 38.93% 62.76% 67.97% 71.90% 73.47% ResNet50Pretrained1k 98.05% 97.81% 97.89% 97.93% 98.24% 84.81% 81.48% 81.48% 81.48% 75.93% 33.29% 53.51% 62.56% 64.45% 67.47% ResNet50Pretrained21k 97.52% 97.45% 97.65% 97.61% 98.18% 85.19% 79.63% 83.33% 87.04% 85.19% 37.45% 59.85% 65.39% 70.66% 73.13% SimCLRPretrained 97.37% 97.43% 97.53% 97.74% 98.07% 87.04% 87.04% 83.33% 87.04% 75.93% 35.50% 55.87% 64.34% 66.79% 69.34% ViTPretrained1k 97.94% 97.87% 97.98% 97.95% 98.31% 88.89% 87.04% 85.19% 77.78% 75.93% 40.13% 62.66% 68.53% 71.32% 73.36% ViTPretrained21k 97.68% 97.61% 97.90% 97.97% 98.27% 88.89% 79.63% 83.33% 74.07% 70.74% 39.75% 61.42% 68.39% 71.51% 73.76% iBotPretrained1k 97.49% 97.55% 97.56% 97.75% 98.02% 88.89% 77.78% 83.33% 75.93% 68.52% 36.30% 60.69% 65.98% 68.22% 70.00% iBotPretrained21k 98.00% 97.79% 97.96% 98.01% 98.19% 88.89% 87.04% 83.33% 85.19% 72.22% 38.86% 62.35% 69.18% 71.96% 73.84%
Table A7: Pose finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Spot hue_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.66% 97.13% 97.40% 97.62% 97.98% 90.74% 88.89% 87.04% 87.04% 81.48% 49.99% 70.06% 75.12% 82.36% 82.48% MAEPretrained 97.04% 96.67% 97.20% 97.41% 97.95% 79.63% 77.78% 72.22% 74.07% 68.52% 30.10% 54.63% 60.74% 66.78% 69.34% MLPMixerPretrained1k 97.32% 96.92% 97.20% 97.36% 97.89% 87.04% 83.33% 77.78% 79.63% 72.22% 48.80% 65.60% 70.82% 76.88% 78.59% MLPMixerPretrained21k 97.80% 97.83% 97.99% 97.99% 98.42% 88.89% 87.04% 81.48% 75.93% 79.63% 49.81% 73.15% 75.23% 79.01% 82.35% ResNet50Pretrained1k 98.02% 97.99% 98.03% 97.98% 98.28% 88.89% 87.04% 83.33% 81.48% 77.78% 48.48% 67.72% 74.65% 76.90% 75.83% ResNet50Pretrained21k 97.68% 97.60% 97.87% 97.79% 98.24% 90.74% 87.04% 83.33% 77.78% 75.93% 54.35% 71.69% 76.70% 81.03% 81.86% SimCLRPretrained 97.60% 97.61% 97.72% 97.87% 98.17% 90.00% 76.30% 85.19% 77.78% 74.07% 45.58% 70.36% 79.09% 78.06% 81.75% ViTPretrained1k 97.68% 97.93% NaN 98.00% 98.37% 94.44% 88.89% NaN 81.48% 81.48% 54.38% 70.94% NaN 82.53% 83.94% ViTPretrained21k 97.77% 97.68% 97.94% 98.00% 98.24% 92.59% 85.19% 77.78% 78.15% 81.48% 52.49% 72.55% 76.58% 79.75% 82.39% iBotPretrained1k 97.91% 97.58% 97.76% 97.88% 98.08% 88.89% 81.48% 79.63% 75.93% 74.07% 48.67% 69.24% 75.01% 79.44% 80.81% iBotPretrained21k 98.25% 97.87% 97.88% 97.96% 98.13% 90.74% 83.33% 92.59% 79.63% 83.33% 49.39% 74.42% 80.67% 83.05% 85.02%
Table A8: Spot hue finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Scale_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.68% 97.08% 97.39% 97.63% 97.95% 90.74% 90.74% 87.04% 83.33% 79.63% 45.47% 66.39% 72.46% 75.63% 78.05% MAEPretrained 96.81% 96.64% 97.19% 97.40% 97.91% 81.48% 70.37% 70.37% 70.37% 53.70% 28.70% 51.73% 60.66% 62.49% 64.47% MLPMixerPretrained1k 97.23% 96.60% 97.07% 97.24% 97.82% 87.04% 85.19% 83.33% 74.07% 70.37% 41.62% 60.01% 65.78% 70.24% 71.64% MLPMixerPretrained21k 97.80% 97.80% 97.96% 97.95% 98.37% 85.19% 87.04% 81.48% 72.22% 77.78% 46.02% 68.37% 73.08% 76.51% 77.38% ResNet50Pretrained1k 97.88% 97.94% 97.95% 97.89% 98.24% 87.04% 85.19% 81.48% 75.93% 79.63% 44.47% 62.86% 71.55% 73.81% 75.80% ResNet50Pretrained21k 97.40% 97.58% 97.84% 97.72% 98.18% 90.74% 79.63% 81.48% 79.63% 79.63% 49.37% 68.72% 70.89% 76.85% 79.18% SimCLRPretrained 97.57% 97.54% 97.65% 97.83% 98.10% 88.89% 83.33% 83.33% 83.33% 74.44% 42.25% 65.04% 71.88% 74.89% 76.25% ViTPretrained1k 97.80% 97.92% 98.05% 97.92% 98.34% 88.89% 83.33% 85.19% 79.63% 79.63% 44.17% 65.86% 71.66% 77.46% 78.46% ViTPretrained21k 97.77% 97.71% 97.85% 97.99% 98.24% 85.19% 85.19% 85.19% 79.63% 79.63% 42.63% 67.71% 70.61% 74.96% 76.14% iBotPretrained1k 97.85% 97.61% 97.74% 97.79% 98.04% 90.74% 87.04% 83.33% 83.33% 79.63% 45.14% 64.09% 71.34% 73.90% 76.99% iBotPretrained21k 97.88% 97.75% 97.86% 97.97% 98.12% 88.89% 87.04% 87.04% 81.48% 75.93% 48.70% 65.52% 72.39% 79.16% 80.04%
Table A9: Scale finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Background path_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 96.49% 97.05% 97.52% 97.81% 98.01% 94.44% 88.89% 92.59% 87.04% 81.48% 50.79% 67.24% 76.25% 79.79% 80.92% MAEPretrained 96.20% 96.61% 97.31% 97.63% 97.99% 81.85% 72.22% 77.78% 72.22% 62.96% 30.49% 52.04% 64.77% 66.67% 68.56% MLPMixerPretrained1k 96.16% 97.03% 97.13% 97.66% 97.76% 90.74% 87.04% 81.48% 75.93% 72.22% 47.41% 63.84% 71.85% 73.76% 75.96% MLPMixerPretrained21k 98.08% 98.15% 98.09% 98.33% 98.71% 88.89% 92.59% 90.74% 85.19% 79.63% 51.84% 72.00% 77.05% 80.46% 81.10% ResNet50Pretrained1k 97.71% 97.76% 97.95% 98.04% 98.38% 92.59% 92.59% 90.74% 92.59% 83.33% 46.50% 63.48% 70.90% 73.73% 77.72% ResNet50Pretrained21k 97.38% 98.09% 97.72% 98.33% 98.34% 88.89% 83.33% 87.04% 81.48% 85.19% 50.87% 71.59% 76.21% 79.13% 83.24% SimCLRPretrained 97.38% 97.31% 97.77% 97.55% 98.05% 77.78% 87.04% 88.89% 90.74% 83.33% 43.32% 61.47% 69.94% 76.99% 79.07% ViTPretrained1k 98.12% 97.76% 97.89% 97.73% 98.44% 88.89% 90.74% 87.04% 81.48% 83.33% 54.25% 71.56% 75.73% 80.58% 84.92% ViTPretrained21k 97.77% 97.49% 97.95% 98.04% 98.37% 91.67% 88.89% 90.74% 87.04% 81.48% 55.17% 73.53% 76.50% 82.28% 81.94% iBotPretrained1k 97.47% 97.44% 97.61% 98.15% 97.86% 88.89% 92.59% 90.74% 81.48% 79.63% 51.19% 71.17% 75.32% 78.37% 79.93% iBotPretrained21k 97.69% 97.91% 97.77% 98.17% 97.93% 93.52% 92.59% 90.74% 85.19% 83.33% 55.00% 73.11% 76.62% 83.56% 82.90%
Table A10: Background path finetuning top-1 accuracy across multiple percentages of varying training instances",0,negative
", 2019), Masked AutoEncoders (MAE, (He et al., 2022)), or ViTs Dosovitskiy et al.",2,positive
"0 0.5 1
−0.8
−0.6
−0.4
−0.2
0
0 0.5 1 0 0.5 1
0 0.5 1
−0.6
−0.4
−0.2
0 0.5 1
CLIPPretrained MAEPretrained MLPMixerPretrained1k MLPMixerPretrained21k ResNet50Pretrained1k ResNet50Pretrained21k SimCLRPretrained ViTPretrained1k ViTPretrained21k iBotPretrained1k iBotPretrained21k
Generalization when Lighting varies in training
percent of diverse Lighting training samples percent of diverse Lighting training samples percent of diverse Lighting training samples
percent of diverse Lighting training samples percent of diverse Lighting training samples
t o p - 1
a c c u r a c y d
r o p
t o p - 1
a c c u r a c y d
r o p",0,negative
"0 50 100 −40
−20
0 50 100 0 50 100 0 50 100 0 50 100
CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k best fit
Percent of varying images across all instances
0 50 100 −40
−20
0
0 50 100 0 50 100 0 50 100 0 50 100",0,negative
"To test this,
0 10 20 30
Position gap
Pose gap
Lighting gap
Size gap Background gap CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k
Relative top-1 accuracy drop when (all factors vary - no factors vary) (linear eval)
0 10 20 30 40
Position gap
Pose gap
Lighting gap
Size gap Background gap CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k
Relative top-1 accuracy drop when (all factors vary - no factors vary) (finetuning)
0 10 20 30 40
Position gap
Pose gap
Lighting gap
Size gap Background gap CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50 21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k
Relative top-1 accuracy drop when (all factors vary - no factors vary) (finetuning)
0 10 20 30 40
Position gap
Pose gap
Lighting color gap
Size gap Background gap CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k
Relative top-1 accuracy drop when (all factors vary - no factors vary) (finetuning)
0 10 20 30
Position gap
Pose gap
Lighting color gap
Size gap Background gap CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k
Relative top-1 accuracy drop when (all factors vary - no factors vary) (linear eval)
0 10 20 30
Position gap
Pose gap
Lighting gap
Size gap Background gap CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k
Relative top-1 accuracy drop when (all factors vary - no factors vary) (linear eval)
Figure 6: Varying all factors during training improves robustness We show show relative generalization gaps when all factors vary during training relative to no instances seeing varying (no variability).
we trained models with increasing amounts of variability for a single factor and evaluated the robustness of other factors.",0,negative
"Using the typical evaluation procedures of self-supervised models (Caron et al., 2021; Dosovitskiy et al., 2021; Chen et al., 2020b) (finetuning and linear evaluation), we examine robustness across a catalog of state-of-the-art vision architectures, such as CLIP (Radford et al., 2021) that have significantly outperformed earlier models on robustness benchmarks such as ObjectNet (Barbu et al., 2019), Masked AutoEncoders (MAE, (He et al., 2022)), or ViTs Dosovitskiy et al. (2021)) among others.",2,positive
"As shown in Table 1 (b), the MAE model is especially susceptible to changes in background, with a-44.",1,neutral
"Learning objective is more impactful than architecture for robustness In general, we found that robustness was similar across models with the notable exception of MAE.",1,neutral
"…a catalog of state-of-the-art vision architectures, such as CLIP (Radford et al., 2021) that have significantly outperformed earlier models on robustness benchmarks such as ObjectNet (Barbu et al., 2019), Masked AutoEncoders (MAE, (He et al., 2022)), or ViTs Dosovitskiy et al. (2021)) among others.",2,positive
We use pre-trained weights from the official repo of He et al. (2022).,0,negative
MAE is also substantially more sensitive to position and lighting color.,1,neutral
"In our experiment we deploy our Lie operator on top of the Variance-Invariance-Covariance Regularization model (VICReg, (Bardes et al., 2021)), and of the very recent masked autoencoders model (MAE, (He et al., 2021)).",2,positive
"We incorporate our proposed Lie operator in two recent SSL approaches: masked autoencoders (MAE, (He et al., 2021)), and Variance-Invariance-Covariance",2,positive
", 2021)), and of the very recent masked autoencoders model (MAE, (He et al., 2021)).",2,positive
"We incorporate our proposed Lie operator in two recent SSL approaches: masked autoencoders (MAE, (He et al., 2021)), and Variance-Invariance-Covariance
Evaluate Robustness
Self-Supervised Lie Operator
Training Data
Regularization (VICReg (Bardes et al., 2021)) to directly model transformations in…",2,positive
", 2020), computer vision (Dosovitskiy et al., 2020; He et al., 2022) and related fields (Lu et al.",1,neutral
"…between vision and language using a pre-trained multimodal transformer (Geng et al., 2022), which is a large masked autoencoding transformer (Vaswani et al., 2017; He et al., 2022) jointly trained on image-text (Changpinyo et al., 2021; Thomee et al., 2016) and text-only data (Devlin et al., 2018).",2,positive
"It consists of two modules: a pre-trained multimodal masked autoencoder (He et al., 2022; Geng et al., 2022) to encode instructions and visual observations, and a transformer-based (Vaswani et al., 2017) policy that predicts actions.",2,positive
", 2022), which is a large masked autoencoding transformer (Vaswani et al., 2017; He et al., 2022) jointly trained on image-text (Changpinyo et al.",2,positive
"It consists of two modules: a pre-trained multimodal masked autoencoder (He et al., 2022; Geng et al., 2022) to encode instructions and visual observations, and a transformer-based (Vaswani et al.",2,positive
"Transformers (Vaswani et al., 2017) have led to significant gains in natural language processing (Devlin et al., 2018; Brown et al., 2020), computer vision (Dosovitskiy et al., 2020; He et al., 2022) and related fields (Lu et al., 2019; Radford et al., 2021; Geng et al., 2022).",1,neutral
"In addition to the differentiation of the network structure pre-trained on ImageNet, we include models with a variety of pre-trained strategies, including SimCLR [6], MoCov2 [8] and
1https://pytorch.org/vision/stable/index.html 2https://github.com/rwightman/pytorch-image-models 3https://github.com/open-mmlab
BYOL [21] for ResNet50, MoCov3 [9] and MAE [26] for ViT-B.",2,positive
"BYOL [21] for ResNet50, MoCov3 [9] and MAE [26] for ViT-B.",1,neutral
The ViT pre-training2 is analogous to the image reconstruction task proposed in MAE [45]: to reconstruct the masked image patches from visible ones.,2,positive
The most popular pretraining scheme for ViTs is called Masked Autoencoders (MAE) [45].,1,neutral
5× compared with the original MAE [45]3 and enables us to scale up ViTs with greater model capability (Figure 1a).,0,negative
The optimal masking ratio is related to the information redundancy in the data: BERT [55] uses a masking ratio of 15% for language and MAE [45] uses a ratio of 75% for images.,1,neutral
"Moreover, studies in both CNNs [87, 113] and ViTs [45, 101] indicate that alternative loss functions (e.",1,neutral
"Following this spirit, we take masked autoencoders (MAE) [45] as pre-training task for its simplicity, efficiency, scalability, and compelling performance.",2,positive
This design reduces time and memory complexity [45]: a masking ratio of 90% (used in our paper) can reduce the encoder complexity to <1/10.,2,positive
"Recently, self-supervised learning [24, 13, 16, 4, 15] has shown remarkable results in representation learning.",1,neutral
"Particularly, among popular pretext tasks are Contrastive Learning (CL) [14, 16, 38, 56, 75] and Masked Image Modeling (MIM) [7, 35, 76] for Convolutional Nets (ConvNet) [37, 47] and vision transformers [25, 69, 72], respectively.",1,neutral
"where LMIM is the MIM loss such as the prediction error used in the MAE [35] when reconstructing missing matches, p[u,v ] is the relative positional embedding with a 2D index of [u, v], g(xi − x j + δx ) and g(yi − y j + δy) are the continuous indexing function for x− and y−coordinate respectively, andM is the index of masked patches.",1,neutral
"A straight idea is to find the worst-case perturbations by jointly maximizing the contrastive loss and the MAE loss of a MIM task, and to use the obtained adversarial perturbations to train a robust representation by minimizing the joint
, Vol. 1, No. 1, Article .",1,neutral
"Publication date: October 2022.
where L𝑀𝐼𝑀 is the MIM loss such as the prediction error used in the MAE [35] when reconstructing missing matches, p[𝑢,𝑣 ] is the relative positional embedding with a 2D index of [𝑢, 𝑣], 𝑔(𝑥𝑖 − 𝑥 𝑗 + 𝛿𝑥 ) and 𝑔(𝑦𝑖 − 𝑦 𝑗 + 𝛿𝑦) are the continuous indexing function for 𝑥− and 𝑦−coordinate respectively, andM is the index of masked patches.",2,positive
"Defined on the image level, the distance can be the pixel-wise mean squared error as in MAE [35].",1,neutral
"However, due to its natural connection with the pretraining of language transformers [8, 23], the Masked Image Modeling (MIM) [7, 35, 76] has attracted extensive attentions recently to pre-train the vision transformers.",1,neutral
"Moreover, Masked Image Modeling (MIM) [7, 35, 76] has attracted increasing attentions for pretraining vision transformers.",1,neutral
"Then one adopts a Masked Auto-Encoder (MAE) [35] reconstruction error
L𝑚𝑎𝑒 (xm, x;𝜃 ) = D(I𝜃 (xm), x) with a distance measure D between reconstructed and original images to pre-train the network 𝜃 .",1,neutral
Then one adopts a Masked Auto-Encoder (MAE) [35] reconstruction error,1,neutral
"They have pushed forward the state of the art in multiple domains (Chowdhery et al., 2022; He et al., 2022).",2,positive
", 2021), and the patches themselves (Dosovitskiy et al., 2020; He et al., 2022).",2,positive
"For example, MAE (He et al., 2022) relies on further fine-tuning to purify image features.",1,neutral
"Motivated by such success, computer vision methods begin to apply Transformer (Vaswani et al., 2017) architectures, and operate on the sequences of raw pixels (Chen et al., 2020a), discrete patch tokens (Bao et al., 2021), and the patches themselves (Dosovitskiy et al., 2020; He et al., 2022).",2,positive
"Transformer framework was first proposed in [15], achieved huge success in natural language process tasks [16], [17] and was recently adopt in image processing tasks [18], [19].",1,neutral
"As the most effective scheme for few-shot learning, the ”pretraining & fine-tuning” paradigm has achieved great success in NLP and CV [11, 12, 13, 14, 15, 16, 17], but no researchers have tried it in spectral detection.",1,neutral
"On the other hand, many researchers [12, 13, 14, 26, 28] have also conducted in-depth research on the application of pre-trained models in the CV field.",1,neutral
"Even though transformers have shown better localization properties than convolutional networks, especially in the self-supervised setting [4, 22, 33], the few studies so far on vision transformers for image retrieval are limited to using the [CLS] token from the last layer of ViT as a global representation [12, 4, 14].",1,neutral
"For student networks, the learning objective is to recover the masked token in feature domain, as the successful progress made by MAE, random mask is implied to learn more generalized body feature.",1,neutral
"As a milestone of visual MIM, MAE [4] could almost recover the general content of the image under the mask rate of more than 75%.",2,positive
"Then inspired by DINO [3] and MAE [4], this paper formulate mask image modeling as knowledge distillation, as most of ViT-based backbone network did, the image pairs are cropped into patches with 16× 16 pixel.",2,positive
"He et al. (2021a), also motivated by the label-deficiency issue in federated learning, developed a series of self-supervised FL algorithms that incorporated the advances of supervised FL, especially those algorithms with personalization, to handle the heterogeneity in data.",1,neutral
"…of unlabeled data and achieved tremendous successes for a wide range of downstream tasks in computer vision (He et al., 2020; Chen et al., 2020; He et al., 2021b), natural language processing (Devlin et al., 2018; Sarzynska-Wawer et al., 2021), and embodied intelligence (Sermanet et al., 2018;…",2,positive
"Very recently, there have been a few contemporaneous/concurrent attempts (He et al., 2021a; Zhuang et al., 2021; 2022; Lu et al., 2022; Makhija et al., 2022) that bridged unsupervised/self-supervised learning and decentralized learning, with focuses on designing better algorithms that mitigate the…",2,positive
"…al. (2022) requires access to an unlabeled public dataset for all models to measure the distance, and requires the communication of some datasets; He et al. (2021a) experiments with multiple methods to do personalization in decentralized learning, and our clustering-based approach can be viewed…",2,positive
"For masked autoencoder (He et al., 2021b) experiment on CIFAR-10, we use Vit-Tiny (Dosovitskiy et al., 2020) with AdamW optimizer for 1000 epochs with batch size 256.",2,positive
"In particular, the works He et al. (2021a); Zhuang et al. (2021; 2022); Lu et al. (2022); Makhija et al. (2022) are closest to ours.",2,positive
"To the best of our knowledge, there have only been a few contemporaneous/concurrent attempts (Zhang et al., 2020a; He et al., 2021a; Zhuang et al., 2021; 2022; Lu et al., 2022; Makhija et al., 2022) that bridged unsupervised/self-supervised learning with unlabeled data and decentralized learning,…",2,positive
"Other significant examples of SSL include masked auto-encoding in language (Devlin et al., 2018) and vision (He et al., 2021b).",1,neutral
"Recently, the self-supervised learning methods, SimMIM [34], UM-MAE [19], BEiT [2], MAE [13], SplitMask [12], MoCo v3 [9], and DINO [5], are effective in pretraining Transformers [11, 22] for learning visual represenar X iv :2 21 0.",2,positive
"Hence, a high masking ratio (75% (He et al., 2022)) may not be necessary to suppress the amount of information the encoder sees, and we consequently attempt lower ratios of 50%, 60% to introduce more information about the subordinate target.",0,negative
"…as pre-text based methods (Doersch et al., 2015; Zhang et al., 2016; Gidaris et al., 2018), contrastive learning with Siamese networks (Oord et al., 2018; He et al., 2020; Chen et al., 2020; Henaff, 2020), masked image modeling (MIM) (He et al., 2022; Bao et al., 2022; Xie et al., 2022), and etc.",1,neutral
"In this work, we focus on MAE (He et al., 2022), which proposes to use a high masking ratio and a non-arbitrary ViT decoder.",2,positive
"We notice that recent work in the literature (He et al., 2022; Bao et al., 2022) performs many experiments in masking strategies, but to the best of our knowledge, we are the first to introduce image mixtures in the pre-training of MIM.",2,positive
"For better classification performance, we use normalized pixels (He et al., 2022) and a high masking ratio (0.75); for better visual reconstructions, we use a lower masking ratio (0.5) without normalizing target pixels.",2,positive
"Vanilla MAE exhibits strong signs of semantics understanding (He et al., 2022).",1,neutral
"Among them, MIM has shown a preponderant advantage in performance, and the representative method Masked Autoencoders (MAE) (He et al., 2022) has attracted much attention in the field.",2,positive
", 2020; Henaff, 2020), masked image modeling (MIM) (He et al., 2022; Bao et al., 2022; Xie et al., 2022), and etc.",2,positive
"Linear Probing: For linear evaluation, we follow MAE (He et al., 2022) to train with no extra augmentations and use zero weight decay.",1,neutral
", in MAE [19]), and is used to generate latent representation of a full image.",1,neutral
"Though MAE and iBOT are both strong MIM-based methods, TEC can still further improve them with the proposed target-enhanced conditional MIM scheme, verifying its effectiveness.",2,positive
"In SSL, a pretext task is first built, e.g., instance discrimination task [20, 8] or masked image modeling (MIM) [2, 19], and then pseudo labels are generated via the pretext task to train a network model without requiring manual labels.",1,neutral
"Instead, our patch-dim normalization stresses the relations among patches, which is compatible to the patch-prediction in the MIM scheme.",1,neutral
", iBOT models with more category semantics while MAE models with more image details [19].",1,neutral
", instance discrimination task [20, 8] or masked image modeling (MIM) [2, 19], and then pseudo labels are generated via the pretext task to train a network model without requiring manual labels.",1,neutral
"We therefore choose a mask-reconstruction [19] SSL scheme for the pretraining of the new model, in which the base model generates reconstruction targets from the full input images and the new model tries to predict base model targets from random masked image input.",2,positive
"After normalization, following [19], new model uses an fully-connected layer followed by the decoder to generate Zf for predicting the base model target Yf on masked patches: Lfea = ∥M ◦ (Yf − Zf )∥(2)2, (4) where M is the mask matrix and ◦ denotes the element-wise product.",1,neutral
"Under the mask-reconstruction [19] framework, TEC consists of a randomly initialized new ViT encoder to be pretrained, conditional adapters for conditional pretraining, and a multi-target decoder for reconstruction targets prediction, an SSL pretrained ViT encoder as the base model and an target-enhancing module to generate patch-relation enhanced reconstruction base model targets.",2,positive
"1 Introduction Self-supervised learning (SSL) has achieved overwhelming success in unsupervised representation learning, with astonishing high performance in many downstream tasks like classification [50, 51], object detection and segmentation [2, 19].",1,neutral
"TEC follows [19, 2], and uses Vision Transformer (ViT) [14] for implementation.",2,positive
"For MIM, this patch-dim normalization can better enhance the spatial relations among tokens than the widely used feature normalization [41, 39, 1] along channel dimension.",1,neutral
"Therefore, it is the relation among patches that helps the MIM training.",1,neutral
We then propose to utilize the self-attention maps as a type of reconstruction targets for MIM to further enhance the semantic relation modeling capability of the new model.,2,positive
"[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
", MoCo [20] trained with 200 epochs while MAE [19] with 16,00 epochs to release its potential.",0,negative
"With the plain backbone pretrained as a MAE [21], our method achieves 4.",2,positive
"Recently, the original ViT has been reintroduced as a competitive backbone for semantic segmentation [8] and object detection [25], with the aid of MAE [21] pretraining and window attention.",1,neutral
"Instead, we simply use the readily available pretrained MAE weights from [21].",0,negative
The three backbones were pretrained on ImageNet-1k as MAEs [21].,2,positive
Backbone PretrainingOur backbonemodels are pretrained as MAEs [21] on ImageNet-1K [11].,2,positive
This simple self-supervised approach turns out to be an efficient and scalable way to pretrain ViT models [21].,1,neutral
"Different from Masked Image Modeling (MIM), which is to reconstruct the masked contents for learning good representation, the Masked Siamese Networks will not predict the information in removed areas, so erasing will only lose information and is not desired in the learning procedure.",1,neutral
"This is different from vision transformer based MIM methods [20, 3] that will recover the masked regions by operating on the pixel space using rear X iv :2 21 0.",1,neutral
[20] proposed a simple transformer-based Masked Autoencoder (MAE) architecture that employs Mean Squared Error (MSE) loss to reconstruct the original image.,1,neutral
"• We show that block-wise masking provides superior performance on Masked Siamese ConvNets to the discrete random masking, commonly used in selfsupervised representation learning frameworks [20, 24, 1].",1,neutral
"Our observation on Masked Siamese ConvNets is opposite to that in MIM methods, which found discrete/random masking is better [20, 39].",1,neutral
He et al. [20] proposed a simple transformer-based Masked Autoencoder (MAE) architecture that employs Mean Squared Error (MSE) loss to reconstruct the original image.,1,neutral
"However, this approach simply employs the masking scheme from MIM [20, 3] without adapting it to the peculiarities of Siamese ConvNet.",1,neutral
This is quite different from the mechanism of Masked Autoencoders (MAE) [20] that predict masked areas to learn good representations.,1,neutral
Such independent patch-wise processing of an image allows to simply drop masked patches thus decreasing the computational cost [20].,1,neutral
"Recently, Masked Image Modeling (MIM) [20, 3, 1] has emerged and proven to be an effective approach to learning useful representation.",1,neutral
"Furthermore, instead of using discrete random masking that is applied in [20], we found using block-wise masking will give better performance since it can better preserve global information important for contrastive loss.",1,neutral
"Spatiotemporal representation learning using MAE was proposed in [15] by Feichtenhofer et al. Several works [18, 2, 16] have explored ways to adapt MAEs to handle multimodal data.",1,neutral
"This observation is different from MIM-based approaches [20, 39], where a discrete mask achieves better performance and highlights the importance of maintaining global features when generating an image mixture as opposed to the case when erasing parts of the image by performing a vanilla masking operation.",1,neutral
"This is different from vision transformer based MIM methods [20, 3] that will recover the masked regions by operating on the pixel space using rear X iv :2
21 0.",1,neutral
"The comparisons are made in Table 5 with previous methods based on Siamese networks [12, 29, 10, 11, 31], and also recently proposed masked auto-encoding methods [30, 90] tailored for Transformers.",1,neutral
"In this sense, SSL [12, 31, 10, 29, 15, 14, 86, 30] has seen great progress in computer vision, and can achieve competitive or even better performance on various downstream tasks compared to its supervised counterparts.",1,neutral
"In computer vision, however, there emerge a much wider variety of pretext tasks [87, 88, 52, 83, 20, 80, 55, 19, 30, 4, 42], since images contain much more information than languages so that there are much more intrinsic structures to be mined as free learning signals Solving pretext tasks requires the model to understand the visual concepts present in images and thus useful representations can be learned.",1,neutral
model MEC iBOT [90] MAE [30] DINO [11] MoCo v3 [15] SimCLR [12] BYOL [29] SwAV [10],0,negative
"MAE [31] presents a masked autoencoder for representation learning, which masks random patches from the input image and trains an encoder to reconstruct the masked patches.",1,neutral
"Meanwhile, MAE [31] is a representative restorative method for ViTs.",1,neutral
"However, such methods are generally less competitive in direct discriminative representation learning tasks, as assessed by linear evaluation and k-NN classification [31], [33].",1,neutral
"Recently, masked image modeling [31], [32] has gained great research attention as an SSL paradigm for ViTs.",1,neutral
"1) Evalutation protocols: Following previous works [14], [28], [31], we adopt three common evaluation protocols, namely fine-tuning evaluation, linear evaluation and k-NN classification [43], to assess the performance of each pretrained model.",2,positive
"Model pre-training and fine-tuning have been shown effective in many vision tasks [17, 46, 67].",1,neutral
"This finding is in line with those in recent model pre-training studies [17, 46, 67].",0,negative
"We also collect 7 ViT-S models with different training regimes, including the original ViT training setup [26]6, a stronger data augmentation setup in the Deit paper [89]-36, the training setup with distillation [89]-36, an improved DeiT training setup [90]-36 , and selfsupervised training fashions by MoCo v3 [12]7, MAE [38]8 and BYOL [33]9.",0,negative
", MOCO [12], MAE [38] and BYOL [33]) or semi-weakly supervised learning [103].",1,neutral
"Similarly, we investigate the effect of different types of supervision, such as self-supervision (e.g., MOCO [12], MAE [38] and BYOL [33]) or semi-weakly supervised learning [103].",1,neutral
"For the low-level target, ViT (Dosovitskiy et al., 2020), MAE (He et al., 2022), SimMIM (Liu et al., 2022b), ConvMAE (Gao et al., 2022), HiViT (Zhang et al., 2022) and GreenMIM (Huang et al., 2022) utilize the original or normalized pixels as the MIM target.",2,positive
"Following MAE (He et al., 2022) and BEiT v2 (Peng et al.",1,neutral
", 2020) Pixel ViT FC / N/A MAE (He et al., 2022) Pixel ViT Decoder LayerNorm `2 SimMIM (Liu et al.",2,positive
"Following MAE (He et al., 2022) and BEiT v2 (Peng et al., 2022), we test the robustness of MaskDistill on three ImageNet validation sets, i.e., ImageNet-Adversarial (Hendrycks et al., 2021b), ImageNet-Rendition (Hendrycks et al., 2021a) and ImageNet-Sketch (Wang et al., 2019).",2,positive
"After that, the main difference between previous work lies in the architecture design (He et al., 2022; Chen et al., 2022a) and reconstruction targets (He et al.",2,positive
"MAE (He et al., 2022) also introduces per-patch normalized pixels (i.e., layer normalization without affine transformation) as the target to boost local pixels contrast, resulting in better performance.",2,positive
"After that, various target supervision has been explored under the masked image modeling framework, such as original or normalized pixels (He et al., 2022; Dong et al., 2021; Liu et al., 2022b; Gao et al., 2022; Liu et al., 2022a; Zhang et al., 2022; Huang et al., 2022), high-level features (Wei et al.",2,positive
", 2022a) and reconstruction targets (He et al., 2022; Liu et al., 2022b; Wei et al., 2021; 2022a; Baevski et al., 2022).",0,negative
"After that, the main difference between previous work lies in the architecture design (He et al., 2022; Chen et al., 2022a) and reconstruction targets (He et al., 2022; Liu et al., 2022b; Wei et al., 2021; 2022a; Baevski et al., 2022).",0,negative
"MAE (He et al., 2022) introduces a decoder to decouple the masked prediction task from the encoder.",2,positive
"After that, various target supervision has been explored under the masked image modeling framework, such as original or normalized pixels (He et al., 2022; Dong et al., 2021; Liu et al., 2022b; Gao et al., 2022; Liu et al., 2022a; Zhang et al., 2022; Huang et al., 2022), high-level features (Wei et…",2,positive
"MAE (He et al., 2022) also introduces per-patch normalized pixels (i.",1,neutral
The key difference between anomaly detection and ordinary benchmarks where MAE excel is that anomaly detection is an unsupervised task.,2,positive
We compare between DINO [5] and MAE [11] as a representation for a kNN based anomaly detection algorithm.,2,positive
It intended to convey that even for easy tasks the MAE doesn’t achieve as good results as DINO.,2,positive
"Recently, masked-autoencoder (MAE) based methods achieved significant improvements on several self-supervised representation learning benchmarks [11].",1,neutral
MAE’s optimization objective may explain why its strong representation does not translate into better anomaly detection capabilities.,2,positive
A comparison between MAE to contrastive self-supervised method (DINO) is presented in Tab.,1,neutral
"Yet, the representations learnt by MAE underperform contrastive self-supervised methods on unsupervised tasks such as anomaly detection.",1,neutral
"For MAE, we experimented both with kNN and reconstruction error for anomaly scoring and found that the latter works badly, therefore we report just the kNN results.",2,positive
"This is also suggested by MAE’s worse performance with linear probing (as reported by the original paper), where the supervised labels cannot be used to improve the backbone representations.",1,neutral
"As MAE’s objective is to reconstruct patches, it may learn a representation that encodes local information needed for reconstructing the image, overlooking semantic object properties.",2,positive
"A.1 Anomaly detection comparison of MAE and DINO
We compare between DINO [5] and MAE [11] as a representation for a kNN based anomaly detection algorithm.",2,positive
Regressing RGB values normalized by the mean and standard deviation within each patch has proven to be effective for MAE [38].,1,neutral
This optimal ratio is higher than what was found for instance in MAE [38] for the auto-completion task.,1,neutral
"In particular, the masked autoencoder (MAE) [38] accelerates pre-training by using an asymmetric architecture that consists of a large encoder that operates only on unmasked patches followed by a lightweight decoder that reconstructs the masked patches from the latent representation and mask tokens.",2,positive
"Reminiscent of denoising autoencoders [78] or context encoders [60], and aiming to reconstruct masked pixels [3, 16, 26, 30, 38, 85], discrete tokens [7, 95] or deep features [5, 82], these methods have demonstrated the ability to scale to large datasets and models and achieve state-of-the-art results on various downstream tasks.",1,neutral
"This is due to the MSE loss, but as noted in MAE [38], beyond a certain point, sharper reconstructions do not necessarily lead to better pre-training.",0,negative
"In practice these models are typically trained on object-centric datasets such as ImageNet [67] and thus tend to learn high-level semantic information; that makes them well suited for tasks such as image classification or object detection [4, 38, 47].",1,neutral
"In particular, we compare to DINO [14], a state-of-the-art self-supervised method based on instance discrimination, and to MIM methods with MAE [38] and MultiMAE [4].",2,positive
"We follow the exact same protocol as MAE [38] for that, with global average pooling for CroCo as we did not include a [CLS] token in our model.",2,positive
"More recently, Masked Image Modeling (MIM) [7, 16, 19, 32, 38, 60, 93] has emerged as a powerful alternative for self-supervision.",1,neutral
"Our model is trained using a simple pixel reconstruction loss over all masked patches, similar to MAE [38].",2,positive
"” In the past decade, representation learning has made significant progress in representing high-dimensional sensory signals, such as images and audio, to reveal the geometric and semantic structures hidden in raw signals (Bengio et al., 2013; Chen et al., 2018; Kornblith et al., 2019; Chen et al., 2020; Baevski et al., 2020; Radford et al., 2021; Bardes et al., 2021; Bommasani et al., 2021; He et al., 2022; Chen et al., 2022).",1,neutral
"…images and audio, to reveal the geometric and semantic structures hidden in raw signals (Bengio et al., 2013; Chen et al., 2018; Kornblith et al., 2019; Chen et al., 2020; Baevski et al., 2020; Radford et al., 2021; Bardes et al., 2021; Bommasani et al., 2021; He et al., 2022; Chen et al., 2022).",1,neutral
"Note that the random masking as a regularization technique has also been successfully used in reconstruction-based self-supervised learning [24, 74].",1,neutral
The head architecture is based on MAE and is composed of two layers: BatchNormalization(affine=False) and Linear.,2,positive
"Therefore, we used ArcFace [9] to train a projection head composed of two layers, BatchNormalization and Linear. this architecture is based on MAE [13].",1,neutral
this architecture is based on MAE [13].,1,neutral
"…tries to maximize the agreement between positive pairs (Chen et al., 2020; He et al., 2020; Grill et al., 2020), or clustering-based methods to generate pseudo labels for data (Caron et al., 2020; 2018), or mask autoencoder to predict the masked patches by the remaining patches (He et al., 2022).",1,neutral
", 2020; 2018), or mask autoencoder to predict the masked patches by the remaining patches (He et al., 2022).",2,positive
"In order to increase robustness to such varying resolution, we utilize up to 2⇥ higher resolution images during training but randomly drop 80% of visual tokens to minimize additional compute overhead (similar to [38, 52]).",2,positive
"Self-supervised models pretrained on ImageNet-1k (He et al., 2022).",2,positive
"Table 1: Token Merging ablation experiments using ViT-L/16 from MAE (He et al., 2022) on ImageNet-1k evaluated off-the-shelf without training, using r = 8.",2,positive
"MAE (He et al., 2022) is a self-supervised pretraining method for ViT with models pretrained and fine-tuned on ImageNet-1k.",2,positive
"…making them faster than their raw flop count would suggest; they support powerful self-supervised pre-training techniques such as MAE (He et al., 2022) that can put up state-of-the art results while being fast to train; given their lack of assumptions about the data, they can be…",2,positive
"We perform several experiments on ImageNet-1k (Deng et al., 2009) using ViT models trained in four different ways: AugReg (Steiner et al., 2022), MAE (He et al., 2022), SWAG (Singh et al., 2022), and DeiT (Touvron et al., 2021).",2,positive
"Yet, vanilla ViTs still have many desirable qualities: they consist of simple matrix multiplications, making them faster than their raw flop count would suggest; they support powerful self-supervised pre-training techniques such as MAE (He et al., 2022) that can put up state-of-the art results while being fast to train; given their lack of assumptions about the data, they can be applied with little or no changes across many modalities (Feichtenhofer et al.",1,neutral
"…in methods that leverage large amount of unlabeled data in domains such as speech, vision and language to produce state-of-the-art results, e.g. Baevski et al. (2020; 2022); Chen et al. (2020a); Caron et al. (2021); He et al. (2022); Cai et al. (2022); Brown et al. (2020); Ramesh et al. (2021).",2,positive
"Bert [8] solves this problem by a small proportion of random token in the mask, and MAE [16] embed latent representation without mask token in the encoder to avoid this problem.",1,neutral
"Different from MAE [16], our encoder operates on the full set.",2,positive
"It has recently achieved incredible success in pre-training vision Transformers [16], motivating researchers to extend its application to molecular graphs.",2,positive
High mask ratio was originally applied in images to encourage learning effective semantic information [16].,1,neutral
"A high mask ratio offers a challenging reconstruction task, which requires the learned model to capture numerous correlations among the tokens and thus encourages learning effective structure information [16].",1,neutral
"Various self-supervised pretrain tasks have been proposed for ViT and applied in downstream tasks [2, 6, 10].",1,neutral
"Transformer structure has been reported to work better than the customized convolutional neural networks or recurrent networks for both vision and language tasks [11, 17], thereby implying the potential as a unified cross-modal encoder.",1,neutral
"For image representation, the supervised model ViT [12] and the self-supervised BEiT [1] and MAE [17] also prove the effectiveness of the transformer in learning visual semantics.",1,neutral
"Although the transformer is good at learning semantic representations, researchers have found that it requires larger scale supervision [11, 12, 17] than the customized networks.",1,neutral
MAE [17] adopts a similar pre-training scheme as BERT and predicts the masked regions of an image with unmasked ones.,1,neutral
"Both in the domains of NLP and computer vision, these large models [13, 56, 68, 25, 94, 95] achieve enormous performance improvements compared to the small-scale models and provide pre-trained weights for downstream tasks.",1,neutral
"The experiments reveal that MAE and U-Net are the best shape denoising methods we evaluated for all six
types of noise.",1,neutral
"The EBM, U-Net, Deeplabv3+ and MAE were trained with clean shapes as well as shapes perturbed by all types of noise except the detection noise.",2,positive
"The paper evaluated seven methods from different areas that could be used for shape denoising: Active Shape Model (ASM) as a classical segmentation method, two generative models based on Boltzmann Machines (Deep Boltzmann Machine (DBM) and Convolutional DBM), another generative model named Energy Based Model (EBM), and three deeplearning based models used for object segmentation: U-Net, DeepLabv3+ and Masked Autoencoder (MAE).",2,positive
"U-Net [11], DeepLabv3+ [12], masked autoencoder (MAE) [13], etc.",2,positive
"We evaluate seven shape denoising methods (ASM [6], DBM [8], CDBM [9], EBM [10], U-Net [11], Deeplabv3+ [12], MAE [13]) for denoising shapes corrupted by the six types of noise introduced in Section 2.",2,positive
"We evaluate seven shape denoising methods (ASM [6], DBM [8], CDBM [9], EBM [10], U-Net [11], Deeplabv3+ [12], MAE [13]) for denoising shapes corrupted by the six types of noise introduced in Section 2.2.",2,positive
"Moreover, deep learning based methods for semantic segmentation can also be used for shape modeling, e.g. U-Net [11], DeepLabv3+ [12], masked autoencoder (MAE) [13], etc. 1.1.",1,neutral
"Recently, view-based self-supervised vision transformers (ViTs) have revealed emerging properties that have not been shown in either the supervised ViTs or previous unsupervised CNNs, attracting a lot of research interest in the community [6], [7], [8], [9], [10].",1,neutral
"Concurrent BERT [35] like approachs (BEiT [36], MAE [10], CAE [37]) propose to reconstruct raw pixels via mask image modeling.",1,neutral
[15] show that the mask ratio has a decisive influence on the downstream performance of MAE.,1,neutral
"Beside the popular contrastive learning methods [4, 14, 12, 27], recently there has been a renaissance of reconstruction-based autoencoders for SSL, for example, MAE [15], BEiT [1], iBOT [32], and SimMIM [29], which demonstrate state-of-theart performance on various downstream tasks.",1,neutral
"For the decoder, we use a flexible one following [15].",1,neutral
"We mainly follow the basic setup of MAE [15]: for the encoder, we adopt different variants of ViT [10], i.",2,positive
"Specifically, MAE adopts a simple mean square error (MSE) loss: LMAE(h) = Ex̄Ex1,x2|x̄ ‖g(f(x1))− x2‖ 2 , (2) where the decoder output x̂2 = h(x1) = g(f(x1)) is assumed to be l2-normalized following the original paper of MAE saying that normalized target x2 yields better performance [15].",1,neutral
", BERT [9]), also shows promising results in visual representation learning, to name a few, BEiT [1], iBOT [32], SimMIM [29], and MAE [15].",1,neutral
We begin by introducing a mathematical formulation of MAE [15].,1,neutral
"As an implication of this theorem, a small U-MAE loss would provably imply a small downstream classification error, which helps explain the good downstream generalization ability of MAE [15].",1,neutral
"[15], the patchwise masking strategy is a key component that distinguishes MAE from standard autoencoders, and different mask ratios ρ have a large impact on the downstream performance of pretrained features of MAE.",2,positive
"Secondly, MAE discards the masked image blocks at the input layer, only extracts the features of the non-masked image blocks, and then adds the mask information to form the image features [31].",2,positive
As inpainter we use a publicly available Masked AutoEncoder (MAE) [21] trained with an adversarial loss.,2,positive
The MAE [21] we use is based on a ViT architecture and has been pre-trained in an adversarial fashion (as opposed to the standard training with an MSE loss) to output more realistic-looking details,2,positive
(MAE) [21] with an adversarial loss to increase the details of the reconstructed images.,0,negative
"Following the recent trend of methods for unsupervised object segmentation [7–12, 22], we build our method on top of SSL features, and, in particular, DINO [4] or MAE [21] features.",2,positive
"Our approach is compared not only with MAE [9] and CPC [7], but also with SimCLR [7], BYOL [7], and MoCov2 [16].",2,positive
"This may be somewhat related to the network (transformer) used in MAE, so although we use a masked representation learning approach similar to MAE, we improve the network structure to make it more adaptable to the learning of ECG representations.",2,positive
An asymmetric encoder-decoder structure is used in MAE [9].,1,neutral
"From Table I, we can know that although MAE has excellent performance in computer vision, it is not able to learn good ECG representations.",2,positive
"This recent trend began with natural language processing when BERT [10] and successive large pretrained language models [31, 7] were released and quickly gained popularity in other domains such as computer vision [19, 9, 11] and graph learning [54, 22, 33].",1,neutral
"A mask-noise is usually used to perturb the images and those approaches predict the masked inputs either at the pixel level (Dosovitskiy et al., 2020; He et al., 2021; Xie et al., 2019) or at a token-level, using a pixel (often patch-level) tokenizer (Bao et al., 2021; Wei et al., 2021).",1,neutral
"We also compare the performance of those models to instance-based methods such as MAE (He et al., 2021) and data2vec (Baevski et al., 2022), which do not employ volume maximization regularizers.",2,positive
"The MAE method employs a simple pixel-reconstruction loss for representation learning, and thus does not explicitly compute mini-batch statistics during pretraining.",1,neutral
"We also compare the performance of those models to instance-based methods such as MAE (He et al., 2021) and data2vec (Baevski et al.",2,positive
"Auto-regressive models, and denoising auto-encoders, in particular, predict clean visual inputs from noisy views (Chen et al., 2020a; Vincent et al., 2010; He et al., 2021; Bao et al., 2021; Baevski et al., 2022).",1,neutral
Our pretraining follow the standard hyperparameters defined in He et al. (2021).,2,positive
"Tables 5, 6,7, 8 and 9, and show the results of SimCLR, MSN, VICReg, data2vec and MAE on the CIFAR100, CIFAR100 1%, Place205, Clevr/Count, Clevr/Dist and KITTI downstream tasks.",0,negative
"We pretrain a ViT-B/16 with data2vec (He et al., 2021) using the AdamW optimizer with a batch size of 2048 for 800 epochs using the official code base, which is publicly available: http: //github.com/facebookresearch/data2vec_vision/tree/main/beit.",2,positive
"By contrast, evaluations with instanced-based methods data2vec and MAE in Table 1 show different trends.",1,neutral
"We pretrain a ViT-L/16 with MAE (He et al., 2021) using the AdamW optimizer with a batch size of 1024 for 800 epochs using the official code base, which is publicly available: https:// github.com/facebookresearch/mae.",2,positive
MAE.,0,negative
"Masking patches individually works well for the original MAE [19]; but for a significantly longer sequence, it can bring about delicacies and degenerate the task even if the same percentage of tokens are masked [48].",1,neutral
"Our long-sequence MAE is a simple and minimallychanged extension of MAE [19], which we summarize first.",1,neutral
"Fortunately, pre-training, especially variants of Masked Autoencoding (MAE) [12, 19], have risen as a domainagnostic approach to reduce overfitting and scale models.",1,neutral
"Pioneered by earlier efforts [6,14,35,43], many recent methods [1, 9, 13, 19, 33, 46, 48, 52] have revisited this idea as a highly effective solution for visual representation learning.",1,neutral
"Notably, MAE [19] employs an explicit encoder-decoder architecture, and drops (instead of ‘replaces’ [1, 12]) tokens for the heavier encoder.",1,neutral
"Extending its success to computer vision, MAE [19] again demonstrates strong model scalability by directly pre-training on raw pixels.",2,positive
"conducted in MAE [19], the same model size is used for both stages), whereas for input dimensions, we can easily change them due to the extensive weight-sharing used in modern model architectures.",2,positive
"This not only helps a clean, scientific understanding in contrast to prior studies that scale both [4,10,28], but also offers a more efficient, practical solution compared to scaling supervised transfers alone [13, 19, 48].",1,neutral
"Compared to [19], the sequence length is increased to 4× (compute is also ∼4×), resulting from an enlarged image.",1,neutral
"Finally, the output sequence of the decoder is used to predict the normalized pixel values [19] in the masked patches.",1,neutral
"Unsupervised representation learning has shown great success in computer vision [27] and natural language processing [10]; however, one challenge is that RL includes both behavior learning and representation learning [32, 53].",1,neutral
"In [22, 59], the input patches are masked and the network is tasked to predict the masked pixels.",1,neutral
"Self-supervised learning: In recent years, several self-supervised techniques have been proposed to pre-train ViTs [1, 3, 22, 36, 59, 66].",1,neutral
"And the patches encoded from input images containing high-level understanding of parts, objects, and scenes [8].",1,neutral
"redundancy, which is unsuitable for representation learning [15].",1,neutral
"Linear evaluation misses the opportunity of pursuing strong but non-linear features, which is indeed a strength of deep learning [15].",1,neutral
"Prior works [5, 37, 84] have successfully introduced the mask-and-predict scheme in NLP [9,23] to pre-train an image transformer.",1,neutral
"To help the model better model the trajectory, we adopt the patch norm method to normalize the trajectory values in each patch into a standard normal distribution, as has also been done in MAE [37].",1,neutral
"First, as the appearance information can be well reconstructed in a single image with an extremely high masking ratio (85% in MAE [37]), it is also feasible to be reconstructed in the tube-masked video frame-by-frame and neglect to learn important temporal clues.",1,neutral
"Since an image with a high masking ratio (85% in MAE [37]) can be well reconstructed [37,75], we conjecture that the masked appearance information of a video can also be reconstructed frame by frame independently.",1,neutral
"These methods vary in different reconstruction objectives, including raw RGB pixels [37], handcrafted local patterns [75], and VQ-VAE embedding [5], all above are static appearance information in images.",1,neutral
"For computation efficiency, we follow MAE [37] to only feed the unmasked patches (and their positions) to the encoder.",2,positive
"2) Pixel: predicting all the pixels of each 3D patch [28, 37, 64].",1,neutral
"Recently, BEiT and MAE [5, 37] show two excellent mask image modeling paradigms.",1,neutral
"We consider two supervised feature backbones: ResNet50 [25] and ViT-B/16 [18], and four selfsupervised backbones: SimCLR [13], MAE [23], MSN [4] and DINO [11].",2,positive
"We consider two supervised feature backbones: ResNet50 [16] and ViT-B/16 [13], and four self-supervised backbones: SimCLR [9], MAE [17], MSN [2] and DINO [7].",2,positive
"[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"long-range interactions on sequential image patches [63], [64], [65], [66].",1,neutral
"According to a recent study [60], self-supervised learning can be as effective as or even superior to supervised learning in various computer vision tasks.",1,neutral
The random mixing and block-wise mixing strategies are inspired by MAE [18] and BEiT [3] and we replace the masking operation with image mixing on patch-level and block-level (both of size 16×16) respectively.,2,positive
"Inspired by MAE [18] and BEiT [3], we implement a random mixing strategy and block-wise mixing strategy.",2,positive
"With the success of Transformers and Bidirectional Embedding Representation for Transformer (BERT) on natural language processing (NLP), the idea of attention mechanism and pre-training and fine-tuning has also been shown effective in the convolutional network, graph network, and RS [18, 21, 22, 31, 32].",1,neutral
"Unsupervised pre-training on big datasets succussed in most tasks in NLP and CV [18, 31, 32] but is studied slightly in RS.",1,neutral
"In particular, we compare to two approaches: R3M [41], which utilizes the Ego4D dataset of human videos to obtain a representation, and MVP [46, 56], which trains a masked auto-encoder [16] on the Bridge Dataset and utilizes the learned latent space as the representation of the new image.",2,positive
"Some other prior approaches that attempt to leverage large, diverse datasets via representation learning [36, 58, 59, 41, 16, 56, 35], as well as other methods for learning from human demonstrations, such as behavioral cloning methods with expressive policy architectures [47].",1,neutral
", 2022), and recent works even explore to use LMs as a general-purpose interface for diverse modalities (Hao et al., 2022; Xie et al., 2022; He et al., 2022).",2,positive
"…(LMs) have demonstrated great success on a wide range of natural language tasks (Devlin et al., 2018; Brown et al., 2020b; Chowdhery et al., 2022), and recent works even explore to use LMs as a general-purpose interface for diverse modalities (Hao et al., 2022; Xie et al., 2022; He et al., 2022).",1,neutral
"For a fair comparison, we compare our DMAE ViT-B model with the MAE ViT-B checkpoint released by He et al. (2022) in the linear probing setting on ImageNet.",2,positive
"The other class is the generative learning approach, which randomly masks patches in an image and learns to generate the original one (Bao et al., 2021; He et al., 2022).",1,neutral
"Following He et al. (2022); Xie et al. (2022), we use ImageNet-1k as the pre-training corpus which contains 1.28 million images.",2,positive
"Like MAE (He et al., 2022), we adopt the asymmetric encoder-decoder design for DMAE.",2,positive
"To enforce the encoder (but not the decoder) to learn robust semantic features, we control the capacity of the decoder by setting a smaller value of the hidden dimension and depth following He et al. (2022).",2,positive
"For the pre-training of the two DMAE models, we set the masking ratio to 0.75 following He et al. (2022).",2,positive
"…auto-encoder (Vincent et al., 2008; 2010) or denoising diffusion model (Ho et al., 2020; Nichol & Dhariwal, 2021) to pre-train θdenoiser, and leverage contrastive learning (Chen et al., 2020; He et al., 2020) or masked image modelling (He et al., 2022; Xie et al., 2022) to pre-train θencoder.",2,positive
", 2020) or masked image modelling (He et al., 2022; Xie et al., 2022) to pre-train θencoder.",2,positive
"Motivated by the Masked AutoEncoder (MAE) (He et al., 2022), which learns latent representations by reconstructing missing pixels from masked images, we design a new self-supervised task called Denoising Masked AutoEncoder (DMAE).",2,positive
"The compared MAE[9] method implemented in this paper follows a similar design as our MV-SSTMA to fit the EEG data, calculating the spatial attention between EEG channels but removing the multi-view and temporal aspects of MV-SSTMA.",2,positive
• MAE[9]: Masked autoencoders as scalable self-supervised learners by reconstructing the missing patches in images for computer vision.,1,neutral
"Nevertheless, MAE cannot be directly applied well with EEG data
considering the complication of EEG signals, and it doesn’t take temporal properties into account.",1,neutral
"Furthermore, the MAE method also exceeds baselinemethods on SEED-IV, whereas performsworse than some supervised models on SEED.",2,positive
He et al. [9] presented masked autoencoders (MAE) as scalable self-supervised learners for computer vision to reconstruct the missing patches in images.,1,neutral
"In the case of only few labeled data for calibration, the proposed MV-SSTMA is evaluated with the self-supervised method MAE and the supervised methodMD-AGCN.",2,positive
[9] presented masked autoencoders (MAE) as scalable self-supervised learners for computer vision to reconstruct the missing patches in images.,1,neutral
"The Paired t-test is also conducted on the performance of MVSSTMA and MAE for all subjects in all situations above, as well as performance of MV-SSTMA and MD-AGCN.",0,negative
"In addition, our model outperforms MAE and MD-ADCN in every scenario.",2,positive
[9] proposed a masked autoencoder for self-supervised learning in the computer vision area achieving excellent performance.,1,neutral
"Moreover, as the temporal information is also considered in the NoHybrid model and SingleScale model, their performance are still better than MAE.",1,neutral
"[18]) are stochastic and can significantly alter the distribution of the data [19, 16, 20].",1,neutral
"data masking [16], cutout [17], mixup Both senior authors contributed equally.",2,positive
"2 show that random mask [16], cutout [17] and our new random rotation augmentation yield comparable generalization error for a wide range of hyperparameters (masking probability, cutout width and rotation angle respectively); the random rotation is a new augmentation proposed in this work and frequently beats ridge regularization as well as interpolation.",2,positive
"This type of augmentation has been widely used in practice [16, 67]8, and is a simplified version of the popular cutout augmentation [17].",1,neutral
"An example is as follows: while the classical noise injection augmentation [21] causes only a constant shift in the spectrum, data masking [16, 24], cutout [17] and distribution-preserving augmentations [15] tend to isotropize the equivalent data spectrum.",1,neutral
", random-masking [16], cutout [17], noise injection [21], and group-invariant augmentations [15].",1,neutral
"2 Comparisons of different types of augmentations In this section, we compare the generalization of three canonical augmentations that we analyzed in this work: 1) Gaussian noise injection [21], 2) random mask [16], and 3) random rotation (which we introduced in Section 3.",1,neutral
"In our framework, we allow for cases in which augmentation leads to significant changes in distribution and provide a path to analysis for such OOD augmentations that encompass empirically popular DA [16, 17].",2,positive
"3, we simulate the biased and unbiased random mask augmentation [16] and test their performance in regression and classification tasks.",2,positive
"Dropout also induces implicit regularization by randomly dropping out intermediate neurons (rather than covariates, as does the random mask [16] augmentation) during the learning process, and has been shown to have a close connection with sparsity regularization [64].",1,neutral
"Note that a natural constraint r ≤ m holds, and the original MAE setting [14] can be regarded as the special case when r = m.",1,neutral
"Examples include MAE [14], SimMiM [61], and BEiT [2] for images, and [10, 11, 52] for videos.",1,neutral
"As observed in the recent self-supervised pre-training work, such as MAE [14] for images and [10, 52] for videos, it is sufficient to use only a small fraction of the input visual tokens to reconstruct the visual signal.",1,neutral
"Inspired by MAE [14], we also use a shallow decoder with much fewer Transformer blocks than the encoder (Φdec(.",2,positive
"such as NLP [28] and vision [29], [30].",1,neutral
"We only calculate the loss on the mask tokens, which is consistent with MAE and SimMIM.",2,positive
"Different from the MAE [8] which utilizes a multilayer converter structure as decoder, we only employ a single-layer MLP is used as decoder.",2,positive
"More recently, Masked Auto-Encoder (MAE) [8] predicts random masked patches of the input image and reconstructs the missing pixels by an autoencoder, and this method has obtained superior performance for the image classification task.",1,neutral
"Given an image-text pair (𝐼 ,𝑇 ), we random sample 25% of the image patches, the other 75% image patches will not be fed into the models following MAE[11].",2,positive
"Given an image-text pair (I ,T ), we random sample 25% of the image patches, the other 75% image patches will not be fed into the models following MAE[11].",2,positive
"Recent research explores intriguing properties of vision transformers [59, 65], jointly modeling multiple modalities [60, 90], and inpainting via masked autoencoders [29, 64].",1,neutral
"The image reconstruction and patch masking follow [12], the difference is that the masking probability is set to 1/3 for ease of implementation.",1,neutral
"Moreover, some works [2, 13, 40] have proven the masking mechanism is efficient to facilitate model to focus on the masked contents, and significantly enhancing the reasoning ability of model.",1,neutral
"The encoder and decoder of PA-former adopt the same structure following [11], in which the encoder transforms the input sub-sampled signal into latent representation and the encoder maps the latent representation back to the full-sampled data.",1,neutral
"Although preliminary studies have shown the promising application of image-based Transformer in classification [26, 15, 14], segmentation [41, 32, 49, 23] or detection [7, 50], transformers do not have inductive bias for images which means that they can not perform well on a small-scale dataset [8].",1,neutral
", raw pixel [16, 43], HOG features [42], visual tokens [2] from a pre-trained dVAE [33], visual tokens from a momentum model [48], etc.",1,neutral
Some masked image modeling methods such as MAE [16] adopt an asymmetric encoder-decoder architecture.,1,neutral
"Most recently, inspired by MAE [14], several concurrent works, e.g., M3AE [10] and VLC [13], transfer the pixel reconstruction task into VLP by simply adding pixel reconstruction tasks on top of VLP models.",1,neutral
"For example, BERT [17] formulates masked language modeling (MLM) to predict masked linguistic tokens, while MAE [14] and BEiT [2] formulate masked image modeling (MIM) to reconstruct raw pixels and dVAE visual tokens [29] of image patches respectively.",1,neutral
"Particularly, for the masked image tokens, instead of reconstructing low-level raw pixels [16] or predicting mid-level pre-defined visual tokens [2] (encapsulating mostly patch details, e.",1,neutral
Some masked image modeling methods such as MAE [14] adopt an asymmetric encoderdecoder architecture.,1,neutral
"For example, BERT [20] formulates masked languagemodeling (MLM) to predict masked linguistic tokens, while MAE [16] and BEiT [2] formulate masked image modeling (MIM) to reconstruct raw pixels and dVAE visual tokens [33] of image patches respectively.",1,neutral
"In self-supervised learning of images, popular MIM targets include raw pixels [16, 43], visual tokens [2] from a pre-trained dVAE [33], etc, as are displayed in the left side of Fig.",1,neutral
"Most recently, inspired by MAE [16], several concurrent works, e.",1,neutral
"Generally, pretraining a vision encoder by generative objectives (Bao et al., 2021; He et al., 2022) or discriminative objectives (He et al.",1,neutral
"However, without the supervision from other modalities, such pretrained models require task-specific finetuning (Bao et al., 2021; He et al., 2022; O Pinheiro et al., 2020; Wang et al., 2022) or linear probing He et al. (2020); Chen et al. (2020) for reasonably domainadapted predictions.",2,positive
"Generally, pretraining a vision encoder by generative objectives (Bao et al., 2021; He et al., 2022) or discriminative objectives (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al., 2021) at the scale of one to ten million images (Deng et al., 2009) is sufficient to yield good…",2,positive
"However, without the supervision from other modalities, such pretrained models require task-specific finetuning (Bao et al., 2021; He et al., 2022; O Pinheiro et al., 2020; Wang et al., 2022) or linear probing He et al.",2,positive
"The impressive recent progresses in unsupervised pretraining (Devlin et al., 2019; He et al., 2022) and contrastive learning (Wu et al.",1,neutral
"The impressive recent progresses in unsupervised pretraining (Devlin et al., 2019; He et al., 2022) and contrastive learning (Wu et al., 2018; He et al., 2020; Chen et al., 2020) are all closely related to this direction.",1,neutral
"For computational efficiency, we adopt an asymmetric MAE architecture (He et al., 2022) based on vanilla Vision Transformers (ViTs) (Dosovitskiy et al.",2,positive
"For computational efficiency, we adopt an asymmetric MAE architecture (He et al., 2022) based on vanilla Vision Transformers (ViTs) (Dosovitskiy et al., 2020): The encoder operates only on visible patches and the decoder on all the patches.",2,positive
"Masked (denoising) autoencoding (MAE) (Vincent et al., 2010; 2008) has resurged as the state-ofthe-art self-supervised image representation learning approach (He et al., 2022; Atito et al., 2021; Bao et al., 2021; Wei et al., 2022; Dosovitskiy et al., 2020; Li et al., 2021).",2,positive
"For each decoder head, specific positional embeddings are applied following MAE (He et al., 2022).",1,neutral
"Interestingly, MAE (He et al., 2022) demonstrates the strength of the straightforward idea of image patch reconstruction, in addition to improving the pretraining efficiency by adopting high masking ratios and encoding only unmasked patches.",1,neutral
"Our decoder is another vanilla ViT deployed on the union of the encoded patch set and a set of mask tokens (He et al., 2022).",2,positive
"Existing image MAE models focus on visual tokenization Bao et al. (2021); Dong et al. (2021), token masking strategy (Li et al., 2021; Atito et al., 2021), reconstruction target (Wei et al., 2022), and architectural efficiency (He et al., 2022).",2,positive
"Following (He et al., 2022; Feichtenhofer et al., 2022), the encoder is applied only on unmasked patches.",2,positive
"Our proposed MotionMAE adopts the asymmetric Transformer based masked autoencoder architecture (He et al., 2022), as depicted in Fig.",2,positive
", 2017) in computer vision, masked autoencoders (MAEs) have recently emerged as a general SSL framework (He et al., 2022; Bao et al., 2021; Radford et al., 2018).",2,positive
"Masked autoencoders With the increasingly wide adoption of Transformers (Vaswani et al., 2017) in computer vision, masked autoencoders (MAEs) have recently emerged as a general SSL framework (He et al., 2022; Bao et al., 2021; Radford et al., 2018).",2,positive
", 2022), and architectural efficiency (He et al., 2022).",2,positive
", 2010; 2008) has resurged as the state-ofthe-art self-supervised image representation learning approach (He et al., 2022; Atito et al., 2021; Bao et al., 2021; Wei et al., 2022; Dosovitskiy et al., 2020; Li et al., 2021).",2,positive
", 2021) and MAE (He et al., 2022) have also achieved impressive performances with much more scalable potential.",2,positive
"Recently, generative models such as BEiT (Bao et al., 2021) and MAE (He et al., 2022) have also achieved impressive performances with much more scalable potential.",2,positive
"This idea is inspired by recent studies of computer vision [7], [23] that engage semantic context learning by masking most parts of the input and predicting missing components.",1,neutral
"Amongst the standard models, MAE outperforms the ResNet50-based models on most measures.",2,positive
"Hence, the robustness of MAE cannot be solely attributed to the transformer backbone.",1,neutral
"MAE uses masked autoencoders (He et al., 2022) and we use its ViT-Large variant.",2,positive
"B we evaluate the transformer-based models MAE (He et al., 2022) and MOCOv3 (Chen et al.",2,positive
"B we evaluate the transformer-based models MAE (He et al., 2022) and MOCOv3 (Chen et al., 2021) as well as adversarially fine-tuned versions of MOCOv3 using the same three attacks as for MOCOv2.",2,positive
MAE also has some of the best robustness against targeted U-PGD attacks and is competitive to PixPro for the untargeted case.,2,positive
"…language processing (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020; Smith et al., 2022; Rae et al., 2021; Hoffmann et al., 2022a; Chowdhery et al., 2022; Kim et al., 2021a), automatic speech recognition (Baevski et al., 2020), and computer vision (He et al., 2022; Xie et al., 2022).",1,neutral
"As for segmentation, we use the ViT-base model provided by MMSegmentatation, which is pre-trained by MAE on ImageNet and then finetuned on the ADE20k dataset.",2,positive
"Based on transformers and masked image modeling, MAE [21] becomes a good alternative for pre-training.",1,neutral
", natural language processing [25, 34, 2], computer vision [4, 20, 21].",1,neutral
"Note that SimR101, SimR101 and MAEViT stand for Resnet101 pretrained by SimCLRv2, Resnet50 pretrained by SimCLRv2 and ViT-base-16 pretrained by MAE, respectively.",0,negative
"We craft pre-trained adversarial perturbations (PAPs) for three pre-trained models (i.e., Resnet50 by SimCLRv2, Resnet101 by SimCLRv2, ViT16 by MAE) and evaluate the attack success rates on ten downstream tasks.",2,positive
We report the results of SimCLR and MAE in Section 4.2.,0,negative
"Note that Resnet50 and Resnet101 [18] are pre-trained by SimCLRv2 [4], and ViT16 [56] is pre-trained by MAE [21].",0,negative
"We conduct extensive experiments on typical pre-trained vision models [4, 21] and ten downstream tasks.",2,positive
"Recent progress in self-supervised learning [9, 21, 24, 25] has resulted in methods that can extract informative visual representations without requiring any supervised labels.",1,neutral
"There are also further SSL approaches that are not limited to instance discrimination, but instead use information from nearest neighbors [17], prototype clustering [8], and image patch reconstruction [25].",1,neutral
"We fine-tune a stateof-the-art vision transformer model pretrained with masked autoencoding (He et al., 2021) for 100 epochs on rotated MNIST (Weiler and Cesa, 2019) (details in Appendix C.5).",2,positive
"We fine-tune a stateof-the-art vision transformer model pretrained with masked autoencoding (He et al., 2021) for 100 epochs on rotated MNIST (Weiler and Cesa, 2019) (details in Appendix C.",2,positive
"We use the base model and default finetuning procedure from (He et al., 2021), finetuning for 100 epochs, halving the learning rate on loss plateaus.",2,positive
"MAE (He et al., 2022) simplifies the pre-training pipeline by only encoding a small set of visible patches.",2,positive
"We opted to use a more flexible transformerbased visual architecture, which has recently achieved stateof-the-art results in computer vision tasks [11, 14], and language tasks [10, 31].",2,positive
"Recently, an in part due the successful usage of transformers in language [31] and vision tasks [11], such a pre-training strategy has been successfully applied to text [10] and vision tasks [14].",1,neutral
"Visual transformers are typically trained either with a supervised loss [11] or a maskingbased objective, followed by fine-tuning [14].",1,neutral
"97 Despite ViT models trained via the MAE framework yield improving performance in vision tasks as 98 model sizes grow [8, 15, 48], previous work [9] does not show improvement from switching a ViT99 Small model to the ViT-Base counterpart of 4x as many parameters.",2,positive
"Masked autoencoding [42, 15] 77 overcomes this issue and has shown superior performance on visual recognition tasks.",1,neutral
125 We pre-train the models via the MAE framework [15].,2,positive
MAE masks out random patches in an image and reconstructs the missing content with a vision transformer (ViT) [9].,2,positive
We leverage the masked 29 autoencoders (MAE) [15] that learn representations by masked prediction.,2,positive
"An important property of our visual pre-training approach is that uses a self-supervised objec194 tive [15] that makes minimal assumptions about the data distribution and does not rely on human195 designed proxy tasks, like data augmentations.",2,positive
At the core of our self-supervised representation learning approach is masked image modeling via the masked autoencoders (MAE) [16].,2,positive
"The training recipe closely follows [15], 126 with dataset specific settings from [9].",0,negative
"While the MAE-trained ViT models yield improving performance in vision tasks as model sizes grow [9, 16, 46], previous work [10] does not show improvement from switching a ViT-Small model to the ViT-Base counterpart of 4x as many parameters.",2,positive
"Simple and free from dataset or task-specific augmentations [41], MAE is the state-of-the-art self-supervised framework in computer vision [42, 43, 44, 45], and has been demonstrated to work well for motor control tasks in simulation as well [10].",2,positive
We pre-train the models via the MAE framework [16].,2,positive
2 Masked Visual Pre-training 90 At the core of our self-supervised visual representation learning approach is masked image modeling 91 via the masked autoencoders (MAE) [15].,2,positive
We train the MAE models for 400 epochs for the combined Ego dataset; 1600 epochs for the HOI dataset; and 1600 epochs for ImageNet dataset.,2,positive
"propose Masked AutoEncoder (MAE) for a largescale self-supervised pre-train [11], which can obviously enhance the performance of the purely attention-based model in vision.",1,neutral
"It is further MAE fine-tuned (He et al., 2021), using the same in-domain data as for the Mask R-CNN object detector.",2,positive
"To prevent catastrophic forgetting, VIMA only fine-tunes the last two layers of the language encoder with layer-wise learning rate decay (He et al., 2021) but freezes all other layers.",2,positive
"Our experiments follow the same architecture, settings, and pre-training recipe as MAE (He et al., 2022), and we find that the simple addition of a teacher (RC-MAE) consistently outperforms MAE in all model sizes (e.",2,positive
"(MAE) (He et al., 2022) is a self-supervised approach with a ViT encoder f and decoder h, which randomly masks a portion of input patches, and then reconstructs the masked patches given the visible patches.",2,positive
"Method Pre-Train Data APbox APmask Superivsed (He et al., 2022) IN1K w/ labels 47.",0,negative
"To validate the effectiveness of the EMA Teacher in RC-MAE, we compare RC-MAE to an otherwise identical MAE (He et al., 2022) baseline.",2,positive
"Inspired by the success of masked language modeling (MLM) pre-training in NLP, recent SSL approaches (Bao et al., 2022; Zhou et al., 2022; Xie et al., 2022; He et al., 2022; Assran et al., 2022) in the vision community have proposed forms of masked image modeling (MIM) pretext tasks, using ViT-based backbones.",2,positive
", 2022) based masked image modeling (MIM) approaches (Zhou et al., 2022; Bao et al., 2022; He et al., 2022; Xie et al., 2022; Assran et al., 2022) for computer vision tasks have been proposed.",2,positive
"For the reconstruction target, we use standard normalized patch pixels as done with MAE (He et al., 2022).",2,positive
"Pixel-level prediction (Chen et al., 2020b; Xie et al., 2022; He et al., 2022) learns visual representations by reconstructing masked input patches at the RGB pixel-level.",1,neutral
", 2022) and MAE (He et al., 2022) provide pixel-level reconstructions of masked patches, and lead to superior performance on dense prediction tasks such as object detection and segmentation.",2,positive
", 2021) and implementation details as MAE (He et al., 2022), we fine-tune Mask R-CNN (He et al.",2,positive
Which masking strategy would work better for depth estimation? ‚ MIM pre-training uses a relatively high mask ratio and mask size [9]–[11] due to more information redundancy ar X iv :2 21 0.,0,negative
"Instead, SiMMIM [10] and MAE [11] show that even random masking with a higher mask ratio or mask size can similarly perform well for selfsupervised pretraining from image data.",1,neutral
"MIM pre-training involves masking a portion of image patches and then using the unmasked portion to predict the masked input [10], [11] or its features [9], [12].",1,neutral
"Self-supervised learning (SSL) has shown great progress to learn informative data representations in recent years (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; Grill et al., 2020; Lee et al., 2021; Caron et al., 2020; Zbontar et al., 2021; Bardes et al., 2021; Tomasev et al., 2022; Caron et al., 2021; Chen et al., 2021; Li et al., 2022a; Zhou et al., 2022a;b; HaoChen et al., 2021; He et al., 2022), catching up to supervised baselines and even surpassing them in few-shot learning, i.",2,positive
"…Bardes et al., 2021; Tomasev et al., 2022; Caron et al., 2021; Chen et al., 2021; Li et al., 2022a; Zhou et al., 2022a;b; HaoChen et al., 2021; He et al., 2022), catching up to supervised baselines and even surpassing them in few-shot learning, i.e., when evaluating the SSL model from only a…",2,positive
"The predicted property can be the original pixels [15], latent representation [29], or visual tokens [6, 8].",1,neutral
"By randomly masking a portion of patches and optimizing the loss between reconstructed masked patches and real patches, MAE achieves state-of-the-art performance.",2,positive
We observe the ASR ranged from 66.34% to 99.18% on both MAE and CAE.,0,negative
"As MIM methods randomly mask a large portion of the input images, i.e., 75% in MAE, the trigger can be masked in the pre-training phase.",1,neutral
We then take MAE as the target model’s architecture and conduct comprehensive ablation studies to understand the impacts of important backdoor attack components in each supply chain’s phase.,2,positive
"Conventionally, after obtaining the released MAE model, Type II attacker would directly apply backdoor attacks on the encoder.",2,positive
"Concretely, for Type I and Type II attacks, as the adversary does not involve in the pre-training phase, we utilize the public MAE 3 and CAE 4 as our target model.",2,positive
"We compare the MAE performance of using AdamW, SGD, and LARS as the optimizer and find AdamW reaches the best clean accuracy (see Table 9).",2,positive
"We consider two MIM architectures as the target models, i.e., Masked Autoencoder (MAE) [15] for end-to-end MIM and Contextual Autoencoder (CAE) [8] for tokenizer-based MIM.",2,positive
VideoMAE: Masked Autoencoders are DataEfficient Learners for Self-Supervised Video PreTraining.,1,neutral
"Recently, with the advent of the Transformer architecture, masked image modeling (MIM), a generative method, has successfully surpassed contrastive learning and reached state-of-the-art performance on self-supervised pre-training tasks [6, 8, 15, 32].",1,neutral
"To promise the results are comparable, we adopt the same linear probing configurations in all three scenarios for both MAE and CAE.",2,positive
MAE-AST: Masked Autoencoding Audio Spectrogram Transformer.,2,positive
Mask is a key component of MAE.,2,positive
"For MAE, the batch size is 32, epoch is 200, mask ratio is 75%, and norm pix loss is False.",0,negative
MultiMAE: Multi-modal Multi-task Masked Autoencoders.,1,neutral
"Also, previous work [8, 15] leverages the pre-training dataset as the downstream dataset as well.",2,positive
", Masked Autoencoder (MAE) [15] for end-to-end MIM and Contextual Autoencoder (CAE) [8] for tokenizer-based MIM.",1,neutral
"The existing techniques used various forms of contrastive learning and different pretext tasks such as predicting the masked portion of the image [7], predicting",1,neutral
We find that threshold [3-7] produces the best results which shows that it is important to keep a balance between similar and dissimilar view-pairs.,1,neutral
"In the past, in single image setting, these included masked image modeling [7], object mask prediction [8], instance discrimination [17] and others.",1,neutral
"2) Specialized ViT backbone (BodyPressureSD distribution
via MAE).",2,positive
Our decoder must be initialized randomly because the MAE authors do not share their decoder model parameters.,2,positive
"For our third specialized model, we first pretrain on in-domain simulated data for 150 epochs, initializing our ViT encoder from MAE’s pretrained model parameters and our ViT decoder randomly.",2,positive
"We introduce steps 2 and 3 that leverage self-supervised MAE training and finetuning, respectively, using additional adult pose datasets.",2,positive
1) ViT backbone (ImageNet-1k distribution via MAE).,2,positive
MAE is an SSL algorithm that pretrains ViTs by encoding a small portion of patches (referred to as visible patches) and then employs a ViT decoder to reconstruct the hidden patches.,2,positive
"When pretraining with MAE, a smaller decoder size is typically chosen to reduce the computational costs of pretraining; specifically, they chose a decoder model width of 512 and two transformer blocks.",1,neutral
"3) Establish, for the first time, the benefit of a hierarchical pretraining strategy for pose estimation using MAE for ViT-based models.",1,neutral
[34] for 1600 epochs on the ImageNet-1k dataset.,1,neutral
"We leverage hierarchical pretraining by continuing to pretrain our ViT encoder on in-domain (i.e., fused depth and pressure) data using the MAE SSL algorithm.",2,positive
"fied by MAE [34], which we leverage in Section V.",0,negative
"This may be due to the relatively small dataset size of SLP, resulting in overfitting on the MAE task.",1,neutral
"For our first two specialized models, we pretrain on in-domain—either simulated or real—data for 150 epochs, initializing our ViT encoder from MAE’s pretrained model parameters and our ViT decoder randomly.",2,positive
We also demonstrate a masked autoencoding (MAE) hierarchical pretraining strategy for ViT that significantly improves accuracy on the SMaL dataset.,2,positive
"This is a popular class of SSL algorithms often referred to as reconstructive learning and is recently exemplified by MAE [34], which we leverage in Section V.",1,neutral
"art performance in image classification [34], [37], semantic",1,neutral
"Given that the ViT backbone is amenable to hierarchical MAE pretraining strategy, we can further improve the performance of the best-performing architecture, ViTPose.",2,positive
This asymmetric encoder–decoder design reduces pretraining FLOPs by approximately 3× [34].,1,neutral
"In more detail, MAE first encodes a randomly sampled 25% of image patches using a ViT encoder and holds the remaining 75% of patches aside.",2,positive
"In ViTPose, they initialize with MAE’s encoder; we use this as a baseline to compare with ViTPose models initialized with our three specialized encoders.",2,positive
"5(left), our method is significantly superior to MAEL, which has the best accuracy of all single deep networks.",2,positive
"Following the procedure described in previous section, we first constructed object embeddings based on ViT (MAE), CNN (DenseNet) and used kNN classifier (our first approach).",2,positive
"A. Classifier comparison
In the first round of experiments, we encode RGB and depth modalities of the object separately using ViT (MAE) and CNN (DenseNet) and assessed the performance of seven classifiers, including k-Nearest Neighbors (kNN) [50], Multi-layer Perceptron (MLP) [51], Support Vector Machine (SVM) [52], Decision Tree (DT) [53], Gaussian Process (GP) [54], Random Forest (RF) [55], and Gaussian Naive Bayes (GNB) [56], on the restaurant fine-grained object dataset.",2,positive
"In particular, the accuracy of the multimodal with ViT (MAE) and DenseNet reached 93.13%.",2,positive
", and ViTs (MAE+MAE-L).",1,neutral
We used MAE (RGB) + DenseNet (RGB-D) to represent each of the objects.,1,neutral
"Subsequently, Swin [34], DeiT [35], and MAE [36] were introduced for various computer vision tasks.",1,neutral
"The accuracy of our multimodal appraoch-II with DeseNet (RGB-D) and MAE (RGB) was 93.51%, which outperformed all single models, CNNs (Dense.+Mnas.)",2,positive
It can be observed that the computation time of the MAEL approach is generally higher compared to the CNN-based approach.,1,neutral
We find that existing theory designed to transfer knowledge of ConvNets trained in a self-supervised manner results in a significant performance gap between teacher and student.,2,positive
"Compared with ConvNet, the attention-based ViTs suffer less from an image-specific inductive bias and have a larger potential when training on large scale datasets.",1,neutral
"Moreover, existing selfsupervised knowledge distillation (SSKD) methods focus on ConvNet architectures and are suboptimal for ViT knowledge distillation.",2,positive
"For future work, we are interested to explore AttnDistill for knowledge distillation between ConvNets and ViT.",2,positive
5-(c)) : A recent work for distillation of selfsupervised representations of ConvNet is CompRess [1].,1,neutral
"Observing that the previous SSKD methods focussed on ConvNet do not work well on ViT, we proposed AttnDistill to distill the knowledge from a pretrained teacher model to its student model.",2,positive
"From the aspect of backbone architectures, the previous methods are all based on ConvNet [5, 8, 26].",1,neutral
A drawback of the attention mechanism is that it is tailored for transformer usage and requires additional computation when applied to ConvNets (namely the computation of the attention maps).,1,neutral
"We draw the following conclusions:
• Based on ViT-T/16 distilled from Mugs(ViT-S/16), our method AttnDistill gets state-of-theart k-NN and Linear probing performance compared with previous knowledge distillation methods based on ConvNet.",2,positive
"The potential of attention distillation has been explored for ConvNet [24, 71], however, since for these networks attention is not explicitly computed, additional computation and attention definition are needed.",1,neutral
"He et al., 2022), we avoid such learnable prefix design with random initialization and propose a parallel attention (PATT) to the original attention module (see Figure 3).",1,neutral
"He et al. (2022) analyzed the unified view among PETL techniques such as prefix-tuning, low-rank (LoRA) adaptation, and Adapter, pointing out the similarity between prefix-tuning and Adapter in terms of calculating the attention.",1,neutral
"He et al., 2022) and VideoMAE (Tong et al., 2022) to conduct further comparison on video and image datasets, which follows the self-supervised pre-training setting2 in S. Chen et al. (2022) except that the batch size is set to 256 instead of 1, 024.",0,negative
"He et al., 2022), etc. ST-Adapter adapts image-text models pre-trained on large scale datasets such as 400M image-text pair proposed by CLIP (Radford et al., 2021) and the IG-3.",2,positive
"He et al., 2022) in the NLP domain, we aim to propose a unified model for the vision domain, especially for video-based downstream tasks.",2,positive
He et al. (2022).,2,positive
"He et al. (2022); Houlsby et al. (2019) for PETL in NLP tasks, Adapter (S. Chen et al., 2022) has been directly used for vision tasks, showing promising performance using far less tunable parameters.",2,positive
"He et al., 2022; Tong et al., 2022).",1,neutral
"He et al. (2022); X.L. Li and Liang (2021) can be regarded as prepending contextual information for downstream tasks, which is similar to the pre-training process aiming to predict masked words in the process of an inner loop (Brown et al., 2020).",1,neutral
"He et al., 2022), fine-tuning VLP models do not lead to results as good as fine-tuning supervised pre-trained vision models.",1,neutral
"He et al., 2022; Tong et al., 2022), adding a prefix for a sentence input in NLP can be structurally different from the visual domain.",1,neutral
"In particular, contrastive learning [5, 18] and masked auto-encoding [12, 17, 48] are two popular self-supervised schemes.",1,neutral
"By removing the local inductive bias [19] from convolutional neural networks [28, 64, 60], vision transformers armed with global self-attention show superiority in scalability for large-scale models and billion-scale dataset [18, 84, 61], self-supervised learning [27, 76, 1], connecting vision and language [53, 34], etc.",1,neutral
Existing masked data modeling approaches include Context Encoders [21] and Masked Autoencoders (MAE) [12].,1,neutral
"conST [31] first concatenates gene expressions and the pre-extracted morphology features that are extracted from images via the Masked Auto-encoder (MAE) into a feature vector [32], which is then fed into a graph convolutional network to learn latent representations.",1,neutral
"Specifically, we use the weight from the original vision MAE model (He et al., 2022) (Weights from https://github.com/ facebookresearch/mae) with only self-supervised learning (SSL) pretraining for all audio, visual, and joint encoder and the decoder.",2,positive
", 2022) Given the success of MAE in the vision domain (He et al., 2022; Bachmann et al., 2022; Girdhar et al., 2022; Tong et al., 2022; Feichtenhofer et al., 2022), several efforts adapt MAE for audio with relatively minor changes to the overall pipeline (Baade et al.",2,positive
"…based on the Audio Spectrogram Transformer (Gong et al., 2021a) and Vision Transformer (Dosovitskiy et al., 2020) backbones, the single-modal Masked Auto-Encoder (MAE) (He et al., 2022) achieved state-of-the-art (SOTA) performance on images and audio tasks (Huang et al., 2022a) individually.",2,positive
"Specifically, we use the weights of the original vision MAE He et al. (2022).",2,positive
"Due to the space limitation, please refer to He et al. (2022); Huang et al. (2022a) for single-modal MAEs.",1,neutral
"…applied on visual and audio domains (Baevski et al., 2020; Hsu et al., 2021; Srivastava et al., 2022) Given the success of MAE in the vision domain (He et al., 2022; Bachmann et al., 2022; Girdhar et al., 2022; Tong et al., 2022; Feichtenhofer et al., 2022), several efforts adapt MAE for audio…",2,positive
This is mainly due to many previous MAE works reporting a masking ratio ∼75% is appropriate for both audio and visual input He et al. (2022); Baade et al. (2022); Huang et al. (2022a); Niizumi et al. (2022).,0,negative
These settings are identical to the original vision MAE He et al. (2022).,0,negative
", 2020) backbones, the single-modal Masked Auto-Encoder (MAE) (He et al., 2022) achieved state-of-the-art (SOTA) performance on images and audio tasks (Huang et al.",2,positive
"Based masked modeling as all autoencoder [18], MAE [6] uses the encoder to map the observed signal to the potential representation, and the decoder to reconstruct the original signal from the latent representation.",1,neutral
"These pre-trained masked modeling has been proved to be well applied to various downstream tasks, among which a simpler and more effective way is masked autoencoders (MAE) [6].",1,neutral
"The demand for large scale data processing has been solved by self-supervised pretraining in natural language processing (NLP) and computer vision (CV) fields [5], [6].",1,neutral
"Masked modeling encourages the model to infer the deleted parts according to the context information, so that the model can learn the deep semantics, which has become the benchmark of self-supervised pre-training in NLP and CV fields [5], [6].",1,neutral
"Implementation Detail For the image classification problem, we use Vision Transformer pretrained on unlabeled ImageNet dataset with masked auto-encoding (He et al., 2022) and fine-tune it on the downstream task with AdamW optimizer (Loshchilov & Hutter, 2019) for 10,000 steps with batch size 32.",2,positive
"Firstly, we remove masked auto-encoding objective LMAE and train the model with only distillation loss LDistill before fine-tuning.",2,positive
"Then we take the copy of the pre-trained model (fθinit , gϕinit) as a student, and match the representations of the student encoder and those of the teacher encoder while optimizing the student with the MAE on the target unlabeled data.",2,positive
"1 INTRODUCTION Pre-trained transformer models (Devlin et al., 2019; Brown et al., 2020; Liu et al., 2019; He et al., 2022) have been effective on various vision and natural language processing tasks.",2,positive
"Masked Auto-Encoding We briefly describe the masked auto-encoding objective (Liu et al., 2019; He et al., 2022) for a language model such as RoBERTA (Liu et al.",1,neutral
"In this work, we focus on the masked auto-encoding (Devlin et al., 2019; He et al., 2022) as a pre-training objective function since its generality compared to other self-supervised methods (Chen et al.",2,positive
"Masked Auto-Encoding We briefly describe the masked auto-encoding objective (Liu et al., 2019; He et al., 2022) for a language model such as RoBERTA (Liu et al., 2019) and Vision Transformer (ViT) (Dosovitskiy et al., 2021).",2,positive
"Specifically, we take the pre-trained model with an encoder fθinit and a decoder gϕinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fθ0 and gϕ0 .",2,positive
"Then we take the copy of the pre-trained initial network gϕinit ◦fθinit as a student and further pre-train the student with masked auto-encoding objective but enforce hidden representation of the encoder of the student fθinit to be close to that of the teacher fθ0 as follows:
(θ1, ϕ1) ∈ argmin θ,ϕ (LMAE(θ, ϕ;Du) + LDistill(θ; θ0,Du))
LDistill (θ; θ0,Du) = 1
n n∑ i=1 ∥∥∥fθ(x(i))− StopGrad(fθ0(x(i)))∥∥∥2 2
(3)
where θ and ϕ are initialized with the pre-trained parameters θinit and ϕinit, respectively and StopGrad denotes the stop-gradient operation which does not back-propagate through the input.",1,neutral
"Then the final objective for masked auto-encoding is defined as follows:
LMAE(θ, ϕ;Du) = 1
n n∑ i=1 Ez(i)∼pγ,T (z)
[ −
K∑ k=1 z (i) k Z(i) · log pθ,ϕ(x(i)k |x̂(i))
] , Z(i) =
K∑ k=1 z (i) k , (2)
where pγ,K(z) denotes a Binomial distribution with its parameters γ for probability that zk = 1 and K for the number of trials.",1,neutral
"In this experiment, we continue to pretrain the encoder of the pre-trained transformer (He et al., 2022) with supervised contrastive loss (Khosla et al.",2,positive
"Specifically, we take the pre-trained model with an encoder fθinit and a decoder gφinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fθ0 and gφ0 .",2,positive
"…model with an encoder fθinit and a decoder gϕinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fθ0 and gϕ0 .",2,positive
"In this work, we focus on the masked auto-encoding (Devlin et al., 2019; He et al., 2022) as a pre-training objective function since its generality compared to other self-supervised methods (Chen et al., 2020b; He et al., 2020; Grill et al., 2020; He et al., 2020; Chen & He, 2021; Caron et al.,…",2,positive
"In this experiment, we continue to pretrain the encoder of the pre-trained transformer (He et al., 2022) with supervised contrastive loss (Khosla et al.,
2020) while fine-tuning a randomly initialized linear classifier with cross-entropy loss.",2,positive
"Pre-trained transformer models (Devlin et al., 2019; Brown et al., 2020; Liu et al., 2019; He et al., 2022) have been effective on various vision and natural language processing tasks.",1,neutral
"For example, MAE-based [24] methods [17, 51] use pixel values of video frames as supervision by masking raw videos with an extremely high ratio and reconstructing them.",1,neutral
"For example, MAE-based [24] methods [17, 49] use pixel values of video frames as supervision by masking raw videos with an extremely high ratio and reconstructing them.",1,neutral
"Xiao et al. (2022) demonstrates that masked-autoencoder (He et al., 2022) trained on diverse video data (Goyal et al., 2017; Shan et al., 2020) can be an effective visual embedding for online RL.",2,positive
"(2022) demonstrates that masked-autoencoder (He et al., 2022) trained on diverse video data (Goyal et al.",2,positive
"Inspired by pre-training successes in computer vision (CV) (He et al., 2020; 2022) and natural language processing (NLP) (Devlin et al., 2018; Radford et al., 2019; 2021), pre-training visual representations on out-of-domain natural and human data (Deng et al., 2009; Grauman et al., 2022) has…",2,positive
"Most methods train for a constant factor more (sometimes an order of magnitude more) [7, 12] than their supervised counterparts, necessitate multiple forward passes per gradient step [6, 7, 12, 16, 32], or use expensive Transformer-based decoders that scale quadratically with image resolution [13].",1,neutral
"We address this gap by profiling four popular self-supervised methods (BYOL [12], SimCLR [7], DINO [6], and MAE [13]) and two supervised methods (CLIP [27] and standard classification).",2,positive
"[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"We use the MAE ViT [13] as the backbone in SimCLR, BYOL, and CLIP (using e.",2,positive
"1 Pre-training Methods and Models We run four common self-supervised methods (BYOL [12], SimCLR [7], DINO [6], and MAE [13]) and two supervised methods (CLIP [27] and standard softmax classification).",2,positive
"Similar to [13], we ran an exploratory learning rate (LR) search across {1e-5, 5e-5, 1e-4, 5e-4} and weight decay (WD) values {0.",2,positive
"BYOL [12], data2vec [2], DINO [6], and ODIN [16]), and masked autoencoding (MAE) [13, 36] are three of the best-known flavours of modern SSL for visual pre-training.",1,neutral
MIM pre-training forces the ViT to learn the local facial action units and global facial structures in various expressions [18].,0,negative
"Recently, several works (He et al., 2022; Fang et al., 2022; Wei et al., 2022; Chen et al., 2022; Xie et al., 2022; El-Nouby et al., 2021; Bao et al., 2022) also explored masked content prediction tasks for self-supervised representation learning.",1,neutral
"Luckily, such signals can nowadays be easily obtained with self-supervised learning algorithms, which have been successful in learning powerful representations for vision tasks such as classification and object detection purely from images (Chen et al., 2020b; Grill et al., 2020; He et al., 2022).",1,neutral
"4.3), we investigate several recent ones (DINO (Caron et al., 2021), MoCo-v3 (Chen et al., 2021), MSN (Assran et al., 2022), MAE (He et al., 2022)).",2,positive
"For the pre-trained weights, we use the timm library for DINO and third-party releases for MoCo-v35, MSN6, and MAE7.",2,positive
"This is interesting because MAE, the worst among the self-supervised methods with the Transformer decoder, yields the overall best results in terms of ARI on COCO object discovery: 42.3.",1,neutral
"To this end, we train DINOSAUR with a ResNet34 encoder (from scratch) on COCO, but reconstruct targets obtained from ViTs pre-trained with different methods: DINO, MoCo-v3, MSN, and MAE.",2,positive
"In the following, we use block 9 for supervised training, DINO and MSN, and block 12 for MoCo-v3 and MAE.",2,positive
Both characteristics contribute to the significant improvement compared to ConvNets on medical image segmentation Tang et al. (2022); Bao et al. (2021); He et al. (2022); Atito et al. (2021).,2,positive
"Except for the MAE method used ViT-Large [5], all other methods used ResNet-50 [7] as the backbone network.",2,positive
"Method Ours SKD BYOL SimSiam MAE Transfer From Scratch Accuracy 82.7% 74.2% 68.3% 66.8% 62.3% 53.9% 28.4%
Figure 3: Examples of real and distilled images.",0,negative
"For comparative methods, we used several SOTA self-supervised learning methods, including SKD [12], BYOL [6], SimSiam [2] and MAE [7].",1,neutral
"While most SSL methods require architectural or training strategies to avoid collapse (Grill et al., 2020; He et al., 2021), Bardes et al. (2022) and Zbontar et al. (2021) only require to modify the loss.",0,negative
"While most SSL methods require architectural or training strategies to avoid collapse (Grill et al., 2020; He et al., 2021), Bardes et al.",1,neutral
"While research in applying plain ViTs to dense vision tasks continues [15, 21], research into hierarchical vision transformers quickly became dominant [26,41] and continues to grow [13, 25].",1,neutral
"Recently, methods based on masked image modeling, such as SimMIM [41] and MAE [20], are proposed to improve the self-supervised ViTs training by masking a large ratio of patches.",1,neutral
"This is also inspired by MAE [20], i.e., the image can be reconstructed with only a few patches thanks to the powerful global attention ability of ViTs, which, if unconstrained, also makes the model more sensitive to some local patches during training.",2,positive
"First, the prosecution network and the defendant network pre-trained by MAE [13] are adopted to initialize CourtNet.",2,positive
"First, the prosecution and defendant networks pretrained by MAE [13] are adopted to initialize CourtNet.",2,positive
"In a contemporary work, He et al. (2022) showed that pretraining with a Masked AutoEncoder (MAE) objective (analogue of MLM objective for images) boosts the performance of ViT models on the Imagenet-1K dataset.",2,positive
"Self-pretraining in Computer Vision Most relevant to our work, recent/concurrent works in computer vision explore self-pretraining (He et al., 2022; El-Nouby et al., 2021).",2,positive
"For each training batch, we compute each objective through a separate forward pass and use the weighted sum of them for the final loss, where λVAM = 1.0 and λMAE = 0.3.
loss = λVAMlossVAM + λMAElossMAE (1)",2,positive
"(1), we use λVAM = 1.0 and λMAE = 0.3.",1,neutral
[26] that is pretrained on ImageNet [14].,0,negative
"Following MAE-AST [7], we randomly mask 75% of the spectrogram patches.",2,positive
"The challenge lies in the difference between text and acoustic signals; text is discrete and dense in information, while acoustic signals are continuous and sparse in information [26; 7].",1,neutral
"We calculate the mean squared error between the reconstructed and original video frames and spectrograms:
lossMAE = 1
NVM ∑ i∈masked ||xVi − x̂Vi ||22 + 1 NAM ∑ j∈masked ||xAj − x̂Aj ||22 (3)
whereNVM andN A M are the number of masked patches for vision and audio, respectively.",2,positive
"4.2, we use separate decoders (with shared weights) for the vision and audio MAE pretraining objectives.",2,positive
[26] and use a shallow decoder that only serves for masked autoencoding objective (Sec.,1,neutral
"Following MAE [26], we randomly mask 75% of the visual patches, and the masking is applied for each video frame independently.",2,positive
"In addition to the VAM objective to learn cross-modal representation, we also use the masked autoencoding (MAE) objective to improve unimodal representations in the vision-and-language settings, by masking random patches of visual frames and the audio spectrogram, and reconstruct missing inputs as shown in Fig.",2,positive
Table 11 shows that each of the pretraining objectives (MAE and VAM) improves finetuning performance over random weight initialization.,0,negative
"In Figure 3 and Figure 4, we show the reconstruction results with the MAE head, described in the main paper Sec.",0,negative
"The combination of VAM and MAE further improves the finetuning performance, and we use this configuration as default for TVLT pretraining.",2,positive
"(a) Using the state-ofthe art reconstruction-based SSL strategy, MAE [8] architecture for pre-training an representation extractor (encoder).",2,positive
"As indicated in [1, 7, 8] that SSL training requires a large amount of data, we begin with training a MAE on SSL Pre-Training Set.",0,negative
"1), we utilize a state-of-the-art reconstructed-based strategy, Masked Auto-Encoder (MAE) [8] to learn lowerdimensional semantic feature embedding.",2,positive
"These pretext tasks are created solely using the input features, such as predicting a missing image patch [8], recovering the color channels of an image from context [19], predicting missing words in texts [12], forcing the similarity of the different views of images [1, 7], etc.",1,neutral
"Motivated by theoretical implication of SSL embedding space, we leverage a Masked Autoencoder [8] for feature extraction.",1,neutral
"The structure of residual MLP is inspired by masked autoencoders (MAE)[22], which is used to reconstruct the random missing pixels.",1,neutral
MAE uses vision transformer (ViT)[23] as encoder to reconstruct random masked patches in a image.,1,neutral
"Researchers first borrow transformer as the base model [17, 53, 37, 56, 12], and then design novel self-supervised algorithms to pretrain them with the help of huge amounts of unlabeled data and modern hardware [10, 5, 30, 22, 8, 32].",1,neutral
[55] further verified the effectiveness of this class of methods by applying self-supervised masks to the computer vision domain.,1,neutral
", 2021), this has been primarily explored in the context of pretraining or designing auxiliary loss (Rasmus et al., 2015; Sabour et al., 2017; Chen et al., 2020; He et al., 2021).",1,neutral
"While the object-centric approach is known to improve the model’s generalization performance (Dittadi et al., 2021), this has been primarily explored in the context of pretraining or designing auxiliary loss (Rasmus et al., 2015; Sabour et al., 2017; Chen et al., 2020; He et al., 2021).",1,neutral
"In this section, we verify that GPaCo can enhance model generalization ability and robustness on full ImageNet and its variants when compared with MAE [14] models.",2,positive
(c) Comparison with MAE [14] ViT models on full ImageNet and out-of-distribution robustness.,2,positive
[14] justifies that masked autoencoders are scalable vision learners.,1,neutral
"To demonstrate the advantages of GPaCo on out-ofdistribution data, we load MAE [14] pre-trained weights and then fine-tune on full ImageNet with same training strategy as in [14].",2,positive
"These augmentations are suitable for usage with deep learning models Kaiming et al. (2022), Mathieu et al. (2015) and have been considered for oil drilling logging data Xingye et al. (2021).",2,positive
"…et al., 2017] that can exacerbate posterior collapse when used for decoding and self-supervised learning, where input (words in NLP, image patches in computer vision) are often masked randomly [Devlin et al., 2019, He et al., 2022], but a learnt policy might improve performance or convergence.",1,neutral
VideoMAE is a video transformer pretrained in a selfsupervised strategy inspired by ImageMAE [27] and proposed customized video tube masking and reconstruction.,2,positive
"We also experiment with the recently proposed masked auto-encoder framework [60], which is based on the ViT",2,positive
"Among the existing models, we select seven models [42], [44], [55], [58]–[60], [62] to show results before and after including SSPCAB and SSMCTB, respectively.",0,negative
"Inspired by the success of masked auto-encoders [60], Jiang et al.",1,neutral
"Another exception is the masked auto-enconder [60] based on the ViT backbone, where we place SSPCAB and SSMCTB before the first transformer block.",2,positive
"The reconstruction of masked information has recently become an attractive area of interest [60], [82]–[85].",1,neutral
"[60] proposed to reconstruct masked (erased) patches as a self-supervised pretext task for pre-training auto-encoders, subsequently using them for mainstream tasks, including object detection and object recognition.",1,neutral
"Although CoMo has unified three types of pre-training, our future direction will focus on integrating more existing pre-trained knowledge, such as the maskgenerated knowledge of MAE (He et al. 2022) and the 3Dcontrastive knowledge of CrossPoint (Afham et al. 2022).",2,positive
"Self-supervised learning is initialized as pretext tasks, such as image restoration from corruption (Vincent et al. 2008; Pathak et al. 2016; He et al. 2022), pseudo labels (Doersch, Gupta, and Efros 2015; Noroozi and Favaro 2016) and clustering (Caron et al.",1,neutral
"Self-supervised learning is initialized as pretext tasks, such as image restoration from corruption (Vincent et al. 2008; Pathak et al. 2016; He et al. 2022), pseudo labels (Doersch, Gupta, and Efros 2015; Noroozi and Favaro 2016) and clustering (Caron et al. 2018).",1,neutral
"Although CoMo has unified three types of pre-training, our future direction will focus on integrating more existing pre-trained knowledge, such as the maskgenerated knowledge of MAE (He et al. 2022) and the 3Dcontrastive knowledge of CrossPoint (Afham et al.",2,positive
"Mixed Feature Prediction Task Motivated by MAE [12] and Point-BERT [11], we wanted to extend this self-supervised method to point cloud segmentation.",2,positive
MAE [12] encoded incomplete patches with an autoencoder and reconstructed the original image through a lightweight decoder.,2,positive
"Mixed Feature Prediction Task
Motivated by MAE [12] and Point-BERT [11], we wanted to extend this self-supervised method to point cloud segmentation.",2,positive
"We pretrained five encoders, one using our proposed method TOV-VICReg and four using state-of-the-art self-supervised methods: MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021), VICReg (Bardes et al., 2022), and MAE (He et al., 2021).",2,positive
"For this study we selected DINO (Caron et al., 2021), MoCo (Chen et al., 2021), MAE (He et al., 2021), and VICReg (Bardes et al., 2022) since they are currently considered state-of-the-art, their official implementations are available in
PyTorch, and each represents a different type of approach.",2,positive
"Lastly, we also consider MAE (He et al., 2021), a masked reconstruction method, which consists in training an auto-encoder based on ViT to reconstruct an image with a set of patches masked.",2,positive
", 2021), MAE (He et al., 2021), and VICReg (Bardes et al.",1,neutral
"The procedures here can be summarized as follow:
y = ClassHead(FF (CLSL))
ViT and other models based on it have found much success in producing state-of-the-art results in image processing domains [23, 24] and have been the building block for many pre-trained models such as, to mention a few, DINO [25], MAE [26], and MOCOv3 [27].",2,positive
"ViT and other models based on it have found much success in producing state-of-the-art results in image processing domains [23, 24] and have been the building block for many pre-trained models such as, to mention a few, DINO [25], MAE [26], and MOCOv3 [27].",2,positive
"Recently, the vision Transformer has demonstrated excellent performances in supervised learning [11, 27, 37, 41], weakly supervised learning [10, 28], and self-supervised learning tasks [16, 38].",1,neutral
"On the other hand, with the release of ViT [7], more and more work apply vision Transformer to achieve better results in different vision areas [1, 10, 15, 16, 18, 24, 38, 39].",2,positive
"In particular, these outstanding studies in the field of weakly supervised localization [10, 28] and self-supervised learning [1, 16, 38] demonstrate the powerful local representation capabilities of tokens in vision Transformer models.",1,neutral
"Based on ViT, MAE [185] first masks random patches and tries to reconstruct them during training, which is a typical selfsupervised learning method.",2,positive
"We take Vit [38], MAE [185], and MoCo [186] in our experiments.",2,positive
"Besides, as Transformer-based methods have gained increasing attention for MedISeg in recent years, we also present experimental results on ViT [38], MAE [185], and MoCo [186] for comparison in this section.",2,positive
"To further reduce the encoder training time and improve robustness, a pre-trained encoder [27], [28] could be used and fine-tuned on our data.",2,positive
MAE [14] is an effective selfsupervised learning model processing images.,1,neutral
"Thus, we treat STMM as images and borrow the training approach of MAE [14].",2,positive
"To this end, we treat STMM as images and borrow the key principle of MAE [14] to process",2,positive
"3 Masked language/image models and the pretraining technique The design of NIERT is also inspired by the recent advances in masked language/image models [14], [15], [26], [27] and pre-trained models for symbolic regression [28], [29].",1,neutral
"More recently, masked image modeling (MIM) methods represented by MAE [11] have been proved to learn rich visual representations and significantly improve the performance on downstream tasks [18].",1,neutral
‘w/ SSL’ denotes training the segmentation network with the model weights pre-trained by a 3D Masked Autoencoder (MAE) SSL method [8].,1,neutral
"Recently, reconstruction-based SSL methods [8,28], which pre-train transformers for patch-level recovering with natural images.",1,neutral
"Implementation Details To pre-train the ASA model, we use center-cropping augmentation, Xavier uniform initializer [5] for SW-ViT blocks and set the hyper-parameters following [8] (see Table 1(a)).",2,positive
"To address this problem, the Masked Autoencoder (MAE) self-supervised training method [41] is used to pre-train the proposed model on unlabeled data.",1,neutral
"Further, the Masked Autoencoder pre-training method [41] is introduced.",1,neutral
"Moreover, whether to perform pixel normalization in the reconstruction loss [41] is compared.",1,neutral
"[41] used the idea of mask encoding from BERT [42], which is a self-supervised pre-training method for NLP.",1,neutral
"The best result is from ViT-Huge pretrained by MAE [19] and finetuned by DAT, which suggests DAT is also effective in downstream fine-tuning tasks.",1,neutral
"By combining MAE [19] pre-training and DAT fine-tuning, our ViT-Huge [20] achieves 31.40 mCE on ImageNet-C [2] and 32.77% top-1 accuracy on Stylized-ImageNet [3].",2,positive
"By combining MAE [19] pre-training and DAT fine-tuning, our ViT-Huge [20] achieves 31.",2,positive
"Besides, we use DAT to conduct supervised finetuning on downstream ImageNet classification task based on a self-supervised ViTHuge pretrained by MAE [19].",2,positive
"Furthermore, in 2021, the Facebook AI Research team led by Kaiming proposed the Masked Autoencoder(He et al. 2021) pre-train model in CV.",2,positive
"Particularly, the task embedding vectors are employed to perform convolution or attention operation together with the encoded image features [25].",1,neutral
"It is commonly acknowledged that masked image modeling (MIM) methods [55, 56, 57] achieve impressive performance in self-supervised learning.",1,neutral
"For the transformer-based CVmodels such as ViT [10], BEiT [2], masked autoencoders(MAE) [14], and swin-transformer [25], the local trigger information is more easily masked by the overall image due to the increased attention span, while the neurons that learn the backdoor features are easily lost in fine-tuning, so they have a certain ability to resist backdoor attacks.",1,neutral
"This paper adopts a purely Transformerbased backbone architecture using the dual-stream fusion with ViT-based grid features and BERT-based dynamic text features and three common pretext tasks (i.e., MLM, MIM, and ITM).",2,positive
"We perform the pre-training on three largescale medical image-text datasets, i.e., ROCO [40], MedICaT [44], and MIMIC-CXR [21].",2,positive
"For ROCO and MedICaT, we filter nonradiology samples, and for MIMIC-CXR, we only keep images in the frontal view.",2,positive
"As for the dataset split, we adopt the official splits of ROCO and MIMIC-CXR.",2,positive
"For pretext tasks, inspired by uni-modal pre-training schemes such as MLM [10, 33] and causal language modeling [6], existing studies explore a variety of pre-training tasks, including MLM [27, 35, 47], MIM [8, 35], ITM [27, 58], image-text contrastive [26] and prefix language modeling [51].",1,neutral
"In general, a VLP system consists of three elements: (i) uni-modal encoders (i.e., a vision encoder and a language encoder) that encode images and texts into image and text features, respectively; (ii) a multi-modal fusion module that performs the fusion of the encoded image and text features; (iii) pretext tasks (e.g., masked image modeling (MIM), masked language modeling (MLM), and image-text matching (ITM)) that assist the learning of VLP models.",1,neutral
"For the pretext tasks, we adopt (knowledge-enhanced) MLM, MIM [17], and ITM, where the masking ratios of MLM and MIM are set to 15% and 75%, respectively.",2,positive
"Pretext Tasks Given the aforementioned structure (denoted as M𝜃 ) with its parameters 𝜃 , the Med-VLP framework develops various pretext tasks (e.g., masked language modeling (MLM), masked image modeling (MIM), and image-text matching (ITM)) to guide the learning of 𝜃 .",1,neutral
"We first fine-tune a ViT-Large model [9, 14] on Places365 [54], which is dubbed PlacesViT.",2,positive
", with masked pretraining [14,12,30] or token pruning [26,18,36]).",0,negative
"Moreover, Hydra attention is a general technique that doesn’t make any assumptions about the relationships between tokens, so it can be applied to further improve the speed of token-sparse applications such as masked pretraining [14,12,30] or token pruning [26,18,36].",1,neutral
[13] claimed that languages are highly semantic and information-dense.,1,neutral
"It recently attracted attention as proxy task for self-supervised pre-training, especially for the ViTs [36, 37].",1,neutral
Salman et al. [15] propose to apply DRS to Vision Transfomers (ViTs).,1,neutral
"1 Introduction Advances in computer algorithms and hardware have made machine learning a great success in the fields of computer vision [1,2], data mining [3], medical diagnosis [4], cyber security [5], etc.",1,neutral
"As for the algorithm readiness, recent years have witnessed a great blossom for general vision, where Transformer [14], ViT [15, 16], Masked Auto-encoders (MAE) [17] and CLIP [18], etc.",2,positive
"As for the algorithm readiness, recent years have witnessed a great blossom for general vision, where Transformer [14], ViT [15, 16], Masked Auto-encoders (MAE) [17] and CLIP [18], etc., achieve impressive gain over conventional methods.",1,neutral
"Thus, for example, state-of-the-art Self Supervised Learning approaches [3, 5, 8, 9] (pre) train the latent representation on so-called pretext tasks that are related to but not identical to the downstream task for which the system is actually intended.",1,neutral
"In MaskFeat [38], authors use HOG [11], MoCo [20] and DINO [7] features to perform MIM; MVP [39] employs a multi-modality model, CLIP [31], which is pre-trained by rich image-text pairs.",2,positive
The architectural settings strictly follow [19].,0,negative
method mIoU mAcc ViT-B ViT-L ViT-B ViT-L supervised [19] 47.,1,neutral
"In addition to using the token obtained from offline or online model as reconstruct target, MAE [19], SimMIM [42],
and MaskFeat [38] achieve good performance in maskedimage reconstruction using low-level pixels or HOG [11] features.",2,positive
"As a special case of MIM, we formulate MKD upon which an empirical investigation is conducted about the influence of different target representations on self-supervised masked autoencoders.",1,neutral
"Masked Image Modeling (MIM) [2, 19, 38, 46] has recently become an active research topic in the field of visual representation learning and establishes strong performance for vision recognition tasks, e.g., image classification, object detection, and semantic segmentation, which also surpasses traditional supervised learning [35] mechanism.",1,neutral
"To answer these questions, we employ the standard masked autoencoder framework [19] to give a system-level study, introduced next.",2,positive
"MAE [19] uses image pixels as the target, which functions likewise to a randomly initialized teacher network, as demonstrated in Appendix B.",1,neutral
", DeiT [35] for supervised learning, DINO [7] for contrastive learning, DALL-E [32] for autoregressive generation, and MAE [19] for autoencoding.",1,neutral
"A crucial problem of MIM is how to choose the reconstructed target, i.e., T (·) in Eq.",1,neutral
"In addition to using the token obtained from offline or online model as reconstruct target, MAE [19], SimMIM [42], and MaskFeat [38] achieve good performance in maskedimage reconstruction using low-level pixels or HOG [11] features.",2,positive
"Nevertheless, it shows negligible gains on MIM-trained models such as MAE.",1,neutral
"To be specific, MIM randomly masks a portion of the input and then reconstructs the masked portion according to the transformed target, formulated as
min θ E x∼D M(T (x (1−M)), fθ(x M)), (1)
where “ ” means element-wise product; M is the patch mask; “x M” represents “unmasked patches” and vice ∗Equal contribution.",1,neutral
method ViT-B ViT-L ViT-H ViT-H448 supervised [19] 82.,1,neutral
"In this work, we paraphrase a term Masked Knowledge Distillation (MKD) to focus our discussion on a special case of MIM where the target is generated by a parameterized network (teacher network), i.e., T (·) = hφ(·).",1,neutral
method data2vec [2] BEiT [3] MAE [19] dBOT,2,positive
"Masked Image Modeling (MIM) [2, 19, 38, 46] has recently become an active research topic in the field of visual representation learning and establishes strong performance for vision recognition tasks, e.",1,neutral
And the painstaking selection of the target representations in the field of MIM.,1,neutral
", DINO [7] for contrastive learning, MAE [19] for masked autoencoding, DeiT [35] for supervised learning, and DALL-E [32] for autoregressive generation.",1,neutral
Concurrent work manages to adopt conventional feature distillation [40] to match contrastive models with MIM-trained ones.,1,neutral
"Similar to other MIM-based SSL work [19, 45], we reconstruct knowledge for those masked patches of input sample x .",2,positive
"For all patterns, the MIM methods SimMIM [45] and MAE [19] tend to group the patches with similar colors regardless of their semantic meaning.",1,neutral
"Following BEiT [3], MAE [19] develops an asymmetric encoder-decoder architecture to reconstruct the normalized masked patches.",2,positive
"MIM [45], MAE [19], and our MimCo on ImageNet-1K dataset.",2,positive
"Some methods [19, 45] propose to directly regress the raw pixels of the masked patches in a simple and effective way.",1,neutral
"What Semantic PatternsDoesMimCoLearn? To further help reveal what patterns does MIM learn, we follow the visualization of iBOT [47] to explore the learned patterns of the pre-trained models of SimMIM [45], MAE [19], and our MimCo via visualization, respectively.",2,positive
"Previous work [19, 45] have shown that the accuracy of linear probing is not always consistent with that of finetuning, especially for MIM-based pretraining methods.",1,neutral
"We randomly choose 10 classes of ImageNet-1K dataset to visualize for simplicity, the visualization of learned representation shows that our MimCo significantly improves the linear separability of representations compared to SimMIM [45] and MAE [19].",2,positive
Recent works in unsupervised learning have largely focused on removing inductive biases from the training process: transformer-based methods have successfully removed the scale-and-shift invariance from CNNs [18] and autoencoders have successfully removed the hardcoded augmentation-based invariances from contrastive learning methods [23].,1,neutral
"BEiT [52, 53, 54], Masked Autoencoder (MAE) [55], Contextual Autoencoder (CAE) [56], and Masked Image Modeling (MIM) [57] are other examples for visual representation learning by masking random patches of an input image and reconstructing the missing pixels.",1,neutral
", 2022), self-attention block (He et al., 2022), and convolutional projector (Yang et al.",1,neutral
"Furthermore, we choose different generative blocks, including cross-attention block (Chen et al., 2022), self-attention block (He et al., 2022), and convolutional projector (Yang et al., 2022e).",1,neutral
"The second way is a ‘self-attention’ block from MAE (He et al., 2022), which also includes 6 transformer layers.",2,positive
", [63, 64]) to get the pre-trained feature network.",1,neutral
"Different from existing works that focus on MVM for pure vision problems [4, 22, 86], we study MVM as a VidL pre-training task.",2,positive
"Meanwhile, self-supervised vision pre-training has been proven highly effective by reconstructing the masked image patches through raw pixel values [22, 78], discrete visual tokens [4, 86], or visual-semantic features [75, 76].",1,neutral
"Among the literature of vision pre-training itself, MAE [22, 67] and SimMIM [78] reconstruct the pixels of the masked image patches to enhance visual representation.",1,neutral
"While MVM has been explored in pure vision tasks [4, 22, 75], it remains an open question whether MVM can facilitate the interactions between video and language modalities.",1,neutral
"In addition, most of the existing Transformers [51], [52], [53] divide images or videos into nonoverlapping patches to generate tokens, resulting in loss of local details and cannot be well applied to video reconstruction tasks in this work.",1,neutral
"Recently, we have seen great success in natural language processing (NLP), as transformer models like BERT, GPT-3, RoBERTa, and other variants have achieved top performance on a wide array of language tasks.",2,positive
", ImageNet) processing, such as Image GPT, Swin transformer [13], and MAE [5].",1,neutral
This appetite for data has been successfully addressed in NLP by self-supervised pre-trained models such as BERT and GPT-3 [5].,1,neutral
"Similar models has been successful in producing strong features for natural images (e.g., ImageNet) processing, such as Image GPT, Swin transformer [13], and MAE [5].",1,neutral
"Conventionally, one would train an auto-encoder neural network comprising an encoder and a decoder, to obtain embeddings that contain summarized information of the encoder’s input [10].",1,neutral
", in 2021, showed that randomly masking pixels of an input image helps an autoencoder learn more robust embeddings, which would be useful for subsequent fine tuning tasks [10].",1,neutral
"As mentioned earlier, recent deep learning research has shown that incorporating self-supervised learning as part of the neural network’s training process can improve model performance [10][11][12].",1,neutral
"Recently, a new self-supervised learning approach named masked autoencoders (MAE) [15] demonstrates a strong generalization capability with remarkable performance in computer vision tasks.",1,neutral
"Inspired by the MAE [15], we propose a spatial-temporal masked autoencoder framework for self-supervised 3D skeleton-based action recognition (SkeletonMAE).",2,positive
"However, MAE [15] can not be directly utilized for selfsupervised skeleton action recognition due to the following reasons:",1,neutral
• The Vision Transformer (ViT) [10] architecture is used in MAE [15] to process the image input.,1,neutral
"The Transformer architecture [18] is now ubiquitous in Natural Language Processing [19, 20, 21, 22], and is also gaining traction in Computer Vision [23, 24], as well as in Offline Reinforcement Learning [25, 26].",1,neutral
"%) of reconstruction achieves the peak of classification performance, which is as high as the masking ratio of Image MAE [9].",1,neutral
"The high masking ratio (75%) of reconstruction achieves the peak of classification performance, which is as high as the masking ratio of Image MAE [9].",1,neutral
"In particular, some recent research on the field of SSL [4, 9, 10] has shown excellent results, yet self-supervision alone is still insufficient due to its limited practical applicability.",1,neutral
"In the field of computational vision, especially in the area of image classification and image generation, PixelCNN [34], VQ-VAE-2 [28], and MAE [9] successfully used the whole input image as the self-supervised target.",1,neutral
This paper links the inductive semi-supervised algorithm (e.g. Pseudo Label [13]) with the generative self-supervision (e.g.MAE [9]).,1,neutral
"Compared to generative SSL, the motivation of contrastive SSL is to measure the similarity of different inputs (e.g., mutual informationmaximization and instance discrimination).",1,neutral
The generative SSL trains a generator consisting of an encoder and decoder to reconstruct the input data.,2,positive
"Like what ImageMAE does in [9], we directly discard a subset (e.g., 50",1,neutral
"As for the generative-contrastive SSL, most works focus on learning knowledge from unlabeled data with generative adversarial networks.",1,neutral
"Like what ImageMAE does in [9], we directly discard a subset (e.",1,neutral
"Besides, Self-Supervised Learning (SSL) is also a powerful learning framework that exploits the generalizable representations from unlabeled data.",1,neutral
"Through Liu’s research [21] on SSL, its main methods can be divided into three categories: generative SSL, contrastive SSL, and generative-contrastive SSL.",1,neutral
"We propose the MAE-VQGAN model, which combines ideas from MAE [20] and VQGAN [15].",2,positive
"Based on the recent success of Vision Transformers (ViTs) [13], multiple works have proposed to hole-filling a self-supervised pretext task for ViTs [1, 20, 54].",1,neutral
"For example, in MAE [20], the goals is to reconstruct the image given a small subset of input patches.",1,neutral
"During training, an input image is patchified, masked and fed into an MAE [20].",1,neutral
"While N3F can work on top of any 2D dense image features, including recent ones based on Vision Transformers (ViT) [11] and variants [2, 5, 14, 19, 31, 58, 59, 68, 76, 81], of particular interest are self-supervised versions such as [4, 8, 17, 28] as they are more generically applicable and can benefit more from the consistency induced by N3F.",1,neutral
as self-supervised learning [53] and foundation models [54].,1,neutral
"Considering that the continuous data flow in each vehicle brings massive unlabelled data, self-supervised learning [53] and",1,neutral
"Recently, to encourage learning useful features from images with spatial redundancy, MAE [8] presents a simple strategy to reduce redundancy by masking a very high portion of random patches.",1,neutral
"The intuitive strategy of combating spatial redundancy is to apply redundancy removal approaches to pre-process images before being input into the networks, including PCA [6], DCT [7], etc. Recently, to encourage learning useful features from images with spatial redundancy, MAE [8] presents a simple strategy to reduce redundancy by masking a very high portion of random patches.",1,neutral
"Among vision learners, vision transformers (ViTs) [9] enjoy the ability to capture intra-image long-range dependencies and exhibit impressive performances, as MAE [8] uses ViTs as encoders.",1,neutral
"…our lexiconbottlenecked masked autoencoder (LexMAE) contains one encoder and one decoder with masked inputs in line with the masked autoencoder (MAE) family (He et al., 2021a; Liu & Shao, 2022), while is equipped with a lexicon-bottlenecked module for document-specific lexicon-importance learning.",2,positive
"Recently, pre-trained language models (PLM), e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), DeBERTa (He et al., 2021b), have been proven generic and effective when transferred to a broad spectrum of downstream tasks via fine-tuning.",2,positive
"We also compared our model with the few-shot counting sota method Fam-
Net [24], and our model outperforms it with a large advance (15.16 MAE and 24.29 RMSE on Val-COCO and 11.87 MAE and 14.81 RMSE on Test-COCO), which demonstrates the superiority of our model.",2,positive
"We use two standard metrics to measure the performance of our model, namely, Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).",2,positive
"MAE = 1 NI
NI ∑ i=1 |Ci−CGTi |, RMSE = √√√√ 1 NI NI ∑ i=1 (Ci−CGTi )2 (6)
Here, NI is the total number of testing images, and Ci and CGTi are the predicted number and ground truth of the ith image.",1,neutral
"In this section, we aim to give the detail of our proposed two-stage training procedure, that is, first pre-train the ViT encoder with MAE [11], and then fine-tune the whole model on supervised object counting.",2,positive
"We propose a two-stage training scheme, with the transformer-based image encoder being firstly pre-trained with self-supervision via masked image modeling [11], followed by supervised fine-tuning for the task at hand.",2,positive
"# Shots Val Test
MAE RMSE MAE RMSE
A0 % % % % 0 24.84 86.",0,negative
"As input for pre-training ViT with MAE, we randomly drop 50% of the visual tokens, and task the model to reconstruct the masked patches with pixel-wise mean square error.",2,positive
MAE Pre-training.,0,negative
Self-supervised Pre-training with MAE.,0,negative
"1 Training Details In this section, we aim to give the detail of our proposed two-stage training procedure, that is, first pre-train the ViT encoder with MAE [11], and then fine-tune the whole model on supervised object counting.",2,positive
"Specifically, we adopt the recent idea from Masked Autoencoders (MAE), to train the model by image reconstruction with only partial observations.",2,positive
"In Section 3.2, we further introduce a two-stage training regime, in which the model is firstly pre-trained with self-supervision via masked image reconstruction (MAE), followed by fine-tuning on the downstream counting task.",2,positive
"Many work in self supervised learning (SSL) have been proposed [10, 30], in particular, the methods based on masked image reconstruction have achieved SOTA results [6, 29].",1,neutral
"The two methods are based on the recent Masked Auto Encoder (MAE) approach [26], where we randomly mask the image ( e.g. , 75",1,neutral
"The two methods are based on the recent Masked Auto Encoder (MAE) approach [29], where we randomly mask the image (e.",1,neutral
"We also exploit more efficiently the input data with Masked Image Modeling, based on the recently proposed Masked Auto Encoder (MAE) [26].",1,neutral
"We also exploit more efficiently the input data with Masked Image Modeling, based on the recently proposed Masked Auto Encoder (MAE) [29].",1,neutral
"We propose a new efficient VLP approach centered on 3 main components; stronger Vision-Language pre-alignment through hierarchical contrastive objective, self supervision via masked image modeling based on MAE, and a new Visual Concepts injection and extraction technique.",2,positive
"We favor the MAE based, unimodal MIM loss which improves the results by 2.4% RSUM.",0,negative
"Unsupervised Learning Techniques Among the most popular unsupervised learning techniques for images, there are exist autoencoders [29], sparse coding networks [30], and Generative Adversarial Networks (GANs) [31].",1,neutral
"Based on the above insight, inspired by the Vision Transformer, which is entirely based on a self-attentive mechanism in machine vision tasks [5],and its good performance in the Masked Auto-Encoder task [6],a network structure named Padded Auto-Encoder for reactor monitoring parameter feature extraction is proposed, taking into account the actual needs in reactor accident diagnosis.",2,positive
"Generative methods [2,16,36] try to reconstruct the original input to learn meaningful latent representation.",1,neutral
"Predicting the enhancing methods utilized for images, predicting the relative placements of image blocks, recoloring sample grayscale maps [21], and restoring the missing parts of images to complete them [22] are some of the gen-",1,neutral
SimMIM [27] and MAE [11] both propose to directly reconstruct the pixel values of the masked image patches.,2,positive
"With the most recent representative work in this direction, masked autoencoder (MAE) [11], we are now able to efficiently and effectively pre-train high-capacity Vision Transformers (ViTs) with strong feature representations, leading to state-of-the-art solutions for a wide range of downstream visual tasks.",2,positive
"Following MAE [11], we first perform self-supervised pre-training on ImageNet-1k [7].",2,positive
We choose a decoder depth of 8 as the default setting as in [11].,2,positive
"Similar to MAE [11], the final performance gets (slightly) increased with a deeper decoder.",2,positive
"Following the setting in MAE [7], we add a batch-norm layer without learnable affine parameters before the classifier to avoid adjusting the learning rate for each model.",2,positive
"CLIP
+
SimCLR
CLIP
+ MAE
Sandwich bread
MaskCLIP
(Ours)
CLIP
Bird
MaskCLIP
(Ours)
CLIP
CLIP
+
SimCLR
CLIP
+ MAE
SnowMountain goats Santa hatBearded Man
BandanaDog",0,negative
We start from CLIP+MAE and add three components of the distillation loss one by one.,2,positive
"Progress in self-supervised pretraining in computer vision has led to improvements on a wide range of transfer learning tasks such as image classification, object detection, and semantic segmentation [6, 8, 25, 26, 28].",1,neutral
We adopted the MAE structure proposed in [14].,2,positive
"Recently, masked autoencoder (MAE) [14] has been proposed, which combines autoencoding and pixel masking to learn discriminative image features.",1,neutral
"8: In pixel-level pretext tasks (§3.2.1), the aim is to reconstruct the original image x̂ from a corrupted input x.
corrupted input images, as represented by three standard low-level image processing tasks: (1) image inpainting [126], [127] learns by inpainting the masked-out missing regions in the input images, which is also known as masked autoencoders (MAE) [127]; (2) denoising [128] learns to denoise the partial destructed input; and (3) colorization [129], [130], [131] aims to predict the colour values of the grayscale images.",1,neutral
"Pretext tasks Pixel-level reconstruction [124], [125], inpainting [126], MAE [127], denoising [128], colorization [129], [130], [131]",1,neutral
"corrupted input images, as represented by three standard low-level image processing tasks: (1) image inpainting [126], [127] learns by inpainting the masked-out missing regions in the input images, which is also known as masked autoencoders (MAE) [127]; (2) denoising [128] learns to denoise the partial destructed input; and (3) colorization [129], [130], [131] aims to predict the colour values of the grayscale images.",1,neutral
"Pixel-level pretext task is generally designed as a dense prediction task that aims to predict the expected pixel values of an output image as a self-supervision signal [124], [125], [126], [127], [128], [129], [130], [131].",1,neutral
"Families of Models Model Rationale Representative Strategies and Methods
Pretext tasks Pixel-level reconstruction [124], [125], inpainting [126], MAE [127], denoising [128], colorization [129], [130], [131] Instance-level predict image rotations [123], scaling and tiling [122], patch ordering [11], patch re-ordering [121]
Discriminative models Instance discrimination negative sampling large batch size (SimLR [12]), memory bank (InstDis [132]), queue (MoCo [16]) input transformation data augmentation (PIRL [133]), multi-view augmentation (CMC [134]) negative-sample-free simple siamese (SimSiam [135]), Bootstrap (BYOL [136]), DirectPred [137]
Deep clustering offline clustering DeepCluster [138], JULE [139], SeLa [140] online clustering IIC [141], PICA [142], AssociativeCluster [143], SwAV [144]
Deep generative models Discriminator-level DCGAN [145], Self-supervised GAN [146], Transformation GAN [147] Generator-level BiGAN [148], BigBiGAN [149]
✓",1,neutral
"Following the previous work [9], we take ViTB [57] as the backbone network, which consists of 12 transformer layers and was pre-trained on ImageNet-21K with the self-supervised method MAE [21].",2,positive
"Note that the patch-level strategy is essentially similar to [8], which was proposed to analyse 2D images.",1,neutral
"Different from the 2D approach [8], a dual-level masking strategy is adopted to explicitly extract features in both the spatial and temporal dimensions.",1,neutral
"Inspired by [8], we design a decoder using a lightweight transformer structure [8] and is only used during the pre-training phase.",2,positive
"For image encoder, we employ a standard ViT-B/16 (Dosovitskiy et al. 2020) with random masking strategy (He et al. 2022).",2,positive
"Following MAE (He et al. 2022), we randomly divide the patches",1,neutral
"Following (He et al. 2022), we take a 4-layer transformer with 512 dim as our image decoder.",2,positive
"MAE (He et al. 2022) and SimMIM (Xie et al. 2022) predict pixel RGB values directly to promote image pre-training, achieving even better performance than complicatedly designed token classification methods.",2,positive
"Unlike MAE (He et al. 2022), we aim to reconstruct the invisible patches with both image and text features to facilitate multi-modal information fusion.",2,positive
"Motivated by recent generative methods in visual self-supervised learning (He et al. 2022; Chen et al. 2022), we propose to introduce pixel-level reconstruction task to VLP.",2,positive
"Following MAE (He et al. 2022), we adopt a lightweight vision transformer as our image decoder.",2,positive
"Following MAE (He et al. 2022), we randomly divide the patches into visible patches { xvisi }N−M i=1
and invisible patches{ xmski }M i=1
according to mask ratio α, where M = αN .",1,neutral
"Recently, with asymmetric encoder-decoder architecture, masked autoencoders (MAE) (He et al. 2022) exhibits high generalizability and remarkable performance in vision tasks.",1,neutral
"According to the finding in MAE (He et al. 2022), unlike contrastive learning models, generative learning models are not prone to saturation in training, and a longer training schedule can further boost model performance.",1,neutral
"The autoencoder architectures [74] based on FCNs achieved encouraging performance on image segmentation tasks and have been widely used in the field of medical image segmentation [5][11], target detection [8] and video object segmentation [55].",1,neutral
", through learning invariant mapping (DrLIM), It is used to learn nonlinear functions of global coherence[7] , paving the way for Kaiming He to propose an expanded self-supervised learning scheme Masked AutoEncoders(MAE)[8].",1,neutral
"[51] proposed an asymmetric encoder-decoder architecture, which the encoder operates only on a subset of visible patches (tokens without masks).",1,neutral
BMM shares a similar idea with MAE [13] and BEIT [2] in that both learn better representation through random masking.,1,neutral
MAE and BEIT aim at image reconstruction using an autoencoding style.,2,positive
"The above method shares a similar idea to Random Erasing [50], MAE [13], and BEIT [2].",1,neutral
"On the other hand, some self-supervised learning-based methods, such as MoCo [48], BYOL [49], and MAE [50], can also alleviate the requirement of large-scale training data.",1,neutral
"The other two better performing transformer architectures are MixMIML and MAE (ViT-H), which have ∼300M and ∼600M parameters and perform at 83.9% and 88.3% respectively.",2,positive
"We compared the performance with other baselines involving Vision Transformers such as ConViT [16], Masked Auto Encoders (MAE) [53], Convolution-enhanced image Transformer (CeiT) [33], LeViT [34].",2,positive
"Additionally, examining different training strategies, such as the masked-autoencoder discussed in [9], might also be beneficial.",1,neutral
", masked autoencoding) has achieved great success for representation learning in the fields of computer vision [56], [57], [58], natural language processing [52], [59], and speech signal processing [60], [61].",1,neutral
", only utilize local neighbor information to accomplish this task instead of inferring global semantics) [17], [58], [62].",1,neutral
"Inspired by (He et al. 2022; Wettig et al. 2022) that a large mask rate may be beneficial for pre-training, we employ a mask rate greater than the BERT’s 15% setting.",2,positive
"Inspired by (He et al. 2022; Wettig et al. 2022) that a large mask rate may benefit pre-training, we adopt up to 30% and 45% mask rate in CoT-MAE’s encoder and decoder, respectively.",2,positive
"Thus, borrowing insights from masked autoencoders [33], we believe most of the points can be redundant and we randomly masked the points with a probability of 75% per visit.",2,positive
", 2020) and has also helped excel at various computer vision tasks (Goyal et al., 2021; Ramesh et al., 2021; He et al., 2022).",2,positive
"This new paradigm has had great success in advancing NLP (Devlin et al., 2019; Conneau et al., 2020; Brown et al., 2020) and has also helped excel at various computer vision tasks (Goyal et al., 2021; Ramesh et al., 2021; He et al., 2022).",2,positive
"The longsequence prediction data set adopts the evaluation indicators of the Mean Squared Error (MSE) and Mean Absolute Error (MAE), and the short-sequence data set adopts the evaluation indicators of the Root Relative Squared Error (RSE) and Empirical Correlation Coefficient (CORR).",2,positive
2020) in Natural Language Processing (NLP) or MAE (He et al. 2022) in computer vision.,1,neutral
"In the long series prediction, we employ Mean Squared Error (MSE) and Mean Absolute Error (MAE) as indicators and follow the setting (Zhou et al. 2021).",1,neutral
"…deep learning models add more functional modules or modify the training process to improve accuracy, which is representative of the trend of self-supervised or unsupervised learning, such as GPT-3 (Brown et al. 2020) in Natural Language Processing (NLP) or MAE (He et al. 2022) in computer vision.",1,neutral
"The above deep learning models add more functional modules or modify the training process to improve accuracy, which is representative of the trend of self-supervised or unsupervised learning, such as GPT-3 (Brown et al. 2020) in Natural Language Processing (NLP) or MAE (He et al. 2022) in computer vision.",2,positive
"The formula is as follows: • Mean Squared Error (MSE):
MSE = 1
n n∑ i=1 (Yi − Ŷi)2
• Mean Absolute Error (MAE):
MAE = 1
n n∑ i=1 |Yi − Ŷi|
In the short series prediction, we employ Root Relative Squared Error (RSE) and Empirical Correlation Coefficient (CORR) as indicators and follow the setting (Lai et al. 2018).",2,positive
"In the absence of prior work applying masked autoencoding to abstract/synthetic imagery, we explore a large masked pixel percentage (75% of the image), thus forcing the model to attempt to recover the image based only on the unmasked 25% of the image, such high percentages have been shown to work well for natural imagery [30].",2,positive
Recent work [30] has shown that masking a large portion of pixels in natural images (such as ImageNet) leads to a challenging self-supervisory task capable of generating useful representations for downstream tasks.,1,neutral
[13] constructed a scalable self-supervised learner which masked random patches of the input image and reconstructed them.,1,neutral
"In CV, both masked autoencoders [15] and contrastive learning [31], with different architectures, obtain promising results.",1,neutral
"ResNet [7], ConvNext [8], ViT [9], 16 Swin [10], MAE [11], Transformer-XL [12] and BERT [13].",2,positive
"All these results show that in most cases, Adan can use half of the training cost (epochs) of SoTA optimizers to achieve higher or comparable performance on ViT, ResNet, MAE, etc.
NLP Results.",2,positive
%) of ViT-B and ViT-L trained by selfsupervised MAE on ImageNet.,0,negative
"MAE-ViT-B MAE-ViT-L Epoch 300 800 1600 800 1600
AdamW 82.9 — 83.6 85.4 85.9 Adan 83.4 83.8 — 85.9 —",0,negative
"More surprisingly, Adan can use half of the training cost (epochs) of SoTA optimizers to achieve higher or comparable performance on ViT, ResNet, MAE, etc, and also shows great tolerance to a large range of minibatch size, e.g. from 1k to 32k. Code is released at https://github.com/sail-sg/Adan.",2,positive
"Extensive experimental results show that Adan surpasses the corresponding SoTA optimizers for vision, language, and RL tasks and sets new SoTAs for many popular networks and frameworks, e.g. ResNet [7], ConvNext [8], ViT [9], Swin [10], MAE [11], Transformer-XL [12] and BERT [13].",2,positive
"2) self-supervised settings: we follow the MAE training framework to pretrain and fine-tune ViT-B and ViT-L, and report results in Table 3.",0,negative
"As shown in Table 3, compared with MAE (He et al., 2022), BEIT V2 achieves dramatic gains across datasets, demonstrating the superiority of the proposed method in terms of model generalization.",2,positive
"Existing MIM approaches can be coarsely categorized to three according to the reconstruction targets: low-level image elements (e.g., raw pixels; He et al. 2022; Fang et al. 2022; Liu et al. 2022), handcrafted features (e.",2,positive
"Masked image modeling (MIM), which greatly relieves the annotation-hungry issue of vision Transformers, has demonstrated great potential in learning visual representations (Bao et al., 2022; He et al., 2022).",1,neutral
"MAE (He et al., 2022) treated MIM as a denoising pixel-level reconstruction task.",1,neutral
"Existing MIM approaches can be coarsely categorized to three according to the reconstruction targets: low-level image elements (e.g., raw pixels; He et al. 2022; Fang et al. 2022; Liu et al. 2022), handcrafted features (e.g., HOG features; Wei et al. 2021), and visual tokens; Bao et al. 2022; Wang…",2,positive
All the compared methods are based on ViT-B/16 and pretrained for 300 epochs except MAE for 1600 epochs.,0,negative
"Table 2 presents the top-1 accuracy for linear probing and compares BEIT V2 with recent methods including BEIT, CAE, MAE, MVP and MoCo v3.",2,positive
VideoMAE [13] further extends MAE to video and shows data-efficient learners for self-supervised video pre-training.,2,positive
"Specifically, motivated by the autoencoding paradigm in BERT [6] in NLP, MAE adopts an asymmetric encoder-decoder architecture with visible patches encoding in the encoder and masked token reconstruction in the decoder.",2,positive
"To obtain the benefits of self-supervised learning in video classification, we utilize VideoMAE pre-trained in Something-Something [8] to extract video features inherent in make-up scenes.",2,positive
2.1.2 MAE-based Features.,2,positive
"As the existing pre-trained models, e.g., VideoSwin [11] and VideoMAE [13], are pre-trained on benchmarks like action recognition, the extracted video features are inevitably more suitable to general-purpose action scenes and not optimal for the specialized-purpose make-up scenes.",1,neutral
"Among them, Masked Autoencoders (MAE) [9] demonstrate superior learning ability and scalability.",1,neutral
"Recently, the pre-training and fine-tuning paradigm [30, 31, 32, 33] achieves promising results.",2,positive
"The reason for lower performance on STL-10 might result from the usage of the self-supervised pre-trained model [33], rather than the supervised pre-trained model is used in other settings.",1,neutral
The models are self-pretrained by MAE[24].,0,negative
"Effect of Self-pretraining The self-pretraining of MAE [24] has a substantial boost in performances, as seen in Table 3.",1,neutral
"When self-pretrained, MAE [24] is mainly used, and we directly use their pretrained models.",2,positive
"When the model is self-pretrained by MAE [24], we first evaluate the fine-tuning performances of MAE on the labeled data only, as the common practice in self/un-supervised learning literature [25, 14, 22], with results shown Table 1.",1,neutral
"In the past few years, Vision Transformers (ViT) [18], which adapt the transformer architectures [60] to the visual domain, have achieved remarkable progresses in supervised learning [59, 41, 69], un/self-supervised learning [16, 12, 24], and many other computer vision tasks [11, 19, 1, 54] (with architecture modifications).",1,neutral
"To ensure the representation quality of the pretrained model, previous methods [2, 25, 56] usually require very long pretraining epochs.",1,neutral
"Full fledged transformer blocks are used in the decoder of MAE [25] to reconstruct masked input patches pixel by pixel, whereas lightweight linear layer is adopted in the decoder of MaskFeat [56] to reconstruct local features of the image.",1,neutral
"Following the training recipe provided by MAE [25], the ViT models pretrained on ImageNet1K dataset serve as the backbone of UperNet [59], and are finetuned together with the segmentation layers.",2,positive
We use a masked autoencoder architecture similar to MAE [25].,2,positive
", grid, block, random) of the input image patches affect the final performance of masked image pretraining [25, 61].",1,neutral
"Following MAE [25], the pretrained ViT backbones are adapted to FPN [36] in the Mask R-CNN framework [27], which is finetuned end-to-end on COCO training set to produce the bounding boxes (evaluated by box AP) and the instance masks (evaluated by mask AP) simultaneously.",2,positive
MAE [25] uses a deep decoder that not only updates the mask tokens but also enhances the features of the unmasked patches.,2,positive
"Majority of prior arts [2, 3, 9, 18, 25, 56, 61] sample the masked patches uniformly at random since it is unbiased and can guarantee coverage.",0,negative
"Using the default 75% masking ratio, our prompting decoder reduces the decoding computation cost by 20% compared to MAE [25].",2,positive
"Works in [3, 25, 56, 61] adopt similar techniques in CV to address the data-hungry issue of ViT models.",1,neutral
"MAE [6], which is designed for more efficient self-supervised pre-training, proposes dropping a high proportion of patches and subsequently inferring the missing patches through an autoencoder setup.",2,positive
"is that visual data, often, contains considerable redundancy or correlation in appearance [6] (see Figure 2).",1,neutral
"Our work takes some inspiration from MAE, however, PatchDropout can be applied to target tasks directly using standard ViTs (unlike MAE).",2,positive
Later studies [28] show higher mark ratio (e.,0,negative
"While scaling up training sets has indeed been integral to the recent progress in large models, advances in weak and self-supervision [14, 29, 30, 12, 20] have led to an abundance of potential training data.",1,neutral
"To ensure the effectiveness of training, we compute the loss on the common parts of the masked patches of T1 and unmasked patches of T2 (following [29]).",2,positive
"For simplicity, we take MAE [29] as an representative example of MIM, in which the similarity measurement is simply l2−distance on the masked patches.",1,neutral
"iBOT [57], MSN [2] and data2vec [4], whose frameworks involve various distance measurements between the siamese branches instead of reconstructing the unmasked parts, however, achieve comparable or even better results than the original reconstructionbased MIMs like [5, 29].",2,positive
"Several works try to interpret MIM from different views, for example, [29] suggests MIM model learns ""rich hidden representation"" via reconstruction from masked images; afterwards, [8] gives a mathematical understanding for MAE [29].",1,neutral
"3In original MAE [29], the encoder network only generates tokens of unmasked patches and the decoder only predict the masked patches during training.",2,positive
9 – MAE [29] patch masking reconstructive 1600 83.,0,negative
"l2-distance [29], cross-entropy [5] or perceptual loss [20] in codebook space.",1,neutral
"Furthermore, taking MAE [29] as a representative example of MIM, we empirically find the success of MIM models relates a little to the choice of similarity functions, but the learned occlusion invariant feature introduced by masked image – it turns out to be a favored initialization for vision transformers, even though the learned feature could be less semantic.",1,neutral
"Compared with conventional contrastive methods, MIM requires fewer effort on tuning the augmentations, furthermore, achieves outstanding performances especially in combination with vision transformers [21], which is also demonstrated to be scalable into large vision models [29, 37].",1,neutral
"Inspired by the above pilot studies, we employ the same MIM pretraining and finetuning routine to investigate the influences of plain ViTs on RS tasks.",2,positive
"Recently, reconstruction-based SSL methods such as MAE [12] have been proposed and shown effective for pretraining plain ViTs and adapting them for downstream tasks [13], [14].",1,neutral
"For more details about MAE, please refer to [12].",0,negative
"Some works resort to self-supervised learning [12], [32]–[34] with different RS characteristics taken into the design, e.",1,neutral
2) MAE: MAE [12] aims to recover the masked image parts given the visible ones with an encoder-decoder structure.,2,positive
"The models are trained for 1,600 epochs if not specified, following the default setting in MAE [12].",0,negative
"Different from these works, we use the representative MAE method [12] for MIM pretraining and specifically focus on advancing plain ViTs for remote sensing tasks.",2,positive
"image modeling (MIM) [12], recent work reveals that such a pretraining process can give a good initialization for plain ViTs to achieve surprising results on various downstream tasks including object detection and pose estimation [13], [14].",1,neutral
"Thanks to the development of the unsupervised learning in masked image modeling (MIM) [12], recent work reveals that such a pretraining process can give a good initialization for plain ViTs to achieve surprising results on various downstream tasks including object detection and pose estimation [13], [14].",1,neutral
"Different from these works, we use the representative MAE method [12] for",1,neutral
"To explore the effectiveness of MIM pretraining in remote sensing tasks, we pretrain our model using MAE [12] with the large-scale RSI dataset, i.e., MillionAID [11].",2,positive
"To explore the effectiveness of MIM pretraining in remote sensing tasks, we pretrain our model using MAE [12] with the large-scale RSI dataset, i.",2,positive
"Inspired by the Self-supervised [9], deeplearning-assisted compressed sensing [10], [11], we propose a new radar imaging method based on Selfsupervised deep-learning-assisted compressed sensing(SS-DL-CS-net).",2,positive
"Index Terms-Self-supervised, deep learning, compressed sensing, radar imaging
1.",1,neutral
"Recently, Transformer’s success in natural language processing and computer vision has a novel inspiration to point cloud learning [5,6].",1,neutral
"Masked vision modeling approaches such as MAE [18] train an encoder–decoder architecture to reconstruct
the original image from the latent representation and mask tokens.",1,neutral
"Considerable efforts have been made on learning high-quality and general visual representations through contrastive learning [31,21], masked vision modeling [18,45,2], and traditional supervised learning [48,32].",1,neutral
"The success of BERT [7] sparked an emerging direction of building large-scale vision models with masked vision modeling [18,45,2].",1,neutral
Masked vision modeling approaches such as MAE [18] train an encoder–decoder architecture to reconstruct,1,neutral
"Recent Vision Transformers (ViTs) have achieved impressive performance on various image understanding tasks (Dosovitskiy et al., 2020; Caron et al., 2021; Wang et al., 2021b; Liu et al., 2021; Chu et al., 2021a; He et al., 2021).",2,positive
Recent Vision Transformers (ViTs) have achieved impressive performance on various image understanding tasks Dosovitskiy et al. (2020); Caron et al. (2021); Wang et al. (2021c); Liu et al. (2021); Chu et al. (2021a); He et al. (2021).,2,positive
"3, which include ResNet50 He et al. (2016), DINO Caron et al. (2021), MAE He et al. (2021), and Twins Chu et al. (2021a).",2,positive
"The involved methods – MAE, DINO, and Twins are relatively representative.",0,negative
"Supervised ViTs are usually pre-trained for the classification task, while self-supervised ones are implemented with masked prediction He et al. (2021); Bao et al. (2021) or contrastive learning Caron et al. (2021); Chen et al. (2021b).",1,neutral
"For different pre-training tasks, ViTs can be categorized into self-supervised ones (Bao et al., 2021; He et al., 2021; Caron et al., 2021) and supervised ones (Dosovitskiy et al.",1,neutral
"Specifically, MAE
and DINO are based on the vanilla plain-ViT, while Twins is based on the hierarchical-ViT.",1,neutral
"To explore the learning ability of ViTs, some quantitative results about ViTs (DINO-small Caron et al. (2021), MAE-base He et al. (2021), Twins-small Chu et al. (2021a)) trained without FPNs for MVS are shown in Tab.",2,positive
"Supervised ViTs are usually pre-trained for the classification task, while self-supervised ones are implemented with masked prediction (He et al., 2021; Bao et al., 2021) or contrastive learning (Caron et al.",1,neutral
"For the comparisons among pre-trained ViTs with the multi-scale training strategy, both DINO-small and MAE-base are frozen during the training.",0,negative
"Specifically, by considering two typical ViT backbones – plain-ViT Dosovitskiy et al. (2020); Caron et al. (2021); He et al. (2021) and hierarchical-ViT Wang et al. (2021c); Liu et al. (2021); Chu et al. (2021a), we implement MVSFormer-P and MVSFormer-H as in Fig.",2,positive
"For different pre-training tasks, ViTs can be categorized into self-supervised ones Bao et al. (2021); He et al. (2021); Caron et al. (2021) and supervised ones Dosovitskiy et al. (2020); Liu et al. (2021); Chu et al. (2021a).",0,negative
"The learning rates of MAE and DINO are 1e-5; the learning rate of Twins is 3e-5, and the ones of all CNNs are 1e-3.",0,negative
"alternative MVSFormer to plain-ViT with vanilla attention (Dosovitskiy et al., 2020; Caron et al., 2021; He et al., 2021), called MVSFormer-P.",2,positive
", 2021), MAE (He et al., 2021), and Twins (Chu et al.",1,neutral
"Inspired by these achievements, transformers are also introduced into the computer vision Dosovitskiy et al. (2020); Liu et al. (2021); He et al. (2021).",2,positive
"Inspired by these achievements, transformers are also introduced into the computer vision (Dosovitskiy et al., 2020; Liu et al., 2021; He et al., 2021).",2,positive
"In particular, 1) we utilize the recent ViTs pre-trained with self-supervised (MAE He et al. (2021), DINO Caron et al. (2021)) and supervised (Twins Chu et al. (2021a)) tasks, while it is an interesting future work of exploring the influence of different pre-training tasks on MVS.",2,positive
[17] treated the object detection task as a prediction task of an ensemble sequence.,1,neutral
[17] used the DETR to input the feature map into a transformer for encoding and decoding operations and output the predicted bounding boxes.,1,neutral
"[17] used the MAEmodel to randomly mask the patches of the input image, thus, reconstructing the image pixels and obtaining richer semantic information.",1,neutral
"…based on large models and big data in natural language processing (NLP) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Raffel et al., 2020; Brown et al., 2020) and computer vision (Chen et al., 2020b,a,c; Chen and He, 2021; Bao et al., 2021; He et al., 2021b).",2,positive
"Following this trend, a series of modification to prompts and adapters (Hu et al., 2022; He et al., 2021a; Jiang et al., 2022; Sun et al., 2022a) for improvements in performance or training efficiency have emerged and made prompt tuning a heated topic in the whole NLP community.",2,positive
"C L
] 4
A ug
2 02
2
derstanding and generation (Liu et al., 2021b; He et al., 2021a).",1,neutral
"Although existing work exploits a light-weight transformer decoder with only self-attention [17] or a simple linear mapping [52] for the image decoder, we use joint information between modalities in decoding.",1,neutral
"In SimMIM [52] and MAE [17], transformers are trained to recover masked patches in an end-to-end fashion.",1,neutral
"MIM has shown to be an effective pre-training task for learning visual representations [2, 52, 17].",1,neutral
"Masked signal modeling is a popular self-supervisory pre-training task [10, 30, 54, 2, 52, 17], which aims at reconstructing the masked signals from the unmasked ones.",1,neutral
"It has been independently explored in the domains of natural language processing (NLP) and computer vision [10, 30, 54, 2, 52, 17].",1,neutral
"For image masking, we follow [17, 52] and use random masking of raw image patches with a masking patch size of 32 × 32.",1,neutral
"In MAE [20], only 25% unmasked patches are applied into the encoder, while learnable masked tokens are shared in the decoder for the reconstruction.",1,neutral
"[20] have released pre-trained MAEs based on ImageNet-1K with random masks, there are still some domain gaps for the inpainting.",0,negative
"From left to right are masked inputs, LaMa [40], MAE [20], and our results.",2,positive
"From left to right are masked images, results of pre-trained Masked AutoEncoder (MAE) [20], results of LaMa [40], and results from our method.",0,negative
"To address these dilemmas, we propose to guide the image inpainting with an efficient Masked AutoEncoder (MAE) pre-training model [20], which is called as prior Feature and Attention enhanced Restoration (FAR).",2,positive
"Benefited by the efficient design from [20] with 25% unmasked input tokens to the encoder, we should claim that the MAE pre-training is not heavier than CNNs.",0,negative
"Our method incorporates effective prior features from the transformer based representation learning [20] to enhance the inpainting, which make our method achieve superior results without overfitting the transformer results.",2,positive
Our MAE loss is the mean squared error (MSE) between MAE predictions and ground truth pixel values for masked patches as in [20].,1,neutral
"Such masking and predicting idea and transformer based architectures have been also well explored in vision tasks [20,45,3,58].",1,neutral
"Moreover, MAE [20] proposes an efficient transformer-based masked autoencoder for visual representation learning.",1,neutral
"Another trend for visual pre-training is masked prediction [5, 92, 39], which originated from NLP pre-training [27, 8].",1,neutral
"Recent works have shown impressive superiority of large-scale pre-trained models in computer vision (CV) [38, 15, 69, 39], natural language processing (NLP) [27, 65, 8, 19], and cross-modal applications [64, 45, 81, 3].",1,neutral
"Recently, a new training idea is provided by image mask self-supervised training such as MAE [19] and IboT [20].",2,positive
"( 1) ( 2) ( 3)M L ViT L ViSELoss T L ViT  (3)
Proc. of SPIE Vol. 12257 1225717-3
Different from MAE, the reconstruction method utilized in this paper is feature-level reconstruction instead of pixel-level reconstruction, the purpose is to make the pre-trained ViT structure fit the CNN network output to the greatest extent, so that achieves underlying feature aggregation and semantic information modeling.",2,positive
"3 (a), MAE [24] and MaskFeat [36] take the whole image as input to produce normalized tokens (normalized raw image pixels) or HOG from the whole images as the reconstruction target.",1,neutral
"(2) Specifically, for MAE [24], fφ is the identity function, and fθ is the masked autoencoder that reconstructs masked patches in the pixel spatial space.",1,neutral
MAE [24] proposes an asymmetric encoder-decoder architecture that can reconstruct the raw pixels from the latent representation.,2,positive
(a) MAE [24] and MaskFeat [36] reconstruct the low-level representations.,1,neutral
"Inspired by MAE [24], we propose a value normalization function upon teacher outputs.",1,neutral
(c) MAE [24] mainly models the masked image modeling task as a simply pixel restoration task.,1,neutral
"For a fair comparison, we directly follow most of the hyperparameters of MAE [24] in our fine-tuning experiments.",2,positive
"Inspired by Masked Language Modeling (MLM) as the pre-training tasks in BERT [17], Masked Image Modeling (MIM) has been proposed in several recent works [4,24,38].",1,neutral
"As mentioned in MAE [24], languages are human-generated signals that are highly semantic and information-dense.",1,neutral
"With the development of vision transformer [20], inspired by natural language processing (NLP), the generative based self-supervised learning (SSL) methods [3,4,14,19,24] using masked image modeling (MIM) task have grown in concern.",1,neutral
Recent MAE [18] observes and utilizes the spatial redundancy of single images.,1,neutral
"Previous methods [2, 18] reduce the redundancy of single images by masking a certain portion of patches.",1,neutral
MAE [18] directly masks the image patches and learns image representations by reconstructing the original image.,1,neutral
The masked frames predicting strategy is inspired by the high temporal redundancy of videos and the image patch masking strategy in recent MAE [18].,2,positive
"By masking and reconstructing image patches, the MAE encoder [11] is forced to learn high-level semantic information.",1,neutral
A recent approach MAE [29] shows that pixel loss is effective for improving Fig.,1,neutral
"A recent approach MAE [29] shows that pixel loss is effective for improving
unimodal encoder representation.",1,neutral
We follow UNITER [9] to set 15% masking ratio for masked language modeling and we follow MAE [29] to set 75% masking ratio for masked image modeling.,2,positive
"Specifically, we use Vision Transformer (ViT) pre-trained by AutoEncoder (MAE) [61] as the pre-trained model , and follow the fine-tuning settings obtained from the Github repository of MAE which is implemented on PyTorch.",2,positive
COIN achieves higher accuracy when implemented on ViT pre-trained by MAE [61].,2,positive
"Despite high similarity regarding pretext task, the masked autoencoder introduced in [1] differs from early denoising autoencoder [12] in numerous ways, which are summarized in Table 1.",1,neutral
denoising autoencoder [12] masked autoencoder [1] Training dataset MNIST ImageNet Model Architecture CNN ViT Corruption size pixels patches Corruption ratio maximum 50% patches,2,positive
"Early denoising autoencoder [12] and recent masked autoencoder [1] both attempt to reconstruct a clean input from a corrupted one, precisely predicting masked input content from unmasked input content.",1,neutral
"Abstract—Masked autoencoders are scalable vision learners, as the title of MAE [1], which suggests that self-supervised learning",1,neutral
TABLE 1 Comparison of denosing autoencoder [12] and masked autoencoder [1],1,neutral
"Its reviving success in recent MAE [1], outperforming joint-embedding methods, inspires numerous works to understand its success in vision and to apply it in various applications, such as video, point cloud, and graph.",2,positive
"In this work, we use MAE exclusively to refer to the method in [1] not as shorthand for masked autoencoder to avoid confusion.",2,positive
"an end-to-end masked autoencoder in the vision is proposed in MAE [1], which has attracted unprecedented attention.",2,positive
"In other words, the success of masked autoencoder in vision paves a path that SSL in vision“may now be embarking on a similar trajectory as in NLP” [1] by generative pretext task with masked prediction.",1,neutral
This challenge was finally broken by BEiT [10] as well as MAE [1] (see Sec.,1,neutral
"…algorithms become increasingly popular recently: it was recognized to be a strong pretrainer for various tasks in computer vision communities (He et al., 2022; Feichtenhofer et al., 2022); later on, it was also proved to be extremely effective for zero-shot dense retrieval in RetroMAE (Liu…",1,neutral
"The MAE-style algorithms become increasingly popular recently: it was recognized to be a strong pretrainer for various tasks in computer vision communities (He et al., 2022; Feichtenhofer et al., 2022); later on, it was also proved to be extremely effective for zero-shot dense retrieval in RetroMAE (Liu and Shao, 2022).",2,positive
"To overcome information redundancy in images, recent approaches suggest masking a very high portion of random patches in order to create a self-supervised task for learning expressive visual features (He et al., 2021; Bao et al., 2021; Zhou et al., 2021b; Bachmann et al., 2022).",1,neutral
"Geometrically, SSL-based label augmentation (Lee et al., 2020) will duplicate one Voronoi cell to be multiple (possibly disjoint) Voronoi cells (see Fig.",1,neutral
"Note that for the SSL-based label augmentation, there are 400 classes in total, which 2D t-SNE is unable to clearly illustrate the distributions of all the classes (including the rotated classes).",1,neutral
"In this section, we demonstrate that the Entropy-based geometric variance (HV) is a good indicator of predictive uncertainty induced by SSL-based label augmentation, and also exhibits high correlation with predictive accuracy.",1,neutral
"However, SSL in CIL is still largely underexplored and restricted to simple operations e.g. rotation.",1,neutral
"Self-supervised Learning (SSL) (Lee et al., 2020; Chen et al., 2020; He et al., 2021), on the other hand, has shown potential for alleviating task-level overfitting by learning representations transferable across phases.",1,neutral
"(B) In PASS, SSL-based label augmentation is applied on all three phases and expands the classes to be 16, 9, and 9 classes.",2,positive
"To enhance the discriminative power of CIL method, SSLbased label augmentation has been used to expand the original Kt classes to 4Kt by rotating the
10 0 12 0 14 0 16 0 18 0 20 0
20
original image x. Specifically, for image x, the rotated image x(α) = rotate(x, π2α), α ∈ {0, 1, 2, 3} will be assigned to a new class y = k(α), k ∈ {1, ...,K},K ∈ ∑t τ=1Kτ .",1,neutral
"Masked image modeling is applied in LayoutLMv3 [13] to interpret visual content by reconstructing the masked image tokens [2, 11].",1,neutral
"Self-supervised pre-training (Grill et al. 2020; Chen and He 2021; He et al. 2021; Yang, Zhang, and Cui 2022; Ohri and Kumar 2021; Zhao and Dong 2020; Larsson, Maire, and Shakhnarovich 2017; He et al. 2020) is attracting growing attentions as it can transfer knowledge learned from large scale…",2,positive
"Self-supervised pre-training (Grill et al. 2020; Chen and He 2021; He et al. 2021; Yang, Zhang, and Cui 2022; Ohri and Kumar 2021; Zhao and Dong 2020; Larsson, Maire, and Shakhnarovich 2017; He et al. 2020) is attracting growing attentions as it can transfer knowledge learned from large scale unlabeled dataset to boost the performance on downstream tasks.",2,positive
"In the 5-shot part segmentation task, our approach outperforms state-of-the-art methods in this dataset with mIoU increases of 3.9%, 0.3%, and 2.7% respectively for DatasetGAN, DDPM, and MAE.",2,positive
"We follow previous work [4] to first train a ViT-Large [12] by MAE, and then extract ViT features for part segmentation.",2,positive
"• MAE [18] — one of the state-of-the-art self-supervised methods, which learns an autoencoder to reconstruct missing patches.",2,positive
Our model can better avoid noises (compared to MAE [19] and,2,positive
"In the 1-shot part segmentation task, our model achieves significant improvement of 4.5%, 8.6%, and 10.5% compared to DatasetGAN, DDPM, and MAE, respectively.",2,positive
"• MAE [19] — one of the state-of-the-art self-supervised methods, which learns an autoencoder to reconstruct missing patches.",2,positive
"Compared with three baselines, DatasetGAN, DDPM,and, MAE, our model achieves significant improvement of 2%, 9.5%, and 13.4%, respectively.",2,positive
"Similar to MAE [24], position embeddings are added to input tokens.",1,neutral
"Based on the reconstruction target, these methods can be devided into: pixel-domain reconstruction [17, 24, 39, 42] and auxiliary features/tokens prediction [3, 11, 15].",1,neutral
"Masked image modeling (MIM) [18, 24, 42] has been attracting increasing attention recently in the self-supervised learning field, due to its method simplicity and capability of learning rich and holistic representations.",1,neutral
"It has been shown in [18, 24, 42] that such a simple framework outperforms previous self-supervised learning methods in both ImageNet classification [13] and some downstream tasks, like object detection and semantic segmentation.",1,neutral
SimMIM [42] and MAE [24] propose to reconstruct the raw pixel values from either the full set of image patches (SimMIM) or partially observed patches (MAE) to reconstruct the raw image.,2,positive
"A bunch of methods have been proposed to advance this technique from different perspectives [4, 8, 17, 21, 24, 34, 46, 50].",1,neutral
"The online encoder adopts the Vision Transformer (ViT) architecture [16], following MAE [24].",2,positive
"This issue has been manifested by experimental results in [24, 42].",1,neutral
"For MIM tasks, color enhancements degrade the results [24], so we do not apply them to the input of the online branch.",2,positive
", MAE [24] and SimMIM [42]), our method further processes the input image via a spatially shifted cropping operation.",2,positive
"The MAE [21] model is a lightweight encoder-decoder model that uses a CNN to encode the image patch, mask most of the patch embedding to extract the global features, and then use a lightweight decoder for fine-grained reduction, which is perfect for global feature extraction is more than appropriate.",2,positive
It follows a standard MAE [21] model architecture.,2,positive
"So, can we extract features using a lightweight transformer model and CNN together? The MAE [21] model is a lightweight encoder-decoder model that uses a CNN to encode the image patch, mask most of the patch embedding to extract the global features, and then use a lightweight decoder for fine-grained reduction, which is perfect for global feature extraction is more than appropriate.",2,positive
"Although some details differ, this is similar to the Perceiver IO[17] and MAE[13] models that do primary computation at a lower resolution and use cross-attention to expand to dense output when necessary.",1,neutral
"We also report the result on COCO test-dev with a large foundation model (ViT-Huge [62, 20, 7]).",2,positive
"When finetuning the detector on COCO, we find that applying learning rate decay [10, 1, 20, 7] for the components of the detector gives a ∼0.",1,neutral
"For MAE with ViT-Base/16, we follow the default end-to-end fine-tuning schedule: AdamW as the optimizer with base learning rate 5e-4 using the cosine learning rate decay; the layer-wise learning rate decay is set to 0.65 and weight decay is set to 0.05; the drop path is set to 0.1 and the warmup epochs are 5.",2,positive
Table 5 shows the recognition accuracies on the VIPriors-10 and NICO datasets with ViT-Base/16 as the feature backbone and MAE [32] as the SSL method.,2,positive
◦ Section C shows the results with MAE pretrained feature (Section 4); the attention map visualizations (Section 5.4) and the algorithm complexities (Section 5.4).,2,positive
We leave the results based on the most recent MAE [32] in Appendix.,0,negative
"For the NICO dataset, we train the MAE for 2000 epochs and adopt the mixup version of IP-IRM [86] to achieve the reasonable performance.",2,positive
"Besides, image inpainting and super-resolution algorithms [25], [41] can be applied.",1,neutral
MAE in the image domain [20] and video domain [18] both demonstrate that reconstructing perpatch normalized pixels works well for self-supervised pre-,1,neutral
MAE in the image domain [20] identified a semantic gap between the lowlevel features required for the reconstruction task and the abstract features required for the recognition task.,1,neutral
"Compared with reconstructing the original video pixels, using normalized pixels [18], [20] always performs better, which is in line with the pre-training settings.",1,neutral
"MAE-based approaches [9], [18], [20] skip the masked tokens in the encoder and apply them with positional embeddings in the lightweight reconstruction decoder.",1,neutral
"Although the self-supervised MAE pretrainings [9], [18], [20] require only multi-scale cropping",0,negative
MAE [20] mentioned that the pixel-level reconstruction and the recognition tasks require latent representations at different abstract levels.,1,neutral
"We are inspired by the powerful completion ability of ViT in reconstruction tasks [9], [18], [20] and propose to adopt this idea to empower supervised action recognition models at both the efficiency and performance levels.",2,positive
"Image MAE [20] investigates an asymmetric encoderdecoder structure, i.",1,neutral
"We use ViT-base as the backbone for both two methods, which is pretrained by MAE [5] on face image benchmark CelebA [21].",2,positive
", BeiT [1], Masked Auto-Encoder (MAE) [5].",1,neutral
"These baselines represent various strategies to incorporate environment information into clip representations ranging from frame features (MAE, FRAMEFEAT, OBJFEAT), to topological graph-based features (EGOTOPO), to pose-based features (EPC).",2,positive
"Note that OBJFEAT, MAE and EPC all benefit from pre-training on the same walkthrough videos as our approach.",0,negative
We use all frames from our generated walkthroughs to train an MAE VIT-large model with 16 × 16 patch size.,2,positive
PlacesCNN [91] FrameFeat ObjFeat MAE [34] EgoTopo [54] EPC [61] Trf (scratch) EgoEnv 0 2 4 6 8 10 12 14 16 easy hard (×103 instances) 44 46 48 50 52 54,0,negative
• MAE [34] trains a state-of-the-art self-supervised learning approach to reconstruct the pixels of masked patches in frames from our simulated video trajectories.,2,positive
"Strong image-level supervision (object labels in OBJFEAT and FRAMEFEAT) results in the largest improvements; MAE, which has access to the same data but trains self-supervised representations, does not show very large improvements.",1,neutral
"Despite having access to all additional pretraining videos and labels as supervision, frame-level features from OBJFEAT and MAE prove to be insufficient for environment-level reasoning.",0,negative
MAE architecture details.,0,negative
"[34] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"The encoder, generator and discriminator here are just the copies of the corresponding vanilla ViTs pre-trained by the method MAE [10] using AffectNet images.",2,positive
"We show the fine-tuning results of the pretrained ViTs, whose representations are obtained by conventional supervised learning (SL) or self-supervised learning as MAEs (MAE) [10] in Table 2.",1,neutral
"Motivated by the success of self-supervised Masked Autoencoders [10] (MAEs), which employs an asymmetric encoder-decoder architecture to implement strong representation learning, and the deficiency analysis of the prior identity-invariant FER work, we propose a novel FER model referred to as Poker Face Vision Transformer (PFViT) .",2,positive
All training configurations are the same as those in MAE [10] unless mentioned otherwise.,0,negative
[10] to reconstruct the normalized pixel values of each masked patch.,1,neutral
"We train ViTs as Masked Autoencoders [10] on AffectNet [22] dataset without noisy and biased emotion labels, so as to obtain stronger representations of expressive faces and narrow the gap between the selfsupervised pre-training task and downstream FER task.",2,positive
"Our PF-ViT benefits much from the strong ViT representations learned by the selfsupervision learning method MAE [10], described in Sec.",2,positive
"We firstly build and report several strong and transparent baselines based on vanilla Vision Transformers (ViTs) [6, 10] for the task of facial expression recognition (FER).",2,positive
"4, where IN1KViT and MAE-ViT represent the plain ViTs pre-trained using ImageNet1K data and AffectNet data as MAEs [10], respectively.",1,neutral
It has been experimentally demonstrated that MAE pre-training [10] can help ViTs learn a large number of visual semantics and acquire excellent performance in different downstream tasks.,1,neutral
"We can categorize self-supervised learning approaches into three groups: contrastive learning approaches [21, 6], predictive learning approaches [20], and others [51, 18].",1,neutral
"proposed a scalable self-supervised learning method Masked Autoencoders (MAE) [22], which splits the image into blocks, and randomly selects a few subblocks of the image as network input (the rest of the subblocks are masked), and then reconstructs the missing pixels.",1,neutral
"He et al. proposed a scalable self-supervised learning method Masked Autoencoders (MAE) [22], which splits the image into blocks, and randomly selects a few subblocks of the image as network input (the rest of the subblocks are masked), and then reconstructs the missing pixels.",1,neutral
"Inspired by pretext tasks for language transformer models, such as masking in BERT (Devlin et al., 2018), (He et al., 2022) recently introduced the Masked Auto-Encoder (MAE) for images, an effective pre-training method, by which an image is split into patches, and about 70 percent of the patches…",1,neutral
", 2018), (He et al., 2022) recently introduced the Masked Auto-Encoder (MAE) for images, an effective pre-training method, by which an image is split into patches, and about 70 percent of the patches are masked.",2,positive
"Inspired by MAE [26], our method directly recovers the input patches.",2,positive
"Instead of predicting the tokens, a latest work, MAE [26], proposes to directly reconstruct raw pixels with a very impressive performance achieved, which provides a new self-supervised pre-training paradigm.",2,positive
"Motivated by BERT [16] and MAE [26], we study the masked modeling strategy for mesh representation learning based on the introduced Mesh Transformer.",2,positive
"And following [26], we set a lightweight decoder, which has 6 layers.",2,positive
"Here, we propose several positional embedding strategies: a) utilizing learnable parameters as MAE [26] does; b) first embedding the center coordinates of each face, then applying max-pooling; c) first reshaping the center coordinates of 64 faces (64, 3) into a one-dimension vector (64×3), then embedding this vector directly; d) first calculating the center coordinates of the whole patches, then embedding the center of the patch directly (ours).",1,neutral
"Recently, BEiT [5] proposes a pretext task that tokenizes the input images into discrete visual tokens firstly and then recovers the masked discrete tokens, while MAE [26] encourages the model to reconstruct those missing pixels directly without the tokenization.",1,neutral
"Inspired by MAE [26], we wonder whether performing masked autoencoding on the unlabeled 3D mesh data could also promote the ability of the network.",2,positive
"Recently, MAE [26] skips the dVAE training process and proposes to reconstruct the pixel values directly, which is simpler and saves much computation overhead.",2,positive
"Unlike MAE [26], which reconstructs the raw pixels naturally, we choose to recover the geometric information of the masked patches.",1,neutral
"Similar to [20], we use the mean squared error as loss function as follow:",1,neutral
"Inspired by [20], we train our image patch generation network with random mask method on ImageNet [21] and our own dataset.",2,positive
"The feature set {MAE, ires100, fau, DenseNet} and the transformer-based structure are chosen for valence, expression and AU prediction.",2,positive
"The feature set {MAE, ires100, fau, DenseNet} and the LSTM-based structure are chosen for arousal prediction.",2,positive
"As for the second model, we first use the EmotionNet dataset to pre-train the MAE model with the reconstruction task, and then use the AffectNet [26] dataset to fine-tune the model further.",2,positive
† https://github.com/pengzhiliang/MAE-pytorch,0,negative
"1) We explore several unsupervised (MAE-based) and supervised (IResNet/DenseNet-based) visual feature representation learning methods for learning effective and robust visual representations; 2) We utilize three types of temporal encoders, including GRU [4], LSTM [29] and Transformer [31], to capture the sequential information in videos; 3) We employ multi-task frameworks to predict the valence, arousal, expression and AU values.",2,positive
MAE-based Features The features of the first type are extracted by MAE [6] models† which use C-MS-Celeb [10] and EmotionNet [3] datasets at the pretraining stage.,2,positive
The recent study of Masked Auto-Encoders [13] proves that deep-learning models can reconstruct the entire image from mere 25% patches due to the redundant visual information.,1,neutral
"The masked-prediction pretraining is now also applied on the spectrogram [10, 26], after being applied as Masked Language Model (MLM) , BERT [3] in NLP and Masked Auto-Encoder (MAE) [8] in CV. MAE proves the Generality of the transformer and the information density of images is lower than that of texts.",2,positive
"The masked-prediction pretraining is now also applied on the spectrogram [10, 26], after being applied as Masked Language Model (MLM) , BERT [3] in NLP and Masked Auto-Encoder (MAE) [8] in CV.",1,neutral
"In addition, recent researches on Transformer architecture [5–7] and Masked Auto-Encoder (MAE) [8] has been studied on audio spectrograms [9,10].",1,neutral
"Although “pretrain-MAE loss” fine-tune the model to resist domain transfer, it uses fewer data in training stages compared to “baseline 20”.",2,positive
• pretrain-MAE loss: This is to use the same prediction loss during the pre-training stage and fine-tuning stage.,0,negative
"Thanks to triplet loss’s ability to enlarge the relative distance of embedding, it improves Pearson corr in both the multitask method(pretrain-MAE+triplet loss) and our method.",2,positive
"In CV, models such as MAE [20] and MoCo [21], [22] also greatly improve performance in the same way.",1,neutral
• pretrain-MAE+triplet loss: This method uses both triplet loss (easy sample) and MAE loss during the pretraining stage.,1,neutral
The intuitive way to use the pretrian-finetune framework (pretrainMAE loss) fails to achieve the performances of “baseline 20”.,2,positive
"Recently, Transformer-based methods have been deployed in many computer vision tasks [3,19,36,42,54,56,57].",1,neutral
[19] proposes an image reconstruction task with masked patches.,1,neutral
Self-Supervised Learning approaches have seen significant interest in computer vision [19] and its core is the different,1,neutral
"masked autoencoders in 2021 [19], making outstanding contributions to the field of self-supervision and greatly contributing to the development of self-supervised learning.",1,neutral
"As mentioned in [16], there are three main categories in SSL according to their objectives: generative [17], [18], contrastive [19], [20], and generative-contrastive (adversarial) [21].",1,neutral
[8] designed an asymmetric encoderdecoder architecture to reconstruct the masked images and they found that a high proportion mask of the input image led to a more efficient result.,1,neutral
"However, existing self-supervised learning approaches [1, 2, 3, 4, 5, 6] are mainly designed for natural images.",1,neutral
Masked Autoencoder The MAE is an autoencoder with asymmetrical encoding and decoding stages [1].,1,neutral
"Inspired by MAE’s state-of-the-art performance on a wide collection of vision benchmarks [1], many follow-up works extend MAE to different data modalities.",2,positive
"In MAE [1] and in many transformers [49, 50], the positional encoding is:",1,neutral
Masked Autoencoder MAE [1] is a recent powerful self-supervised learning method.,1,neutral
"In recent years, self-supervised learning techniques have quickly become the norm for pre-training models on large-scale natural image datasets [1, 2, 3, 4, 5, 6, 7, 8], and have demonstrated strong performance on downstream tasks including image classification [3, 4, 9, 10], image segmentation [3, 11], representation learning [12, 13, 14], image compression [12, 15], and image reconstruction [1].",1,neutral
Only the last row of table 3 includes additional data augmentations used during finetuning as in [1].,1,neutral
"To address this issue, in this paper, we propose SatMAE, a self-supervised learning framework based on masked autoencoders (MAEs) [1] which naturally handles temporal and multi-spectral input data.",2,positive
"The decoder outputs a reconstructed image Î ∈ RC×H×W , which is compared to the original image using the mean-squared error (MSE) loss, computed per-pixel only on the masked patches [1].",1,neutral
State-of-the-art methods are deep learning methods that are trained or fine-tuned over annotated datasets [6].,1,neutral
"For problems in which there are no annotated data available, the solution is to use pretrained models [6], unsupervised segmentation methods, e.",1,neutral
"Self-supervised learning is garnering traction with recent advances showing that, under the right conditions, self-supervised pre-training can outperform fully supervised pre-training in terms of transfer performance in downstream tasks (He et al., 2022, 2020).",2,positive
"Self-supervised learning (SSL) is one of the most popular paradigms in the unsupervised scenario, which can learn transferable representations without depending on manual labeling (Gidaris et al., 2018; He et al., 2022; Grill et al., 2020).",1,neutral
"…magnitude can indicate discrimination confidence of the CL model, then the alignment-adaptive temperature dynamically controls penalty strength (arrow length) to negative samples to balance uniformity and tolerance for samples.
labeling (Gidaris et al., 2018; He et al., 2022; Grill et al., 2020).",1,neutral
"6 ViT-L MAE (He et al., 2021) 200 50 2D absolute 83.",0,negative
"MAE (He et al., 2021) further simplifies the recipe of BEiT by directly predicting the masked patches with a regression loss in the pixel space.",1,neutral
"3 ViT-L MAE (He et al., 2021) 1600 50 2D absolute 85.",0,negative
"We also consider two Transformer based selfsupervised pretraining methods, MOCO V3 (Chen et al., 2021) and MAE (He et al., 2021).",2,positive
"When combined with Transformers, MAEs have shown great success as an unsupervised pretraining technique, with BERT (Devlin et al., 2019), BEiT (Bao et al., 2021) and MAE (He et al.,
2021) as notable examples.",2,positive
"6 ViT-B MAE (He et al., 2021) 150 150 2D absolute 82.",0,negative
", 2020) and, recently, in MAE models in Vision (Bao et al., 2021; He et al., 2021), forces the model to learn about relationships between the content in different parts of the input using autoencoding related objectives.",2,positive
"2 ViT-B MAE (He et al., 2021) 1600 100 2D absolute 83.",0,negative
"Following the asymmetrical design in [15], we also design a small decoder which is 50% narrower and shallower than the encoder.",2,positive
"Encoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only operates on unmasked patches (i.e.,≈ 396 patches at 25% masking) and a special CLS embedding with its positional encoding c = x[cls] +E0pos ∈ R1×Denc is prepended to the sequence: h0 = [c, x̃vis] ∈ R(1+bR·Nc)×Denc .32 Let {hi}Li=1…",2,positive
"Following He et al. (2022), we use a masked autoencoder with a ViT architecture and a lightweight decoder for pretraining (left).",2,positive
"Encoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only processes unmasked patches (i.",2,positive
"Encoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only operates on unmasked patches (i.e.,≈ 396 patches at 25% masking) and a special CLS embedding with its positional encoding c = x[cls] +E0pos ∈ R1×Denc is prepended to the sequence: h0 = [c, x̃vis] ∈ R(1+bR·Nc)×Denc .32 Let {hi}Li=1 be the encoder hidden states after each of the L = 12 encoder transformer layers, and h0 denotes the input sequence.",2,positive
"PIXEL-base is a 112M parameter ViT-MAE architecture (He et al., 2022) with a 12-layer ViT encoder (Dosovitskiy et al., 2021) and an 8-layer Transformer decoder (Vaswani et al., 2017).",2,positive
"The latter, in particular, is common in computer vision applications to finetune on higher resolution images (Touvron et al., 2019; Kolesnikov et al., 2020; Dosovitskiy et al., 2021; He et al., 2022).",1,neutral
"Encoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only processes unmasked patches (i.e., ≈ 396 “visible” patches at 25% masking) rather than on a sequence including mask tokens, which not only reduces memory requirements and increases training speed, but also has the advantage of not…",2,positive
"Our Pixelbased Encoder of Language (PIXEL) is built on the Masked Autoencoding Visual Transformer (ViTMAE; He et al., 2022).",2,positive
"This mismatch would occur when training the encoder with inserted mask tokens because they are not inserted during finetuning (He et al., 2022).",1,neutral
"Encoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only operates on unmasked patches (i.",2,positive
"2 ARCHITECTURE PIXEL-base is a 112M parameter ViT-MAE architecture (He et al., 2022) with a 12-layer ViT encoder (Dosovitskiy et al.",2,positive
"Encoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only processes unmasked patches (i.e., ≈ 396 “visible” patches at 25% masking) rather than on a sequence including mask tokens, which not only reduces memory requirements and increases training speed, but also has the advantage of not creating a mismatch between pretraining and finetuning.",2,positive
A known issue [5] with pixel shuffling with larger upsampling ratios (P > 8) was that output images tend to contain evident borders between image patches as seen in Figure 2.,1,neutral
"Image Colorization with Transformers Unlike the widely-used convolution-based approach for image synthesis, recent studies [5, 12, 14, 35] made efforts to synthesize images by only utilizing the Transformer architecture.",1,neutral
Images generated with large upsample ratios [5] tends to suffer from evident borders between image patches.,1,neutral
The regressor aims to recover the missing pixels as in [28].,1,neutral
Then we study two widely used masking strategies in masked image modeling: random block-wise masking in [4] and random masking in [28].,1,neutral
"Inspired by MAE [28], the encoder only handles the visible patches Xv for training efficiency and outputs the latent representation Zv.",2,positive
"The efforts include (i) framework design, such as MAE [28], SplitMask [24], SimMIM [54], CAE [13]; (ii) prediction targets, such as PeCo [21], MaskFeat [51], data2vec [3], iBOT [59]; (iii) video extension BEVT [50]; (iv) integration with vision-language contrastive learning FaRL[57].",2,positive
We use normalized pixels as the reconstruction target for groundtruth as MAE [28].,2,positive
It has been observed in MAE [28] that block-wise masking degrades at such a large ratio for their model.,1,neutral
"The total masking ratio is 75%, same with that in MAE [28].",0,negative
"A very recent work, masked autoencoder (MAE) [28], introduces an asymmetric encoder-decoder structure where the encoder only operates on visible patches, and the output representation of the encoder along with masked tokens are fed into a lightweight decoder.",1,neutral
"Recently, lots of works [28,26] exploring MIM have been concurrently developed from different perspectives.",2,positive
"Moreover, the elimination of the special mask tokens bridges the gap between pre-training and fine-tuning as the fine-tuning stage sees real visible patches without any mask token [28,26].",1,neutral
"We benchmark four types of self-supervised models, including contrastive (MoCov2 [11]), clustering-based (SwAV [7], DINO [8]), feature de-correlation (BarlowTwins [67]), masked autoencoder (MAE [22], BeiT [1]) models.",2,positive
MAE [1] reduces sequence length by masking a large portion of image patches randomly and encoding only non-masked ones for reconstruction of pixel color information.,1,neutral
Abstract This paper studies a simple extension of image-based Masked Autoencoders (MAE) [1] to self-supervised representation learning from audio spectrograms.,1,neutral
"Similarly in the CV community, Vision Transformers (ViT) [9] have become popular for CV tasks, and, for self-supervised image representation learning, Masked Autoencoders (MAE) [1] have brought the CV community closer to the success of BERT in NLP.",1,neutral
"We observe, similar as in MAE for images [1], that a high pre-training masking ratio (80% in our case) is optimal for audio spectrograms.",2,positive
"For self-supervised learning, MAE [1] efficiently encodes only a small portion (25%) of visual patches while the majority of patches is discarded.",1,neutral
"Based on ViT [9] that applies Transformers to image patches, BEiT [17] and MAE [1] present masked image modeling frameworks.",2,positive
"Transformers [2] and self-supervised learning [3, 4, 5, 6, 7, 1] are dominating computer vision (CV) and natural language processing (NLP) research.",1,neutral
"The masking mechanism, as introduced in MAE [1], is the key ingredient for efficient self-supervised learning.",1,neutral
"In CV, visual masked pre-training has made recent progress [23, 24, 1, 20].",0,negative
"As in MAE [1], we add fixed sinusoidal positional embeddings to the embedded patches.",1,neutral
"Generative approaches focus on reconstructing original data based on the partial [4,18,19], corrupted [1,20] or shuffled [2] input, etc.",1,neutral
"Transformer-based pretraining has become the dominant approach in natural language processing [11, 20], and is being actively introduced to other research fields such as vision-language [23, 32, 14], images [12, 13, 7, 4], and videos [36, 21].",1,neutral
"Many ViT-based variants also demonstrated the success in computer vision tasks [47,19,13], such as object detection [5], video recognition [1], and image synthesis [26].",1,neutral
"ViT relies on globally-contextualized representation, in which each patch is attended to all patches of the same image, as opposed to local-connectivity in CNNs. ViT and its variants have shown promising superiority in modeling non-local contextual relationships as well as good efficiency and scalability, though they are still in their infancy.",2,positive
"S2(a), utilizing a pretrained ViT encoder contributes to the improvements of FID and IS by 2.418 and 0.204, respectively.",2,positive
Transformer Encoder Our encoder is a standard ViT [9].,2,positive
The superiority of the Transformer architecture is presented in ViT fully utilizing the advantage of pretraining on large-scale datasets compared with the CNN-based methods.,2,positive
"The pretrained ViT encoder is capable of capturing the long-term dependencies, which may benefit the patch prediction.",2,positive
Transformer Encoder and Decoder We compare the impact of the pretrained ViT-based encoder and the number of transformer decoder layers M .,2,positive
"We develop a novel hybrid query-based encoder-decoder transformer framework, named Query Outpainting TRansformer (QueryOTR), to extrapolate visual context all-side around a given image taking advantages of both ViT [9] and pure transformer [41] in the image outpainting task, as shown in Fig.",2,positive
"Inspired from ViT, the input image is first converted to several non-overlapping patches represented as a sequence of patch tokens Xp.",2,positive
"On the ImageNet benchmark, Dosovitskiy et al. [9] developed the Vision Transformer (ViT) interpreting a picture as a sequence of tokens, which can achieve comparable image classification accuracy while requiring less computational budgets.",2,positive
ViT [9] is a convolution-free Transformer that conducts image classification over a sequence of image patches.,1,neutral
"M FID↓ IS↑ - 4 22.784 3.751 X 2 20.731 3.931 X 4 20.366 3.955 X 8 20.373 3.852
(a) Ablation of the pretrained ViTbase encoder and the number of transformer decoder layers M .",2,positive
We initialise the weights of generator encoder by utilizing the pre-trained ViT [17].,2,positive
"To learn good representation from visible observations, recent works in masked image modeling tasks [4,18] use random masks with a specific masking ratio to construct a self-supervised learning task.",1,neutral
"the self-supervised strategy recovers original 2D inputs from the occluded motion map, which formulates the problem as a masked image modeling (MIM) task [4,18] and can learn an expressive motion representation for the occluded problem.",1,neutral
"Different from the augmentation techniques in the previous methods [51,5,50,31], we propose a self-supervised strategy by recovering original 2D inputs from the occluded motion map, which can learn an expressive motion representation for the occluded problem via the MIM task [4,18].",1,neutral
"With the self-supervised learning, the prior learns the expressive representation for the human motion [18,4] with only the partial observations.",1,neutral
"In fact, similar to images (He et al., 2022), many time series are natural signals with temporal information redundancy—e.g., a missing timestep value can be interpolated from neighboring time-steps.",1,neutral
"In fact, similar to images (He et al., 2022), many time series are natural signals with temporal information redundancy—e.",1,neutral
"Unsupervised Semantic Segmentation The first line of work (Van Gansbeke et al., 2021; He et al., 2022; Cho et al., 2021; Ji et al., 2019; Hwang et al., 2019; Ouali et al., 2020; Hamilton et al., 2022; Ke et al., 2022) aims to learn dense representations for each pixel in the image and then cluster them (or their aggregation from pixels in the foreground segments) to get each pixel label.",2,positive
"Nevertheless, recent breakthroughs in applying attention-based models (Dosovitskiy et al., 2020; Tolstikhin et al., 2021; He et al., 2021) to computer vision have fundamentally challenged the significance that symmetries play in model design.",1,neutral
"Additionally, we will combine the symmetric pattern in Swin-Unet [48] with the coding pattern in MAE [57], to retain sufficient vessel details and make the performance of our model more outstanding on each metric.",2,positive
"Moreover, we will further focus on feature learning using some state-of-the-art methods, such as MAE [57] and ViT [38].",2,positive
", 2021) and masked auto encoders (MAE) (He et al., 2021) are becoming increasingly popular and achieve state-of-art performance in solving vision problems, it is imperative to understand the efficacy of our alignment strategy on feature representations obtained from such largescale pre-trained transformer encoders.",2,positive
"The table shows the FTTA performance on DomainNet with representations from MAE (He et al., 2021).",2,positive
", 2021)-based encoder (trained using masked auto-encoders (He et al., 2021)) finetuned on the ImageNet dataset.",2,positive
"(iii) We conduct, for the first time, a FTTA experiment on the large-scale DomainNet (Peng et al., 2019) dataset, based on self-supervised representations from the recent ViT-based masked autoencoders (He et al., 2021).",2,positive
"CATTAn with Pre-Trained ViT Embeddings
As Transformer-based solutions such as vision transformers (ViT) (Dosovitskiy et al., 2021) and masked auto encoders (MAE) (He et al., 2021) are becoming increasingly popular and achieve state-of-art performance in solving vision problems, it is imperative to understand the efficacy of our alignment strategy on feature representations obtained from such largescale pre-trained transformer encoders.",2,positive
"Furthermore, we also experiment with a vision transformer(ViT) (Dosovitskiy et al., 2021)-based encoder (trained using masked auto-encoders (He et al., 2021)) finetuned on the ImageNet dataset.",2,positive
"…ViT Embeddings
As Transformer-based solutions such as vision transformers (ViT) (Dosovitskiy et al., 2021) and masked auto encoders (MAE) (He et al., 2021) are becoming increasingly popular and achieve state-of-art performance in solving vision problems, it is imperative to understand…",2,positive
", 2019) dataset, based on self-supervised representations from the recent ViT-based masked autoencoders (He et al., 2021).",2,positive
"To this end, we consider the encoder from MAE (He et al., 2021) fine-tuned on ImageNet as our feature extractor 3.",2,positive
"As illustrated in Figure 3, MAE first masks a large portion of the image and attempts to reconstruct the complete image from the masked image.",1,neutral
Previous studies in the field of computer vision (CV) have proved that AutoEncoder is a powerful frame of feature extraction and reconstruction [7].,1,neutral
"Here, following the MAE [63], we adopt the MIM-based pretext task for self-supervised pretraining both in steps (1) and (2).",1,neutral
Mask-RCNN(MAE) [63] SSP(IN1K) ViT-B 66.,1,neutral
"Thus, some works, such as MAE [63], BEiT [64], SimMIM [65] have begun to turn to mask partial image patches and then reconstruct the original image (i.",1,neutral
"Moreover, some pioneering studies [63]–[65], [71], [72] have also demonstrated the superiority of MIMbased pretext task applied for self-supervised pretraining on",1,neutral
"For intuitive analysis, the pretrained weights of ViTB [14] from MAE [63] are adopted to individually generate visualized attention scores from two unseen images.",1,neutral
"Subsequently, to ensure effectiveness, we keep the same setting of MAE [63] by utilizing 75% tokens randomly masked and then reconstructed",2,positive
"MAE [63] for a self-supervised pretraining study on medical image analysis and showed that when using the MIM-based pretext task, the pretrained model can significantly improve the fine-tuning performance of diverse medical downstream tasks.",1,neutral
MAE [63] CVPR2022 SSP(AID or NR45) ViT-B 86.,0,negative
"In our study, the process of task-agnostic representation of MIM follows [63], and it can be expressed by the following equations:",1,neutral
"In particular, MAE [63] has been utilized for self-supervised pretraining on the unlabeled nature scene dataset of ImageNet [18] based on the ViT [14] architecture.",2,positive
", via pθ(x̂T|x̂S) with x̂ = g(x) or pθ(h(xT)|k(xS)) [21, 53], where g(·), h(·), and k(·) are domain-knowledge-inspired transformations.",1,neutral
"AI is undergoing a paradigm shift with the rise of big/foundation models [5, 57], e.g., BERT [47], GPT3 [7], MAE [21], DALL-E [43, 42], Imagen [45], Stable Diffusion [44], UniDiffuser [3], ChatGPT [39, 40], etc. Foundation models, often based on mask-and-predict pretraining and downstream finetuning, are capable of benefiting from pretraining on broad data at scale and accordingly, demonstrate diverse downstream task capabilities with impressive robustness [47], adaptability [21], and generalization [43].",2,positive
", BERT [47], GPT3 [7], MAE [21], DALL-E [43, 42], Imagen [45], Stable Diffusion [44], UniDiffuser [3], ChatGPT [39, 40], etc.",1,neutral
"Following the MAE [21], we design the GAN generator pθ(xT|xS) with an autoencoder-like architecture, which employs an encoding G-Encoder and a decoding G-Decoder, as shown in Fig.",2,positive
"Most existing foundation models are pretrained3 with (i) masked LM (or masked auto-encoding; like BERT and MAE), (ii) causal/auto-regressive LM (like GPTs and DALL-E), and (iii) permutation LM (like XLNET [56]).",2,positive
"MAE [21] MaskFeat [53] Eq(S,T)KL[q(h(xT)|xS)||pθ(h(xT)|xS)] q(S,T) = U{(S,T) : S is a 25% random subset of L, and T = L\S pθ(h(xT)|xS) = N (h(xT)|μθ(xS), I) h(·) is a normalization/HOG transformation for MAE/MaskFeat",1,neutral
"Foundation models, often based on mask-and-predict pretraining and downstream finetuning, are capable of benefiting from pretraining on broad data at scale and accordingly, demonstrate diverse downstream task capabilities with impressive robustness [47], adaptability [21], and generalization [43].",2,positive
"Recent advancements in self-supervised learning (Mikolov et al., 2013; Devlin et al., 2018; Chen et al., 2020a; He et al., 2021) have enabled effective representation learning without curated, labeled datasets (Goyal et al.",1,neutral
"Recent advancements in self-supervised learning (Mikolov et al., 2013; Devlin et al., 2018; Chen et al., 2020a; He et al., 2021) have enabled effective representation learning without curated, labeled datasets (Goyal et al., 2021).",1,neutral
"For computer vision and NLP domains, pretraining is a de facto standard and is shown to be necessary for the state-of-the-art performance [18, 10].",2,positive
"For domains with structured data, like natural images or texts, pretraining is currently an established stage in the typical pipelines, which leads to higher general performance and better model robustness [18, 10].",1,neutral
", these methods require the model to predict certain parts of the input given the remaining parts [18, 10].",1,neutral
"In the vision community, the self-prediction based methods are shown to be superior to the methods that use contrastive learning objectives [18].",1,neutral
"SSL aims to learn efficient feature representation from unlabeled training samples using self-generated supervision signals [27, 28, 7, 6, 21, 11, 70, 40].",1,neutral
"nals from the data themselves by adopting various pretext tasks, such as contrastive learning [28, 6], masked autoencoding [11, 27, 70], rotation estimation [18, 43], jigsaw puzzles [39] and so on [1, 21].",1,neutral
"Reconstructing Masked Surfels or All Surfels? Similar to observations in [27], better results are achieved by reconstructing masked parts only, as shown in Fig.",1,neutral
"For each downstream task, we adopt various fine-tuning strategies [28, 27], including transferring features protocol, linear classification protocol and non-linear classification protocol.",2,positive
"Among those pretext tasks, masked auto-encoding has demonstrated its effectiveness in many applications [11, 27, 62, 57, 70, 40], including point cloud learning [70, 40].",1,neutral
"MAE [He et al., 2021] is created based on method adopted by autoencoder to train a vision transformer.",2,positive
"Adopting Transformer engineering techniques from vision and language domains, such as data scaling [7, 18], deepening [70, 74], hybrid architectures [16, 80], and self-supervision [17, 7, 24], are promising.",1,neutral
"Unlike MAE [12] that calculates loss only on masked patches, we will compute the loss on the entire point cloud.",2,positive
"More recently, MAE [12] has proposed a more effective asymmetric encoder-decoder architecture for MIM tasks, where the encoder operates on the visible patches to extract the latent representation by a linear projection with concatenated positional embeddings.",1,neutral
"An image can be spilt into a sufficient number of patches, and each of them contains enough pixel information [12].",1,neutral
"Similar to MAE [12], we discard the masked patches directly and do not use any mask tokens in the encoder part, which will save the computational time and memory effectively.",2,positive
"3D point clouds are characterized by random order and irregular structure, and the irregularity of the input point clouds is aggravated by masking parts of the point cloud in the MPM pre-text task, which makes it difficult to obtain the embedding features by a simple linear projection as MAE [12] and ViT [24] do for image patches.",1,neutral
"[12] presents the masked Autoencoders (MAE) method, which randomly masks the patches of the image and inputs the visible patches subset to the encoder to obtain the latent representations, which are then concatenated with the mask tokens and input to the decoder to reconstruct the missing pixels of the original input image.",1,neutral
"In addition, it has been observed that the self-attention maps of vision transformers pre-trained by advanced unsupervised representation learning methods, such as DINO [176], BEiT [177], MAE [178] and iBoT [179], contain rich information about the segmentation of an image,",1,neutral
"In addition, it has been observed that the self-attention maps of vision transformers pre-trained by advanced unsupervised representation learning methods, such as DINO [176], BEiT [177], MAE [178] and iBoT [179], contain rich information about the segmentation of an image, which provides a potential way to generate reliable pseudo dense labels without any supervision.",1,neutral
"Inspired by the major success of transformer in NLP field, researchers have recently applied this architecture to CV fields [3, 21, 28, 39, 49, 53].",1,neutral
"Noting this challenge and the potential for improved effectiveness and applicability, in the work here, we thus propose (and also fully develop) a method based on the idea of masked autoencoders [16] which will utilize unlabeled data to do selfsupervision.",2,positive
"Inspired by the success of the self-supervised way transfer learning, the CV community is also exploring the self-supervised way to explore new possibilities, one recent work which is similar to the BERT in NLP is MAE [40].",2,positive
"Inspired by the success of the self-supervised way transfer learning, the CV community is also exploring the self-supervised way to explore new possibilities, one recent work which is similar to the BERT in NLP is MAE [28].",2,positive
"Inspired by their success, multiple methods have applied similar techniques to the image domain [2, 7, 13, 19, 40].",1,neutral
"For the pre-training, we follow the paradigm of MAE [19] and equip the model with a lightweight decoder that is structurally similar to the encoder.",2,positive
"This can partially be motivated by the reduced time needed for pre-training, but we also find the encoder to achieve higher downstream task performance when trained in conjunction with a smaller decoder, similar to the results in [19].",2,positive
Figure 1: MAE [19] (left) divides images into nonoverlapping patches of fixed size.,1,neutral
"Fueled by robust representations, self-supervised models have seen great success in fields such as Natural Language Processing (NLP) [3, 12, 32] and computer vision (CV) [5, 8, 19].",1,neutral
This work aims to extend the MAE-style pre-training [19] to voxelized point clouds.,2,positive
"Specifically, masked language modeling [12] and masked image modeling [2, 19, 45] have proven themselves as simple, yet effective, pre-training strategies.",1,neutral
"One of the key differences between Voxel-MAE and MAE for images [19], is the reconstruction task and its accompanying losses.",1,neutral
"Recently, the authors of [19] proposed MAE, a simple approach where random image patches are masked and their pixel values are used as reconstruction targets.",1,neutral
MAE uses mask technology and Transformer widely used in Natural Language Processing (NLP) to mask most areas of the image and encode only a small amount of unmasked areas.,1,neutral
"Benefiting from the development of Transformer [11] in recent years, generative learning now has an option to avoid these pitfalls: Masked Autoencoder (MAE) [12].",2,positive
"In the first stage, the MAE scheme is introduced to pre-train the classification model using unlabeled pathological images.",2,positive
"After using the MAE method to pre-train a VIT-Base model on the pathological dataset, we obtain an accuracy of 83.02% and an F1-score of 0.8299 in patch classification.",0,negative
"1) unsupervised pre-training based on MAE: To pretrain the ViT model, we employ an encoder-decoder architecture with a pretext task of image reconstruction.",2,positive
MAE [19] and SimMIM [61] proposed masked image modeling algorithms for vision transformers.,1,neutral
MAE [19] and SimMIM [60] proposed masked image modeling algorithms for vision transformers.,1,neutral
"Recent self-supervised pre-training frameworks have been used to learn useful representations that can be applied to downstream tasks [6,10,3,9,21].",1,neutral
", pixel-level image reconstruction for denoising autoencoder [48], patch-level image reconstruction for masked autoencoder [49], and language probability model for word vector embedding [50].",1,neutral
"This motivates us to adopt this approach for visual model-based RL, but we find that masked image modeling with commonly used pixel patch masking [13] often makes it difficult to learn fine-grained details within patches, e.",1,neutral
"1 Visual Representation Learning It has been observed that masked image modeling with a ViT architecture [13, 34, 36] enables compute-efficient and stable self-supervised visual representation learning.",1,neutral
"Interestingly, we find that masking convolutional features can be more effective than pixel patch masking [13], by allowing for capturing fine-grained details within patches.",1,neutral
Figure 8: Frames reconstructed with the masked autoencoders (MAE) [13] trained on Meta-world (Top) Coffee Pull and (Bottom) Peg Insert Side.,0,negative
"Specifically, we provide reconstructions from MAE [13] trained on Coffee-Pull and Peg-Insert-Side tasks from Meta-world [16] in Figure 8.",2,positive
Masked autoencoder Masked autoencoder (MAE) [13] is a self-supervised visual representation technique that trains an autoencoder to reconstruct raw pixels with randomly masked patches consisting of pixels.,1,neutral
"On the other hand, masked autoencoders (MAE) [13] have recently been proposed as an effective and scalable approach to visual representation learning, by training a self-supervised vision transformer (ViT) [14] to reconstruct masked patches.",1,neutral
"Training self-supervised ViTs with masked image modeling [13, 34, 35, 36, 37, 38, 39] has also been successful.",1,neutral
com/view/mwm-rl which contains videos for (i) reconstructions from masked autoencoders (MAE) [13] and (ii) predictions from latent dynamics models.,2,positive
[13] proposed a masked autoencoder (MAE) that reconstructs masked pixel patches with an asymmetric encoder-decoder architecture.,1,neutral
MAE [58]: reconstruct randomly masked patches with vision transformer.,1,neutral
"Most recently, masked autoencoder (MAE) [58] raised great attention in the computer vision community with a breakthrough in autoencoding self-supervised pre-training of vision transformers.",1,neutral
"Since ViT has shown promising results in selfsupervised learning [77, 52, 85], and recent advances are exploring the link between large-scale pretraining for both language and vision [58, 86], it deserves more exploration",2,positive
"Inspired by denoising autoencoder, MAE masks out random patches of the input image, sends visible patches to the encoder, and reconstructs the missing patches from the latent representation and masked tokens.",2,positive
"Therefore, efficient data compression [252], data loading, model design [58] and hardware acceleration are necessary to be explored.",2,positive
"He et al. proposed Masked Autoencoders [13] mask random patches of the input image and reconstruct the missing pixels, which attempting to bridge the gap of progress of autoencodingmethods between CV andNLP.",1,neutral
"proposed Masked Autoencoders [13] mask random patches of the input image and reconstruct the missing pixels, which attempting to bridge the gap of progress of autoencodingmethods between CV andNLP.",1,neutral
"To assist the training of mixed-supervised network, we adopted an effective sampling strategy (random masking) to optimize the training process, inspired by MAE [6].",2,positive
"Next, we deviate from the common autoregressive training objective and pre-train a bidirectional transformer with window-restricted attention via masked visual modeling (MVM).",2,positive
"Unlike prior work on MVM [42, 40] that uses a fixed masking ratio, we propose to use a variable masking ratio that reduces the gap between pre-training task and inference leading to better evaluation results (see § 3.",2,positive
"The pre-training objective is to minimize the negative log-likelihood of the visual tokens given the masked video as input: LMVM = − E
x∈D
[∑ ∀i∈NM log p(zi|ZMp , Zc) ] , where D
is the training dataset, NM represents randomly masked positions, and ZMp denotes the output of applying the mask to Zp, and Zc are latent tokens corresponding to context frames.",1,neutral
"Let Zp = [zi]Ni=1 denote the latent tokens corresponding to future video frames, where N = Tp×h×w. Unlike prior work on MVM [42, 40] that uses a fixed masking ratio, we propose to use a variable masking ratio that reduces the gap between pre-training task and inference leading to better evaluation results (see § 3.4).",2,positive
"Inspired by the success of masked language [8] and image [42, 40] modeling, and in the spirit of unifying methodologies across domains, we pre-train MaskViT via MVM for video prediction.",2,positive
"The success in NLP has also been replicated in vision by masking patches of pixels [40, 41] or masking tokens generated by a pretrained dVAE [42, 43].",1,neutral
The MVM training objective is different from the causal autoregressive training objective as the conditional dependence is bidirectional: all masked tokens are predicted conditioned on all unmasked tokens.,1,neutral
The shared prompt templates lead to less discriminative text features since images are much more diverse [13] compared to highly-semantic and information-dense texts.,1,neutral
"R ECENT years have witnessed the success of the transformer structure in natural language processing [1], [2], [3] and computer vision [4], [5], [6].",1,neutral
Setting 16×16 patch size 8 × 8 patch size MAE [19] SemMAE MAE [19] SemMAE Linear probing 63.,1,neutral
"Inspired by the success of Masked Language Modeling (MLM) [5; 12] in pre-training of the NLP field, Masked Image Modeling (MIM) has been proposed recently and exhibits promising potential for visual pre-training [3; 7; 19; 30].",1,neutral
"5% top-1 classification accuracy, outperforming SimMIM[37] and MAE[19] by 0.",0,negative
"Our model can surpass the most competitive MAE [19] with a clear margin, i.",2,positive
We follow MAE [19] and adopt an encoder-decoder structure to perform MIM.,2,positive
"In this paper, we follow MAE [19] to adopt the most simple and intuitive raw pixels regression.",1,neutral
MAE [19] and SimMIM [37] argue that predicting RGB values of raw pixels by direct regression performs no worse than the patch classification approaches with complex designs.,1,neutral
MAE [19] and SimMIM [37] use random masking with a large masked patch size or a large proportion of masked patches.,1,neutral
"Context encoder [28], an inpainting-based masked image modeling (MIM) pioneer, proposes to use a random and fix-shaped mask; SiT [2] and BEiT [3] use random “blockwise” masks, where patches in the local neighbourhood are masked together (also called GMML: group mask model learning); MAE [19] randomly masks out 75% patches of an image.",2,positive
"proach has been particularly effective in learning representative features, where the task is to reconstruct masked data from unmasked input [9, 18, 6, 36].",1,neutral
"Our voxel masking strategy thus reduces the memory complexity of training, similar to how the Transformer network works in NLP [9], 2D vision [18], and small-scale point clouds [26, 23, 42, 43].",2,positive
"Masked autoencoding has achieved success in NLP [9] and image [18], leading to the development of masked autoencoding techniques for point clouds in the last year.",1,neutral
"The Transformer network used in masked autoencoding for NLP [9], 2D vision [18], and small-scale point clouds [26, 23] performs self-attention on unmasked portions of the training data, which are unaffected by the masking.",1,neutral
"Recently, MAE [18] has shown promising results in selfsupervised learning by first masking random patches of the input image and then reconstructing the missing pixels with a simple autoencoder framework.",2,positive
"Inspired by the excellent performance of masked autoencoding [9, 18, 26], we design the masked voxel autoencoding network for 3D perception.",2,positive
"Similarly, in 2D vision, masked autoencoding has outperformed supervised pre-training counterparts [18].",1,neutral
MAE Masked autoencoders.,0,negative
"The difference of Mask-CE from the Mask strategies used in MAE and BEIT is that the [Mask] token only affects the words behind it but not itself while predicting words, and the model needs to predict all words, not only words with the [Mask] token.",1,neutral
Masking is an effective learning strategy which has already been used in MAE [45] (Masked autoencoders) and BEIT [46] (bidirectional encoder representation from image Transformers) to enhance the model learning ability and to explore the relationships among objects.,1,neutral
"In comparison, in MAE and BEIT, the model mainly reconstructs the masked patches, and the normal patches will not be affected.",0,negative
"Pre-training methods are widely exploited to learn effective and compact representations for molecule and protein recently, which take the spirit of pre-training and fine-tuning methodology from natural language processing (NLP) [43, 14] and image processing [35, 15, 21].",1,neutral
", object detection [13,134,43, 22], semantic segmentation [130,17,98,116,126,29,28], generative adversarial network [59,102,58], low-level vision [16,72,127], video understanding [81,7,94,104,118], self-supervised learning [2,24,80,53,4,14,38,23,117,110, 3,45], neural architecture search [103,67,15,19,20], etc.",1,neutral
"Motivated by the above analyses and recent computer vision models [9], especiallyMasked AutoEncoder (MAE) [13], we propose a masked autoencoding model for time series based on Transformer blocks (i.e., TSFormer).",2,positive
"Specifically, we design an efficient unsupervised pre-training model for Time Series based on TransFormer blocks [33] (TSFormer), which is trained through the masked autoencoding strategy [13].",2,positive
"As the semantics of time series are more straightforward than languages, we use four layers of Transformer blocks, far less than the depth of Transformer-based models in computer vision [9, 13] and natural languages [3, 8].",1,neutral
"Unlike MAE [13], we no longer add positional embeddings here since all patches already have positional information added in the encoder.",2,positive
"which is in line with other pre-training models [8, 13].",0,negative
"Another important difference between TSFormer and MAE [13] and the original Transformer [33] is the
learnable positional embedding.",1,neutral
"Furthermore, another noteworthy difference from MAE [13] is that we pay more attention to the representations of the patches.",1,neutral
"Moreover, unlike the deterministic, sinusoidal embeddings used in MAE [13], we use learnable positional embeddings.",1,neutral
"Motivated by the above analyses and recent computer vision models [9], especiallyMasked AutoEncoder (MAE) [13], we propose a masked autoencoding model for time series based on Transformer blocks (i.",2,positive
MAE enables us to train large models efficiently and effectively and outperforms supervised pre-training.,2,positive
"On the contrary, MAE [13] uses selfsupervised learning based on the masked autoencoding strategy.",1,neutral
"To address this problem, a simple strategy that works well is to mask a very high portion of the model’s input to encourage the learning of high-level semantics, motivated by recent development in computer vision [13].",1,neutral
"We use uniform distribution to initialize the positional embeddings, and we use truncated normal distribution with 𝜇 = 0 and 𝜎 = 0.02 to initialize the mask token, similar to MAE [13].",1,neutral
Another important difference between TSFormer and MAE [13] and the original Transformer [33] is the,1,neutral
"We evaluate the performances of all baselines by three commonly used metrics in multivariate time series forecasting, including Mean Absolute Error (MAE), Root Mean Squared Error (RMSE) and Mean Absolute Percentage Error (MAPE).",2,positive
This approach was originally proposed in the image domain [63] where patches of an image are masked.,1,neutral
"These tasks originated in the image domain, where the most common techniques are Generative Adversarial Networks (GANs) [51, 210] and Masked Autoencoders (MAEs) [63].",1,neutral
"Inspired by [18, 17], we added the whole initial embedding patches back to the last layer’s embedding patches to retain global information and maintain the correlations of all patches.",2,positive
"The masked image denoising task follows Bao et al. (2022) and He et al. (2022) – randomly masked 75% of the image patches, and the goal is to recover the whole image.",2,positive
"Notably, we achieve this without the extensive data augmentations methods typically used by SOTA models (Yu et al., 2022a; He et al., 2022).",2,positive
", running masked auto encoder models like MAE[22] is not straightforward).",1,neutral
"SimA without LPI: Although XCiT[2] shows that LPI layer can improve the accuracy by 1.2 point, it limits the application of vanilla transformer (e.g., running masked auto encoder models like MAE[22] is not straightforward).",1,neutral
"For instance, the distribution of the token values is relatively robust compared to CNNs when we mask (drop) 75% of the tokens in masking auto-encoder (MAE[22]).",1,neutral
We propose to use the state-of-the-art self-supervised learning framework masked autoencoders (MAE) [10] to extract the feature representations and initialize the feature space.,2,positive
"Specifically, we use the self-supervised masked autoencoders (MAE) [10] to map all instances into an initial feature space and iteratively refine it for better data distribution.",1,neutral
"For the MAE, we only modified its input size of the ViT model in its encoder and decoder (from 224 to 512), and other model details are the same as those in [10].",2,positive
"Following MAE, our Masked Autoencoder for Generic Event Boundary Detection(MAE-GEBD) is inspired by the recent ImageMAE [5] and Spatiotemporal-MAE [3].",2,positive
"VideoMAE [13] inspired by the ImageMAE and propose customized video tube masking and reconstruction, and show that video masked autoencoders are data-efficient learners for self-supervised video pre-training (SSVP).",1,neutral
"MAE [5] develop an asymmetric encoder-decoder architecture with masked tokens hiding a high proportion, e.",1,neutral
"We compare our model with [1], [2], [24] using objectlevel appearance frames on Avenue dataset.",2,positive
"Moreover, we are inspired by MAE[24] for the simultaneous multi-frame prediction.",1,neutral
"MAE[24] generalizes an image by
leveraging ViT.",1,neutral
"[53] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
5) on the pre-trained weights and layer-wise learning rate decay for better finetuning [53].,1,neutral
"Due to architectural constraints, vision transformer-based methods also naturally use a patch-based representation (He et al., 2022; Bao et al.).",1,neutral
"Following [40], we feed the non-masked N −M patches (and their positions) to the encoder to produce per-patch embeddings.",1,neutral
"To this end, we leverage the findings of several recent works on the use of the masked pretraining [23] to greatly improve the training and performance of Transformers in the domain of images [6, 40, 80, 84], videos [76, 82, 80] or across text, audio and images [5].",2,positive
"[40] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"As we follow [40] and only pass unmasked patches to the encoder and have a lightweight decoder, extreme masking leads to a dramatically lower computational cost for training the encoder, and consequently the model as a whole.",2,positive
"Unlike our experiments, prior work found that extremely high masking ratios lead to degradation in performance, for instance MAE [40] saw a significant degradation in performance when masking more than 75% of the patches.",2,positive
"We start from the default 75% masking for each modality [40], and increase it upto 90% for images and 95% for videos.",2,positive
"Following [40], we re-train OmniMAE without normalizing the pixel targets to obtain easy to visualize RGB reconstructions.",2,positive
"We compare OmniMAE’s representations trained jointly on images and videos to MAE, trained solely on images [40].",2,positive
"In particular, we consider the Masked Auto-Encoding (MAE) approach [40] to train an Omnivorous visual encoder [33].",1,neutral
Figure 1: OmniMAE is a single model for images and videos that is trained using masked autoencoding [40].,1,neutral
"The model learns to recover the visual tokens [4, 70] or pixels [26].",1,neutral
"Recently, inspired by BERT [15], which was developed for masked language modeling, masked image modeling has been proposed for visual representation learning [4, 26, 70].",1,neutral
"Despite the fast progress in image-based SSL [8, 12, 24, 26], its success has not yet been fully matched by video-based SSL techniques.",1,neutral
Part of this observation has been spotted in a study on masked visual transformers [24].,1,neutral
"2 [24] Kaiming He, Xinlei Chen, Saining Xie, et al.",1,neutral
Further analysis on several robustness benchmarks also exhibits more appealing robustness of the studied corruption tasks than MIM.,1,neutral
"Several methods [79, 23] also integrate MIM into contrastive-based Siamese frameworks.",1,neutral
"The architecture of our encoder is quite flexible since we do not insert any mask tokens on the corrupted non-overlapping patch embeddings as in MIM [1, 29, 71, 68].",2,positive
We believe that our MFM can also complement recent MIM approaches to further improve the performance.,2,positive
"To cope with heavy spatial redundancy in images, MAE [29] shows that one would need to mask a very high proportion (e.",1,neutral
"In this paper, we mainly use a standard ViT [22] as our encoder for a direct comparison with MIM methods.",2,positive
"This suggests that predicting the invisible signals is a more favourable task in representation learning, which is in accordance with the observation in recent MIM approaches.",1,neutral
"3) Extensive experiments show that our MFM can achieve competitive performance among existing patch-based MIM approaches on standard ImageNet-1K classification benchmark, while not using mask tokens or other more complex designs.",2,positive
Both MLM and MIM follow a common corruptand-predict paradigm – randomly masking a portion of input data and then learning to predict the missing parts.,1,neutral
"2) Corruption-based tasks (e.g., SR, Deblur, Denoise and MFM) are generally more robust than the MIM task (e.g., MAE).",1,neutral
"Following the success of Masked Language Modeling (MLM) such as BERT [17] in natural language processing (NLP), Masked Image Modeling (MIM) [1, 29, 68, 71] has shown promising performance in self-supervised pre-training of visual models.",1,neutral
"Our work differs from previous approaches in that we perform masking in the frequency domain, which relies on none of the following: (i) extra data [1, 21, 24], (ii) extra model [79, 23, 24, 57, 10], or (iii) mask token [1, 29, 71, 68, 10].",2,positive
Recent promising results of MIM motivate us to investigate the effectiveness of these corruption operations in the context of representation learning.,2,positive
"Pioneered with stacked autoencoders [64] and context encoders [52] using CNNs, recent approaches [1, 29, 71, 68, 10] follow the mask-word strategy in NLP to randomly mask image patches in the spatial domain using the vision Transformers [22, 40].",1,neutral
"By default, current MIM methods such as BEiT [1], MAE [29] and SimMIM [71] perform masking in the spatial domain by excluding image patches randomly, a strategy inspired by MLM that performs masking on words (Figure 1(a-b)).",1,neutral
"Along with this mask-patch strategy, different types of prediction targets have been studied, including discrete tokens [1, 21], raw pixels [29, 71], and hand-crafted features [68].",1,neutral
"On the other hand, the recent success of generative language pre-training (Brown et al., 2020) and generative vision pre-training (He et al., 2022; Bao et al., 2021) motivates us to explore generative vision-language pre-training to learn more versatile and scalable vision-language models.",2,positive
", 2020) and generative vision pre-training (He et al., 2022; Bao et al., 2021) motivates us to explore generative vision-language pre-training to learn more versatile and scalable vision-language models.",2,positive
"We use an output dimension of 8192 for the projection heads across all models, and employ random Masked Image Modelling with prediction ratios (0, 0.3) and variances (0, 0.2).",2,positive
", Masked Image Modelling [2, 3, 23, 16, 34, 43, 59]), can be used in a standalone manner to learn more generalized features for few-shot learning on small-scale datasets.",1,neutral
To overcome the lack of such fine-grained annotations we employ self-supervised training with Masked Image Modelling as pretext task [59] and use a Vision Transformer architecture [10] as encoder due to its patch-based nature.,2,positive
"In contrast, FewTURE demonstrates that the self-supervised pretext task (i.e., Masked Image Modelling [2, 3, 24, 17, 35, 44, 59]), can be used in a standalone manner to learn more generalized features for few-shot learning on small-scale datasets.",1,neutral
"Motivated by the success in NLP [17, 8] with transformers [44], many visual representation learning methods [26, 5, 54] using ViTs have also shown benefit from masked inputs.",1,neutral
"Self-supervised learning with masking has demonstrated more scalable properties when combined with ViTs [26, 5, 54, 3].",1,neutral
"Recently, masked autoencoders [26] have demonstrated impressive performances in visual representation learning, only with ViTs.",1,neutral
"For example, masked autoencoder [26] benefits from longer training up to 1600 epochs.",1,neutral
"After DINO, other self-supervised training frameworks have been proposed such as Masked Autoencoders (He et al., 2021) and BEiT (Bao et al.",1,neutral
"After DINO, other self-supervised training frameworks have been proposed such as Masked Autoencoders (He et al., 2021) and BEiT (Bao et al., 2021).",1,neutral
"Moreover, instead of replacing the masked regions with [MASK] tokens[17, 58], we use the feature patches of teachers to fill the masked regions of students, thereby making our distillation teacher knowledge-aware.",2,positive
"We also consider grid-wise masking, which regularly retains one of every four patches, similar to MAE [17].",1,neutral
"Particularly, MIM-based approaches generally i) divide an image or video into several non-overlapping patches or discrete visual tokens, ii) mask random subsets of these patches/tokens, and iii) predict the patches masked visual tokens [2], the feature of the masked regions such as HOG [57], or reconstruct the masked pixels [17, 58].",1,neutral
"Inspired by MAE [7], we treat volumetric SR as a task to recover the masked regions from the visible regions, where the visible regions refer to the slices in the LR volume and the masked regions refer to the slices in the corresponding HR volume.",2,positive
"Recently, large-scale pre-training and fine-tuning of transformers [54] have been successful in various domains [8, 36, 44, 38, 53, 7, 20, 1, 10].",1,neutral
This is in contrast to the common observation in CV that linear probing lags behind fine-tuning by a large gap [25].,1,neutral
The vision transformer backbones in ViTPose are initialized with the pre-trained weights from the self-supervised MAE pre-training [13].,2,positive
"MIM enhances the token-level representations, while spatial self-relation focuses on improving the ability to model inter-token relations.",1,neutral
"We also demonstrate that our proposed method, SERE, outperforms and complements various masked image modeling (MIM) based methods.",2,positive
", discrete tokenizer [60], [62], raw pixels [14], [59], [63], [64], [65], HOG features [66], patch representations [18], etc.",1,neutral
"These visualizations demonstrate that SERE produces more precise and less noisy attention maps than various methods, including MIM-based meth-
ods, i.e., MAE [14] and iBOT [18].",1,neutral
"MIM reconstructs masked patches from unmasked parts, with different forms of reconstruction targets, e.g., discrete tokenizer [60], [62], raw pixels [14], [59], [63], [64], [65], HOG features [66], patch representations [18], etc. Compared to ID, patch-level reconstruction in MIM enhances token-level representations [18], [61].",2,positive
Experiments also demonstrate that SERE can outperform and complement various MIMbased methods.,1,neutral
Comparison between spatial self-relation and MIM.,1,neutral
"Additionally, we strengthen the ability to model inter-channel relations, which MIM is missing.",2,positive
Comparison with masked image modeling (MIM).,1,neutral
"Both spatial self-relation and MIM act on the spatial dimension, but their effects significantly differ.",1,neutral
"6, SERE achieves consistent improvements compared to different MIM-based methods, strongly confirming the effectiveness of SERE compared to MIM.",2,positive
"4 shows that SERE can locate the frog, but MAE primarily focuses on the background.",0,negative
MAE produces noisy attention maps that highlight almost all tokens in an image.,1,neutral
"Meanwhile, SERE and MIM can be complementary.",1,neutral
"DINO+SERE achieves comparable performance compared to MIM based methods (iBOT and MAE), requiring less pre-training/fine-tuning epochs.",2,positive
"4, SERE generates more precise and less noisy attention maps than MAE [14] and iBOT [18].",1,neutral
"Concurrent with our work, self-supervised learning by masked image modeling (MIM) [14], [33], [60], [61] has become a popular alternative to instance discrimination (ID) for self-supervised ViT.",1,neutral
These results strongly confirm the effectiveness of SERE compared to MIM-based methods.,1,neutral
"Pioneering works have shifted the methods designed for self-supervised CNN to ViT and revealed the great potential of self-supervised ViT [13], [14], [15].",1,neutral
"Next, rather than training the explainer from scratch, we fine-tune an existing ViT: we can use the original classifier or the surrogate as an initialization, or we can use a ViT pre-trained on a different supervised or self-supervised learning task [17, 58, 24].",2,positive
"Then, the unmasked portion is embedded with the information about their position in the original image, which then goes to Transformer blocks to extract the image features [68].",1,neutral
The encoder employs a masked autoencoder (MAE) with vision Transformer (ViT) architecture [68].,2,positive
"Then, the pretrained masked autoencoder (MAE) [24] completes the original image relying on the visible image patches.",1,neutral
"In this paper, we closely follow the model architecture of MAE [24].",2,positive
We pretrain the autoencoder module on ImageNet [15] for 200 epochs following the hyper-parameters of MAE [24].,2,positive
"1, we first revisit the pretraining framework based on masked autoencoder [24].",2,positive
"In detail, we pretrain an extremely light-weight autoencoder via a self-supervised mask-reconstruct strategy [24].",2,positive
"Closely following the recent selfsupervised method MAE [24], we first divide the images into patches and mask out a set of patches from the input images, meaning only portions of the images are input to the autoencoder.",2,positive
"Image inpainting as a proxy task recently attracts a new wave of self-supervised learning [24, 3, 18, 10, 2] owing to the success of the vision transformer [17].",1,neutral
Recent study on Masked AutoEncoder [32] (MAE) adapts the reconstruction task for patch-based Vision Transformers [18].,1,neutral
The objective in MAE includes reconstructing the entire image from input masked image patches.,2,positive
MVP [80] follows MAE [32] to train a visual encoder and even outperforms supervised pre-training.,2,positive
"Inspired from this, we adapt SAC+AE into SAC+MAE by replacing the augmented input image with its masked version, and only penalizing the reconstruction error for the masked patches.",2,positive
", [5, 6, 8, 10, 14, 15, 17, 21, 31, 32, 37, 40, 52, 54, 55, 61, 71]), some RL methods [1, 42, 46, 59, 63, 69, 81, 88] take advantage of self-supervised learning.",1,neutral
For MAE we start from augmented SAC+AE and first divide the augmented image into nonoverlapping patches in the spatial domain with a size of 4 × 4.,1,neutral
"For transformers, we leverage pretrained models on ImageNet [16] from ViT [17], DeiT [58], DINO [4], MoCo-v3 [11], and MAE [26] (which makes an interesting comparison as it is based on reconstruction rather than contrasting).",2,positive
"1While CL algorithms are the main focus here, the generative SSL approach MAE [26] is also investigated in the downstream robustness test.",1,neutral
"In addition to augmentation invariance, generative pre-training [2, 26, 49] and visual-language pre-training [47] are promising ways to",1,neutral
"While the exact reason why pre-trained CL models are more robust to downstream corruptions remains unclear, our analysis of the learning dynamics through feature space metrics reveals one piece of the puzzle: CL yields larger overall and steadily-increasing per-class feature uniformity
1While CL algorithms are the main focus here, the generative SSL approach MAE [26] is also investigated in the downstream robustness test.
and higher stability than SL.",2,positive
"The generative method, MAE [26], is slightly more robust than CL to patch shuffling on CIFAR, but inferior on STL10 and more vulnerable to gamma distortion.",1,neutral
"Aside from addressing the above limitations, we also point out that our methods could be extended in a number of directions, including: 1) training the whole network in a self-supervised way, perhaps using recent advances in transformers for vision [58, 59] and contrastive approaches for neural activity [34, 39]; 2) improving domain transfer through structured transport methods [60].",2,positive
"BYOL 82.6 68.8 InfoNCE 82.8 66.8
Table 8: ViT-B Wall-clock time comparison using a single node of 8×V100 GPUs.
methods epochs time DINO 400 300 hrs BEiT 800 240 hrs MAE 1600 650 hrs
ExtreMA (80% ratio ×1) 300 29 hrs ExtreMA (80% ratio ×2) 300 36 hrs ExtreMA (80% ratio ×5) 300 60 hrs
finetuning and 55.1% linear probing performance.",0,negative
"A notable distinction of this model is its effectiveness with an extremely large masking ratio (75% 90%), while typical masked modeling approaches work best between the range of 50% to 75% [14, 2, 11].",1,neutral
"Our model is even comparable to MAE, which is trained with a much heavier schedule (300 epochs vs. 1600 epochs).",2,positive
"We compare with representative contrastive methods MoCo-v3 [16] and DINO [15], as well as masked image modeling methods BeiT [2] and MAE [11] on ImageNet classification.",2,positive
ExtreMA outperforms the masked image modeling approaches MAE and BEiT by a large margin of 12% and 30% using 1% of the labels.,2,positive
The evaluation protocol mainly follows BEiT and MAE.,0,negative
"For the masked area, its potential degrees of freedom grow combinatorially large with its size, allowing for richer self-supervision than conventional augmentations such as cropping and scaling, which are heavily biased towards areas around the image center [11].",1,neutral
"In computer vision, block-wise masking [2] and random masking [11] are investigated to cope with the 2D nature of images.",1,neutral
Both DINO and MAE fail to inpaint proper colors due to their use of color augmentation and normalized pixels.,1,neutral
"This allows us to train ViT-Base models of 300 epochs using a single node of 8×V100 GPUs for 29 hours
Table 9: Semantic segmentation on ADE20K.
methods epochs mIoU DINO 300 47.2
MoCo-v3 300 47.3 BEiT 800 47.1 MAE 1600 48.1 ExtreMA (1k) 300 47.9 ExtreMA (22k) 30 48.4
Table 10: Semi-supervised classification.
methods epochs 1% 10% scratch - 9.0 44.8 BEiT 800 35.9 69.7 DINO 400 64.7 75.9
MoCo-v3 300 57.2 75.8 MAE 1600 52.7 72.1 ExtreMA (1k) 300 67.3 76.1
to 60 hours depending on the choice of multi-masking.",0,negative
"Aside from BERT-like training approaches [2, 11, 14, 27, 28], a special type of random masking in the form of small local image crops has also been adopted in contrastive models [15, 31].",1,neutral
"Past works demonstrate generalization through k-nearest-neighbors [5] and zero-shot classification on the learned features [9], or finetuning the model for a limited schedule [18, 11].",1,neutral
"A fixed sinusoidal positional encoding is added to each embedded patch, and a few of the embedded patches are sampled randomly [11] according to the masking ratio.",1,neutral
Such representations are shown to surpass prior art when finetuned for downstream tasks [11].,1,neutral
"tokens [2], raw pixel values [11], or features [26] from an online learned encoder [27, 14, 28, 29].",1,neutral
"We vary the masking ratio and compare results with supervised ViT [41], DINO [15] and MAE [11] using this inversion method.",1,neutral
This is in contrast to masked image modeling where the performance degrades when the masking ratio exceeds 75% [11].,1,neutral
"Specifically, we use ViT-B/16, ViT-L/16 and ViTH/14 according to the settings from [18].",2,positive
"Similar to SwinV2 models pre-trained with SimMIM, we observe the same
overfitting phenomenon when training with small datasets or large models, which makes MAE still demand for large-scale data.",2,positive
"Recently, iGPT [6], BEiT [3], MAE [18] and SimMIM [44] recall this approach on training vision transformer. iGPT [6] sequentially predicted the pixels by auto-regressive manner.",2,positive
We also conduct experiments with other MIM frameworks like MAE [18] and other vision encoder like the widely used ViT [13] to verify the generalizability of our findings.,2,positive
"In addition, we conduct experiments using MAE as the masked image modeling approach to verify the methodological generalizability of our findings.",2,positive
MAE [18] and SimMIM [44] concurrently found predicting the raw pixels with a high masking ratio could work well.,2,positive
Results with MAE and ViT Figure 4 demonstrates the results of ViT pre-trained with MAE.,0,negative
"Recently, iGPT [6], BEiT [3], MAE [18] and SimMIM [44] recall this approach on training vision transformer.",2,positive
"ImageNet-1K We follow [3] to evaluate the quality of learnt representations by fine-tuning the pre-trained models on ImageNet-1K [11] image classification task, which is the most commonly used scenario and evaluation criterion for pre-trained models [18, 44].",2,positive
"In this work, we use SimMIM as the default masked image modeling approach, because of its simplicity and no restrictions on the architecture of vision encoder like MAE.",2,positive
These experiments strictly follow the settings in [18].,1,neutral
"Masked Image Modeling (MIM) [3, 18, 44], which has recently emerged in the field of self-supervised visual pretraining, has attracted widespread interest and extensive applications throughout the community for unleashing the superior modeling capacity of attention-based Transformer",1,neutral
"Since masked token modeling is successful self-supervised learning for texts [8] and images [3, 17], a unified model for both generative and discriminative tasks [24] is worth exploration for future work.",1,neutral
"The study on self-supervised learning [17] also reports similar results, where a masked auto-encoder reconstructs the global contexts of an image after masking half of the image.",1,neutral
"Other recent works which use a “masked-patch” pretext task are [34, 77, 75, 24, 40, 18, 3, 27, 85, 45].",1,neutral
"For transfer learning, the representative task is classification task Tcls (He et al., 2020b; 2021) which pretrains a model on a large-scale unlabeled data Dpre and then fine-tunes the pretrained model on a classification downstream dataset Dfine.",2,positive
Here we mainly consider the masked autoencoder (MAE) structure He et al. (2021).,1,neutral
", 2018) and mask-reconstruction pretraining (MRP) (He et al., 2021; Baevski et al., 2022).",2,positive
", 2021) and object detection (He et al., 2021), and is seeing more applications because of its effectiveness and strong compatibility.",2,positive
"Now MRP has surpassed the end-to-end supervised learning on many downstream tasks, e.g. image classification (Dong et al., 2021) and object detection (He et al., 2021), and is seeing more applications because of its effectiveness and strong compatibility.",2,positive
"2 MASK-CONSTRUCTION PRETRAINING FRAMEWORK As a representative MRP, MAE (He et al., 2021) randomly masks the patches of an input image and then reconstructs the pixels of these masked patches via an auto-encoder.",2,positive
"This data assumption accords with the practice in many SSL works (Wei et al., 2021; Dong et al., 2021), e.g. MAE (He et al., 2021), SimMIM (Xie et al., 2021) and data2vec (Baevski et al., 2022), which pretrain and fine-tune on the same dataset, e.g. ImageNet, but with significant improvement over…",2,positive
"According to the pretext tasks, the current SSL approaches can be grouped into contrastive learning, e.g. (Hjelm et al., 2018; Oord et al., 2018), clustering learning, e.g. (Caron et al., 2018; Wu et al., 2018) and mask-reconstruction pretraining (MRP) (He et al., 2021; Baevski et al., 2022).",1,neutral
"We use SimMIM rather than MAE (He et al., 2021), as MAE removes the masked patches before encoder but the convolution operations in CNN encoder cannot handle masked input, while SimMIM replaces the masked patches by a mask token and can use CNNs.",2,positive
"5, the left image is the visualization of SL, while the middle one is from MAE (He et al., 2021) and the right one is from data2vec (Baevski et al.",2,positive
"As a representative MRP, MAE (He et al., 2021) randomly masks the patches of an input image and then reconstructs the pixels of these masked patches via an auto-encoder.",1,neutral
"In this work, we are particularly interested in the recently proposed mask-reconstruction pretraining (MRP) of SSL families (Xie et al., 2021; Dong et al., 2021), e.g. MAE (He et al., 2021) and data2vec (Baevski et al., 2022).",2,positive
"After pretraining, following the practice in (Wei et al., 2021; Dong et al., 2021; He et al., 2021; Xie et al., 2021; Baevski et al., 2022), we only fine-tune the student encoder with an extra linear layer on the labeled training data of the downstream datasets.",0,negative
"This MRP family, e.g. MAE (He et al., 2021) and SimMIM (Xie et al., 2021), randomly masks image patches and then reconstructs the masked patches via an auto-encoder.",2,positive
"5, the left image is the visualization of SL, while the middle one is from MAE (He et al., 2021) and the right one is from data2vec (Baevski et al., 2022).",2,positive
"The “random sampling” strategy in [38] randomly samples patches following a uniform distribution, i.",1,neutral
"Then, the masked vector quantized-variational autoencoder (VQ-VAE) with vision Transformer (ViT) blocks [37], [38] is designed as the architecture of the robust semantic communication system.",1,neutral
"Although MAE is designed for transformer [3], pre-training CNN with MAE is meaningful since CNN can also restore masked images, as illustrated in Appendix A.4.",2,positive
"To this end, many unsupervised pre-training algorithms are proposed, such as MoCo [10] and MAE [10].",1,neutral
"Therefore, we think the unsatisfactory performance of the fine-tuned model is because MAE is unsuitable as an M3DO pre-training task.",0,negative
"A.4 MAE Reconstruction Results of CNN
In Section 5.2, we pre-train a model on KITTI-raw using MAE.",0,negative
"As shown in Table 4, compared with the model without pre-training, both CL and MAE degrade the performance, and MAE decreases the APs dramatically, which implies that the learned representation is unsuitable for M3OD.",0,negative
This phenomenon could be caused by two reasons: (1) The representation learned by MAE is dissimilar to the representation of M3OD.,1,neutral
"The studied pre-training tasks include CL, MAE [10], depth estimation, and 2D object detection.",1,neutral
"(2) Since MAE is a task designed for pre-training Transformer, CNN fails to learn to recover masked images through MAE.",1,neutral
"To satisfy the demand of different downstream tasks, various pre-training tasks are designed, such as contrastive learning (CL) [5], masked autoencoders (MAE) [10], super resolution [15], pixel prediction [4], etc.",1,neutral
"To confirm which one is the real cause, we randomly mask 80% pixels of images from the KITTI-3D validation set, and use the model pre-trained on KITTI-raw with the MAE pre-training task to recover them.",2,positive
"Among them, CL is implemented based on MoCo [11].",1,neutral
"Current deep learning paradigm tends to increase the scale of the pre-trained model to embed more knowledge into the model, which can be transferred to more diverse tasks [22, 4].",1,neutral
"While some methods train the model to predict discrete tokens (Bao et al., 2022) or contextualized representations (Baevski et al., 2022) for masked image patches, MAE (He et al., 2021) and SimMIM (Xie et al., 2022) achieve competitive performance by simply predicting the pixel values.",2,positive
", 2022) for masked image patches, MAE (He et al., 2021) and SimMIM (Xie et al.",2,positive
"Masked image modeling (Bao et al., 2022; He et al., 2021; Xie et al., 2022), which trains the model to predict missing information from masked image patches, has recently emerged as the superior self-supervised learning method for vision transformers (ViT) (Dosovitskiy et al.",1,neutral
"The mCE of MUST is only slightly higher than a model that is first trained with self-supervised MAE (He et al., 2021) followed by supervised finetuning on ImageNet.",2,positive
"Masked image modeling (Bao et al., 2022; He et al., 2021; Xie et al., 2022), which trains the model to predict missing information from masked image patches, has recently emerged as the superior self-supervised learning method for vision transformers (ViT) (Dosovitskiy et al., 2021).",2,positive
"Following Bao et al. (2022); He et al. (2021), we use a layer-wise learning rate decay (Clark et al., 2020) of 0.65 for both ViT models.",2,positive
", 2021) and self-supervised learning (Caron et al., 2021; Chen et al., 2021; Bao et al., 2022; Zhou et al., 2022; Xie et al., 2021; He et al., 2022), showing a seemly inevitable trend on replacing CNNs in computer vision.",2,positive
"…2021; Yuan et al., 2021; Zhai et al., 2021; Touvron et al., 2021b; Xue et al., 2021) and self-supervised learning (Caron et al., 2021; Chen et al., 2021; Bao et al., 2022; Zhou et al., 2022; Xie et al., 2021; He et al., 2022), showing a seemly inevitable trend on replacing CNNs in computer vision.",2,positive
"Both encoders in the siamese encoder share the same weights, and we adopt the vision transformer (ViT) [2,6] and convolutional neural network (CNN) [7] as the backbone of the siamese encoder.",2,positive
"We apply the AdamW optimizer [11] to train our model, in which CNN encoders adopt ResNet34-3D [7] and are trained from scratch, and ViT encoders are trained on the pre-trained model which uses the MAE method [6].",2,positive
"Here, m indicates the learnable embedding of the mask token, which follows [24].",1,neutral
"SSL has provided competitive results against supervised learning baselines in a variety of downstream tasks, including ImageNet [15] fine-tuning [4, 24] and transfer learning on detection/segmentation tasks [23, 28].",1,neutral
"For example, given enough training epochs, BEiT [4] and MAE [24] can surpass other pre-training paradigms on detection tasks.",1,neutral
MAE [24] successfully performs pre-training via predicting raw pixels for the first time.,2,positive
"We further apply sin(·) and cos(·) operators to get the 2-D sin-cos positional embeddings, following the practice in MAE [24].",1,neutral
"Recently, another SSL framework has gradually attracted more attention, namely Masked Image Modeling (MIM) [4, 24].",1,neutral
"For masking strategy, we follow MAE [24] to use random masking with a masking ratio of 75%.",1,neutral
"Whereas recently, MAE [24] reports that color augmentations are not beneficial for MIM pre-training.",0,negative
"However, as [24] has pointed out, the dense prediction performance of ID methods on Vision Transformers is not superior to supervised learning, especially on object detection tasks.",1,neutral
"Unlike ID methods, MIM methods excel in transfer learning with full model fine-tuning with Vision Transformers, but lack good linearly-separated representations [24].",1,neutral
", predictive (Devlin et al., 2018; He et al., 2021), contrastive (Chen et al.",1,neutral
"…but also in recent self-supervised setting (SSL), in which models are trained with a surrogate loss (e.g., predictive (Devlin et al., 2018; He et al., 2021), contrastive (Chen et al., 2020; Caron et al., 2020; He et al., 2020) or noncontrastive loss (Grill et al., 2020; Chen & He, 2020))…",1,neutral
"rying out the masked image modeling task (He et al., 2022; Xie et al., 2022).",2,positive
"Generative pretraining has achieved great success in natural language processing (Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019; Liu et al., 2019; Conneau et al., 2020; Chi et al., 2021) and computer vision (Bao et al., 2022; He et al., 2021).",1,neutral
"Moreover, recent research in the field of CV also has shown that larger masking rates in the pre-training resulted in better results on the downstream tasks [21].",1,neutral
"Abbreviations
LSCI—laser speckle contrast imaging; VIT—vision transformer; CNN—convolution neural networks; MAE—masked autoencoder.",2,positive
"Using the MAE pretraining method, we found the VIT could be well initialized with dataset-specific hidden representation, which significantly improved the result in the small dataset.",2,positive
"Specifically, when using MAE as the pretraining method, the performance of VIT was further improved with the DICE value of 0.895 and outperformed CNNs by 2%.",0,negative
"Instead of using the weight pretrained in the ImageNet directly, we adopt a pretraining method, namely Masked AutoEncoder (MAE) [29].",2,positive
6 MAE384 [24] IN1K 1600 384×384 ∼232 84.,1,neutral
"Recently, self-supervised learning approaches [14, 7, 2, 24, 9, 1, 56, 41, 28] have achieved enormous success in learning representations conducive to downstream applications, such as image classification and object detection.",1,neutral
We follow the same experimental settings as MAE [24]; detailed hyperparameters can be found in the supplementary material.,2,positive
"For example, pretraining a MAE-Huge [24] on the 1.",1,neutral
"The Masked Autoencoder (MAE) model [24], shown on the left side of Fig.",1,neutral
"3 and Table 1 show the comparison of the computational efficiency among LoMaR, MAE [24] and BEiT [2].",1,neutral
"LoMaR relies on a stack of Transformer [53] blocks to pretrain a large amount of unlabeled images by recovering the missing patches from corrupted images similar to MAE [24], but LoMaR differentiates",2,positive
"Figure 1: We visualize the attention patterns employed by MAELarge [24] in the reconstruction of a random target patch, indicated by orange.",1,neutral
"Among these, several generative self-supervised learning methods such as Masked Autoencoder (MAE) [24] and Bidirectional Encoder Representation from Image Transformers (BEiT) [2], which reconstruct the input image from a small portion of image patches, have demonstrated excellent performance.",1,neutral
MAE [24] reconstructs directly the missing pixels.,1,neutral
", BEiT, iGPT, and MAE [26]) have difficulties in capturing high-level semantics.",1,neutral
"Actually, BEiT, iGPT [11], and MAE [26] can only perform well in pretraining tasks but fail to do well in direct discriminative representation learning tasks, e.",1,neutral
[26] thinks that the occasional inconsistency between linear evaluation and fine-tuning indicates that fine-tuning should be emphasized (see footnote4).,1,neutral
"Thereby, we focus on VITs to test the effectiveness of our method (see footnote3 for more reasons), similar to [3, 10, 16, 26].",2,positive
"ViTs are very big (≫ResNets) and tends to overfit [15, 26, 52], so we choose ViTs rather than ResNets.",1,neutral
"Besides, some studies also suggested that a D model might hold some disadvantages compared to a G model in generalization and interpretability [3, 6, 26].",1,neutral
"Besides, the literature hinted that D models might be less general and explainable than G models [3, 6, 26].",1,neutral
"However, the community reached a consensus that AE new varieties like denoising AE [53], masked AE [3, 26], and variational AE [35] are G models, because they can generate things that are not included in an input, e.",1,neutral
"1Following [26], we refer to semantics as visual concepts, e.",1,neutral
", MoCov3 [16], BEiT [3], MAE [26], and iBOT [65]), and our choice is in line with them.",2,positive
"Future work should examine how to integrate our method to other recent self-supervised masked image modeling methods, especially for the masked image modeling based methods [2,20,49].",2,positive
"In viewing patch-based sequence modeling of WSIs in relation to ViTs, we note that the architectural design choice of using Transformer attention enables pretraining of both the tokenization and aggregation layers in ViT models, which is important in preventing MIL models from over- or underfitting in low-data regimes [5, 13, 23, 33, 46].",1,neutral
The solution proposed by SIGMA is an ensemble of a Masked AutoEncoder (MAE) [16] and LaMa [50],1,neutral
SIGMA solution is an ensemble of a Masked Autoencoder (MAE) [16] and LaMa [51] (See Figure 5).,1,neutral
"SimMIM [57], BEiT [1], and MAE [20] all unveil that masked image modeling does not require heavy augmentations, and simple RRC is often sufficient to sample the input distribution for the reconstruction objective.",1,neutral
We choose MAE [20] with ViT base [13] backbone as our baseline.,2,positive
"6.1 Experimental setup
We choose MAE [20] with ViT - base [13] backbone as our baseline.",2,positive
"B.3.2 Fine-tuning
Our fine-tuning setting is identical to MAE [20].",2,positive
"B.3 Masked Image Modeling
B.3.1 Pre-training
We employ MAE [20] with ViT-base [13] backbone as our baseline.",2,positive
"Recently, motivated by the success of masked language modeling and vision transformers, masked image modeling methods [6, 1, 20, 13] have achieved superior performance.",1,neutral
"Similarly, Mockingjay [11] and MAE [6] are the audio and visual version of BERT ([4]), respectively.",1,neutral
", further simplifies the training paradigm in that it trains an asymmetric auto-encoder to construct the masked patches ([6]).",1,neutral
", GPT3 [26] in NLP, MAE [6] in CV, or GPT-GNN in graph learning ([54]), and gained considerable achievements.",1,neutral
[15] GPT-3 [26] processing TF-ID [14] SA-LSTM [16] ELMO [17] ULMFiT [18] RoBERTa [19] XLNeT [20] ALBERT [21] ELECTRA [22] DeBERTa [23] T5 [24] Computer Patch position prediction [8] DIM [35] MAE [6] – vision SIFT [27] AM-DIM [36] BEiT [44] HOG [28] CMC [37] MaskFeat [45] SURF [29] MoCo [38] Exemplar CNN [30] SimCLR [39] Counting [31] MoCo v2 [40] Jigsaw puzzle [32] SimSiam [41] Unsupervised tracking [33] DINO [42] Ego-motion [34] BYOL-A [43] Graph DeepWalk [46] DGI [48] Graph-GAN [53] – learning node2vec [47] InfoGraph [49] GPT-GNN [54] GRACE [50] CMVR [51] GROVER [52] Audio Mel-spectrograms [55] HuBERT [7] Mockingjay [11] wav2vec-C [9] processing CPC [56] APC [63] PASE [66] wav2vec [57] VQ-APC [64] PASE+ [67] COLA [58] TERA [65] CLAR [59] CLMR [60] BYOL-A [43] vq-wav2vec [61] wav2vec 2.,1,neutral
"For the network part, the pre-training is on the whole network
CV
Examplar CNN
Patch Position
Prediction
Jigsaw Puzzle
Counting
Unsupervised
Tracking
Ego-Motion
SIFT/ HOG/ SURF
Semantic Information
Spatial Information
Larger
Patch Camera Motion
DIM BYOL
DINO BEiT
MAE
SimSiam MaskFeat
With
Full Image
In Domain
Full
Image
Remove Negative
Transformer
Generation
Larger
Mask
All Patches
Word2VecNLP GPT BERT
Milestone Important
Improvement
Local
Modeling
Global
Modeling
Traditional Feature Engineering
AM-DIM/CMC
MoCo/MoCo v2/ SimCLR
Fig.2.",2,positive
"With the rapid growth of computational power, modern neural architectures endowed with self-supervised algorithms can even improve supervised models trained with over a million labeled data ([6]).",1,neutral
"Recent advances of SSL are converged to the generative modeling, e.g., GPT3 [26] in NLP, MAE [6] in CV, or GPT-GNN in graph learning [54], and gained considerable achievements.",1,neutral
"Similarly, Mockingjay [11] and MAE [6] are the audio and visual version of BERT [4], respectively.",1,neutral
"A simpler yet effective method, MAE, proposed by He et al., further simplifies the training paradigm in that it trains an asymmetric auto-encoder to construct the masked patches [6].",1,neutral
"It is worth noting that such an interpretation only relates to contrastive learning, not including generativebased self-supervised learning methods such as Masked AutoEncoder (MAE) (He et al., 2021).",1,neutral
", the encoder), and appends a decoder to it, aiming at recovering the original image contents, either tokenized features [1] or pixels [20], at the end of the decoder.",2,positive
Figure 1: Self-supervised pre-training of HiViT is significantly faster than Swin with SimMIM [54] and the result is better than ViT trained with MAE [20] and BEiT [1].,0,negative
"6% over ViT-B (using MAE [20], pre-training for 1600 epochs) and +0.",0,negative
"A typical example lies in masked image modeling (MIM), a recent methodology of pre-training vision transformers [1, 20, 54] – a random part of image patches are hidden from input, and it is difficult for the hierarchical models to",1,neutral
"We are interested in a particular generation-based method named masked image modeling (MIM) [1, 20].",1,neutral
"The existing pretext tasks are roughly partitioned into three categories, namely, geometry-based proxies that were built upon the spatial relationship of image contents [50, 33, 17], contrast-based proxies that assumed that different views of an image shall produce related visual features [21, 6, 18, 3, 2, 53, 41], and generation-based proxies that required visual representations to be capable of recovering the original image contents [58, 34, 20, 1, 40].",2,positive
", 4 in the regular setting of MAE [20]).",1,neutral
", global self-attentions), hence the units’ spatial coordinates can be discarded and the units can be serialized for efficient computation, like in MAE [20].",1,neutral
MAE [20] improved the MIM framework by only taking the visible tokens as input and computing loss at the pixel level – the former change largely accelerated the training procedure as the computational costs of the encoder went down.,2,positive
"For example, when the input image size is 224× 224, the masking unit size for MAE [20] is 16× 16.",1,neutral
"Existing self-supervised approaches have demonstrated that visual representations can be learned from unlabeled data by constructing pretexts such as transformation prediction [22], instance discrimination [72, 32], and masked image modeling [2, 31, 69], etc.",1,neutral
"Recently masked autoencoders [33], based on the ViT formulation, have been proposed for self-supervised learning and pretraining [6].",1,neutral
"In Figure 4, we show the results of training from scratch and uptraining from the MAE checkpoint [27].",0,negative
"Towards the end of 2021, SIMMIM [5] and MAE [6] applied GMML with reconstruction loss using huge vision",2,positive
The two notable post arts are SIMMIM [5] and MAE [6].,1,neutral
"For example, in computer vision (CV), one can split an image into sub-images and align them in a sequence as if they were tokens in NLP tasks [8, 9, 11].",1,neutral
"Many recent works have used vision Transformers for self-supervised learning [Bao et al., 2021, Ahmed et al., 2021, He et al., 2021, Caron et al., 2021, Li et al., 2021b,a].",1,neutral
"Decoder is a key component in the MAE framework, the design of which determines the semantic level of the learned latent representations [8].",1,neutral
", MAE [8], and find it also significantly improves downstream performance on lightweight ViTs.",2,positive
It has been validated that MAE works well across a wide range of the decoder’s width and depth under fine-tuning protocol when the encoder is large [8].,2,positive
"Our experimental setup on MAE-lite also largely follows those of MAE [8] which include optimizer, learning rate, batch size, argumentation, etc.",2,positive
"However, prior works point out that linear evaluation does not always correlate with utility [8, 15].",1,neutral
"We introduce MAE-lite to facilitate our study, which largely follows the design of MAE [8] except that the encoder is altered to ViT-Tiny.",2,positive
"We further examine a large pre-trained model, MAE-Base [8], and find it achieves a better alignment to DeiT-Tiny, as shown in the left column of Fig.",2,positive
"Specifically, a pre-trained MAE-Base [8] is introduced as the teacher network.",1,neutral
The proposed SupMAE is a supervised pre-training method built upon the recently introduced MAE (He et al. 2021).,2,positive
"To unearth the potential of ViT, self-supervised pretraining methods (Chen*, Xie*, and He 2021; Caron et al. 2021; Bao, Dong, and Wei 2021; He et al. 2021; Xie et al. 2021b) are emerging as an alternative.",2,positive
"Following He et al. (2021), we mask a large portion of patches (e.g., 75%).",2,positive
"Recently, self-supervised Masked Autoencoders (MAE) (He et al. 2021) have attracted unprecedented attention for their impressive representation learning ability.",1,neutral
"Following prior work (He et al. 2021; Devlin et al. 2018), we compute the loss only on masked patches.",2,positive
"Among them, the Masked Autoencoder (MAE) (He et al. 2021) is the stateof-the-art method that adopts a BERT-type masked autoencoding scheme (Devlin et al.",2,positive
"We compare SupMAE with three supervised counterparts: ViT (Dosovitskiy et al. 2020), DeiT (Touvron et al. 2021) and naive supervised results from He et al. (2021).",2,positive
"We use a one-layer transformer decoder for SupMAE, unlike an 8-layer transformer decoder for MAE (He et al. 2021).",2,positive
"When compared with the self-supervised method, such as MoCo-v3 (Chen*, Xie*, and He 2021), BEiT (Bao, Dong, and Wei 2021) and MAE (He et al. 2021), SupMAE can achieve comparable results with much lower training compute.",2,positive
"In this section, we compare the robustness performance of MAE (He et al. 2021), DeiT (Touvron et al.",2,positive
We follow the hyperparameters in MAE (He et al. 2021).,1,neutral
"Among them, the Masked Autoencoder (MAE) (He et al. 2021) is the stateof-the-art method that adopts a BERT-type masked autoencoding scheme (Devlin et al. 2018).",2,positive
(He et al. 2021) for the setup and training hyper-parameters.,1,neutral
"The reconstruction objective can be raw pixels (He et al. 2021; Xie et al. 2021b), discrete visual tokens (Bao, Dong, and Wei 2021; Dong et al. 2021), low-level local features (Wei et al. 2021), or latent representations (Baevski et al. 2022).",2,positive
"The reconstruction objective can be raw pixels (He et al. 2021; Xie et al. 2021b), discrete visual tokens (Bao, Dong, and Wei 2021; Dong et al.",2,positive
"We basically follow MAE
(He et al. 2021) for the setup and training hyper-parameters.",2,positive
He et al. (2021) show that standard supervised pre-training underperforms even with more data augmentations (Touvron et al. 2021) or stronger regularization (Steiner et al. 2021).,1,neutral
"For a fair comparison, we follow the same training configuration and code as MAE (He et al. 2021) with MMSegmentation (MMSegmentation 2020) framework.",2,positive
"Naive supervised indicates the supervised pre-training done from scratch, in which we directly use the reported mIoU from He et al. (2021).",2,positive
"Robustness evaluation on ImageNet variants In this section, we compare the robustness performance of MAE (He et al. 2021), DeiT (Touvron et al. 2021) and our SupMAE on four ImageNet variants.",2,positive
"Following the fine-tuning recipe in He et al. (2021), the pre-trained models are further fine-tuned on ImageNet for 100 epochs.",2,positive
"Overview of our ObjMAE based on MAE (He et al., 2021) for fast pre-training.",0,negative
"Unlike MAE (He et al.,
2021) that shuffles or unshuffles the token list, we gather the vector of target patches in the correct order.",1,neutral
"til recently, the self-supervised pre-trained models (Caron et al., 2021; Xie et al., 2021a; He et al., 2021; Bao et al., 2021) surpass the supervised models on computer vision tasks with the help of transformer architectures (Dosovitskiy et al.",2,positive
"The reconstruction result (left) of MAE (He et al., 2021) for the masked image (right).",1,neutral
"Note that BLACK patches are discarded.
til recently, the self-supervised pre-trained models (Caron et al., 2021; Xie et al., 2021a; He et al., 2021; Bao et al., 2021) surpass the supervised models on computer vision tasks with the help of transformer architectures (Dosovitskiy et al., 2021; Touvron…",2,positive
"MAE (He et al., 2021) applies an encoder and a lightweight decoder to reconstruct the missing pixels by masking random patches of the input image.",2,positive
"Unlike MAE (He et al., 2021) that shuffles or unshuffles the token list, we gather the vector of target patches in the correct order.",1,neutral
"Typically, Masked Autoencoders (MAE) (He et al., 2021) suggests that it is a good self-supervised task to randomly mask 75% of patches in an image.",1,neutral
"Learning to represent from unlabeled data without annotations, known as self-supervised learning, has attained great success in natural language processing [10, 32, 33, 5], computer vision [19, 7, 8, 18] and multi-modality learning [31, 50, 21].",1,neutral
"On top of that, MAE [18] directly reconstructs the raw pixel values of masked tokens and performs great efficiency with a high mask ratio.",2,positive
"Motivated by masked language modeling [32, 10], MAE [18] and some other methods [46, 53, 3] adopt asymmetric encoder-decoder transformers [13] to apply masked autoencoding for self-supervised learning on 2D images.",1,neutral
"Point-BERT [49] and Point-MAE [27] respectively introduce BERT-style [10] and MAE-style [18] pre-training schemes for 3D point clouds with standard transformer networks and performs competitively on various downstream tasks, but both of them can only encode point clouds with a single resolution and ignores the local-global relations between 3D shapes.",2,positive
"Other work [33, 34] center on Unsupervised learning and transfer learning.",1,neutral
"For occlusion, we consider the patch based random masking as adopted in most MIM works [21, 51, 44].",1,neutral
"For the masking strategy, we follow the common practice of existing works [16, 21, 51, 44] where the input image is divided into non-overlapping patches, and a random subset of patches are masked.",1,neutral
"For evaluation on Transformer, we follow MAE [21], which efficiently fine-tunes Mask R-CNN with ViT-B backbone using 1× schedule.",1,neutral
"Currently proposed works [16, 21, 51] adopt raw RGB values as the prediction target.",2,positive
MAE [21] and BEiT [4] mask out random patches of the input image and reconstruct the missing patches with ViT.,1,neutral
"More recently, inspired by the masked autoencoding in NLP such as GPT [36] and BERT [13], Masked Image Modeling (MIM) methods [21, 44, 51] have brought about new advances for selfsupervised pre-training for CV tasks.",1,neutral
We experiment on ADE-20K [58] using UperNet [48] following MAE [21] to fine-tune the model for 100-epoch with the batch size of 16.,2,positive
"Note that existing MIM works adopt a medium or high masking ratio [51, 21] (e.",1,neutral
"For evaluation on Transformer, we employ the fine-tuning as MAE [21], which uses DeiT [39] augmentation setting, an AdamW optimizer for 100-epoch training, and adopt a layer-wise learning rate decay of 0.",2,positive
"Masked token prediction has enabled highly successful methods for pre-training in NLP and vision, including Transformer [41], GPT [3], BERT [10], and MAE [17].",2,positive
"Similar to MAE [17] and BERT [10], we compute the loss only on the masked image",1,neutral
"hardware performance, self-supervised pre-training has made tremendous progress in natural language processing (NLP) and vision [17, 10, 2, 3].",1,neutral
"5% accuracy on linear classification, which exactly matches the original reported performance [17].",0,negative
"Following MAE [17], our decoder is lightweight and has 8 blocks of width 512.",2,positive
"For image patches, we use a learnable linear projection to convert them to image embeddings that have the same dimension as the language embeddings, and then apply 2D positional encodings, following the practice of MAE [17].",1,neutral
"Following MAE [17], we use a lightweight Transformer-based decoder on the full set of tokens consisting of (i) encoded visible image patches, (ii) encoded visible text tokens, and (iii) mask tokens.",2,positive
"Based on MAE [17], M3AE is trained purely via masked token prediction.",2,positive
"Thus, while our results cannot be directly compared to the original MAE results [17] pre-trained on ImageNet due to distribution mismatch, they demonstrate the strengths of multimodal training of M3AE for learning transferable representations across datasets.",2,positive
The success of MIM begs for the question: why does MIM perform so much better in fine-tuning?,1,neutral
"This usage is so important that solid improvements in this direction can usually attract significant attention, which is right the case for the recent masked image modeling methods [1, 17, 45].",1,neutral
"(a) Average attention distance (b) Accuracy and loss landscapes Figure 6: The average attention distances and loss / accuracy landscapes before (left) and after (right) feature distillation on a masked image modeling approach, MAE [17].",1,neutral
"We consider 5 pre-training methods of DINO [3], EsViT [26], CLIP [34], DeiT [36], and MAE [17], using their public checkpoints.",0,negative
"On masked image modeling (MIM) Figure 6 shows the average attention distance and the loss / accuracy landscapes of an MIM-based approach, MAE [17], before and after the feature distillation process.",1,neutral
"In terms of its scalability, the ability of MIM tasks to train large-capacity models has been well demonstrated in [17, 45, 29], however, the ability to benefit from larger data currently sounds negative, as shown in [12].",1,neutral
We observe that the feature distillation converts the old representations to new ones embodied a few desirable properties just like those representations produced by MIM.,1,neutral
It made the contrastive based self-supervised learning methods as competitive in fine-tuning as the state-of-the-art masked image modeling (MIM).,2,positive
We follow [17] to use the LARS optimizer [48] with a base learning rate of 0.,2,positive
"We made the following reflections to these approaches:
• Masked image modeling (MIM).",1,neutral
"In other words, are there key ingredients that can be added to other pre-training approaches to make them as successful as MIM in fine-tuning?",1,neutral
"Shared relative position bias In the original ViT [10], relative position bias (RPB) did not show any benefit over the absolute position encoding (APE), and so absolute position encoding (APE) is usually used for ViT architectures [10, 3, 7, 17].",1,neutral
These results may suggest that the good fine-tuning performance brought by the feature distillation post-processing have certain overlap in functionality with the masked image modeling (MIM) method.,1,neutral
"Recently, the masked image modeling (MIM) [4, 1, 45, 17] has achieved remarkable performance in fine-tuning evaluations [1, 45, 17] and attracted widespread attention.",1,neutral
"Over the past several years, self-supervised pretraining has attracted more and more attention, and achieved finetuning performance on par with the supervised counterparts on several representative downstream tasks [31, 8], including two representative ones, contrastive learning [18, 31, 8, 5, 27] and masked image modeling [7, 2, 30, 77].",1,neutral
"After (masked) language modeling repainted the NLP field [14, 53], recently, such task has also been shown to be a competitive challenger to the supervised pre-training in computer vision [7, 17, 2, 30, 85, 77].",1,neutral
"But if the model gets larger, MIM models still have the unique advantage that MIM tasks are harder to be overfitted than supervised tasks [30, 77], which is beyond the scope of this paper.",1,neutral
"Fortunately, most MIM pre-trained models [17, 2, 30, 85, 77] are established upon the Vision Transformers, where self-attention block is its major component.",1,neutral
"85, which conforms with the observation of [22].",1,neutral
"In sharp contrast to these methods, Masked Autoencoders (MAE) [22] proposes throwing the masked patches at the masking stage, i.",1,neutral
"Among them, the representative work Masked Autoencoder (MAE) [22] exhibited competitive performance as well as impressive efficiency.",2,positive
"Until recently, aided by the significant advancement of Vision Transformers [15], several MIM methods presented promising results [15, 2, 22, 66, 62] and became the state-of-the-art of self-supervised learning in CV.",1,neutral
", color bins [15], discrete tokens [2, 14] from pre-trained VAEs [57, 52], raw pixels [22, 66], and handcrafted features [62].",1,neutral
This study favors the simple prediction head design of SimMIM [66] with hierarchical models and is in contrast to the observation of MAE [22] with isotropic ones.,1,neutral
"8 Scratch, MAE [22] ViT-B 86M 0 300 82.",0,negative
"Among these approaches, MAE [22] exhibited competitive performance as well as impressive efficiency as it discards the masked tokens and operates only on the visible ones.",2,positive
", predicting the discrete tokens [2], the latent features [73, 62, 1], or the raw pixels [22, 66] of the randomly masked input image patches.",1,neutral
"Following the prior work MAE [22], we use a lightweight decoder that consists of nd (by default nd = 1) transformer blocks with an embedding dimension of 512.",2,positive
"The experimental settings of image and video mainly follow the ones utilized in MAE [43] and VideoMAE [78], respectively.",1,neutral
"We use MAE [43] as our self-supervised pre-training method in the image domain, a simple yet effective method that first masks nearly 75% patches of the input image and then reconstructs the missing pixels.",2,positive
"However, linear probing tends to have an unsatisfactory performance and misses the opportunity of pursuing strong but non-linear features [43], which indeed benefit deep learning.",1,neutral
"VideoMAE utilizes the plain ViT [31] architecture of joint space-time attention mechanism [2, 64] and an extremely high proportion of masking ratio (i.e., 90% to 95",2,positive
"Our default configurations follow the linear probing settings in [21, 43], which do not utilize many common regularization strategies, such as mixup [94], cutmix [91], color jittering and so on.",1,neutral
"We use VideoMAE [78] as our self-supervised pre-training method in the video domain, which is an direct extension of MAE to the video domain.",2,positive
"Furthermore, transformers have advanced the vision recognition performance by a large-scale pretraining [21, 67, 13, 36, 43, 78, 71].",1,neutral
"For video, we take both supervised and self-supervised pre-trained models from VideoMAE [78].",2,positive
"Recent arts in self-supervised learning [12, 5, 43, 97, 85, 78, 35] can serve as a solution to this challenge.",1,neutral
"Specifically, for image, we directly use the ImageNet-21k [26] supervised pre-trained model3 and MAE [43] self-supervised model4.",2,positive
", by modeling the dependency among image patches [24, 51], capturing the semantic similarity between image pairs [14, 16] or modeling image clustering patterns [7, 8].",1,neutral
"Inspired by the masked language modeling (MLM) approach [19] for pre-training text Transformers, masked image modeling (MIM) based methods [10, 3, 24, 45, 51] are proposed to learn self-supervised image representations with Vision Transformers.",1,neutral
"Supervised pretraining [27, 49, 77] casts representation learning as a multi-class/label classification problem, while un/self-supervised learning learns representation via proxy tasks like instance classification [92] and reconstruction [29, 60].",1,neutral
"Inspired by this, some works [15,21,23,27,35] showed diverse application scenarios that benefit from the usage of suitable PEs.",1,neutral
"Inspired by [12, 11], we add the whole initial embedding patches back to the last layer’s embedding patches to retain global information and maintain the correlations of all patches.",2,positive
"It has proven crucial for contemporary vision tasks [16, 2, 52, 1, 39] to first pretrain models on large data and then transfer the learned knowledge to downstream tasks.",1,neutral
"Motivated by the tremendous success of network pretraining in various vision tasks [17, 7, 16, 39] and natural language processing [10, 4], we propose a new paradigm that uses pretraining to improve image-to-image translation.",2,positive
MAE-ViT-B/16 [59] uses a masked image modeling task for self-supervised pre-training of the patch-based transformer architecture.,2,positive
"For self-supervised models, we use the linear probing result as the IN accuracy as reported in their paper [59, 66, 57].",1,neutral
"[59] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
", in 2021, showed that randomly masking pixels of an input image helps an autoencoder learn more robust embeddings, which would be useful for subsequent fine tuning tasks [10].",1,neutral
"As mentioned earlier, recent deep learning research has shown that incorporating self-supervised learning as part of the neural network’s training process can improve model performance [10][11][12].",1,neutral
"an auto-encoder neural network comprising an encoder and a decoder, to obtain embeddings that contain summarized information of the encoder’s input [10].",1,neutral
"Following their mainstream utilization in language tasks [51, 14, 40, 4], transformers have also started to powerhouse many state-of-the-art models in vision tasks [16, 25, 35, 52].",1,neutral
"In the training stage, we adapt masked autoencoders (MAE) [21] to reconstruct a new dataset from randomly masked face dataset.",2,positive
"Then, we briefly revisit the masked autoencoders (MAE) [21] and formally introduce masked face reconstruction.",2,positive
"Masked Autoencoders (MAE) [21] is a simple but efficient self-supervised pretraining strategy for image classification and its downstream tasks, such as object detection, instance segmentation, and semantic segmentation.",1,neutral
", 2021), MAE (He et al., 2021), and others (Zhou et al.",1,neutral
"Recently, there are also a family of emerging methods based on masked auto-encoding, such as BEIT (Bao et al., 2021), MAE (He et al., 2021), and others (Zhou et al., 2021; Dong et al., 2021; Chen et al., 2022).",1,neutral
"Similar problems are also discussed in self-supervised learning on images (He et al., 2022).",1,neutral
"The decoder fD maps the latent code H back to the input X , and its design would depend on the semantic level [12] of target X .",2,positive
"Actually, the idea of employingmasking as the corruption inmasked autoencoders has found wide applications in CV [1, 12] and NLP [4].",1,neutral
"Different from most GAEs’ efforts in structure reconstruction, GraphMAE only focuses on reconstructing features with masking, whose effectiveness has been extensively verified in CV [12] and NLP [4, 30].",2,positive
"The feature reconstruction criterion varies for masked autoencoders [4, 12] in different domains.",1,neutral
"While contrastive SSL methods have experienced an emergence in the past two years, such as MoCo [13], generative SSL has been gaining steadily increasing significance thanks to several groundbreaking practices, such as the well-established BERT [4] and GPT [30] in NLP as well as the very recent MAE [12] in CV.",1,neutral
"But in vision, previous studies [12] discover that a more advanced decoder (e.",1,neutral
"An exception is the MAE work [12] in CV, which directly predicts pixels in the masked patches using the mean square error (MSE); Nevertheless in fact, pixels are naturally normalized to 0–255, functioning similarly to tokenizers.",1,neutral
"Additionally, similar to MAE [12], a relatively large mask ratio (e.",1,neutral
"For large and huge models, we fine-tune them for 50 epochs following existing work (Bao et al., 2021; He et al., 2021).",2,positive
", 2020) and the results with strong data augmentation (He et al., 2021).",2,positive
"For training ViT from scratch, we report the original results (Dosovitskiy et al., 2020) and the results with strong data augmentation (He et al., 2021).",0,negative
"To compare these two objectives, we use supervised vision transformer (ViT) (Dosovitskiy et al., 2020) and vision masked autoencoder (MAE) (He et al., 2021) as two representative platforms to show our insights.",2,positive
", 2020) and vision masked autoencoder (MAE) (He et al., 2021) as two representative platforms to show our insights.",2,positive
"…conduct experiments on ImageNet-1K and compare with recent supervised vision models e.g., DeepViT (Zhou et al., 2021a) and DeiT (Touvron et al., 2021), and self-supervised vision models e.g., DINO (Caron et al., 2021), MoCo v3 (Chen et al., 2021), BEiT (Bao et al., 2021) and MAE (He et al., 2021).",2,positive
"With the masked autoencoder, recent studies (Bao et al., 2021; He et al., 2021) successfully train large-scale transformers, even without using additional training data compared to supervised learning.",2,positive
"For supervised learning models (i.e., ViT), we train base scale models for 300 epochs and large scale models for 200 epochs following He et al. (2021).",2,positive
"Most representation learning methods focus on generic feature vectors for entire images to initialize deep networks for improved object classification [98, 10, 72, 29, 28].",1,neutral
"2 Image Reconstruction The masked image Im is generated by first uniformly dividing the image I into a 16× 16 grid, and randomly masking out 80% of the grid cells, similar to [28].",1,neutral
"We compared CMAE with MOCO-v1 [16], MOCO-v2 [5], and MAE [15].",2,positive
proposed a new auto-encoder called Masked AutoEncoders [15] for training transformer by partially masking the image.,1,neutral
"In this work, we seek to continue the success of MLM and MIM by introducing masked graph modeling (MGM) as a principled pretext task for graph-structured data.",2,positive
"We compare against baselines belonging to the three categories: (i) generative learning methods, including (V)GAE [17], AR(V)GA [21], and GraphMAE [13]; (ii) contrastive learning methods, including DGI [38], GMI [23], GRACE [52], GCA [53], MVGRL [10],
Table 4: Node classification accuracy (",2,positive
"In addition, MaskGAE also outperforms the state-of-the-art method GraphMAE on six out of seven datasets.",2,positive
MaskGAE versus GraphMAE.,1,neutral
"Recently,
masked image modeling (MIM) [11, 44] follows a similar principle to learn representations by predicting the missing parts at the pixel or patch level.",1,neutral
"In this section, we present the MaskGAE framework for the MGM pretext task, which is developed by taking inspiration fromMLM [4] and MIM [11].",2,positive
"As shown in Figure 1(a) and 1(b), masked language modeling (MLM) and masked image modeling (MIM) have been widely applied to text and image data, with prominent examples including BERT [4] and MAE [11], respectively.",1,neutral
"Since GraphMAE is not initially designed for link prediction tasks, we instead trained an MLP decoder on the learned representations for GraphMAE to solve the link prediction task.",2,positive
"(ii) GraphMAE only presents empirical results, without in-depth analysis or theoretical justification on the benefits of the masked autoencoding scheme on graphs.",1,neutral
"In work concurrent with the present paper, MGAE [32] and GraphMAE [13] seek to apply this idea directly to graph data as a self-supervised learning paradigm, by performing masking strategies on graph structure and node attributes, respectively.",1,neutral
GraphMAE and MaskGAE share similar intuitions on the self-supervised learning scheme.,1,neutral
"MIM is gaining renewed interest from both industries and academia, leading to new state-of-the-art performance on broad downstream tasks.",2,positive
"In a recent exploration [50], the authors discovered an implicit connection between MIM and contrastive learning.",1,neutral
"However, they are technically different in some aspects (see also Table 6): (i) GraphMAE focuses on feature reconstructionwhile MaskGAE focuses on structure reconstruction, including edge and degree reconstructions.",1,neutral
Masked autoencoding is one such learning task: masking a portion of input signals and attempting to predict the contents that are hidden by the mask [11].,1,neutral
"Cora CiteSeer Pubmed Photo Computer arXiv MAG
MLP 47.90 ± 0.40 49.30 ± 0.30 69.10 ± 0.20 78.50 ± 0.20 73.80 ± 0.10 56.30 ± 0.30 22.10 ± 0.30 GCN 81.50 ± 0.20 70.30 ± 0.40 79.00 ± 0.50 92.42 ± 0.22 86.51 ± 0.54 70.40 ± 0.30 30.10 ± 0.30 GAT 83.00 ± 0.70 72.50 ± 0.70 79.00 ± 0.30 92.56 ± 0.35 86.93 ± 0.29 70.60 ± 0.30 30.50 ± 0.30 GAE 74.90 ± 0.40 65.60 ± 0.50 74.20 ± 0.30 91.00 ± 0.10 85.10 ± 0.40 63.60 ± 0.50 27.10 ± 0.30 VGAE 76.30 ± 0.20 66.80 ± 0.20 75.80 ± 0.40 91.50 ± 0.20 85.80 ± 0.30 64.80 ± 0.20 27.90 ± 0.20 ARGA 77.95 ± 0.70 64.44 ± 1.19 80.44 ± 0.74 91.82 ± 0.08 85.86 ± 0.11 67.34 ± 0.09 28.36 ± 0.12 ARVGA 79.50 ± 1.01 66.03 ± 0.65 81.51 ± 1.00 91.51 ± 0.09 86.02 ± 0.11 67.43 ± 0.08 28.32 ± 0.18 GraphMAE 84.20 ± 0.40 73.40 ± 0.40 81.10 ± 0.40 93.23 ± 0.13 89.51 ± 0.08 71.75 ± 0.17 32.25 ± 0.37 DGI 82.30 ± 0.60 71.80 ± 0.70 76.80 ± 0.60 91.61 ± 0.22 83.95 ± 0.47 65.10 ± 0.40 31.40 ± 0.30 GMI 83.00 ± 0.30 72.40 ± 0.10 79.90 ± 0.20 90.68 ± 0.17 82.21 ± 0.31 68.20 ± 0.20 29.50 ± 0.10 GRACE 81.90 ± 0.40 71.20 ± 0.50 80.60 ± 0.40 92.15 ± 0.24 86.25 ± 0.25 68.70 ± 0.40 31.50 ± 0.30 GCA 81.80 ± 0.20 71.90 ± 0.40 81.00 ± 0.30 92.53 ± 0.16 87.85 ± 0.31 68.20 ± 0.20 31.40 ± 0.30 MVGRL 82.90 ± 0.30 72.60 ± 0.40 80.10 ± 0.70 91.70 ± 0.10 86.90 ± 0.10 68.10 ± 0.10 31.60 ± 0.40 BGRL 82.86 ± 0.49 71.41 ± 0.92 82.05 ± 0.85 93.17 ± 0.30 90.34 ± 0.19 71.64 ± 0.12 31.11 ± 0.11 SUGRL 83.40 ± 0.50 73.00 ± 0.40 81.90 ± 0.30 93.20 ± 0.40 88.90 ± 0.20 69.30 ± 0.20 32.40 ± 0.10 CCA-SSG 83.59 ± 0.73 73.36 ± 0.72 80.81 ± 0.38 93.14 ± 0.14 88.74 ± 0.28 69.22 ± 0.22 27.57 ± 0.41 MaskGAE𝑒𝑑𝑔𝑒 83.77 ± 0.33 72.94 ± 0.20 82.69 ± 0.31 93.30 ± 0.04 89.44 ± 0.11 70.97 ± 0.29 32.75 ± 0.43 MaskGAE𝑝𝑎𝑡ℎ 84.30 ± 0.39 73.80 ± 0.81 83.58 ± 0.45 93.31 ± 0.13 89.54 ± 0.06 71.16 ± 0.33 32.79 ± 0.32
BGRL [33], SUGRL [20], and CCA-SSG [48].",0,negative
"masked image modeling (MIM) [11, 43] follows a similar principle to learn representations by predicting the missing parts at the pixel or patch level.",1,neutral
"Similar to MLM and MIM tasks in language and vision research, MGM aims to assist the model to learn more useful, transferable, and generalized representations from unlabeled graph data through masking and predicting.",1,neutral
"In a contemporary work [50], the authors also made a “contrastive interpretation” of the masking procedure in MIM as conducting contrastive learning in an implicit form, in which a contrastive-type loss is used to lower bound the reconstruction loss in the MIM setting.",1,neutral
"Recent works have started to apply reconstruction-based self-supervised learning on image and other data [40], [41], which are straightforward but show great performance.",1,neutral
"Like the recent work [40], our decoder is a shallower Transformer (which has only two layers), because a lighter decoder can help reduce pre-training time without influencing negatively the performance on down-stream tasks [40].",2,positive
"Masking parts of data and reconstructing them is a common paradigm in self-supervised learning tasks [32], [40].",1,neutral
"Moreover, MAE [23] is proposed to mask most of the image patches and apply an unsupervised image reconstruction method to pre-train transformer to improve its generalizability for downstream tasks.",2,positive
"[29, 23], we add the embeddings of all image patches before the first encoder layer to the input of the last encoder layer as in Figure 2.",1,neutral
"[23], we add an image patch mask (detailed in",2,positive
"Different from masking random patches in MAE [23], our design masks the task-irrelevant ones to focus on the task-relevant prior knowledge for better generalizability.",2,positive
(a) Random Sampling (RS) in MAE [19]; (b) Grid-wise Sampling (GS) in MAE; and the proposed (c) Uniform Sampling (US); (d) Uniform Masking (UM) that includes US and Secondary Masking (SM).,1,neutral
We follow MAE [19] where the operation units are 16× 16 image patches.,1,neutral
", high-dimensional semantic labels) of image patches, the Masked Autoencoder [19] (MAE) method is the first to demonstrate that high-quality image representations can also be obtained by directly recovering the original pixels of an image.",1,neutral
"Compared to Random Sampling as adopted in MAE [19], Uniform Sampling (US) samples the image patches uniformly distributed over the 2D space that makes it compatible for representative Pyramid-based ViTs.",1,neutral
"A series of works [2, 13, 49, 6, 19, 42, 39, 35] has been proposed in past months, where Masked AutoEncoder (MAE) [19] becomes",1,neutral
Sampling Strategy (25%) Pyramid SM Pre-train ImageNet-1K ADE20K COCO Support Ratio Loss Top-1 Acc mIoU aAcc AP AP50 AP75 (a) RS (MAE [19] Baseline) × – 0.,0,negative
"Similarly following MAE [19], the left 75% masked patches are dropped and would not participate in the feed-forward process of the encoders.",0,negative
"In this paper, as suggested in MAE [19], we focus on the fine-tuning accuracy (instead of linear probing) over a series of downstream tasks, including image classification using ImageNet-1K (IN1K) [10], semantic segmentation using ADE20K [47] and object detection using COCO [26].",2,positive
Here 25% sample ratio empirically refers to the optimal mask ratio 75% in MAE [19].,1,neutral
"MIM has been successfully applied to transformers to capture local context while preserving global semantics in natural image analysis tasks[17,18,19,20,19,21].",1,neutral
"We follow the masking strategy of MAE [16], i.",2,positive
"We achieve much better performance than MAE [16], especially on the ViT-Small backbone.",2,positive
"The pretraining-and-then-finetuning paradigm has been proven to be effective for visual representation learning and various downstream tasks, where self-supervised pre-training [17, 4, 3, 2, 5, 15, 1, 9, 16, 39, 43, 45, 13, 36] is the most popular.",1,neutral
The recent work MAE [16] casts masked image modeling as a pixel-level reconstruction work rather than vision dictionary look-up.,1,neutral
"Towards this goal, the recent work MAE [16] proposes to mask a large proportion of patches.",2,positive
", MAE [16]) or a self-distillation task (e.",1,neutral
"To accelerate training, we follow MAE (He et al., 2022) and skip the mask token [MASK] in the encoder and only apply it in the lightweight decoder.",2,positive
"To this end, we experiment with our VLC checkpoint as well as other three pretrained checkpoints: ViT (Dosovitskiy et al., 2021), ViLT (Kim et al., 2021) and MAE (He et al., 2022).",2,positive
"In contrast, we follow MAE (He et al., 2022) to randomly mask image patches with a probability of 0.",1,neutral
", 2022) and MAE (He et al., 2022) predict RGB values of raw",1,neutral
", 2021) initialized from MAE (He et al., 2022) (ImageNet-1K without labels) as our Transformer backbone; (3) Task-specific decoder.",2,positive
"In contrast, we follow MAE (He et al., 2022) to randomly mask image patches with a probability of 0.6, and reconstruct the missing pixels based on both non-masked tokens w\m and patches v\m.",2,positive
"We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders (He et al., 2022) that does not require this supervision.",2,positive
"Compared with MAE (He et al., 2022), our model learns competitive multi-modal representations from vision-language pretraining while retains high-quality image representations.",2,positive
"An unsupervised visual semantics is learned via Masked Auto-Encoders (He et al., 2022) before language is integrated.",1,neutral
"SimMIM (Xie et al., 2022) and MAE (He et al., 2022) predict RGB values of raw
pixels by direct regression.",1,neutral
"To this end, we choose encoder-decoders pre-trained by MAE [11] and migrate them to conventional two-stage detectors, e.",2,positive
MAE pre-trains encoder-decoder representation models based on the pretext task of masked image modeling [11].,1,neutral
These models are pre-trained on ImageNet-1K using the self-supervised MAE method [11] for 1600 epochs.,1,neutral
"It was validated that the MAE decoder has the ability to reconstruct masked pixels under a high mask ratio of 75% [11], demonstrating strong capacity to model image context information.",2,positive
", masked autoencoder (MAE) [11], demonstrated great potential.",1,neutral
"Recently, ViTDet [17] and MIMDet [8] tried the powerful representations pre-trained by MAE [11] for object detection.",2,positive
The extracted features are then embedded with location information by summarizing with position embeddings [11].,1,neutral
"Such generalization capability was pushed to a new height by MAE [11], which constructed not only representation models for feature extraction but also decoders for image reconstruction.",2,positive
"1(lower), imTED employs the ViT encoder pre-trained with MAE [11] as backbone, and uses the decoder as the detector head.",2,positive
"The vision transformers [1, 11, 27, 39] trained with self-supervised paradigms were validated to have higher generalization capability.",1,neutral
"TL is also fast-growing in NLP beginning at BERT [20], which often leverages web-scale unlabeled texts for self-supervised pretraining and then applies to specific tasks [34, 54, 55, 56, 57].",1,neutral
"Although there are numerous papers on zero-shot learning (ZSL) in CV and NLP [33, 34, 35], we notice that ZSL was hardly mentioned in tabular domain.",1,neutral
"In computer vision, the basic elements are pixels [15] or patches, [16, 17]; in natural language processing (NLP), the basic elements are words [18] or",1,neutral
"In computer vision, the basic elements are pixels [15] or patches, [16, 17]; in natural language processing (NLP), the basic elements are words [18] or
1Code is available at https://github.com/RyanWangZf/transtab.",1,neutral
The gated tabular transformer is an adaption of the classical transformer in NLP [23].,1,neutral
"SSL uses unlabeled data with pretext tasks to learn useful representations and most of them are in CV and NLP [20, 17, 15, 16, 58, 23, 59, 60, 61, 62, 63].",1,neutral
"With the development of neural networks, deep learning methods are adopted in many areas [9, 22, 29], in addition to abundant works based on CF, many deep learning methods have been proposed for recommendation, covering the shortage of traditional models [10, 13, 16, 34, 39].",1,neutral
"2), higher than the masking ratio of 75% for its image counterpart [31].",0,negative
"Put together with a small decoder [31], the MAE pre-training can achieve a theoretically 7.",2,positive
It is hypothesized in [31] that the masking ratio (i.,1,neutral
"Following the MAE in [31] that applies the encoder only on visible tokens, a masking ratio of 90% reduces the encoder time and memory complexity to <1/10.",1,neutral
"Our study suggests that the general framework of masked autoencoding (BERT [15], MAE [31], etc.",1,neutral
"Following this philosophy, we study extending Masked Autoencoders (MAE) [31] to the problem of spatiotemporal representation learning.",1,neutral
This paper studies a conceptually simple extension of Masked Autoencoders (MAE) [31] to spatiotemporal representation learning from videos.,1,neutral
"75% on images [31]), supporting the hypothesis that this ratio is related to information redundancy of the data.",1,neutral
"For self-supervised representation learning, the denoising/masked autoencoding methodology [68] in BERT [15] has been shown effective on learning visual representations from images [31].",1,neutral
"These results suggest that self-supervised learning on videos can be tackled in a way similar to its counterparts on language [15] and images [31], under a unified framework.",1,neutral
"Following [14], the hyper-parameters of GCMAE method are set: τ = 0.",1,neutral
"Recently, self-supervised visual representation learning has achieved great success in the field of natural images [13,5,6,12,7,2,8,14].",1,neutral
"(1), the weighted sum of the MSE loss [14] of tile feature extraction and the NCE loss [26] of global feature extraction is used as the cost function, which can reduce the distance between similar features while learning the high-level features of images, so as to improve the generalization of the model and improve the accuracy of the cross-dataset transfer learning task.",1,neutral
"the optimal result of model fine-tuning is achieved at a higher mask ratio of 80%, which shows that the mask ratio suitable for linear classification in pathological image field is not suitable for model fine- tuning, which further proves the result in [14].",1,neutral
"In 2021, as an extensible SSL method, Mask Autoencoder (MAE) achieved SOTA results in ImageNet dataset [14].",2,positive
"This method randomly masks part of the input image and uses the lightweight decoder to rebuild the obscured pixels, which can not only improve the accuracy but also speed up the training, and the learning efficiency is better than that of contrastive learning [14].",1,neutral
"However, the above-mentioned self-supervised method based on contrastive learning has the problems of large consumption of hardware resources, difficulty in training multi-task learning, and lower performance of cross-data set transfer learning than supervised learning [14,24].",1,neutral
"Recently, BEiT [4] and MAE [27] extended the scope of ViT to self-supervised learning with masked image modeling, demonstrating the powerful potential of the pure ViT architecture.",1,neutral
"For example, simply replacing the DeiT weights [71] with the MAE weights [27] gives us an extra gain of 0.8 APb and 0.5 APm.",1,neutral
"It’s worth noting that our method could enjoy diverse advanced pre-training for free, such as the mask image modeling in MAE [27] and the multi-modal pre-training in Uni-Perceiver [86].",2,positive
"Specifically, we also use the MAE [27] pre-trained weights to initialize the ViTB, and apply the upgraded Mask R-CNN [28] and a stronger training formula (i.e., large scale jitter [25] and cosine learning rate decay).",2,positive
"Specifically, we also use the MAE [27] pre-trained weights to initialize the ViTB, and apply the upgraded Mask R-CNN [28] and a stronger training formula (i.",2,positive
"For example, simply replacing the DeiT weights [71] with the MAE weights [27] gives us an extra gain of 0.",1,neutral
"Following the setup in BEiT [29] and MAE [30], the input is split into 14 × 14 image patches and the same number of visual tokens for BEiT and 16× 16 patches for MAE.",1,neutral
"and MAE on Retina and Derm is 40% and 60%, respectively, which is consistent with previous findings on ImageNet [29], [30].",0,negative
"Specifically, we integrate two popular masked image modeling methods, BEiT [29] and MAE [30], into our generalized federated framework.",2,positive
"With the recent advance in Vision Transformer (ViT) [16], multiple works such as BEiT [29] and MAE [30] have been proposed to learn visual representations by signal reconstruction given corrupted images.",1,neutral
"their supervised counterparts when fine-tuning with ImageNet in centralized learning [29], [30].",1,neutral
while achieving competitive performance [30].,0,negative
"geNet) [15], (3) ImageNet pre-training using BEiT [29] (BEiT ImageNet), and (4) ImageNet pre-training using MAE [30]",1,neutral
"We implement two popular masked image modeling methods, BEiT [29] and MAE [30], as the SSL module in our federated framework.",2,positive
"Specifically, we implement two masked image modeling methods, BEiT [29] and MAE [30], as the SSL module in our federated framework.",2,positive
"Furthermore, by equipping a masked autoencoder (MAE) [18] as a backbone, our proposed model is more robust in capturing occluded text instance regions, which makes it more suitable for visual place recognition.",2,positive
It is worth mentioning that we use a pre-trained MAE [18] (ViT-Base/16) as the backbone for feature extraction.,2,positive
"Model Recall 0.2 0.4 0.6 0.8 0.9 Our model 1 1 1 0.97 0.93 TextPlace 1 1 1 0.96 0.91 NetVLAD-10 1 1 1 0.95 0.93 NetVLAD-20 1 1 1 0.91 0.87 NetVLAD-30 1 1 0.97 0.85 0.83 ToDayGAN-10 0.50 0.55 0.58 0.57 0.56 ToDayGAN-20 0.40 0.40 0.40 0.38 0.38 ToDayGAN-30 0.26 0.24 0.24 0.25 0.24 FAB-MAP-10 0.79 0.69 0.67 0.65 0.63 FAB-MAP-20 0.76 0.69 0.67 0.63 0.60 FAB-MAP-30 0.68 0.67 0.67 0.62 0.58 SeqSLAM 0.30 0.24 0.18 0.13 0.13
has leveraged a robust and SOTA backbone of pre-trained MAE and a modified multi-task transformer detector.",0,negative
"This method utilized an MAE in their pipeline equipped with a powerful detector, namely deformable-DETR [70], to capture the arbitrary shape of occluded text instance in the wild images.",1,neutral
It was inspired by the concept of BERT [83] and masked autoencoder [111].,2,positive
"In SSL, deep networks are trained with pretext tasks such as clustering [22], mutual information maximization [23], image colorization [24], masked token prediction [25], [26], contrastive learning [27], [28], [29] and so on.",1,neutral
[21] proposed masked autoencoders to achieve scalable vision task learning.,1,neutral
"These models include ViT-RF, Seasonal ViT-RF, Seasonal DeepViT-RF, and Seasonal MAE-ViT-RF, where ViT is pre-trained by a Masked Autoencoder (He et al., 2022).",2,positive
They are similar to the samplings in MAE [14].,1,neutral
"Subsequently, MAE [90] tries to partly mask the image with the assistance of self-supervised learning.",1,neutral
"In recent years, Transformer [40] and Transformer-based pretrained models [12, 35] have revolutionized natural language processing [33] and there have been growing interests in extending the successful paradigm to broader artificial intelligence areas including computer vision [8, 23, 32], speech processing [4] and program analysis [18].",1,neutral
"In more detail we introduce a series of vision models, including Residual Convolutional Neural Networks (CNNs) [12] and Vision Transformers (ViT) variants [13,14,10], adapted for sourcecode understanding (Figure 1).",2,positive
"Following recent advancement of self-supervised learning for Vision Transformers [11], we would like to test and scale up the CV4Code transformer with the abundance of unlabelled sourcecode snippets in the public domain.",2,positive
"Given content representation z, to reconstruct the patches in the i-th domain, we feed both the content representation z and the learnable masked tokens [17] into the i-th domain specific decoder Gi, i.",2,positive
", generative self-supervised learning, and propose a new Domain invariant Masked AutoEncoders (DiMAE) for learning domain-invariant features from multi-domain data, which is motivated by the recent generative-based self-supervised learning method Masked Auto-Encoders (MAE) [17].",1,neutral
"A very relevant work, MAE [17] proposes to train the autoencoder to capture the semantic representation by recovering the input image from very few neighboring patches.",2,positive
"Similar to MAE [17], our content encoder also follows the vision transformer design, which extracts content representations only by visible patches.",2,positive
"As pointed in MAE [17], the decoder design plays a key role in determining the semantic level of the learnt latent features.",1,neutral
"Generally, SSL can be categorized into discriminative [5, 7, 9, 10,15,16,18,29,41] and generative methods [17,23,24,30].",1,neutral
", such as the number of layers, can determine the semantic level of the learned latent representations as pointed out in MAE [17],",1,neutral
"For this reason, S is usually the augmented images [2, 5, 7, 15, 17, 31, 36] or random masked images [2, 4, 16, 35].",1,neutral
"Alternatively, based on ViT [12] framework, we optimize the objective function on both pixel-level reconstruction [4, 16, 35] and features-level regression [2] to predict the content of masked regions.",2,positive
"We, however, take a fundamentally different approach by not targeting to optimize the contrastive function [20, 31] but focusing on maximizing the mutual information between masked inputs and self-supervised signals at pixel-level reconstruction [4, 16, 35] and features-level regression [2].",1,neutral
"In addition, while most existing works [2, 4, 16, 35] utilize a random image masking strategy, our method uses adaptive sampling to more effectively minimize the conditional en-",1,neutral
"x = {xi : i / ∈ M}i=1 ∪ {ei : i ∈ M}i=1, where e is the learnable embedding replacing for masked patches, we feed only the unmasked patches xU = {xi : i / ∈M}i=1 which is similar to MAE [16].",1,neutral
"The method maximizes the mutual information between masked inputs and self-supervised labels automatically generated by pretext tasks [2, 4, 16].",1,neutral
MAE [16] and SimMIM [35] learn to reconstruct missing image patches from uncorrupted patches.,1,neutral
"We propose a novel SSL framework that optimizes both pixel [4, 16, 35], and feature-level losses [2,20,31] for brain cell image analysis, called DAMA as shown in Fig.",2,positive
"Recent works built upon Vision Transformer (ViT) [12] framework, such as BeiT [4], MAE [16], SimMIM [35] have shown potential of learning rich features.",2,positive
"S can be the augmented image [5, 7, 17] or the target of image reconstruction [4,16,35].",1,neutral
"For example, the BERT [Devlin et al., 2018]-based pre-training model has achieved remarkable results in many NLP tasks, and similar works are also proposed in CV [He et al., 2021].",2,positive
", 2018]-based pre-training model has achieved remarkable results in many NLP tasks, and similar works are also proposed in CV [He et al., 2021].",2,positive
"For example, some popular SSL-based methods generate abnormal images by random pasting [10], [21] or masking [19], [23], [26], [33].",1,neutral
"In natural image classification datasets such as ImageNet [1], some SSL-based models have achieved accuracies comparable to conventional supervised learning [24]–[26].",1,neutral
"Pre-training is proven effective on many high-level vision tasks [1, 14, 16].",1,neutral
"Self-supervised learning frameworks, such as DINO [5], MOCO-V3 [8], MAE [26], unleash the potential of Vision Transformers (ViT) and achieve strong performance on various vision downstream tasks [31, 28, 60].",1,neutral
"Masked auto-encoding [2, 1, 26, 57] for feature pretraining and multi-scale hybrid convolution-transformer architectures [10, 20, 51, 32, 59] can further unleash the potentials of ViT, leading to state-of-the-art performances on image classification, detection and semantic segmentation.",1,neutral
Mask Autoencoders (MAE) [26] is the recent representative self-supervised method for training ViT.,2,positive
"The masking strategy adopted in current maskautoencoding frameworks, such as BEiT [2], MAE [26], SimMIM [61], cannot be naively used for ConvMAE as all tokens need to be kept in the later transformer stages.",2,positive
"Different from MAE [26], the encoder of ConvMAE progressively abstracts the input image into multi-scale token embedding of 1/4, 1/8, 1/16 input resolutions at stages 1, 2, 3, respectively, while the decoder reconstruct the pixels corresponding to masked tokens by utilizing multi-scale token embeddings.",2,positive
Masked Autoencoders (MAE) [26] is a self-supervised method for pretraining ViT by reconstructing masked RGB patches from visible patches.,1,neutral
"In contrast to MAE [26], well-performing multi-scale backbones built upon local and global operations are mainly trained with supervised learning approaches.",2,positive
"Compared with MAE-Base [26], ConvMAE-Base improves the ImageNet-1K finetuning accuracy to 85.",2,positive
"To build a strong and scalable image learner, MAE [26] introduced an asymmetric encoder and decoder architecture where masked tokens is skipped in computation-heavy encoder and only pass all tokens through a light-weight decoder.",2,positive
"A natural question is whether multi-scale backbone with local and global operations, which show promising performance on supervised learning can be enhanced by the masked auto-encoding paradigm [26, 13, 2, 66].",1,neutral
"The derived image is the input, and the original image or its features are predicted, and related methods include MAE [14], Simmim [38], Maskfeat [37] and so on.",1,neutral
"Recent approaches can be categorized as discriminative [10,1,16,6,14] or generative [24,46,2,15].",1,neutral
", nearby patches are semantically similar) [18], applying attention to all the spatial tokens, even in a down-sampled feature map, is inefficient.",1,neutral
"With the advancement of deep generative models, parametric analysisby-synthesis techniques are having a renaissance, with some top-down [93, 55, 95] as well as bottom-up [60, 22] techniques.",1,neutral
"As also discussed in [23], linear-evaluation struggles to accurately measure learned representations and we find the attentional poolers are more practical for real-world applications.",1,neutral
MAE [23] and SimMIM [24] remove the need for an image tokenizer and directly use a light-weight decoder or projection layer to regress pixel values.,2,positive
"Figure 9 supplements our analysis of self-supervised methods with the recent MAE models of (He et al., 2021).",2,positive
We fine-tune the MAE [22] using cropped characters of SynthText [66] and applied it to the occluded characters.,2,positive
"works [22, 23, 31, 32] to directly predict character and word instances from a given image.",1,neutral
"computer vision tasks, like classification, detection, and segmentation [22, 23].",1,neutral
"We train all our final STR and scene text spotting models on 4 GPUs of NVidia A100, and we use a pre-trained encoder backbone of MAE (ViT-Base/16) [22] and fine-tune",2,positive
"However, unlike a conventional CNNs like ResNet [41] with multiscale feature maps used in [32], the ViT encoder in MAE [22] has a ”columnar” structure [71] and generates a singlescale features f specifically designed for classification tasks, make it unsuitable for our character detection that require multi-scale features.",2,positive
This model achieves SOTA performance in occluded text instances by leveraging a pre-trained masked backbone [22].,1,neutral
"We utilize MAE [22, 23] as our feature extraction backbone, which is based on a standard Transformers [49] architecture, namely ViT [64].",2,positive
"Recently, a Transformer-based network, namely Masked Auto-encoders (MAEs), was proposed in [22] for masking a large portion (∼ 75%) of an input image and reconstructing the missing pixel.",1,neutral
"In our proposed scene recognition pipeline, we first fine-tune the pre-trained version of MAE (ViT-Base) [22] on 36 classes of cropped characters of Synth-Text [66], We then remove the decoder,",2,positive
"Next, we use a pre-trained encoder model of MAE [22] that use a ViT Transfomer (ViT B/16) as our Network’s backbone to extracts the 2D features from these input patches.",2,positive
"Transformers are now the state of the art machine learning paradigm for natural language understanding, computer vision, biological sequence analysis, and context sensitive reasoning tasks [19, 54, 5, 7].",1,neutral
"Downstream Image End-to-end Fine-tuning In addition to the five downstream tasks mentioned in the main text, following MAE [50], we also conduct the end-to-end fine-tuning experiments on downstream classification task.",2,positive
"Transformer sequence models (Vaswani et al., 2017) have been successfully applied in other domains such as natural language processing (Devlin et al., 2018; Radford et al., 2018; Brown et al., 2020) and computer vision (Dosovitskiy et al., 2020; He et al., 2021).",1,neutral
"In addition, to encourage the networks to more focus on context information, we artificially hide a patch Ipatch in the image I and make the networks recover the patch, which is proven to be effective in vision tasks [69, 70].",2,positive
"Therefore, a relatively lightweight decoder [23] can be applied for efficiency.",1,neutral
"[23] for image self-supervised learning, we present masked spectrogram prediction (MaskSpec), a pre-training objective that directly recovers the masked patches of spectrogram.",2,positive
"We explore different reconstruction targets for MVM in video-text pre-training including raw frame pixels as in [21], discrete visual tokens from a learned image “tokenizer” [39] as in [7], and the textaligned features in this work.",2,positive
"For example, [21] reconstructs the masked image patches in",1,neutral
2022b) and masked image modeling (He et al. 2022; Xie et al. 2022) becomes a new trend on ViTs.,2,positive
Training with masks shows no merit Patch-wise augmentation has been adopted in recent work (Ge et al. 2021; Mao et al. 2022b) and masked image modeling (He et al. 2022; Xie et al. 2022) becomes a new trend on ViTs.,2,positive
", 2022) have shown promising performance such that “Selfsupervised learning in vision may now be embarking on a similar trajectory as in NLP” (He et al., 2022).",2,positive
"A reasonably deep decoder can help make latent representations from the encoder output more abstract (He et al., 2022; Cao et al., 2022).",1,neutral
"Figure 1: MAE (He et al., 2022) pre-training ow; we redraw Figure 1 in the MAE paper, in which we replaced the input image with a spectrogram and added loss calculation ow.",2,positive
"The reconstructions of three sounds in Figure 2 show results similar to those in the MAE paper (He et al., 2022), which reconstruct inputs well but with blurry details, indicating that our models were successfully trained in the experiments.",0,negative
"In the image domain, recent progress of MIM such as BEiT (Bao et al., 2022) and Masked Autoencoders (MAE) (He et al., 2022) have shown promising performance such that “Selfsupervised learning in vision may now be embarking on a similar trajectory as in NLP” (He et al., 2022).",2,positive
"MAE (He et al., 2022) reconstructs the original signal given its partial observation.",2,positive
", 2022) and Masked Autoencoders (MAE) (He et al., 2022) have shown promising performance such that “Selfsupervised learning in vision may now be embarking on a similar trajectory as in NLP” (He et al.",2,positive
"As data volume and model scale evolve, foundation model [159], [160] shows great potential as a member of pre-trained models.",2,positive
"Images, on the contrary, are natural signals with heavy spatial redundancy [5].",1,neutral
"The MAE develops an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens.",2,positive
"This also implies that using a larger masking rate to increase the training difficulty should achieve better performance under large-scale corpora, which is similar to the conclusions drawn from pre-training tasks in machine vision, such as MAE [He et al., 2021].",1,neutral
"Furthermore, our work is inspired by recent pre-training research in the field of machine vision, such as BIET [Bao et al., 2021], IBOT [Zhou et al., 2021a], and MAE [He et al., 2021].",2,positive
"In addition, we reconstruct each pixel on the masked image including the masked pixel and the unmasked pixel, which is similar to the [He et al., 2021] method.",2,positive
"Since natural images are heavy in spatial information redundancy, as shown in [He et al., 2021], in which missing pixels of an image can be reconstructed by the state-of-theart MAE model even though a large amount of input pixels are masked.",1,neutral
"• Information density is often low in images, as missing patches from an image can often be mostly recovered by a neural network structure [He et al., 2021] or by a human brain (as you look at something through a fence).",1,neutral
"• Information density is often low in images, as missing patches from an image can often be mostly recovered by a neural network structure [He et al., 2021] or by a human brain (as you look at something through a fence).
ar X
iv :2
20 4.",1,neutral
"Motivated by a recent paper [He et al., 2021] which restores missing pixels of an image, we devise a new adversarial defense scheme called a Mask-based Adversarial Defense training method (MAD) for DNNs that classify images.",2,positive
"Since natural images are heavy in spatial information redundancy, as shown in [He et al., 2021], in which missing pixels of an image can be reconstructed by the state-of-theart MAE model even though a large amount of input pixels are masked.1 Since the task of classification is supposed to be…",1,neutral
"outstanding performance and outperforms the state-of-theart models in many vision tasks, including image classification, object detection, semantic segmentation, human pose estimation and video classification [1, 3, 12, 16, 26, 26, 34, 34,44].",1,neutral
04 MAE [19] Autoregressive Transformer 77.,1,neutral
"For autoregressivebased method, we choose MAE [19].",1,neutral
"Thus, in this work, we aim to maximize the performance of the model on downstream tasks with an end-to-end fine-tuning protocol [3, 9, 19].",2,positive
"Following the commonly adopted fine-tuning (FT) protocol [17, 19] for fully-supervised and semi-supervised scenarios, we fine-tune the full model for 40 epochs on labeled data.",2,positive
58 MAE [19] Autoregressive Transformer 68.,1,neutral
MAE [19] and BEiT [3] mask out random patches of the input image and reconstruct the missing patches with a Vision Transformer (ViT).,1,neutral
"The no-sweat approach would be to directly reconstruct the amplitude of the masked neurophysiological signal in the spatiotemporal domain following the standard practice of [3, 19].",1,neutral
"Numerous deep learning models have been proposed, such as Vision Transformer (ViT) and masked auto-encoder in the computer vision domain, which have shown the versatility of self-attention-based methods in processing images and videos [10, 11].",1,neutral
"Inspired by the success of the Transformers [48] in natural language processing, many works adopt it to visual tasks including image classification [49], [50] and semantic segmentation [51], [52], offering breakthrough performance.",1,neutral
"The Pretrain may be any SSL model, while we experimant with MoCo-v2, SwAV, SimCLR, DINO and MAE.",2,positive
"Specifically, the upper trajectory, originated in MAE, represents a case where CLI maintains the feature diversity while improving Imagenet score as the main mean for improving the diversity.",2,positive
", MoCov2 [14], SimCLR [13], SwAV [10], DINO [10] and MAE [31]), two formulations of Feature Diversity, several downstream vision tasks, including multi-label classification on the MS-COCO [50] dataset and a variety of 14 single-label classification datasets.",2,positive
"ViT models follow the ViT-B architecture and finetuning scripts from [31] with batch size 32, AdamW optimizer [54] with base learning rate 5e−4, layer decay 0.",2,positive
"We validate our approach over both CNNs (ResNets [34]) and vision transformers (ViT [25]), several self-supervised pre-training methods (e.g., MoCov2 [14], SimCLR [13], SwAV [10], DINO [10] and MAE [31]), two formulations of Feature Diversity, several downstream vision tasks, including multi-label classification on the MS-COCO [50] dataset and a variety of 14 single-label classification datasets.",2,positive
"Similarly to Figure 3 and Figure 13, Figure 15 (presenting Table 8) shows how the control label injection (CLI) can start off from different SSL pre-trained ViT models of both higher (MAE) and lower (DINO) feature diversity and generate ViT models of different levels of Imagenet accuracy and feature diversity for different control cycle values.",2,positive
", 2020) to studying the properties of ViTs (Shao et al., 2021; Mahmood et al., 2021; Naseer et al., 2021; Salman et al., 2021; He et al., 2022).",2,positive
2M) ViT-B MAE [30] 7 7 Self-Supervised ImageNet-1K (1.,1,neutral
"Alternatively, we come up with a solution by directly replacing the pixel target in MAE [18] with the Fourier spectrum where each component carries the global information.",1,neutral
"Among, MAE [18] and SimMIM [52] are two simple and straightforward methods, which regard raw pixels as targets and encourage the model concentrating on semantics by large-ratio masking strategy.",1,neutral
"For the pre-training on ImageNet-1K (IN1K) training set, we inherit the experimental settings in MAE [18].",2,positive
"However, the language naturally has semantics highly abstracted by human [18], where the elemental unit is also discrete.",1,neutral
"On the pixel-level aspect, MAE [18] and SimMIM simply mask the pixels in the patches and then predict them to encourage model focus on the semantics.",1,neutral
"Following the baseline method MAE [18], the IN1K pre-trained encoder is adopted for initializing the ViTlike backbone of Mask R-CNN [20] framework.",2,positive
"As suggested by MAE [18], the decoder design is crucial to the MIM model, as it not only learns representations of masked tokens, but also determines the semantic level of the whole learned latent representations.",1,neutral
"For the pixel loss Lpix, we compute Mean Square Error (MSE) between the reconstructed and raw images in pixel space, which is similar to MAE [18].",2,positive
"eling (MIM) [6, 12, 18, 49, 52, 63], exhibits promising potential, which inherits the “mask-and-reconstruct” thought from masked autoencoding methods in natural language processing (NLP) field, such as BERT [11].",1,neutral
"To achieve this goal and substantially retain the merit of “simple yet effective”, we build our MIM method, termed Geminated Gestalt AutoEncoder (Ge(2)-AE), upon canonical MAE [18] and simply modify it with one extra lightweight frequency decoder (FD) added to simultaneously perform gestalt tasks of the local masked region and global frequency.",2,positive
"Notably, we suggest tuning hyper-parameters among these two protocols independently because the optimal hyper-parameters for two protocols are different [90, 123] or even uncorrelated [124].",1,neutral
The new autoencoder training mechanism makes it a type of Denoising Autoencoder [16].,1,neutral
A large corpus of work indicates masked input modeling as a suitable choice for learning better representations in autoencoders [16].,1,neutral
"Index Terms—Movement Quality Assessment, Deep Learning, Skeletal Data Augmentation, Transformer, Performance Score Generation, Denoising Autoencoder
1The authors are with Department of Electrical Engineering, Indian Institute of Technology Madras, Chennai, India, 600036.",2,positive
"While scaling up ViTs with billions of parameters [21, 9, 44, 39, 13] is a wellproven way to improve the capacity of the ViTs, it is more important to explore more energy-efficient approaches to build simpler ViTs with fewer parameters and less computation cost while retaining high model capacity.",2,positive
For example MAE [19] report its most impressive results when pre-training on ImageNet-1k with a full finetuning on ImageNet-1k.,2,positive
The selfsupervised approach referred to as masked auto-encoder (MAE) [19] proposes an improved supervised baseline for the larger ViT models.,2,positive
For BerT like pre-training we compare our method with MAE [19] and BeiT [2] because they remain relatively simple approaches with very good performance.,2,positive
"Indeed, BeiT [2] or MAE [19] significantly outperform the fully-supervised approach, especially for the largest models.",1,neutral
In that respect it effect is comparable to that of MAE [19].,1,neutral
From this perspective it offers similar scaling properties as mask-autoencoders [19].,1,neutral
"• For ViT-B and Vit-L models, our supervised training approach is on par with BerT-like self-supervised approaches [2, 19] with their default setting and when using the same level of annotations and less epochs, both for the tasks of image classification and of semantic segmentation.",2,positive
"Masked auto-encoders in particular, which learn representations by reconstructing randomly masked patches from an input, have been successfully applied in vision (He et al., 2021; Xie et al., 2021; Wei et al., 2021; Bao et al., 2021).",1,neutral
"In particular, MSN achieves good classification performance using 100× fewer labels than current mask-based auto-encoders (He et al., 2021; Xie et al., 2019).",2,positive
"These approaches corrupt images with mask-noise and predict missing input values at the pixel level (Dosovitskiy et al., 2020; He et al., 2021; Xie et al., 2019) or using a tokenizer (Bao et al.",1,neutral
"For comparison, we also report the performance of a fine-tuned ViT-B/16 pre-trained using MAE (He et al., 2021), along with a supervised ResNet50 baseline, which is available in the PyTorch Torchvision package6.",2,positive
"We compare MSN to the joint-embedding approach, DINO (Caron et al., 2021), the auto-encoding approach, MAE (He et al., 2021), and the hybrid approach, iBOT (Zhou et al., 2021),",2,positive
"All low-shot evaluations (including the 1% ImageNet-1K evaluation) are computed with this procedure, except for models pre-trained using MAE (He et al., 2021), which benefit from using partial fine-tuning (He et al., 2021).",2,positive
"Specifically, we follow the setup of (Touvron et al., 2021; Bao et al., 2021; He et al., 2021).",2,positive
"These approaches corrupt images with mask-noise and predict missing input values at the pixel level (Dosovitskiy et al., 2020; He et al., 2021; Xie et al., 2019) or using a tokenizer (Bao et al., 2021; Wei et al., 2021).",1,neutral
"Auto-regressive models and denoising auto-encoders instantiate this principle in vision by predicting the missing parts at the pixel or token level (Chen et al., 2020a; Vincent et al., 2010; He et al., 2021; Bao et al., 2021; Baevski et al., 2022).",1,neutral
"Nevertheless, masked auto-encoders have enabled the training of large-scale models and demonstrated state-of-the-art performance when fine-tuning on large labeled datasets, with millions of labels (Bao et al., 2021; He et al., 2021; Xie et al., 2021; Baevski et al., 2022).",2,positive
"As observed in (He et al., 2021), MAE benefits from partial fine-tuning.",1,neutral
"In contrast to previous work on masked image modelling, the mask-denoising process in MSN is discriminative, rather than generative (He et al., 2021; Xie et al., 2021; Wei et al., 2021; Bao et al., 2021; Zhou et al., 2021).",1,neutral
"He et al. (2021) conjecture that using pixel reconstruction in their MAE objective results in encoder
representations of a lower semantic level than other methods, which may explain their difficulty in training a linear classifier on the frozen features.",1,neutral
", 2021), the auto-encoding approach, MAE (He et al., 2021), and the hybrid approach, iBOT (Zhou et al.",2,positive
"Recent works revisit this idea and investigate the pre-training of ViTs with masked auto-encoders (Chen et al., 2020a; He et al., 2021; Xie et al., 2021; Wei et al., 2021; Bao et al., 2021).",2,positive
"Note also, that larger MAE-pretrained models achieve stronger performance on all four datasets (He et al., 2021).",0,negative
"To explain this observation, we refer to the Masked Auto-Encoders paper (He et al., 2021) which conjectures that using a pixel reconstruction loss results in encoder representations of a lower semantic level than other methods.",1,neutral
"For MAE, we rely on partial fine-tuning (He et al., 2021), except for the 1 image per class setting, and all results with the ViT-H/14 architecture, which use a linear classifier.",2,positive
"We compare both protocols in more detail in Appendix C.
A.3 Linear Evaluation
For linear evaluation, we use a similar procedure as He et al. (2021).",2,positive
the STL layers are more relevant for recognition rather than reconstruction [26].,1,neutral
"as image reconstruction ( [41]), jigsaw puzzle solving [42] and Rubik cube solving [43], [44].",1,neutral
"For auxiliary ViT tasks proposed in [38], [35], we apply random masking as the augmentation stategy, and we adopt another randomly cropped view as augmented sample for contrastive learning [35], [12].",1,neutral
"contrasting the visual representations of different augmented views of the images [31], [32], discriminating the latent representations [33], [34], and predicting targets with masked inputs [35], [36], [37], [38].",1,neutral
"unchanged across the experiments, except for the masking ratio used in [35], [31].",1,neutral
"MAE [35] adopts a simple-yet-effective idea: given an image with masked patches, the self-supervised objective is to reconstruct the original unmasked image.",1,neutral
"More specifically, we employ the idea of Data2Vec [38], MAE [35], and momentum contrastive learning [32], [12].",1,neutral
"Adding reconstruction-based auxiliary tasks [38], [35] with ViT encoders can significantly enhance performance",1,neutral
"Please refer to the original paper for more details [38], [35], [32], [12].",0,negative
"We evaluate different choices of self-supervised learning losses, including Data2Vec [38], MAE [35], and contrastive",1,neutral
"The reconstruction target t is the pixel value of the masked patches only, and the pixels are normalized per patch to improve the performance as the authors of [35] suggest.",2,positive
"The recent success of vision transformers (ViTs) [11] in computer vision transfers to ViTs as SSL model backbones [9, 12, 13].",1,neutral
"In this short experiment (50 epochs of pretraining, and 70 epochs of linear training) we only used random resized cropping (like MAE (He et al. 2021)) on IN-1K for both MoCoV3 and DILEMMA.",2,positive
A technique that has proven very beneficial to improve the training efficiency of vision transformers is token dropping (Akbari et al. 2021; He et al. 2021; El-Nouby et al. 2021; Chen et al. 2022).,2,positive
"In all the linear probing experiments, we use the embedding of the CLS token of the last layer and perform a coarse grid search over learning rates, batch sizes and whether to normalize the data before feeding it to the linear layer or not (similarly to the added BatchNorm layer (Ioffe and Szegedy 2015) in MAE (He et al. 2021)).",2,positive
"2018) have recently also been adapted to the image domain (Bao, Dong, and Wei 2021; Zhou et al. 2021; He et al. 2021; Zhou et al. 2021).",2,positive
2021) and MAE (He et al. 2021) to reduce the computational workload of training with ViTs (Dosovitskiy et al.,2,positive
some of the MAE (He et al. 2021) inputs and added DILEMMA loss to the encoder of MAE in addition to the reconstruction loss of the decoder.,1,neutral
", 2021), including self-supervised learning methods (e.g. Chen et al., 2020; He et al., 2021), to further improve the performance of DFR.",2,positive
"Masking has been shown to be an effective method of pre-training visual representations [23,15].",1,neutral
"This property of ViT encourages us to act bold: we uniformly random sample a subset of patch embeddings serving as the input set of a MAE [20] pre-trained vanilla ViT encoder, i.",2,positive
"This work aims to tame and unleash the MIM pre-trained vanilla Transformer’s representation [4, 20, 47, 49] for object-level recognition without modifying the pre-training process and the architecture’s nature.",2,positive
"Inspired by the MIM pre-training [4, 20], this work pursues a different solution to transfer a vanilla ViT for object-level recognition: We feed the MIM pre-trained ViT encoder with only a partial input, e.",2,positive
The output sequence fragments are then complemented with learnable mask tokens and processed by a small MAE [20] pre-trained ViT decoder (4× Transformer layers) to recover the full image feature.,2,positive
"Moreover, our approach can achieve even better results compared with other adapted vanilla ViT [25] based on the same pre-trained representation [20] using a more modest fine-tuning recipe while being 2.",2,positive
We initialize the vanilla ViT encoder via MAE [20] pre-trained weight on ImageNet-1K [35].,2,positive
"Entering the 2020s, Vision Transformer (ViT) [13] is rapidly transferring the viewpoint of computer vision in both architectural design [10, 14, 29, 42, 48] as well as representation learning [4, 16, 20, 47, 49].",2,positive
"[25] is the first work to conduct a large-scale study of vanilla ViT on object detection with powerful MIM pre-trained representations [4, 20], demonstrating the promising scalability of vanilla ViT.",1,neutral
"In this case, it is promising to develop self-supervised pretraining methods [46]–[49] and some related methods have been developed in the remote sensing area [50]–[53].",1,neutral
"With the rise of deep learning, deep neural networks (DNNs) have been widely used in many fields such as computer vision [30], machine translation [31] and speech recog-",1,neutral
"The Transformer architecture not only achieves superior performance in different vision tasks [29] (e.g., image classification, object detection, and semantic segmentation), but also bring novel paradigms for some fundamental tasks (e.g., DETR [4] for object detection and MAE [16] for self-supervised learning).",1,neutral
"B.3 Self-Supervised Learning
Here, we also utilize a simple experiment to evaluate BatchFormerV2 on recent self-supervised learning framework, i.e., Masked Auto Encoder [16](MAE).",2,positive
", DETR [4] for object detection and MAE [16] for self-supervised learning).",1,neutral
"Recently, Visual Transformer [13,29] has gradually become a new backbone for visual tasks, and massive large models based on Transformers have emerged in computer vision, including CLIP [34], MoCo [17,8], DINO [5], DALL-E [35], BEiT [2], and MAE [16].",1,neutral
Table 10 demonstrates BatchFormerV2 is also beneficial for MAE.,2,positive
We insert BatchFormerV2 into all layers in the decoder in MAE [16].,2,positive
"Masked Autoencoders (MAEs) [28] have recently been demonstrated to be a powerful, yet conceptually simple and efficient, self-supervised pre-training strategy for Vision Transformers [22] (ViTs).",1,neutral
"Just as in the RGB-only MAE [28], we only pass the small randomly sampled subset of all tokens to the Transformer encoder as part of the masked autoencoding objective.",2,positive
[28] showed that the choice of mask sampling strategy can have a large impact on transfer performance.,1,neutral
"As in MAE [28], these visible tokens are decoded jointly with a set of mask tokens, which serve as placeholders for the decoders to write the reconstructed patches (as shown in Fig.",1,neutral
"Uniform sampling has been shown to work well for masked autoencoders, compared to less random alternatives [28].",1,neutral
"[28] show, they perform similarly to deeper decoders on ImageNet-1K fine-tuning.",2,positive
"In particular, the masked autoencoder (MAE) [28] approach accelerates pre-training by using an asymmetric architecture consisting of a large encoder that operates only on unmasked patches",2,positive
"Since all our modalities have a 2D structure, we add 2D sine-cosine positional embeddings [14, 28] after the linear projection.",1,neutral
"With the introduction of Vision Transformers (ViT) [22] and motivated by the success of BERT [20] in NLP, many recent works propose a variety of masked image prediction methods for pre-training vision models in a self-supervised way, using reconstruction targets such as pixels [5,13,22,25,28,74], discrete tokens [7,81], and (deep) features [6,70].",1,neutral
"To solve this task sufficiently well, it is assumed [28] that the network needs to learn representations that capture more than just low-level image statistics.",1,neutral
"Besides the supervised training methods, we also compare with the methods that pre-train on the ImageNet1K training set in a self-supervised manner and then perform supervised finetuning, including DINO [8], MoCo v3 [12], BEiT [1], and MAE [25].",2,positive
"Specifically, we use the hyperparameters in [25] for training ViT-B, and train ViT-T and ViT-S with the same hyper-parameters except for throwing away EMA, resulting in strong baselines.",2,positive
"For training ViT-B, we adopt the hyperparameters in [25] in all cases.",1,neutral
[53] trained the network by directly reconstructing the original image patches.,2,positive
"Furthermore, following the success of recent advances in masked language modeling and auto-regressive pre-training in NLP, a number of recent efforts [7,25,14] have demonstrated the feasibility of self-supervised BERT-style [10] visual representation learning with outstanding performance in downstream dense prediction tasks such as image segmentation.",1,neutral
"The experiment of [11,23] show that simply computed on the whole image (both visible tokens and masked tokens) could be harmful on ImageNet dataset.",1,neutral
We follow almost the same protocol in MAE [11] to train our SD-MAE.,2,positive
MAE [11] is leveraged as our masked image modeling block.,2,positive
"With the advancement of deep learning, self-supervised learning (SSL) has received increasing research attention [9,7,11,6].",1,neutral
"The same as [7,23,11,21,25], both pre-training and fine-tuning are carried out on the same dataset.",0,negative
"For a fair comparison, we use the respective pre-training methods to train the models [5,7,20,11], and then use a unified approach [11] to fine-tune them.",2,positive
[11] proposed a new SSL paradigm termed masked autoencoders (MAE).,1,neutral
"With Masked Autoencoder (MAE) [23] pretraining, our plain-backbone detector can outperform the hierarchical counterparts that are pre-trained on ImageNet-1K/21K [11] with supervision (Figure 3).",2,positive
We implement a näıve extension of MAE pre-training [23] for the hierarchical backbone ablation (Sec.,2,positive
The ViT models are pre-trained using MAE [23] on IN-1K.,0,negative
We use normalized pixels as the MAE reconstruction target [23] and set the decoder depth as 2.,2,positive
We initialize the backbone with MAE [23] pre-trained on IN-1K without labels.,2,positive
"In contrast, MAE [23] pre-training on IN-1K (without labels) shows massive gains, increasing AP by 3.",0,negative
", block designs [50,51], self-supervised learning [2,23], and scaling [55].",1,neutral
"In this study, our plain-backbone detector has benefited from the readily available pretrained models from MAE [23].",2,positive
MAE enjoys the efficiency benefit from plain ViT by skipping the encoder mask token [23].,2,positive
"MAE-AST models use 2 decoder layers, which show highquality results in table 2 and maximal fine-tune performance in the MAE paper [12].",2,positive
"Of particular relevence to us is the Masked Autoencoder [12], which completely discards masked input tokens during the encoding step, resulting in significant increases in computational efficiency.",1,neutral
"Consistent with [12], we use fixed sinusoidal positional embeddings for both patch-based and frame-based tokenization.",1,neutral
"[12], which asymmetrically applies BERT-like [1] pretraining to the visual domain with an encoderdecoder architecture.",1,neutral
"Following the MAE [12], by default we throw away the decoder during fine-tuning, training solely with the encoder.",2,positive
"Instead of discretizing the visual information, MAE [27] and SimMIM [28] propose to directly predict the pixel-level value as the reconstruction target.",2,positive
"Masked Image Modeling: Motivated by the great success of BERT, masked image modeling (MIM) [1, 8, 27] becomes a new trend in self-supervised visual pre-training, which randomly masks parts of images and reconstructs them based on the corrupted image.",1,neutral
"The pretext task of MIM enables a more fine-grained understanding of the local visual semantics compared to the contrastive counterparts [5, 18].",1,neutral
"Given the observation of the above two issues, we argue that performing MIM with a strict mapping between patch predictions and unique token ids in the form of a hard-label classification loss in BEiT limits the visual context capturing and the pre-training performance.",2,positive
"(1)
4 mc-BEiT
BEiT provides inspiring insights of casting masked image modeling (MIM) as a classification problem to bridge the gap between discrete words in NLP tasks and continuous visual signals in computer vision tasks.",1,neutral
"MIM requires randomly masking a proportion of the image patches and then training the vision Transformer to recover the corrupted image via reasoning
among the visual context.",1,neutral
"Vision Transformers pre-trained with MIM objectives can be well transferred to a wide range of downstream tasks, i.e., classification, segmentation, and detection, after fine-tuning.",1,neutral
"Inspired by the great success of BERT [12] in natural language processing (NLP) tasks, masked image modeling (MIM) has been introduced for visual pre-training as a new pretext task.",1,neutral
"Recent works [1, 8] reproduce the success of BERT by employing the proxy task of masked image modeling (MIM) on image pre-training of vision Transformers [9, 10, 11].",1,neutral
We will go over how to produce such refined multi-choice answers for MIM pre-training in the following section.,2,positive
"However, as there are no perfect answers for visual discretization, performing a strict mapping between patch predictions and unique token ids as a single-choice classification problem is actually a sub-optimal solution for MIM pre-training.",1,neutral
"However, these methods either reconstruct the original images in original pixels [8], or predict the quantized discrete tokens [2].",1,neutral
"Inspired by masked language modeling [5] in NLP, recent attempts [2, 8] learn representations by masked image modeling.",1,neutral
"On the other hand, the recent success of generative learning paradigm [8] shows its better scalability and robustness.",1,neutral
"MAE pre-training for KELIP took 16 hours, while fine-tuning the multimodal model took 362 hours.",0,negative
"Most of the hyper-parameters of MAE pre-training are followed by (He et al., 2021), while we follow (Radford et al.",2,positive
"As shown in Table 1, fine-tuning CLIP with MAE pre-trained model achieves better performance in both classification and retrieval.",0,negative
"Instead, we employ the self-supervised method of Masked Autoencoder (MAE) (He et al., 2021) with our collected 1.",2,positive
The details of hyper-parameter used in MAE pre-training and multimodal fine-tuning is in Table 4.,1,neutral
"As shown in Table 1, although the MAE+CLIP+SimCLR shows the best performance in zero-shot classification, it shows
lower performance than MAE+CLIP+MultiCrop in zero-shot cross-lingual retrieval, which requires a higher understanding of cross-lingual relation.",0,negative
"The pre-training and then fine-tuning technique has shown remarkable achievement in natural language processing (Radford et al., 2018; Devlin et al., 2019; Brown et al., 2020) and computer vision (Chen et al., 2020a; He et al., 2020; Grill et al., 2020; Caron et al., 2020; He et al., 2021).",2,positive
"Instead, we employ the self-supervised method of Masked Autoencoder (MAE) (He et al., 2021) with our collected 1.1 billion images, which has been shown fast training and powerful performance.",2,positive
"Most of the hyper-parameters of MAE pre-training are followed by (He et al., 2021), while we follow (Radford et al., 2021b) for the hyper-parameters of KELIP fine-tuning.",2,positive
"Following the convention [24,2], we fine-tune the pre-trained models for image classification on ImageNet-1K (the same dataset used for pre-training).",2,positive
", BEIT [2], MAE [24], and our work) and image-based pre-training (in particular, contrastive learning, e.",1,neutral
"Note that the integrated task requires the same computational overhead as MAE [24], which is about 4× more efficient than MoCo-v3 [11] and DINO [5].",2,positive
"The setting follows MAE [24], where we set a ratio (e.",1,neutral
", both BEIT [2] and MAE [24] used the ViT [18] architecture, where f(·) contains a projector and 12 transformer blocks (which are often referred to as the encoder).",1,neutral
"3%), while BEIT [2] and MAE [24] report much lower numbers (37.",0,negative
"A representative methodology is known as masked image modeling (MIM) [2,24,9], where a part of image patches are removed from input and the goal is to recover the missing contents or some kind of statistics [52].",1,neutral
"This property gives birth to a new learning objective named masked image modeling (MIM)(5), in which part of image patches (corresponding to some tokens) are masked and the target model is required to recover the original image patches [24] or statistics [2,52] from the incomplete inputs.",1,neutral
"[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Similar to MAE [7], we find that the occlusion ratio of 75% performs the best on both the linear accuracy and supervised fine-tuning accuracy.",2,positive
"Similar to BERT [44] and MAE [7], we don’t pay much attention to the reconstruction ability of visible parts, and only compute loss between the points of predicted patches P and the ground truth point cloud of these occluded patches denoted as P.",2,positive
"Partly inspired by MAE, we design a new self-supervised learning framework to recover the complete shapes from the highly occluded shapes.",2,positive
[7] propose a new form of auto-encoders named MAE by masking regular patches of images and learning to recover the masked parts.,1,neutral
"Similar to BERT [44] and MAE [7], we don’t pay much attention to the reconstruction ability of visible parts, and only compute loss between the points of predicted patches Po and the ground truth point cloud of these occluded patches denoted as Pt.
LCD(Po,Pt) = 1 |Po| ∑
po∈Po min
pt∈Pt ‖po − pt‖2
+ 1 |Pt| ∑
pt∈Pt min
po∈Po ‖pt − po‖2.",1,neutral
The recent improvements of mask-based 2D auto-encoders [7] have proved that masked auto-encoders are effective in image representation learning through the inference of the overall image information based on the visible local patches.,1,neutral
"Recently, in 2D vision, He et al. [7] propose a new form of auto-encoders named MAE by masking regular patches of images and learning to recover the masked parts.",1,neutral
"Moreover, while our work focuses on representation learning via generative pre-training, another interesting future direction would be to investigate the performance of representation learning schemes such as masked prediction (He et al., 2021; Xiao et al., 2022; Yu et al., 2022), latent reconstruction (Yu et al.",2,positive
"Various representation learning methods, including reconstruction (He et al., 2021), rotation (Gidaris et al., 2018), solving zigsaw puzzles (Noroozi & Favaro, 2016), and contrastive learning (He et al., 2020; Chen et al., 2020), have reduced the gap with supervised pre-training with labels.",1,neutral
"Various representation learning methods, including reconstruction (He et al., 2021), rotation (Gidaris et al.",1,neutral
"…another interesting future direction would be to investigate the performance of representation learning schemes such as masked prediction (He et al., 2021; Xiao et al., 2022; Yu et al., 2022), latent reconstruction (Yu et al., 2021; Schwarzer et al., 2021a), and contrastive learning…",2,positive
"Some authors also proposed a scalable self-supervised learning from the pretrained large ViT models, which could also be adopted into semi-supervised GANs architecture to further improve the performance.(46) Further studies will involve using other transformer or GAN models such as retinal vascular GAN or vision transformer GAN.",2,positive
This result is difference from BERT [18] in NLP and MAE [31] in images.,1,neutral
"First, we pre-train the ViT-B on ImageNet-1K for 1600 epochs, following the recipes in [31].",2,positive
"[31] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"More recently, the success of vision transformer has led to investigation of Transformer-based architectures for masked visual modeling [4, 20, 31, 80, 82, 90].",1,neutral
MAE [31] introduced an asymmetric encoder-decoder architecture for masked image modeling.,1,neutral
1 Revisiting Image Masked Autoencoders ImageMAE [31] performs the masking and reconstruction task with an asymmetric encoder-decoder architecture.,2,positive
"In particular, compared with previous masked video modeling [31, 77, 65], we present a simpler yet more effective video masked autoencoder by directly reconstructing the pixels.",1,neutral
"3 Proposed Method In this section, we first revisit ImageMAE [31].",2,positive
We are inspired by the recent ImageMAE [31] and propose customized video tube masking with an extremely high ratio.,2,positive
"Following the success of masked autoencoding in NLP [18] and images [31, 4], we present a new selfsupervised video pre-training (SSVP) method, termed as Video Masked Autoencoder (VideoMAE).",2,positive
Backbone Pre-trained Objective Pre-trained Dataset # params (M) Feature dim d Batch Size Pre-trained Model ViT-B/16 [16] Supervised ImageNet-21k 85 768 2048 / 1280 / 128 / 128 / 64 checkpoint ViT-L/16 [16] 307 1024 2048 / 640 / 64 / 64 / 32 checkpoint ViT-H/14 [16] 630 1280 1024 / 240 / 28 / 28 / 14 checkpoint ViT-B/16 [16] MoCo v3 [9] ImageNet-1k 85 768 2048 / 1280 / 128 / 128 / 64 checkpoint ViT-B/16 [16] MAE [26] checkpoint Swin-B [48] Supervised ImageNet-21k 88 1024 1024 / 1024 / 128 / 80 / - checkpoint ConvNeXt-Base [49] Supervised ImageNet-21k 88 1024 1024 / 1024 / 128 / 128 / - checkpoint ResNet-50 [27] Supervised ImageNet-1k 23 2048 2048 / 2048 / 384 / 256 / - checkpoint,0,negative
"For MAE, other parameter-efficient methods, e.g ., Partial-1, outperform both VPT and Linear.",1,neutral
Different pre-trained objectives: MAE [26] and MoCo v3 [9] with a ViT-B backbone.,0,negative
"In addition to the backbones pre-trained with labeled data, we experiment with two self-supervised objectives: MAE [26] and MoCo v3 [9].",2,positive
"Following the linear scaling rule [38,23,9,26], the learning rate is set as",1,neutral
"Transformers are also being widely used in recent self-supervised pre-training methods [9,26,2].",1,neutral
"– Partial-k: fine-tune the last k layers of backbone while freezing the others, as adopted in [78,82,56,26].",1,neutral
sponding pixels by direct regression; MAE [24] randomly masks a large portion of patches and predicts the corresponding pixels using an autoencoder; MST [38] masks low-attended patches and reconstructs the entire input with a decoder; iBOT [78] extends the self-distillation loss of DINO to dense features corresponding to block-wise masked patches.,1,neutral
"Other than that, MIM methods use continuous representations: SimMIM [72] randomly masks large patches and predicts the corre-
sponding pixels by direct regression; MAE [24] randomly masks a large portion of patches and predicts the corresponding pixels using an autoencoder; MST [38] masks low-attended patches and reconstructs the entire input with a decoder; iBOT [78] extends the self-distillation loss of DINO to dense features corresponding to block-wise masked patches.",1,neutral
"We compare AttMask with random block-wise masking [2], which is the default in iBOT, random patch masking with the same ratio, as well as with a more aggressive ratio, following MAE [24].",2,positive
"(b) is used by SimMIM [72], (c) by MAE [24], (d) by BEiT [2] and (g) by MST [38].",1,neutral
‡: aggressive random masking strategy from MAE [24].,1,neutral
"A prominent paradigm is to mask a portion of the input tokens—words in text or patches in images—and train the transformer to predict these missing tokens [2,15,24,72,78].",1,neutral
"Interestingly, random patch masking outperforms the default iBOT strategy, while the more aggressive MAE-like strategy is inferior and AttMask-Low performs the lowest.",2,positive
"We compare RandSAC with contrastive transformer training approaches (DINO [5] & MoCo v3 [11]), masked image encoding (BEIT [2] & MAE [22]), and our autoregressive counterpart iGPT [6].",2,positive
"Masked image modeling & autoregressive image encoding, of which our method is an instance, tend to perform better in such circumstances [2, 22].",1,neutral
"2 This is, in part, motivated by [22] which observe that in BERT-style pre-training high amount of masking (as much as 75%), which corresponds to harder predictive tasks, leads to better feature learning.",1,neutral
"Alternatively, predictive models learn to predict elements of the scene, either in parallel by reconstructing masked regions/tokens [2, 22] (a.",1,neutral
"We pretrain blob-RandSAC with hierarchy 11 →7 →3 using ViT-Base [17] on ImageNet1K following [2, 22].",2,positive
"We also set the decoder for MAE [22] to have the same depth, attention head, and dimension as ours.",2,positive
"For example, some approaches discreteize images [2, 6], while others patchify them [12, 17, 22, 53].",1,neutral
"Nether the lass, RandSAC outperforms all predictive (non-contrastive methods) in linear probing, despite having 36% fewer parameters in the decoder as compared to MAE [22].",2,positive
"For both benchmarks, we use the normalized pixel loss introduced from [22] as our patch regression target.",2,positive
We simply adopt a mean squared error (MSE) between the predicted and target pixel values for all our experiments following [22].,2,positive
"5.2 Results
Table 9 shows low-data classification performance for contrastive pretraining (DINO [5]), masked image encoding (MAE [22]) and our segment autoregressive coding (RandSAC).",0,negative
The MAE and DINO are pretrained using their official implementations.,0,negative
Masked Autoencoder (MAE) [22] suggests a 75% random masking ratio for image modeling; and SimMIM [53] studies different masking strategies for pretraining.,1,neutral
We adopt minimal data augmentation strategy following [22]: resize cropping with scale range of [0.,2,positive
For MAE we use a 75% masking ratio as suggested in their paper.,2,positive
"In addition, linear probing for DINO and iGPT is evaluated using the last 4 and 5 transformer blocks, respectively, while MoCo v3, MAE, and RandSAC only
evaluate the last block output.",2,positive
"– To the best of our knowledge, this is the first memory-based UAD method that relies on MAE [9]; – A new memory-augmented self-attention operator for our MAE transformer encoder to explicitly encode and memorise the normality patterns; and – A novel decoder architecture that uses the learned multi-level memoryaugmented encoder information as prior features to a cross-attention operator.",2,positive
"1, is based on the masked autoencoder (MAE) [9] that was recently developed for the pre-training of models to be used in downstream computer vision tasks.",2,positive
MemMC-MAE is a transformer-based approach based on masked autoencoder (MAE) [9] with of a novel memory-augmented self-attention encoder and a new multi-level cross-attention decoder.,2,positive
"Implementation Details For the transformer, we follow ViT-B [6, 9] for designing the encoder and decoder, consisting of stacks of transformer blocks.",2,positive
We also adopt a linear projection layer after the encoder to match the different width between encoder and decoder [9].,1,neutral
The vanilla ViT-Base [10] model pre-trained with MAE [14] is adopted as the backbone for joint feature extraction and relation modeling.,2,positive
We further investigate the effect of different pre-training methods on the tracking performance by comparing four different pre-training strategies: no pre-training; ImageNet-1k [8] pre-trained model provided by [37]; ImageNet21k [35] pre-trained model provided by [36]; unsupervised pre-training model MAE [14].,2,positive
"Adopting the existing Vision Transformer architecture also provides a bunch of publicly available pre-trained models [14, 36], freeing us from the time-consuming pre-training stage.",2,positive
Zhang [57] has shown a theoretical connection between masked autoencoder [18] and contrastive learning.,1,neutral
", 2022) uses Masked autoencoding loss (Pathak et al., 2016; He et al., 2022).",2,positive
"For image understanding, the state-of-the-art masked autoencoding Transformer approach MAE [20] masks out a large random subset of image patches, applies the Transformer encoder to the unmasked patches, and trains a small Transformer decoder that takes in the positional encodings of themasked patches to reconstruct their original pixel values.",2,positive
"In this section, we delve into the details on why a reconstruction objective (i.e., reconstructing the original point cloud from the unmasked points) as used in the related Masked AutoEncoder (MAE) [20] approach for images would not work for our point cloud setting.",2,positive
"Masking out content has been used in various ways to improve model robustness including as a regularizer [45,17], data augmentation [13,44,72], and self-supervised learning [7,12,20].",1,neutral
", 90%) is required for point cloud data compared to the image domain (75% in [20]).",1,neutral
"First, in MAE, the self-supervised learning task is to reconstruct the masked patches, based on the input image’s unmasked (visible) patches.",1,neutral
"Instead of reconstructing tokens, the recent Masked AutoEncoder (MAE) [20] reconstructs the masked patches at the pixel level, and with a much higher mask ratio of ≥ 70%.",2,positive
"Masked image modeling works [2,20] adopt a similar idea for image pretraining.",1,neutral
", predicting the masked word in a sentence or masked patch in an image, based on surrounding unmasked context) is the dominant self-supervised learning approach for text understanding [12,26,27,63] and has recently shown great promise in image understanding [2,20] as well.",1,neutral
"We now have methods in NLP [41,12,40] and computer vision [21,5,20,8,2] that can produce stronger features than those learned on labeled datasets.",2,positive
", reconstructing the original point cloud from the unmasked points) as used in the related Masked AutoEncoder (MAE) [20] approach for images would not work for our point cloud setting.",2,positive
"Inspired by the success of the self-supervised way tansfer learning, the CV community is also exploring the self-supervised way to explore new possibilities, one recent work which is similar to the BERT in NLP is MAE [14].",2,positive
Pre-train a model by using the Self supervised approach such as [14] based on one modal and then fine tune the model in another modal will be another interesting direction to investigate.,1,neutral
"Inspired by the success of self-supervised pretraining approaches, CV researchers are also trying to use self-supervised pretraining approaches [14] to build better transfer learning systems.",1,neutral
"Recently, several Transformer [52] based approaches [7,11,4,58,19] have proved it effective to learn representations from masked images.",1,neutral
"Different from the layer-wise pre-training [14] and MAE [19], M(3)PT is to pre-train all layers of the deep network, and does not change the network architecture in the pre-training and fine-tuning stages.",2,positive
"Recently, the single-modal masked autoencoders [19,58] are also applied into object detection and semantic segmentation, achieving amazing improvements on their benchmarks.",1,neutral
"SimMIM [58] and MAE [19] propose to recover raw pixels of randomly masked patches by a lightweight one-layer head and an asymmetric decoder, respectively.",2,positive
"Compared to MAE [19], M(3)PT has no architectural difference between the pre-training and fine-tuning stages, where they differ in only the prediction density of target depth.",2,positive
"The mask is randomly generated following [58,19] with different sizes and ratios.",1,neutral
"Compared to the vision pre-training counterparts [19,58], this design has two obvious advantages: (i) it closes the gap between pre-training and fine-tuning tasks, as they differ only in the prediction density; (ii) it leads to no architectural modification between pre-training and fine-tuning stages, which can potentially make the transfer learning more smooth and effective.",2,positive
"It is quite different from the popular masked pretraining methods [19,58] in vision where the missing image pixels are predicted.",1,neutral
"Following the recent work of MAE, stage one pre-training is built with an auto-encoder approach, as shown in figure 1.",0,negative
And MAE [7] aims at reconstructing masked patches based on visible image patches.,2,positive
This paper is inspired by the recent MAE [7] work.,2,positive
"Importantly, in spite of limited built-in priors, it has demonstrated great potential when combined with self-supervised learning, either with contrastive methods [8, 10] or for reconstruction-based techniques like BeiT [3] or other forms of masked auto-encoders [15, 17, 24, 68, 72, 76].",1,neutral
"However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT [3] or MAE [24].",1,neutral
"MAPLE showed improved tissue architecture detection for posterior and anterior mouse sagittal brain datasets [2], detected distinct tissue architecture of ER + and triple-negative breast cancer datasets [46], and revealed anatomical development trends in developing chicken heart samples [2].
conST [31] is a multi-modal, interpretable contrastive learning framework that learns low-dimensional embeddings of ST data and utilizes it for downstream analyses such as clustering, trajectory inference, cell-cell interaction, etc. conST takes ST data’s gene expression, spatial coordinates, and the H&E images, if applicable, as input. conST represents the input data as a graph where the node attributes are either principal components of gene expression data or morphological feature vectors extracted using MAE (Masked Autoencoder) [47], a powerful computer vision tool.",2,positive
Autoencoder; k-NN ST data; scRNA-Seq Computationally scalable to large sample sizes or gene numbers.,2,positive
"conST represents the input data as a graph where the node attributes are either principal components of gene expression data or morphological feature vectors extracted using MAE (Masked Autoencoder) [47], a powerful computer vision tool.",2,positive
"encoder-decoder structure to reconstruct input, but it is not practical as fitting the low signal-to-noise ratio fMRI features may overfit to spurious features [16].",1,neutral
"Reconstruction-based SSL [16], [17] contains an",1,neutral
"Moreover, the accuracy is slightly low when the dimensionality is set in range of [16, 64].",1,neutral
Reconstruction-based SSL methods [16] apply reconstruction loss (i.,1,neutral
[16] develop an SSL encoder-decoder model to reconstruct the original image from the latent representation and mask patches.,2,positive
"More recent iterations of SSL methods, using strong augmentations such as cropping, color distortion and masking, started to show results that approached fully supervised state-of-the-art models [14-19].",1,neutral
"Visual Transformer Masked Autoencoder (ViT-MAE) [16] performs a different kind of augmentation from the above methods by splitting the original image into patches, heavily masking a significant portion of them and using the remaining patches to reconstruct the input through a visual transformer (ViT) [23].",1,neutral
"To avoid the burden of data labeling, self-supervised learning has emerged as an alternative for pre-training 2D models without labels [3, 6, 7, 16, 18, 19].",1,neutral
"Recent work has revived interest in masked-autoencoding [7, 24, 38] and masked-distillation [6] as viable alternatives to contrastive learning.",1,neutral
"The prevailing methods in self-supervised learning take the form of either contrastive- [7, 20, 21], reconstruction- [22, 23], or nonnegative (positive pairs only)-based learning [24–26].",1,neutral
"Previous approaches [11,5,16,1] randomly mask a portion of input data and then recover the masked content.",1,neutral
"This ratio is higher than MAE [16] in CV and BERT [11] in NLP, whose masking ratios are 75% and 15%, respectively.",1,neutral
An obvious cross can be seen in each attention map because we utilize the same implementation as [16].,1,neutral
"Some works [12,16,1] transfer self-supervised pre-trained models to image-based CV tasks, such as classification, object detection, semantic segmentation, etc.",1,neutral
"Similar to [16], to improve the efficiency of the model, we only use the unmasked frames as inputs to the encoder, excluding the temporal padding embeddings.",2,positive
"Following [16], we adopt an asymmetric encoder-decoder design.",2,positive
"For the future work, we can consider combining the reconstruction models [3, 19] and contrastive learning for convolutional neural networks or vision transformers, since reconstruction can learn more sufficient information and contrast can make the representations more discriminative.",1,neutral
"For example, MAE [17] randomly masks input patches, and pre-train the model to recover masked patches in pixel space.",1,neutral
"Our autoencoder’s backbone is entirely based on standard Transformers, with an asymmetric encoder-decoder design [17].",2,positive
"In computer vision, both MAE [17] and SimMIM [49] propose a similar masked image modeling, which randomly masks input image patches.",1,neutral
"Among them, masked autoencoding [17,49,2], illustrated in Figure 1, is a promising scheme for both languages and images.",1,neutral
"For example, BERT [11] in NLP and MAE [17] in computer vision both apply masked autoencoding and adopt a standard Transformer architecture as autoencoder’s backbone to achieve state-of-the-art performance.",1,neutral
"These methods have dominated until recent generative SSL methods [17,49,45] result in more competitive performance.",1,neutral
"Specifically, our autoencoder’s backbone is entirely built by standard Transformer blocks and adopts an asymmetric encoder-decoder structure [17].",2,positive
"Languages contain high-density information, while images contain heavy redundant information [17].",1,neutral
"Afterwards, Transformers spring up and make splendid breakthroughs on various vision tasks [42, 23, 63, 70, 11, 36, 20].",1,neutral
"We use the MAE framework for the selfsupervised counterpart (He et al., 2021).",2,positive
"Left: We first pre-train visual representations using self-supervision through masked image modeling (He et al., 2021) from real-world images.",2,positive
"We adopt masked modeling as our self-supervision objective—specifically, we use masked autoencoder (MAE) (He et al., 2021).",2,positive
"Specifically, we adopt the Masked Autoencoders (MAE) (He et al., 2021) have shown excellent performance on recognition tasks.",2,positive
"Masked image autoencoding (Chen et al., 2020b; Bao et al., 2022; He et al., 2021) pursues a different direction by learning to recover masked pixels.",1,neutral
"We use an auxiliary dummy classification token in the MAE for downstream finetuning and transfer (He et al., 2021).",2,positive
"We learn the visual representation by performing masked image modeling through the masked autoencoder (MAE) (He et al., 2021).",2,positive
"Different from these works of training two networks simultaneously, [47] proposed to first train masked autoencoders for restoring the masked images, and then exploited the encoder of the trained masked autoencoders to extract features directly for classification tasks.",1,neutral
"Therefore, we use sine-cosine [4, 10] position embedding in the pre-training stage.",2,positive
"Recent advancements in self-supervised learning (SSL) show masked image modeling (MIM) [3, 4] is an effective pre-training strategy for the Vision Transformer (ViT) [5].",1,neutral
", both visible and masked patches, MAE only predicts the pixel/voxel values of the masked patches, which is proven to achieve better results [4].",2,positive
"Among the different MIM frameworks, Masked Autoencoder (MAE) [4] is both simple and effective.",1,neutral
"Second, unlike the high mask ratio [4] adopted in natural images, the two segmentation tasks show different preference to the mask ratios.",1,neutral
", by dVAE [39] or VQ-VAE [32]), and the other [16,37] is to predict pixel-level information.",1,neutral
"However, this paper hopes to deliver the message that pure vision pre-training, especially the recent MIMbased approaches [2,16], suffers the limitations of learning semantic information – this seems not to be solved by simply using larger datasets (e.",2,positive
"%) and MAE (67.8%), demonstrating its strong ability of semantic learning.",1,neutral
6% accuracy in the linear probing test on ImageNet-1K; MAE [16] improves it to 67.,1,neutral
"Following most MIM-based approaches [16,2,12], we mainly utilize a series of ViT backbones [14] to evaluate the effectiveness of MVP.",2,positive
"MIM [2,16,34,37,12,41] removes part of image patches from input and requires the target model to recover the missing contents.",2,positive
"MVP is also evaluated on the linear probing test of ImageNet-1K, and it achieves 75.4% Top-1 accuracy, which significantly outperforms the current MIMbased methods (e.g., BEIT reports a 37.6% accuracy and MAE achieves a 67.8% accuracy).",2,positive
"Moreover, with the pixel-level information reconstruction of each masked patch, MAE [16] further improved the final results.",2,positive
"For example, as for the recent MIM-based methods with different visual feature or pixel information reconstruction pretext tasks, they achieve the nearly same transfer performance, e.g., 45.6% mIoU of BEIT and 48.1% mIoU of MAE on ViT/B-16, respectively.",1,neutral
"But, we note that such models are weak when the backbone is frozen – for example, BEIT [2] reports a 37.6% accuracy in the linear probing test on ImageNet-1K; MAE [16] improves it to 67.8%, but it is still significantly lower than that reported by contrastive learning (e.g., DINO reports 78.2%).",1,neutral
"Same with most previous works [2,16,12], MVP is mainly evaluated on image classification and semantic segmentation tasks.",1,neutral
"recent methods, including DeiT [41], MOCO-V3 [10], SLIP [33], CLIP [36], and MAE [19].",1,neutral
"More recently, generative approaches like Masked Autoencoders (MAE) (He et al. 2021) are introduced to predict a masked latent representation of patches.",1,neutral
"Inspire by BERT, MAE [27] leads a new hotspot of self-supervised methods in CV, and its mask-reconstruction pretask is also very instructive for related
ar X
iv :2
20 3.",2,positive
"Inspired by the form of the HSI sample, we propose the Center Mask (CM) Pre-training pretask, which is similar to MAE and adopts an asymmetric structure but is simpler and easier to implement.",2,positive
"Inspire by BERT, MAE [27] leads a new hotspot of self-supervised methods in CV, and its mask-reconstruction pretask is also very instructive for related ar X iv :2 20 3.",2,positive
"And CPMAE, through the mask-reconstruction process of the center pixel of the input patch, enables the backbone network to effectively model the center pixel and neighborhood pixel relationship in the self-supervised learning process.",2,positive
"For example, [2] and [15] employ MIM and get pre-trained on ImageNet-1k [9], which respectively achieve 86.",1,neutral
"Recently, novel pretext tasks have been explored, such as the Rubik’s Cube Recovery task [31] and the Masked Image Modeling (MIM) task [15,28].",1,neutral
", 2016], and MAE-ViT-Base/Large/Huge (3) [He et al., 2021] with pretrained weights1.",2,positive
"…model as an initialization like most existing methods on domain generalization, this paper seeks a better way to leverage the vast amount of the existing pretrained models [He et al., 2016, Krizhevsky et al., 2012, Iandola et al., 2014, Zoph et al., 2018, He et al., 2021, Radford et al., 2021].",2,positive
"1 (2) [Iandola et al., 2016], and MAE-ViT-Base/Large/Huge (3) [He et al., 2021] with pretrained weights1.",1,neutral
"In contrast, other visual recognition tasks [3,13] have already greatly benefited from very deep models.",1,neutral
"A similar trend exists for vision models, where SSL image representations achieve state-of-the-art performance on ImageNet benchmarks [13], [3], [14].",1,neutral
"BEiT [1] and MAE [12] successfully transferred the pre-training paradigm of natural language processing [8, 19, 20] to vision, further improving the accuracy on ImageNet.",1,neutral
"95 following [12], and adjust the learning rate to 5e-4× 256 and batch sizes to 64 (base), 128 (small), and 256 (tiny).",1,neutral
We trained with the setting of MAE on ImageNet but slightly modified it on Cifar.,2,positive
Here our ViT-P uses the same training hyperparameters as MAE [12].,2,positive
"Many recent ViT-based methods refer to the training technique of DeiT [24], and we note that MAE [12] proposes a better training technique based on it.",1,neutral
"On this basis, we can easily and confidently apply selfsupervised learning methods [12, 1, 19, 8, 20], as these methods often increase training data diversity, and predicting or comparing image details allows the network to learn local information autonomously.",2,positive
"It is observed that the DeiT-B, BEiT-B, and MAE-B are obviously better than ResNeXt-101, and DiT-B is even stronger than these powerful image Transformer baselines.",2,positive
"Image Transformer [3, 9, 12, 13, 17, 31, 36, 47] has recently achieved great success for natural image understanding including classification, detection and segmentation tasks, either with supervised pre-training on the ImageNet or self-supervised pretraining.",1,neutral
"Next, we use the Mask R-CNN framework to compare different backbone networks (CNN and ViT) including ResNeXt-101, DeiT, BEiT, MAE, and DiT.",2,positive
"For image Transformers, we choose the base version of DeiT [36], BEiT [3] and MAE [17] which are pre-trained on ImageNet-1K dataset with a 224×224 input size.",2,positive
"On the contrary, a missing image patch can be recovered from neighboring patches without crossmodality understanding (He et al., 2021).",1,neutral
"Besides, such models seem to be very similar to a masked auto-encoder.(68) Similarly, a specific non-auto-regressive predictive coding (NPC)(69) has been recently proposed, which also applies a mask on its model input but learns representations based on local dependencies of an input sequence rather than globally.",1,neutral
"the areas of natural language processing (NLP) [5], [6], 2D computer vision [7], [8], [9], [10], etc.",1,neutral
"Nonetheless, generation-based methods achieve very impressive progress in 2D images [10] recently, demonstrating their great potential for handling 3D point-cloud data.",2,positive
"Recently, recovering missing parts from masked input as the pre-text task of URL has been proved remarkably successful in NLP [5], [6] and 2D computer vision [10].",1,neutral
"Unlike an end-to-end unsupervised pre-training in [13], which requires an extremely large model and high mask rate to avoid cheating model, a much smaller decoder is trained in our method for a specific encoder.",2,positive
"Recently, as the image reconstruction task is proposed as a new method in self-supervised pretraining [13], autoencoder structure become useful in pretraining task.",1,neutral
"[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
1 Autoencoder Training Image reconstruction is a conventional computer vision task but was introduced as a pretraining method recently[13].,1,neutral
"These methods focus on the recovery of corrupted images [26, 31, 13].",1,neutral
"Inspired by the recent progress in self-supervised learning [13], we introduce the image reconstruction task to find the universal tickets, and a two-stage training paradigm is proposed to obtain the desired ticket.",2,positive
"This success has recently started to percolate into specific perception domains: computer vision [17, 18], point-cloud understanding [19], and speech recognition [20].",1,neutral
Compared to previous work we did not try layerwise learning rate decay in finetuning [17] which may boost performance further.,2,positive
For masked auto-encoding pre-training we use the optimization hyperparameters from [17] and pretrain for 500 epochs with 85% masking for most experiments because that seems to do slightly better than 75% on ImageNet in preliminary experiments.,2,positive
"We consider both the classic masking approach [14] of replacing the masked inputs with a learned token (uniform-masking), and the more efficient alternative of dropping the masked inputs [17] – we call this groupwise-masking.",1,neutral
We experimented also masking random 75% of 16x16 patches [17] instead of pixels (while still operating on pixels) and finetuning performance was about 2% worse.,2,positive
"Using only ImageNet-1K training data, MAE obtains impressive performance.",2,positive
"In this paper, we adopt MAE [27] to train the scaled-up ViTAE model due to its simplicity and efficiency.",2,positive
"MAE (He et al., 2022) simplifies the requirement of tokenizers and simply treats the image pixels as the targets for reconstruction.",1,neutral
"The scaled-up models are pre-trained for 1600 epochs using MAE [27], taking images from the ImageNet-1K training set.",2,positive
MAE [27] simplifies the requirement of tokenizers and simply treats the image pixels as the targets for reconstruction.,1,neutral
"It should be noted that the original MAE is trained on the TPU machines with Tensorflow, while our implementation adopts PyTorch as the framework and uses NVIDIA GPU for the training.",2,positive
"In this paper, we adopt MAE (He et al., 2022) to train the scaled-up ViTAE model due to its simplicity and efficiency.",2,positive
", 2020) with either supervised learning or self-supervised learning like MAE (He et al., 2022), MaskFeat (Wei et al.",1,neutral
"Self-supervised learning (He et al., 2022), on the contrary, can eliminate this issue and facilitate the training of scaled-up models.",1,neutral
"On the other hand, VAN can dynamically adjust the output according to the input image which is suit for self-supervised learning and transfer learning [4,26].",1,neutral
", vision transformers) [16,7,51,21,68,82,94,46,47,4,49,86,50,26] have achieved significantly better performance than the mainstream CNNs on different visual tasks, showing",1,neutral
"…such masked prediction have shown even great progress in learning representations that can perform well on a wide range of downstream tasks [Devlin et al., 2018, Radford et al., He et al., 2021], as well as seeming learning structural properties of the data, e.g. syntax [Hewitt and Manning, 2019].",1,neutral
"We focus on the widely used self-supervised learning method of predicting masked tokens, which is popular for both natural languages [Devlin et al., 2018] and visual data [He et al., 2021].",2,positive
"Classic ideas such as contrastive and masked prediction remain powerful in their modern realizations [Hénaff et al., 2019, Chen et al., 2020b,a, He et al., 2021], pushing the state of the art performance and even surpassing supervised pretraining in various aspects [Lee et al., 2021, Liu et al.,…",1,neutral
"Inspired by BERT [24] and MAE [26], we propose a selfsupervised graph transformer model named Graph Masked Autoencoders (GMAE).",2,positive
"A similar approach has been explored by masked autoencoders in vision (He et al., 2022), where 75% of the input patches are masked and removed from the input of the heavy encoder to achieve a 4.1× speedup.",2,positive
"Recently, a number of works extend MLM training to images and videos and demonstrate strong pre-training results (He et al., 2022; Zhou et al., 2022; Feichtenhofer et al., 2022; Tong et al., 2022) .",2,positive
"Therefore our setting differs from vision, where good reproductions are possible with high masking rates (He et al., 2022).",2,positive
"A similar approach has been explored by masked autoencoders in vision (He et al., 2022), where 75% of the input patches are masked and removed from the input of the heavy encoder to achieve a 4.",1,neutral
"While He et al. (2022) recently pioneered such high masking rates in the vision domain, and they reason that images are natural signals with heavy redundancy, while language is highly semantic and information-dense.",1,neutral
"We distill ViT-T or ViT-S using a regular setting of training for 300 epochs, and distill ViT-base or ViT-large for 600 epochs (MAE (He et al., 2021) uses much more epochs,
1https://github.com/HobbitLong/RepDistiller
i.e. 1600).",2,positive
"Training large ViT models is nontrivial (Steiner et al., 2021), and previous works proposed different techniques (Touvron et al., 2021a; He et al., 2021) to achieved good results.",2,positive
"We distill ViT-T or ViT-S using a regular setting of training for 300 epochs, and distill ViT-base or ViT-large for 600 epochs (MAE (He et al., 2021) uses much more epochs,",2,positive
"9% better than training without distillation (He et al., 2021).",0,negative
", 2021), and previous works proposed different techniques (Touvron et al., 2021a; He et al., 2021) to achieved good results.",2,positive
"To compare with state-of-the-art ViT performances (He et al., 2021; Steiner et al., 2021), we use a stronger teacher BEiT-L (Bao et al., 2021) with students ranging from ViT-T to ViT-L (Dosovitskiy et al., 2020).",2,positive
"Our MKD distilled ViT-L obtains 86.5% accuracy, +3.9% better than training without distillation (He et al., 2021).",2,positive
"To compare with state-of-the-art ViT performances (He et al., 2021; Steiner et al., 2021), we use a stronger teacher BEiT-L (Bao et al.",2,positive
"More recently, a similar paradigm has been used in the context of masked-patch-prediction models [4, 56, 132], showing that scalable pre-training can be achieved.",1,neutral
"Inspired by recent works of self-supervised learning on vision [12, 42], we propose to mask out image patches with larger proportion and follow MaskFeat [41] to reconstruct other views of the whole image rather than recovering those masked regions only.",2,positive
"We first follow [12, 42] to use a larger masking ratio of 50% (instead of 15% as in [5, 25, 36]).",1,neutral
This type of self-supervised task has been shown to be a meaningful training method for computer vision models[21].,1,neutral
"The network architecture g parameterized by θ is composed of backbone f (before the final linear classifier) and of two heads h, h for disease classification and self-supervision, respectively: g θ = h cls ◦ f, g θ = h ◦ f (1) Given an input image x, the models in (1) yield two predictions P cls θ (x) and P ss θ (x) with the dimensionsK andK, respectively, by normalizing the network output with the softmax function with temperature parameter τ cls and τ : P cls θ (x) = Softmax ( g θ (x) ) , P ss θ (x) = Softmax (g ss θ (x)) (2)",1,neutral
The advancement of self-supervised learning (SSL) has been shown to be successful and efficient in both natural language processing (NLP) and computer vision (CV) [5].,1,neutral
"Following the practice in [31], we also omit the shared class token added to the embedding that can be removed from the",2,positive
“Masked Autoencoders (MAE) Are Scalable Vision Learners” [31] (illustrated in Figure 1) recently introduces a ground-breaking self-supervised paradigm for image pretraining.,1,neutral
"For MAE [31], each input image patch is projected as a 1D token embedding.",1,neutral
"Thus, MAE paves a path that “Selfsupervised learning in vision may now be embarking on a similar trajectory as in NLP” [31].",2,positive
Its great success is attributed to “a rich hidden representation inside the MAE” [31].,0,negative
"Masked Autoencoder (MAE) [31] is essentially a denoising autoencoder [45], which has a straightforward motivation that randomly mask patches of the input image and reconstruct the missing pixels.",1,neutral
The great success of MAE is interpreted by its authors as “We hypothesize that this behavior occurs by way of a rich hidden representation inside the MAE” [31].,2,positive
"Figure 1: Architecture of the Masked Autoencoder (MAE) in [31], with Vision Transformer (ViT) backbone.",2,positive
"In vision, the masked modeling in BERT [11] has been extended to image representation learning [2,21] with images quantized to discrete tokens.",1,neutral
"…and then recovers the masked content by conditioning on the visible context, is able to learn rich visual representations and shows promising performance on various vision benchmarks (Zhou et al., 2021; He et al., 2021; Xie et al., 2021; Dong et al., 2021; Wei et al., 2021; ElNouby et al., 2021).",2,positive
"3% better than the normalization method proposed in (He et al., 2021).",1,neutral
"with non-overlap window” is proposed in (He et al., 2021).",1,neutral
He et al. (2021) and Xie et al. (2021) re-explore pixel regression in MIM.,2,positive
"As shown in Table 6, the proposed sliding window normalization improves the fine-tuning accuracy by 0.5% vs. the reconstruction target without normalization, and is also 0.3% better than the normalization method proposed in (He et al., 2021).",2,positive
"As shown in Table 4, we observe CIM works better with simple random masking (He et al., 2021; Xie et al., 2021) compared with the blockwise masking strategy proposed in BEiT.",2,positive
", 2021), which randomly masks out some input tokens and then recovers the masked content by conditioning on the visible context, is able to learn rich visual representations and shows promising performance on various vision benchmarks (Zhou et al., 2021; He et al., 2021; Xie et al., 2021; Dong et al., 2021; Wei et al., 2021; ElNouby et al., 2021).",2,positive
"For each image triplet, we visualize the original image (left), the template of using non-overlapping window normalization (He et al., 2021), and the template of the proposed sliding window normalization paradigm.",1,neutral
"Instead of directly regressing the original pixel, MAE (He et al., 2021) suggests learning the normalized counterpart.",1,neutral
"5: The computational graphs for (a) a context autoencoder (CAE), (b) BEiT [4], (c) a denoising autoencoder (DAE), and (d) MAE [38] and the one stream in SplitMask [29].",1,neutral
"Following [38], we adopt an extra BatchNorm layer [48] without affine transformation",1,neutral
"In MAE [38], the socalled decoder may play a partial role for representation learning as the representations of the visible patches are also updated in the MAE decoder.",1,neutral
"Several subsequent MIM methods are developed to improve the encoder quality, such as designing pretraining architectures: Masked Autoencoder (MAE) [38], SplitMask [29], and Simple MIM (SimMIM) [91]; adopt-",2,positive
Our approach is clearly different from MAE [38],2,positive
"The subsequent method, masked autoencoder (MAE) [38] adopts an encoder-decoder architecture, partially decoupling the two tasks.",1,neutral
"We provide the computational graph for CAE, BEiT [4], denoising autoencoder, Masked Autoencoder [38] and SplitMask [29] (one stream) in Figure 5.",2,positive
"The technical report 1 of our approach was initially published as an arXiv paper [19], and was concurrent to data2vec [3], MAE [38], and other methods, such as [29,91].",2,positive
"OFA achieves state-ofthe-art performances in a series of vision & language downstream tasks, including image captioning, visual question answering, visual entailment, referring expression comprehension, etc.
• OFA, as a multimodal pretrained model, achieves comparable performances on unimodal tasks with SOTA pretrained models in language or vision, e.g., RoBERTa, ELECTRA and DeBERTa for natural language understanding, UniLM, Pegasus and ProphetNet for natural language generation, and MoCo-v3, BEiT and MAE for image classification.",2,positive
"Compared with pretrained models based on masked image modeling, e.g., BEiT-L (Bao et al., 2021) and MAE-L (He et al., 2021a), OFA can achieve similar performance.",2,positive
"Recently, mirroring masked language modeling (MLM) in language pretraining, generative pretraining (Bao et al., 2021; He et al., 2021a) with ViT architecture (Dosovitskiy et al., 2020) boosts downstream performance.",2,positive
"In comparison with the state-of-the-art natural language pretrained models, including RoBERTa (Liu et al., 2019), XLNET (Yang et al.,
2019), ELECTRA (Clark et al., 2020), and DeBERTa (He et al., 2021b), OFA reaches a comparable performance.",2,positive
"Recent advances in generative self-supervised learning for computer vision show that masked image modeling is an effective pretraining task (Bao et al., 2021; He et al., 2021a).",1,neutral
"It removes the masked patches and embeds the unmasked patches with their positional information in the original image, and then processes them via a series of Transformer blocks [14].",2,positive
"2, we employ the MAE with vision Transformer (ViT) architecture, where we randomly mask patches from input images and aim to reconstruct the missing patches [14].",2,positive
"• Then, the masked autoencoder (MAE) with Transformer blocks [14] is designed as the architecture of robust semantic communication systems, where a portion of the original image is masked and only the unmasked portion is encoded and transmitted.",1,neutral
"Using such quantization results as prediction target for self-supervised learning share a similar structure as the masked autoencoder (MAE) (He et al., 2021), which directly reconstruct the masked input signals.",1,neutral
"image understanding (e.g., Caron et al., 2021; Bao et al., 2022; He et al., 2022), and has led there to significantly improved performance on a variety of tasks.",1,neutral
"…which permits unrestricted re-use, distribution and reproduction, provided the original article is properly cited.
image understanding (e.g., Caron et al., 2021; Bao et al., 2022; He et al., 2022), and has led there to significantly improved performance on a variety of tasks.",2,positive
"This is in contrast to, for instance, natural language processing (Vaswani et al., 2017; Devlin et al., 2019) or computer vision (Dosovitskiy et al., 2020; Caron et al., 2021; He et al., 2022).",1,neutral
"Denoising autoencoders are commonly used in data processing in many fields, such as image processing [39].",1,neutral
"We follow the same setting as the visual transformer (ViT) which recently attracts much attention and shows remarkable performance in the computer vision area [6, 15, 20, 46, 72, 73].",2,positive
"To remove the object detectors from the captioning models training pipeline, the natural method is to leverage the pre-trained vision transformer [6, 15, 20, 72] and directly separate an image into several patches which are then fed into the model.",2,positive
"Recent works, such as Vision Transformer, Masked Auto-Encoder [15], etc., have brought new ideas for introducing attention mechanisms to neural processes and avoiding the input sequence length limitation problem.",1,neutral
"Recent works, such as Vision Transformer, Masked Auto-Encoder [15], etc.",1,neutral
"In this way, similar to the spatial reconstruction for images in (He et al., 2021; Xie et al., 2021), MLR enhances the awareness of the agents to the global context information of the entire input observations and promotes the state representations to be predictive in both spatial and temporal…",2,positive
"Similar to the designs in (He et al., 2021; Xie et al., 2021), it is appropriate to use a lightweight decoder in MLR, because we actually expect the predicting masked information to be mainly completed by the encoder instead of the decoder.",2,positive
"In this way, similar to the spatial reconstruction for images in (He et al., 2021; Xie et al., 2021), MLR enhances the awareness of the agents to the global context information of the entire input observations and promotes the state representations to be predictive in both spatial and temporal dimensions.",2,positive
"As discussed in (He et al., 2021; Xie et al., 2021), the choice of this ratio varies for different modalities and depends on the information density.",1,neutral
"With the recent popularity of the transfomer-based architectures, a series of works (Chen et al., 2020; Bao et al., 2021; He et al., 2021; Xie et al., 2021; Wei et al., 2021) dust off the idea of MIM and show impressive performance on learning representations for vision",2,positive
"…Neural Language Processing (NLP) (Devlin et al., 2018; Radford et al., 2018; 2019; Brown et al., 2020) and Computer Vision (CV) (Bao et al., 2021; He et al., 2021; Xie et al., 2021), we make the first endeavor to explore the idea of mask-based reconstruction in RL.
Masked pre-training aims to…",2,positive
"In recent works of masked image modeling (MIM) (He et al., 2021; Xie et al., 2021), the mask ratio is found crucial for the final performance.",1,neutral
"Inspired by MLM and MIM, we explore the masked modeling for RL to exploit the high correlation in vision data to improve the awareness of agents to global-scope dynamics in learning state repesentations.",2,positive
"Most importantly, we propose to predict the masked contents in the latent space, instead of the pixel space like aforementioned MIM works, which better coordinates the representation learning and the policy learning in RL.",2,positive
"For computer vision (CV) tasks, similar to MLM, masked image modeling (MIM) learning representations for images/videos by pre-training the neural network to reconstruct masked pixels from visible ones.",1,neutral
"Similar to the encoder designs in (He et al., 2021; Xie et al., 2021), the online encoder in our proposed MLR reconstructs the representations of masked contents based on visible ones in an implicit way.",2,positive
"With the recent popularity of the transfomer-based architectures, a series of works (Chen et al., 2020; Bao et al., 2021; He et al., 2021; Xie et al., 2021; Wei et al., 2021) dust off the idea of MIM and show impressive performance on learning representations for vision
𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀(�)
𝒐𝒐𝑡𝑡…",2,positive
", 2020) and Computer Vision (CV) (Bao et al., 2021; He et al., 2021; Xie et al., 2021), we make the first endeavor to explore the idea of mask-based reconstruction in RL.",2,positive
"With the recent popularity of the transfomer-based architectures, a series of works (Chen et al., 2020; Bao et al., 2021; He et al., 2021; Xie et al., 2021; Wei et al., 2021) dust off the idea of MIM and show impressive performance on learning representations for vision
𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀(�)
𝒐𝒐𝑡𝑡 𝒐𝒐𝑡𝑡+1 𝒐𝒐𝑡𝑡+2 𝒐𝒐𝑡𝑡+3 𝒐𝒐𝑡𝑡+𝐾𝐾−1�𝒐𝒐𝑡𝑡 �𝒐𝒐𝑡𝑡+1 �𝒐𝒐𝑡𝑡+2 �𝒐𝒐𝑡𝑡+3 �𝒐𝒐𝑡𝑡+𝐾𝐾−1
𝐴𝐴𝐴𝐴𝐴𝐴(�)
𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂 𝐸𝐸𝑂𝑂𝐸𝐸𝐸𝐸𝐸𝐸𝑂𝑂𝐸𝐸
�𝒔𝒔𝑡𝑡 �𝒔𝒔𝑡𝑡+1 �𝒔𝒔𝑡𝑡+2 �𝒔𝒔𝑡𝑡+3 �𝒔𝒔𝑡𝑡+𝐾𝐾−1
𝑀𝑀𝐸𝐸𝑀𝑀𝑂𝑂𝑂𝑂𝑀𝑀𝐴𝐴𝑀𝑀 𝐸𝐸𝑂𝑂𝐸𝐸𝐸𝐸𝐸𝐸𝑂𝑂𝐸𝐸
�𝒔𝒔𝑡𝑡 �𝒔𝒔𝑡𝑡+1 �𝒔𝒔𝑡𝑡+2 �𝒔𝒔𝑡𝑡+3 �𝒔𝒔𝑡𝑡+𝐾𝐾−1 𝒂𝒂𝑡𝑡:𝑡𝑡+𝐾𝐾−1
Original Observation Seq.",2,positive
"Different from the natural language, which is highly semantic and information-dense [39], images are generally natural signals with quite objective descriptions.",1,neutral
"Contrary to deterministic masked auto-encoders [33], Transformers can produce diverse image completions [67] that are sharp in masked regions by adopting the BERT [21] training objective.",1,neutral
"Thus, some works [25,30] interpret an image as a sequence of words and process them by the Transformer’s encoder solely based on the scaled dot-product attention.",1,neutral
"Concurrent MAE [He et al., 2021] and SimMIM [Xie et al.",1,neutral
"Concurrent MAE [He et al., 2021] and SimMIM [Xie et al., 2021b] propose to reconstruct raw pixels via mask image modeling.",2,positive
"Due to the small number of the available Hematoxylin and Eosin (H&E) stain images from ST, MAE is pretrained using ImageNet in a self-supervised manner.",0,negative
"Compared with supervised pretrained ResNet used in previous methods (14; 17), we argue that the MAE is more powerful for extracting features of the H&E stain images.",2,positive
"The morphological features of each spots are extracted by a pretrained MAE model (20), which achieves SOTA performance and transferability on various datasets.",2,positive
"A state-of-the-art computer vision model, MAE (20), is used to extract informative features from the morphology of each spot.",1,neutral
"MAE contains an asymmetric encoder-decoder architecture, with a Vision Transformer (ViT) (28) encoder.",2,positive
The detailed explanation of MAE can be found in the supplementary material.,0,negative
"Despite the great success of SSL in both NLP [2] and Image Transformers [156], they are not as widespread in the video domain, which could be attributed to the large costs involved in such a process.",2,positive
"All in all, VideoMAE and MaskFeat seem to point out pixel-and feature-based MTM approaches compare favorably with “SVT” (instance-based invariance learning) or “BEVT” (quantization-based MTM) despite the latter also using image-based pre-training.",2,positive
"Despite the great success of SSL in both NLP [2] and Image Transformers [156], they are not as widespread in the video",0,negative
"Con-cretely, MaskFeat (feature-based MTM) obtains the best results on K400 and is second best on SSv2, which is dominated by VideoMAE (pixel-based MTM).",2,positive
"In particular, “VMAE (ViT-L) 32@2” (75.3",1,neutral
"The only model pre-training on SSv2 is VideoMAE (“VMAE”), which turns out to be the best performing one.",0,negative
", 40%-60% in MaskFeat [28] or even 75%-90% in VideoMAE [29]), especially compared to NLP (15%-20% in BERT [2]) or images (20%-50% in MAE [156]), indeed force the network to capitalize on global relationships of the data, as seen by improved performance on high-level semantic tasks (see Section VI-B).",2,positive
"It has been found that high masking ratios (e.g., 40%-60% in MaskFeat [28] or even 75%-90% in VideoMAE [29]), especially compared to NLP (15%-20% in BERT [2]) or images (20%-50% in MAE [156]), indeed force the network to capitalize on global relationships of the data, as seen by improved performance on high-level semantic tasks (see Section VI-B).",2,positive
VideoMAE [29] comes second in this category consisting of a ViT backbone with 3D inﬂation of the patch I embeddings.,2,positive
"In particular, MaskFeat [28] (with MViTv2 [109] backbone) and VideoMAE [29] (with a plain ViT [9]) outperform those pre-trained on video in a supervised way.",1,neutral
", 2021a), have higher linear probing accuracy while worse fully fine-tuning results than generative pre-training, such as MAE (He et al., 2021), indicating that the linear separability of the pre-trained features is not the sole metric for evaluating transferability.",1,neutral
"Towards this problem, Zhang and Sabuncu (2018) propose the Generalized Cross-Entropy (GCE) loss as an effective solution (Rusak et al., 2021; Liu et al., 2021a),
LGCE(x, ỹ) = 1/q · (1− hỹ(x)q), (37)
where q ∈ (0, 1] is a hyper-parameter to trade-off between the CE loss and the MAE loss.",1,neutral
"To tackle this issue, MAE randomly masks a very large portion of patches, forcing the model to go beyond low-level understanding and reconstruct the whole image based on a small subset of visible patches, which improves its transferability to semantic-level tasks.",2,positive
"He et al. (2021) show that features from contrastive pre-training, such as MoCo v3 (Chen et al., 2021a), have higher linear probing accuracy while worse fully fine-tuning results than generative pre-training, such as MAE (He et al., 2021), indicating that the linear separability of the pre-trained features is not the sole metric for evaluating transferability.",1,neutral
"He et al. (2021) show that features from contrastive pre-training, such as MoCo v3 (Chen et al., 2021a), have higher linear probing accuracy while worse fully fine-tuning results than generative pre-training, such as MAE (He et al., 2021), indicating that the linear separability of the pre-trained…",1,neutral
"For instance, Masked Autoencoders (MAE) (He et al., 2021) pre-trains vision transformers on large-scale unlabeled image datasets using the image generation task.",1,neutral
"…such as MoCo v3 (Chen et al., 2021a), have higher linear probing accuracy while worse fully fine-tuning results than generative pre-training, such as MAE (He et al., 2021), indicating that the linear separability of the pre-trained features is not the sole metric for evaluating transferability.",1,neutral
"First, from pixels to continuous features, inspired by MAE [He et al., 2021], we can know that the image has heavy spatial redundancy, and the complete image information can be reconstructed only by random masking part of the image.",1,neutral
MAE shows that images are highly redundant representations of natural information that a missing patch can be recovered by its neighboring patches.,1,neutral
"Recently, Masked Auto-Encoder (MAE) [He et al., 2021] is proposed based on randomly masking some patches in an image and trying to reconstruct them using self-supervised learning.",1,neutral
"Inspired by MAE, we also use the subset of non-overlapping patches to train the feature extractor
x̃i = RM(xi;m), (1)
where RM denotes random masking operation and m denotes the masking ratio.",2,positive
"Recently, masked autoencoder (MAE [He et al., 2021]) has been received considerable attention in self-supervised pretraining task.",1,neutral
"1 Feature-Preserving Module Recently, masked autoencoder (MAE [He et al., 2021]) has been received considerable attention in self-supervised pretraining task.",1,neutral
"Firstly, unlike MAE, which requires a high mask ratio (e.g., 75%), our IPHash reaches the highest level when the mask ratio is in the range of 25% to 50%, because the task of reconstructing the original image of MAE is low-level.",2,positive
"In [17, 18, 19], a decoder plays two roles: (i) recovering information for missing positions.",1,neutral
"The solutions based on masking [17, 18, 19] are conceptually simple: removing a portion of data and learning to impute the removed content.",1,neutral
"We also prune stacked Transformer blocks [20] which are pretty popular in previous masking-based models [17, 18, 19].",2,positive
"Unlike [18, 19] which only computes loss on masked patches, we compute loss on all patches.",1,neutral
"We use the supervised training results from DeiT [73] for ViT-S/B and MAE [26] for ViT-L, as they employ improved training procedures over the original ViTs [20].",2,positive
Masked autoencoding has been proven to be effective and efficient in modeling texts [17] and images [18].,1,neutral
This setting is inspired by recent self-supervised learning in computer vision [18].,1,neutral
"Notably, significant effort has been dedicated to developing self-supervised learning methods for 2D images [6, 12, 31, 41, 68], with autoencoders being one of the most popular tools [4, 21, 41, 58,62].",1,neutral
"Recent methods for self-supervised learning also propose a variety of alternatives to the contrastive objective such as self-distillation [25, 6], or input reconstruction [1, 26].",1,neutral
"[58]), as well as using random greyscale, solarization, Gaussian blur and color jittering as additional forms of data augmentation.",1,neutral
"Moreover, the stronger modeling capability of transformers allows for large-scale and sophisticated pre-training, which has shown great success in both NLP and computer vision [Radford et al., 2018; Radford et al., 2019; Brown et al., 2020; Devlin et al., 2018; He et al., 2021; Liu et al., 2022; Zamir et al., 2022; Chen et al., 2022].",2,positive
"Inspired by the recent work [16], we randomly drop a part of input pixels to speed-up the training of image encoders.",2,positive
Note that we can alternatively add a normalization layer before the linear classifier following [30] instead of using larger learning rate values.,1,neutral
"The involved methods can be roughly divided into two categories: 1) pretext task design (Colorization [42], Jigsaw [43], PSL [57], MAE [72]) and 2) contrastive learning (SimCLR [50], SwAV [45], BYOL [46], SimSiam [49], Barlow Twins [47], MocoV3 [73], and DenseCL [74]).",1,neutral
"design (Colorization [42], Jigsaw [43], PSL [57], MAE [72]) and 2) contrastive learning (SimCLR [50], SwAV [45],",1,neutral
"Although transformers have very exceptional representation capacity, it is still a data-hungry solution for recognition tasks, even require more data than CNNs (He et al., 2021; Tang et al., 2021b; You et al., 2022).",2,positive
This hypotheses is further supported by the recent work of He et al [5] on unsupervised learning using masked autoencoders.,1,neutral
"For this reason, even though deep learning-based models applying autoencoders showed excellent performance [6,7], the models were not suitable for this task in that the models produce blurred images [8].",1,neutral
"Moreover, a recent article [16] suggested that Linear Probe evaluation misses the opportunity of pursuing strong but non-linear features and thus they proposed a new partial fine-tuning protocol.",1,neutral
"94%, considerably below RGB’s > 90% accuracy [13, 42, 76, 109].",0,negative
One can observe that the proposed DDPM-based method demonstrates higher robustness and preserves its advantage over the SwAV and MAE models even for severe image distortions.,2,positive
"2 https://github.com/facebookresearch/mae 3 https://github.com/facebookresearch/swav
Method Bedroom-28 FFHQ-34 Cat-15 Horse-21 CelebA-19∗ ADE Bedroom-30∗
ALAE 20.0 ± 1.0 48.1 ± 1.3 — — 49.7 ± 0.7 15.0 ± 0.5 VDVAE — 57.3 ± 1.1 — — 54.1 ± 1.0 —
GAN Inversion 13.9 ± 0.6 51.7 ± 0.8 21.4 ± 1.7 17.7 ± 0.4 51.5 ± 2.3 11.1 ± 0.2 GAN Encoder 22.4 ± 1.6 53.9 ± 1.3 32.0 ± 1.8 26.7 ± 0.7 53.9 ± 0.8 15.7 ± 0.3
SwAV 42.4 ± 1.7 56.9 ± 1.3 45.1 ± 2.1 54.0 ± 0.9 52.4 ± 1.3 30.6 ± 1.6 MAE 45.0 ± 2.0 58.8 ± 1.1 52.4 ± 2.3 63.4 ± 1.4 57.8 ± 0.4 31.7 ± 1.8
DatasetGAN 31.3 ± 2.3 57.0 ± 1.1 36.5 ± 2.3 45.4 ± 1.4 — — DatasetDDPM (Ours) 47.9 ± 2.9 56.0 ± 0.9 47.6 ± 1.5 60.8 ± 1.0 — —
DDPM (Ours) 49.4 ± 1.9 59.1 ± 1.4 53.7 ± 3.3 65.0 ± 0.8 59.9 ± 1.0 34.6 ± 1.7
Generative pretrained models.",0,negative
"First, we learn pixel classifiers on the clean images using the DDPM, SwAV and MAE representations on the Bedroom-28 and Horse-21 datasets.",2,positive
"• MAE (He et al., 2021) — one of the state-of-the-art self-supervised methods, which learns a denoising autoencoder to reconstruct missing patches.",2,positive
• The MAE baseline is the strongest competitor to the DDPM-based segmentation and demonstrates comparable results on the FFHQ-34 and Cat-15 datasets.,2,positive
"Notably, the recent advance of supervised learning has shown that the effectiveness of learning methods can be maximised when they are provided with very large modelling capacity, trained on very large and diverse datasets (He et al., 2021; Liu et al., 2021b; Kim et al., 2018).",1,neutral
"It has demonstrated impressive performance compared to convolutional neural networks (CNNs) on various visual domains, including image classification [42,10], object detection [7,57], semantic segmentation [25], and action recognition [13,4], using both supervised and self-supervised [19,2] training configurations.",1,neutral
"Vision Transformer (ViT)[12] introduced a set of pure Transformer backbones for image classification, and its follow-ups have soon modified the vision transformer to dominate many downstream tasks for computer vision, such as object detection[7,57], semantic segmentation [25], action recognition [4,13], 2D/3D human pose estimation [50,56], 3D object detection [33], and even self-supervision [19].",1,neutral
Another concurrent work [29] extends it from recovering patch tokens to raw pixels.,1,neutral
"This motivates a few recent studies to explore the BERTstyle pretraining for image representation learning by recovering raw pixels [29] or latent codes [3, 18] of masked image patches.",1,neutral
") in a supervised learning [12, 18, 29, 34] or self-supervised learning manner [3, 8, 15, 16], and then finetunes the models on various downstream tasks.",1,neutral
"Masked autoencoder (MAE) masks random patches of the input image and then reconstruct the mixing pixels using an encoder-decoder architecture, which reveals the great potential of masking in feature extraction[4].",1,neutral
"Meanwhile, motivated by the great potential of masking in [4], we mask random patches of original frames and reconstruct them in the loss function.",1,neutral
"In computer vision, this approach has mainly been studied in the context of self-supervised representation learning (Bao et al., 2022; He et al., 2022).",1,neutral
"On the other hand, discriminative approaches, typically implemented in a contrastive learning framework [4, 6–9, 14, 25, 30, 47, 53] or using pre-text tasks [2, 3, 24, 26, 31, 40], demonstrate the ability to obtain better generalised representations with modest computational requirements.",1,neutral
"[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
[26] verified that GMML and masked autoencoder provide a strong pretext task in the case of full ImageNet leading to state-of-the-art performance of multiple downstream tasks.,2,positive
"Although the proposed method can effectively improve the performance of standard Transformers on point clouds, the entire ‘pre-training + fine-tuning’ procedure is rather time-consuming, like other Transformers pre-training methods [2, 8, 13].",2,positive
"More recently, MAE [13] presents a masked autoencoder strategy for image representation learning.",1,neutral
"Different from the ran-
dom masking used in BERT [8] and MAE [13], we adopt a block-wise masking strategy like [2].",2,positive
"dom masking used in BERT [8] and MAE [13], we adopt a block-wise masking strategy like [2].",1,neutral
"Masked image modeling (MIM) [5, 21, 52, 106] demonstrates strong representation learning capability surpassing contrasting views.",1,neutral
"All views contain one subregion representing the same content, but with different pixel appearances and surrounding context.
fθ (X̃ (m))' Z ∀m ∈ (1, . . . ,M) (1)
We relate our approach to discovering semantic meanings for pixels to discovering semantic meanings for words in NLP similar to recent MIM works [5, 21, 106, 116].",2,positive
"‘*’ denotes that we end-to-end finetune RegionCL-M pretrained models for 50 epochs [2, 17].",2,positive
"cial from the progress of self-supervised representation learning while the new trends such as MAE [27] have presented more superior performance on downstream tasks, which deserves studying as future work.",2,positive
"Current state-of-the-art methods are mostly beneficial from the progress of self-supervised representation learning while the new trends such as MAE [27] have presented more superior performance on downstream tasks, which deserves studying as future work.",2,positive
"Recently, mask image modeling (MIM) such as MAE [27] arises a new trend for self-supervised learning that leverages ViT [28] to directly reconstruct mask images.",1,neutral
", time reversal, mask auto-encoder of time series [12]) that can be formulated as self-supervised task for inertial odometry.",1,neutral
"Importantly, we witness an exciting new result: masking-based methods (BEiT and MAE) show considerable gains over both supervised and random initialization and these gains increase as model size increases.",2,positive
We find that MAE and BEiT provide the first convincing results of substantial COCO AP improvements due to pretraining.,0,negative
ResNet-101 is not yet able to benefit from BEiT or MAE pre-training and therefore lags behind ViT-B in APbox (∼1 point) when those methods are used for initialization.,2,positive
"We use the DeiT released weights [36] for ViT-B and the ViT-L weights from [16], which uses an even stronger training formula than DeiT to avoid overfitting (moreover, the DeiT release does not include ViT-L).",2,positive
"This is an important direction, but it also complicates the simple ViT design and may impede the exploration of new unsupervised learning directions, such as methods that sparsely process unmasked patches [16].",1,neutral
The analysis reveals more granular information about where MAE and BEiT improve overall AP relative to the other initializations.,0,negative
"(4) For ViT-B, BEiT and MAE outperform both random initialization by up to 1.4 APbox (50.3 vs. 48.9) and supervised initialization by up to 2.4 APbox (50.3 vs. 47.9).",1,neutral
"For ViT-L, the APbox gap increases, with BEiT and MAE substantially outperforming both random initialization by up to 2.6 APbox (53.3 vs. 50.7) and supervised initialization by up to 4.0 APbox (53.3 vs. 49.3).",2,positive
MAE: We use the ViT-B and ViT-L weights pre-trained on unsupervised ImageNet-1k from the authors of [16].,2,positive
"We overcome these obstacles and present strong ViT-based Mask R-CNN baselines on COCO when initializing ViT from-scratch [18], with pre-trained ImageNet [8] supervision, and with unsupervised pre-training using recent methods like MoCo v3 [7], BEiT [1], and MAE [16].",2,positive
"For these experiments, we use MAE and 50 epoch fine-tuning by default.",2,positive
"While these values may not appear comparable, the reality is unclear: not all methods may benefit equally from longer training and not all methods have the same per-epoch training cost (e.g., BEiT uses roughly 3× more flops than MAE).",0,negative
In Figure 3 we study the impact of MAE pre-training epochs on COCO APbox by sweeping pre-training epochs from 100 to 1600 (the default).,2,positive
"In summary, we observe that all initializations lead
to roughly the same classification performance for correctly localized objects, however the MAE and BEiT initializations improve localization compared to the other initializations.",1,neutral
"Recently, MoCoV3 [84], DINO [180], BEiT [85] and MAE [169] have demonstrated that attention-based models are also well suited to visual tasks.",1,neutral
"Following ViT, many transformer-based architectures such as PCT [27], IPT [79], T2T-ViT [44], DeepViT [167], SETR [81], PVT [45], CaiT [168], TNT [82], Swin-transformer [46], Query2Label [83], MoCoV3 [84], BEiT [85], SegFormer [86], FuseFormer [169], and MAE [170] have appeared, with excellent results for many kind of visual tasks including image classification, object detection, semantic segmentation, point cloud processing, action recognition, and self-supervised learning.",2,positive
"Following ViT, many transformer-based architectures such as PCT [27], IPT [79], T2T-ViT [44], DeepViT [166], SETR [81], PVT [45], CaiT [167], TNT [82], Swintransformer [46], Query2Label [83], MoCoV3 [84], BEiT [85], SegFormer [86], FuseFormer [168] and MAE [169] have appeared, with excellent results for many kind of visual tasks including image classification, object detection, semantic segmentation, point cloud processing, action recognition and self-supervised learning.",2,positive
"Recently, MoCoV3 [84], DINO [180], BEiT [85], and MAE [170] have demonstrated that attentionbased models are also well suited to visual tasks.",1,neutral
"Self-Supervised Transformer Generative: iGPT [69], MST [70], BEIT [71], MAE [72].",1,neutral
"Specifically, Point-BERT [114] and PointMAE [115] directly transfer the previous works [71], [72] to point clouds, while MaskPoint [116] changes the generative training scheme by using a contrastive decoder as similar as DINO (2022) [93] for binary noise/part classification.",2,positive
"Many self-supervised frameworks, such as MoCo [28, 29, 142], SimCLR [27, 143], SimMIM [144], MAE [145], MaskFeat [146], have already provided a big boost to CNNs and Transformer.",2,positive
"In this section, we discuss how to apply our framework to MAE (He et al., 2022), CLIP (Radford et al.",2,positive
"To explain, visual features of these modular networks are usually redundant and noisy [20], and inevitably hurt the efficiency of multi-modal reasoning.",1,neutral
"based on masked image modeling [3], [26], [51], [64] have achieved remarkable performances in the fine-tuning setting.",1,neutral
"Motivated by BERT in NLP, masked self-supervised learning has attracted attention in the computer vision field [21, 98, 336].",1,neutral
"Further, with the recent success of Transformer architectures in computer vision [6], [7], [8], NLP [9], [10], [11] and audio [12], [13], [14], there has been a pivot recently on",1,neutral
"for an input signal and use this latent representation for a variety of applications using a classification head [22], [23], [8].",1,neutral
"SORNet is able to outperform competing pretraining methods (CLIP, MVP, R3M and MAE) in a similar fashion.",2,positive
"We also provide a variant of the MAE model finetuned on our data, denoted as MAE-sor.",2,positive
• MAE-sor is a variant of MAE that is trained on our data (Leonardo and Kitchen) using the masked autoencoding objective.,2,positive
"Interestingly, we did not find a big difference compared to the MAE model pre-trained on ImageNet, which indicates that dataset bias is not the key issue in our comparisons.",2,positive
MAE We use the ViT-B/16 model in the official MAE implementation https://github.com/ facebookresearch/mae.,2,positive
• MAE [41] pretrains visual transformers [30] on the ImageNet data using reconstruction from a masked input (referred to as masked autoencoding) as supervision and demonstrated state-of-the-art transfer results to image classification.,2,positive
Another work [276] utilizes Masked Auto-Encoders [93] to further enhance unsupervised representation learning.,1,neutral
"In addition, since masked image modeling has surpassed the performance achieved by contrastive methods recently [2, 11], a future study may focus on acquiring additional training signals from masked modeling.",1,neutral
"Recently, more works extending the idea of contrastive learning and non-contrastive learning are emerging [4], [34], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51].",1,neutral
"When applying our method to other self-supervised learning methods, like GAN [18], BYOL [11], and MAE [19], some additional revisions are required.",2,positive
"There are also some works [63], [64], [65] aiming to use self-",1,neutral
provides an alternative to learn the spatial feature correspondence with the image reconstruction task [15].,1,neutral
"Backdoor attack on MAE: We attack Masked Autoencoders (MAE) [21], which is a very recent SSL method.",2,positive
We also try masked autoencoders (MAE) [21] which is a very recent SSL method.,2,positive
"Particularly, we compare with MoCo-V3 [72], Dino [73], MAE [57], and SimMIM [56].",1,neutral
Two notable post arts are SimMIM [56] and MAE [57].,1,neutral
Masked Autoencoders (MAE) [88] instead uses an encoder-decoder architecture based on Transformers trained with masked image modeling (i.,1,neutral
"Inspired by the dramatic success of transformers in NLP [10, 40], recent works have explored applying a standard transformer for vision tasks [32], such as image classification [8,11,14], object detection [6,64], semantic segmentation [48,62], image generation and translation [8,12,16,19], and completion [31, 47].",1,neutral
"[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick, “Masked autoencoders are scalable vision learners,” arXiv preprint",2,positive
"Therefore, it is important to consider whether emerging technologies can contribute to the growing requirement for more powerful neural networks [19].",1,neutral
"With the development of self-supervised learning (He et al., 2021), exploiting the unlimited amounts of unlabeled data for model pre-training, then transferring the pre-trained model into OVIS will largely enhance the discriminative power of frame embeddings.",2,positive
"After iGPT, masked image modeling is proposed such as MAE [32] and SimMIM [111] which achieves competitive performance on downstream tasks.",2,positive
"Self-supervised pretraining iGPT [14] Pixel prediction self-supervised learning, GPT model ICML 2020 MoCo v3 [31] Contrastive self-supervised learning, ViT ICCV 2021 MAE [32] Masked image modeling, ViT CVPR 2022",2,positive
"Recently, masked auto-encoders with transformer-based network architectures have demonstrated great performance on unsupervised representation learning [23, 4].",1,neutral
"Similar to the recent unsupervised learning methods [57], [58], [59], we compare the effectiveness of the features extraction using a ResNet50 backbone with our approach to those obtained with a supervised ImageNet pre-trained model.",2,positive
"Therefore, existing pre-training tasks of backbones [10], [11], [37] can not be simply applied to DETR.",1,neutral
"In our work, we pre-train the model on ImageNet dataset at patch level, as this is a common practice in pre-training on ImageNet and transferring to COCO or VOC datasets [10], [37].",2,positive
"Differently, our PLM is optimized to model the action distribution while MAE is optimized for general purposes and requires extra task-speciﬁc tuning for downstream applications.",2,positive
"Similar to our PLM, a recent study in self-supervised learning, masked auto-encoder (MAE) [32] also observe that latent learning generally has better generalization ability.",1,neutral
"to our PLM, a recent study in self-supervised learning, masked auto-encoder (MAE) [32] also observe that latent learning generally has better generalization ability.",1,neutral
"…concerning Transformers, including BERT-Base (Devlin et al., 2019) for texts, ViT-Base (Dosovitskiy et al., 2021) for images, CLIP (Radford et al., 2021) for bi-modality, ViT-Large (Dosovitskiy et al., 2021) for the large model, and ViT-MAE (He et al., 2022) for the global pooling strategy.",2,positive
"The evaluation settings cover different modalities, different sizes and different pooling strategies concerning Transformers, including BERT-Base (Devlin et al., 2019) for texts, ViT-Base (Dosovitskiy et al., 2021) for images, CLIP (Radford et al., 2021) for bi-modality, ViT-Large (Dosovitskiy et al., 2021) for the large model, and ViT-MAE (He et al., 2022) for the global pooling strategy.",2,positive
"The masked autoencoder (MAE) (He et al., 2022) is a self-supervised learning approach of masking random patches of the input images and reconstructing the missing pixels.",1,neutral
"In spite of its good performances in explaining ViT-Base, the visualizations on other ViT variants show some contradictions to the models’ properties, where the model with better performances (ViT-L) is visualized with a focus on less semantic pixels, and the explanation results do not correspond with the semantics enhancement brought by the masked pretraining technique (ViT-MAE).",1,neutral
"We show in Figure 1 some visualizations with Generic Attribution to different types of Vision Transformers (Dosovitskiy et al., 2021; He et al., 2022).",2,positive
ViT-MAE.,0,negative
"Besides ViT-Base in Sub-section 4.2, we conduct evaluation experiments on another two variants: ViT-large-16-224 (Dosovitskiy et al., 2021), denoted ViT-Large and ViT-MAE with the global pooling strategy instead of [CLS] pooling in vanilla ViT-base-16-224 model.",2,positive
", 2021) for the large model, and ViT-MAE (He et al., 2022) for the global pooling strategy.",2,positive
The visualization of ViT-MAE is shown in the third line of Table 8 with the same photo and predicting class.,0,negative
"Our method is further validated qualitatively and quantitatively through the faithfulness evaluations across different settings: single modality (BERT and ViT) and bi-modality (CLIP), different model sizes (ViT-L) and different pooling strategies (ViT-MAE) to demonstrate the broad applicability and clear improvements over existing methods.",2,positive
"ViT-MAE adopts the same architecture as ViT-Base, but with a different pretraining approach and pooling strategy.",2,positive
"Most Transformers perform the classification task via the last [CLS] embedding, while some use global pooling instead (He et al., 2022).",1,neutral
"We can see the proposed Edge MAE achieves the highest results, and link prediction models perform better as they model the whole return process entirely.",2,positive
"For pre-training Edge MAE, we tuned the learning rate as 1.5e-4, weight decay as 0.05, the mask ratio as 1/3, and batch size as 2048.",0,negative
"From the two tables, we can see that, Edge Transformer outperforms almost all baseline models, and the results can be further improved by pre-training Edge MAE on unlabeled edges.",2,positive
"Furthermore, we propose a novel Edge Transformer model and pre-train the model via Masked Auto-Encoders (MAE).",2,positive
The decoder layer of Edge MAE is tuned as 1.,1,neutral
The encoder of an Edge MAE is an Edge Transformer but only applied on unmasked tokens.,1,neutral
All four edge tokens are the input to the Edge MAE decoder.,1,neutral
"In this section, we evaluate our Edge Transformer and Edge MAE in two settings.",2,positive
"Following MAE, Edge MAE randomly masks a proportion of input tokens.",1,neutral
"4.2 Parameter Settings For Edge Transformer and Edge MAE, we set the latent vector size 𝐷 as 256, the dropout rate as 0.0, and the number of attention heads as 3.",1,neutral
The reconstruction target in MAE allows the model to learn the prior distribution of node and edge features.,1,neutral
The ViT models are further improved by pre-training masked autoencoders on unlabeled images [7].,2,positive
"Inspired by MAE [7] in CV which pre-trains masked auto-encoders on unlabeled images, we propose to improve Edge Transformer with an Edge MAEmodel.",2,positive
"For the pretraining and retraining stages, the loss functions, optimizers, stop criterion, and their hyperparameters of MAE and BERT are the same as that of PEAMATL.",2,positive
"D convolutional neural networks (CNNs)-based methods (named 1CDTL [21] and 3CDTL [16]), two masked autoencoders-based methods (named MAE [32] and bidirectional encoder representations from transformers model (BERT) [37]), and one partial least squares regression (PLSR)-based method (named PLSUS [10]), are compared with PEAMATL in terms of the prediction performances for tablet, melamine, and apple samples in the target domain.",2,positive
"3) There are fewer parameters to be learned in the PEAMATL model than in the MAE and BERT models, making the PEAMATL model easier to train [43].",1,neutral
"The decoder of MAE is a four-head selfattention module with an H /I = 1 MLP, while the decoder in BERT for each patch is implemented by an MLP with four output nodes.",2,positive
"MAE and BERT: the transfer strategy of MAE and BERT is the same as that of PEAMATL, which is to pretrain the encoder and transfer it as the backbone network of the prediction model in the second stage.",2,positive
"Three DTL methods, including two 1-D convolutional neural networks (CNNs)-based methods (named 1CDTL [21] and 3CDTL [16]), two masked autoencoders-based methods (named MAE [32] and bidirectional encoder represen-",1,neutral
"Index Terms— Domain adaptation, masked autoencoders (MAEs), near-infrared (NIR) technology, prediction model, self-supervised learning (SSL), transfer learning (TL).",1,neutral
"1CDTL, 3CDTL, MAE, BERT, and PEAMATL are Python 3.7 applications built on the Pytorch framework, while PLSUS is a MATLAB R2019a application (Mathworks, Inc., Natwick, MA, USA).",2,positive
"From Tables s4 and s5 in the supplementary material, for the three datasets with domain shift, 1CDTL, 3CDTL, PLSUS, MAE, and BERT models obtain R (RPD) values of 0.866–0.987 (1.79–8.61), 0.855–0.988 (1.78–9.24), 0.870– 0.988 (1.84–9.38), 0.862–0.985 (1.77–9.22), and 0.867–0.986 (1.78–8.98), respectively.",0,negative
"2) Compared with the single-scale global features of MAE and BERT, PEAMATLs multiscale features obtained by fusing low- (local) to high-level (global) features
Authorized licensed use limited to the terms of the applicable license agreement with IEEE.",2,positive
"To implement this goal, this study proposes a novel self-supervised TL (SSTL), called pyramid external attention model and masked autoencoder (MAE)-based TL approach (PEAMATL), where the pretraining of the source model (i.e., encoder) is implemented through self-supervised learning (SSL) method, which learns domain-invariant features under the supervision of the spectral data itself.",2,positive
"The hyperparameters of the PEAMATL, MAE, and BERT are summarized in the following.",1,neutral
Three possible reasons may result in the results: 1) classical self-attention (the basic module of MAE and BERT) only concentrates on the self-affinities between different bands of a single sample.,1,neutral
3) The PEAMATL outperforms the MAE method with 4.31%–34.25% and the BERT method with 3.32%–32.13% on the test RMSE of all scenarios.,0,negative
"We mainly compare our method with ClusterFit (Yan et al., 2020), SNCA+ (Wu et al., 2018), Grafit (Touvron et al., 2021b), iBOT (Zhou et al., 2022), DeiT (Touvron et al., 2021a) and MAE (He et al., 2021).",2,positive
"However, for the previous methods like MAE (He et al., 2021), there’s no regularization added on the [CLS] token.",1,neutral
"Current MIMbased methods (He et al., 2021; Chen et al., 2022; Xie et al., 2021) basically split raw images into several patches, and randomly select patches to mask.",2,positive
"However, CAE divides them randomly, which is similar to MAE (He et al., 2021), limiting their performance.",1,neutral
"In our experiments, the block-wise dividing strategy gets a bit lower accuracy than random dividing, which is in line with previous methods MAE (He et al., 2021) and SimMIM (Xie et al., 2021).",2,positive
"Based on the reconstruction objective, these methods can be divided into: pixel-wise reconstruction (He et al., 2021; Xie et al., 2021) and auxiliary features/tokens prediction (Dong et al.",2,positive
"…over MAE: Using ViT-B (Dosovitskiy et al., 2020) as a standard protocol in MIM, ccMIM achieves 83.6%, 84.2% top-1 accuracy with 300 and 800 epochs pre-training, outperforming MAE (He et al., 2021) 0.8% (82.8% for 300 epochs) and 0.6% (83.6% for 1600 epochs) accuracy on ImageNet-1K, respectively.",2,positive
"2% top-1 accuracy with 300 and 800 epochs pre-training, outperforming MAE (He et al., 2021) 0.",0,negative
"MIM-based methods (He et al., 2021; Xie et al., 2021) learn vision representation by reconstructing the masked patches from the partial observations.",1,neutral
"However, CAE divides them randomly, which is similar to MAE (He et al., 2021), limiting their performance. ccMIM partitions the two sets by importance sampling.",1,neutral
", 2021) and MAE (He et al., 2021) are the first two methods applying mask modeling in the visual domain.",1,neutral
"Evaluation protocols. i) Linear probing is widely used as a proxy of pretraining quality evaluation for SSL (Chen et al., 2020b; He et al., 2021; Chen et al., 2022).",2,positive
"…classifier over the image-level representation output from the pretrained encoder by using the labels of the images, and then tests the performance on the validation set. ii) Fine tuning is often used to evaluate the backbone in reconstructed-based methods (He et al., 2021; Chen et al., 2022).",1,neutral
"We perform self-supervised pre-training on the ImageNet-1K (Deng et al., 2009) training set, as commonly used in SSL (He et al., 2021; Zhang et al., 2021).",2,positive
"In this paper, following MAE (He et al., 2021), we mainly consider ViT (Dosovitskiy et al., 2020) as backbones.",2,positive
"We mainly compare our method with transformer-based contrastive learning methods (Caron et al., 2021; Chen et al., 2021) and MIM-based (He et al., 2021; Xie et al., 2021) methods.",2,positive
"Based on the reconstruction objective, these methods can be divided into: pixel-wise reconstruction (He et al., 2021; Xie et al., 2021) and auxiliary features/tokens prediction (Dong et al., 2021; Chen et al., 2022).",2,positive
"In this paper, we mainly follow the settings in MAE (He et al., 2021).",1,neutral
"With the success of recent ViT-based vision backbones, which divide images into several patches, Mask-Image-Modeling (MIM) methods (He et al., 2021; Fang et al., 2022; Chen et al., 2022) randomly mask some patches and use the self-attention mechanism to ∗Junchi Yan is the correspondence author.",2,positive
"The decoder
of ccMIM follows the settings of MAE (He et al., 2021), which includes two-layer MHSA blocks.",2,positive
"SimMIM (Xie et al., 2021) and MAE (He et al., 2021) are the first two methods applying mask modeling in the visual domain.",1,neutral
"With the success of recent ViT-based vision backbones, which divide images into several patches, Mask-Image-Modeling (MIM) methods (He et al., 2021; Fang et al., 2022; Chen et al., 2022) randomly mask some patches and use the self-attention mechanism to
∗Junchi Yan is the correspondence author.",2,positive
"In this paper, following MAE (He et al., 2021), we mainly consider ViT (Dosovitskiy et al.",1,neutral
"Recently, Masked Image Modeling (MIM) becomes a popular topic for its scalability as well as promising performance (Bao et al., 2022; He et al., 2022; Dong et al., 2021; Chen et al., 2022; Xie et al., 2022), especially for MAE (He et al., 2022) which significantly accelerates training via only…",2,positive
"MAE (He et al., 2022) and SimMIM (Xie et al., 2022) directly predict the raw pixel value, and MaskFeat (Wei et al., 2022) predicts the HOG (Dalal & Triggs, 2005) feature of masked patches.",2,positive
"He et al. (2022) shows that MAE suffers serious performance degradation once its mask ratio exceeds 75%, which prevents MAE keep fewer tokens in the encoder.",2,positive
"Note that, as a reminiscent method, MAE (He et al., 2022) expects to reconstruct all patches, so the number of mask tokens is 147 (take ViT-B/16 as an example).",1,neutral
"…a popular topic for its scalability as well as promising performance (Bao et al., 2022; He et al., 2022; Dong et al., 2021; Chen et al., 2022; Xie et al., 2022), especially for MAE (He et al., 2022) which significantly accelerates training via only operating on 25% visible patches in the encoder.",2,positive
"Recently, the vision transformer (Dosovitskiy et al., 2021) bridges the architecture gap between vision and language, and more MIM methods (Bao et al., 2022; Dong et al., 2021; Xie et al., 2022; He et al., 2022; Wei et al., 2022; Chen et al., 2022) are proposed inspired by Masked Language Modeling.",2,positive
"Specifically, PCAE is able to accelerate training 2.25 times compared with MAE (He et al., 2022) (739.7 img/s v.s. 328.4 img/s, ViT Base, 32 GB V100), while enjoying much faster converge speed (PCAE 300 epoch 83.6 vs MAE 1600 epoch 83.6) or higher performance (PCAE 800 epoch 83.9 vs MAE 1600 epoch…",2,positive
We follow the implementation of He et al. (2022) for the decoder part.,2,positive
"Recently, Masked Image Modeling (MIM) becomes a popular topic for its scalability as well as promising performance (Bao et al., 2022; He et al., 2022; Dong et al., 2021; Chen et al., 2022; Xie et al., 2022), especially for MAE (He et al.",2,positive
"We follow the setting of hyper-parameters of MAE (He et al., 2022).",1,neutral
"We follow the fine-tuning schedule and hyper-parameters in MAE (He et al., 2022) out of fair comparison.",2,positive
", 2022), especially for MAE (He et al., 2022) which significantly accelerates training via only operating on 25% visible patches in the encoder.",2,positive
", 2021) bridges the architecture gap between vision and language, and more MIM methods (Bao et al., 2022; Dong et al., 2021; Xie et al., 2022; He et al., 2022; Wei et al., 2022; Chen et al., 2022) are proposed inspired by Masked Language Modeling.",2,positive
"The results in Table 15 show that the finetuning performance of PCAE is robust to decoder depth but the performance in downstream tasks relies on deeper decoders, which is consistent with MAE (He et al., 2022).",2,positive
"MIM methods (Chen et al., 2022; He et al., 2022; Wei et al., 2022) exhibit superior performance to previous contrastive learning methods in downstream fine-tuning tasks based on the modern vision transformers.",1,neutral
"The settings of data augmentation and optimization keep consistent with MAE (He et al., 2022).",2,positive
"Method Network mIoU Supervised (He et al., 2021) ViT-B 47.",1,neutral
", 2022), and MAE (He et al., 2021) by a significant margin of at least 4.",2,positive
"Then, HiViT models are tested using masked image modeling self-supervised methods (MIM) (He et al., 2021).",1,neutral
"Under the MAE framework (He et al., 2021), with 1600 epochs of pre-training and 100 epochs of fine-tuning, HiViT-B reports a 84.",2,positive
"We are interested in a particular visual pre-training method named masked image modeling (MIM) (Bao et al., 2021; He et al., 2021).",2,positive
"…et al., 2020; Caron et al., 2021; 2020; Xie et al., 2021a; Tian et al., 2021), and generation-based proxies that required visual representations to be capable of recovering the original image contents (Zhang et al., 2016; Pathak et al., 2016; He et al., 2021; Bao et al., 2021; Tian et al., 2022).",2,positive
"The objective is either computed on tokenized features (Bao et al., 2021) or pixels (He et al., 2021).",2,positive
"Under the MAE framework (He et al., 2021), with 1600 epochs of pre-training and 100 epochs of fine-tuning, HiViT-B reports a 84.6% top-1 accuracy on ImageNet1K, which is +1.0% over ViT-B (trained with MAE) and +0.6% over Swin-B (trained with SimMIM (Xie et al., 2021b)).",2,positive
"Similar to the situation on COCO, HiViT-B benefits from hierarchical features and reports a 52.8% mIoU, surpassing ViT-B pretrained by MoCo-v3 (Chen et al., 2021c), BEiT (Bao et al., 2021), CAE (Chen et al., 2022), and MAE (He et al., 2021) by a significant margin of at least 4.0%.",0,negative
"The pre-training settings follow those of MAE (He et al., 2021).",0,negative
"When fine-tuning, we follow the settings from (He et al., 2021) where the models are trained for 100 epochs using the AdamW optimizer with a warm-up for 5 epochs, a weight decay 0.05, and the input size 224× 224.",2,positive
"A typical example lies in masked image modeling (MIM), a recent methodology of visual pre-raining (Bao et al., 2021; He et al., 2021; Xie et al., 2021b), in which a random subset of image patches are masked from input and the model learns by reconstructing the masked contents.",1,neutral
", 2021), and generation-based proxies that required visual representations to be capable of recovering the original image contents (Zhang et al., 2016; Pathak et al., 2016; He et al., 2021; Bao et al., 2021; Tian et al., 2022).",2,positive
"MAE (He et al., 2021) improved MIM by only taking the visible tokens as input and computing loss at the pixel level – the former change largely accelerated the training procedure as encoder’s computational costs went down.",2,positive
"However, compared to convolutional approaches, vision transformers used in masked autoencoders (MAE) (He et al., 2022) have been shown to better integrate global information (Trockman and Kolter, 2022).",1,neutral
"Interestingly, the MAE model trained on 1.2 million natural RGB images (ImageNet) with a 75% masking ratio by He et al. (2022) did not exhibit this overfitting issue when evaluated on false-color RGB images of SST from the LLC4320 validation set
with different test-time masking ratios (Figure 3).",0,negative
"This approach, adopted from He et al. (2022), trains an encoder network to extract a compact representation of masked input data and simultaneously trains a decoder to reconstruct the original input from the masked representation.",2,positive
"While the original MAE implementation He et al. (2022) uses the mean squared error (MSE) between the reconstructed and original pixel values, MAESSTRO uses the root-mean-square error (RMSE) in order to recover the same units
(◦C) as the SST field being reconstructed.",2,positive
"Inspired by this progress, He et al. (2022) adapted masked autoencoding, a self-supervised learning technique rooted in language modeling Peters et al. (2018), to computer vision, proposing MAE for self-supervised pretraining on unlabeled images.",2,positive
MAE [13] removes random patches to reconstruct pixels under a high masking ratio (75%) and works well.,2,positive
"1: Pad the left and top regions of Xori with a width of 16 pixels to the right and bottom: Xori → X∗ ori ∈ R224×224; 2: Horizontally shift the padded image X∗ ori by ∆x pixels, and vertically shift by ∆y pixels; 3: Embed X∗ ori to the feature: X ∗ ori → Fori ∈ RB×P×C ; 4: Complementarily grid-mask Fori twice with a masking ratio of 50%: Fori → (F1, F2), Fi ∈ RB×P× C 2 , F1 ∪ F2 = Fori; 5: for Fi in (F1, F2) do 6: Reconstruct Fi by MAE [13]: Fi → F i ∈ RB×P× C 2 ;",1,neutral
"Inspired by MAE [13], we propose a naive background reconstruction method (NBR).",2,positive
"MAE [13] developed an asymmetric encoder-decoder architecture, which masks random patches of the input image and reconstructs the missing pixels.",2,positive
Other hyperparameters are the same as MAE [13].,1,neutral
"For BR, we directly use pre-trained MAE with extra GAN loss [13].",0,negative
"Therefore, we utilize a mature but large Vision Transformer (MAE-ViT-large [13]) for background reconstruction.",2,positive
This ‘goal’ is also encoded by the same MAE encoder and concatenated with the bounding box feature.,2,positive
"A masked auto-encoder (MAE) (He et al., 2022), with pre-trained weights, was applied as a feature encoder.",2,positive
"Inspired by previous works which do selfsupervision by predicting rotation angles and context reconstruction (He et al., 2022), two proxy tasks were exploited: masked volume inpainting and image rotation.",1,neutral
"The two MAE concatenated features are then provided as input to a regression model (three layers of a vision transformer block (Dosovitskiy et al., 2020) plus two fully connected layers) to predict the updated location of the bounding box.",2,positive
trained using pretrained features [14].,0,negative
"For instance, ResNet-101 (from CLIP) provides much better accuracy than the two transformer-based models from MAE and SimMIM for UC-Merced and NWPURESISC45, respectively.",2,positive
"For CLIP, a bigger model tends to provide better results, which is not the case for ViT from MAE and Swin from SimMIM because the CLIP-based models are trained with a huge multimodal dataset and have higher capacities; thus, they are less prone to overfitting.",2,positive
"However, the large MAE-based ViT and SimMIM-based Swin models tend to overfit and have low generalizability.",1,neutral
We compared CLIP-based extractors to the other two methods (masked autoencoders (MAE) [14] and a simple framework for masked image modeling (SimMIM) [15]).,2,positive
"Autoencoders are generative models [14], [15] in which the loss is measured in the output space.",1,neutral
"Furthermore, Alice also achieves at least 2.22% improvement of DSC relative to hybrid SSL methods IBOT and CMAE, which combine contrastive learning and MIM.",2,positive
"Following existing siamese frameworks [61, 45, 25], we introduce a target encoder to generate contrastive supervision for the online encoder to further strengthen the representation learned by MIM with semantic discriminability.",2,positive
"More recently, some works explore how to learn representations by combining contrastive learning and MIM [45, 61, 25].",1,neutral
"There is also a growing trend to combine CL and MIM in a self-distillation way to design more powerful SSL frameworks [61, 44, 45, 25].",1,neutral
"Similar to MIM, image restoration is also commonly used as pre-text task to memorize spatial context from medical images.",1,neutral
"Following MAE [21], we use the normalized pixel values as targets in the reconstruction task.",2,positive
"Combining such advantage with the MIM objective, the online encoder learns to capture both high-level discriminative features and fine-grained localizationsensitive details favored by the downstream segmentation tasks.",2,positive
"Following MAE [21], we only feed the visible, unmasked patches to the online encoder Es.",0,negative
"Masked Image Modeling (MIM) accepts input image corrupted by masking and predicts the target of the masked content, which has been actively studied recently in self-supervised learning.",1,neutral
"Now we analyze the drawbacks and irrationalities of existing hybrid SSL approaches, which combine CL with MIM, from the following aspects:
ar X
iv :2
30 2.",2,positive
"In this work, we propose a novel self-supervised learning method (Alice) for improving the learned image representation of contrastive learning and MIM by modeling the class-specific invariance of intrinsic anatomical semantics in 3D medical images.",1,neutral
"In this work, Alice takes one step further by exploiting cooperations between contrastive learning and MIM to learn effective representations with both strong instance discriminability and local detail sensitive perceptibility, from varying or different image views.",1,neutral
"Existing work mainly differ in their regression objectives [2, 14, 47, 49, 21, 52] or masking strategies [27, 30, 42].",1,neutral
"Here, u and r are random masking, which is similar to the “random sampling” adopted in MAE [21].",1,neutral
"For MIM tasks, we apply augmentations and masking strategy following MAE [21] to the input of the online encoder.",2,positive
"Driven by the aforementioned limitations, we present a simple, effective, and dedicated selfsupervised learning framework for 3D medical segmentation tasks, Alice, by explicitly fulfilling Anatomical invariance modeling and semantic alignment through elaborately combined contrastive learning and MIM.",2,positive
"Compared with strong Transformer-based contrastive learning methods MoCo v3 and DINO, Alice outperforms them by at least absolute 3.74% in DSC. Compared with MIM methods MAE and SemMAE, the DSC score of Alice surpasses that of MAE and SemMAE by a large margin.",2,positive
"Nowadays, contrastive learning (CL) [3, 11, 56, 39] and masked image modeling (MIM) [2, 52, 21], together with Vision Transformers (ViTs) [16, 32], have revolutionized the field of SSL in computer vision and medical imaging, which achieve the state-of-the-art (SOTA) performance for a variety of tasks [3, 61, 21, 51, 44].",1,neutral
"This mask-and-regenerate idea has been extensively applied in many current models in the CV domain for self pre-training tasks, such as MAE [16].",1,neutral
"[16] formally propose Masked Autoencoder to increase the difficulty of pre-training Transformer-based models by a very high mask ratio, e.",1,neutral
"In this subsection, we discuss the details of self pre-training and transfer learning using MaskSL, by introducing an instance based on MAE.",2,positive
"As mentioned previously, we use MAE as the loss function of reconstruction tasks in this paper.",1,neutral
"Empirically, the number of Transformer layers of encoder should be much more than that of decoder’s, with a ratio between about 5-10 times [16].",1,neutral
The emergence of the masked autoencoder (MAE) [16] has greatly influenced our community due to its simplicity and effectiveness.,2,positive
"To evaluate the effectiveness of the modification, we compare the proposed strategy with that of MAE [16].",2,positive
"Compared to MAE [16], which randomly masks the most parts of the image, DeepfakeMAE introduces a simple masking strategy conditioned on face feature points to randomly mask the region of a facial part at one time, as shown in Figure 1.",1,neutral
We modify the masking strategy in MAE [16] to improve the generalization.,2,positive
Our masked autoencoder is based on an asymmetric encoder-decoder architecture [16].,2,positive
"In this stage, we customize MAE [16] and its masking strategy with minimal but effective modifications, so that the model can learn generic facial part consistency features and can also prevent over-fitting to specific forgery patterns.",2,positive
"The original MAE [16] masks random patches of the input image, but the same strategy is not suitable for our DeepfakeMAE for the following reasons.",2,positive
"Inspired by [16], we propose a new self-supervised pretraining method based on masked autoencoder, named DeepfakeMAE.",2,positive
MAE [16] is designed for pretraining Vision Transformers (ViT) rather than for Deepfake detection.,2,positive
We customize the original masking strategy from MAE [16] and design a facial part masking strategy to ensure that the model can learn the consistencies of all facial parts.,2,positive
It implies that the information density of relation is at the medium level [38].,1,neutral
"Recent advances indicate that masked modeling [38], [39] can learn general representations by decreasing the redundancy of information.",1,neutral
"Through predicting the masked words or patches, models tend to understand the high-level semantic representations with few inductive biases [38].",1,neutral
Masked Autoencoders (MAEs) [38] are the pioneering methods that learn visual representation by reconstructing masked image patches.,1,neutral
"by observation [38], [72] that low redundant representations learn the intrinsic features, we decrease the redundancy of relational information to learn compact and generic facial features.",1,neutral
"studies [38], [72] indicate that masked modeling is also effective for computer vision tasks.",1,neutral
"Furthermore, inspired by the success of masked autoencoders (MAE) [17], the mask operation is introduced to alleviate the overfitting problem.",1,neutral
"In this section, we illustrate how an interpretable image prior is formulated by borrowing the idea of DIP, self-attention [56] and MAE [17].",1,neutral
"is formulated by borrowing the idea of DIP, self-attention [56] and MAE [17].",1,neutral
"By simplifying the notation, the equation can be equivalent to the optimization problem in [17], [44], and the following equation is derived:",1,neutral
"of masked autoencoders (MAE) [17], the mask operation is introduced to alleviate the overfitting problem.",1,neutral
formers (Dosovitskiy et al. 2020; He et al. 2021)) directly fine-tune the original pretrained parameters through a task head (normally with a cross entropy (CE) loss).,1,neutral
"In particular, we compare the LaCViT-trained models and baseline models that do not include contrastive learning, including LaCViT-MAE verse MAE (He et al. 2021), LaCViTSimMIM verse SimMIM (Xie et al.",1,neutral
2022) and MAE (He et al. 2021)) over five standard image classification datasets.,2,positive
"2020) and Masked Autoencoders (MAE) (He et al. 2021) having been central to advancing the state-of-the-art for many vision tasks, such as image classification and object detection.",2,positive
"Moreover, as the most discriminative information for the target tasks, the task label information is normally ignored in their pretrained representations (He et al. 2021; Xie et al. 2022).",1,neutral
Those native advantages of transformer models brings markedly reduced training times and cost (Dosovitskiy et al. 2020; He et al. 2021).,2,positive
"In 2021, Masked Autoencoders (MAE) (He et al. 2021) were proposed, which addressed the cost of training via the use of a high image masking strategy with an encoder-decoder self-supervised pre-training schema, which enables MAE to learn how to reconstruct the original image based on only partial observations of that image.",2,positive
"MAEBase (He et al. 2021), in our paper, since it is the fastest model to train with our resources.",2,positive
"2022) and MAE (He et al. 2021), which are the state-ofthe-art models in image classification.",2,positive
"Specifically, the predictive performance of AdCo-extracted features yielded a correlation coefficient of 0.30, while MAE reached only 0.17.",0,negative
"For feature extraction, we tested two types of self-supervised learning, adversarial contrastive learning (AdCo) and masked autoencoder (MAE)[31], and compared their performance.",2,positive
"2.7 Contrastive learning and attentive pooling improve performance We also conducted a comparative analysis of two self-supervised learning models, namely masked autoencoder (MAE) and adversarial contrastive learning (AdCo), with respect to their performance in extracting tile-level features.",2,positive
"As depicted in Figure4 (A), the results indicated that the performance of AdCo performed significantly superior to MAE.",0,negative
We speculated that the transformer-based[42] MAE is more adept at capturing global associations such as in natural images.,2,positive
(A) Performance comparison in predicting tumor immune infiltration level by AdCo and MAE feature extractors.,1,neutral
"For feature extraction, we tested two types of self-supervised learning, adversarial contrastive learning (AdCo) and masked autoencoder (MAE)[30], and compared their performance.",2,positive
We follow the parameters setting of MAE [13] and CCT [42] for our model training.,2,positive
"1) Image Masking Strategy: Following the masked AEs (MAE) [13], an anomaly image input in the proposed AnoDet is divided into nonoverlapping image patches.",2,positive
"The MAE indicates that given the strong capability of building long-term dependencies, the transformer can reconstruct a whole image with a few visible patches.",2,positive
"Motivated by MAE, we adopt a ViT-based [12] encoder– decoder network as the backbone of each channel network.",2,positive
reconstruction tasks [13] than convolutional neural networks (CNNs).,1,neutral
"By contrast, the random mask patches (e.g., 75% patches) in MAE aim to obtain a well pretrained model through image reconstruction to optimize downstream tasks, such as object detection, object classification, or instance segmentation.",1,neutral
Masked image modeling approaches such as masked auto-encoders (MAEs) [43] train,1,neutral
"such as texts [42], images [43], and image–text pairs [44].",1,neutral
There has been a shift toward VIT architectures and masked prediction methods that can be very efficient by not encoding masked patches in SSL. Masked image modeling approaches such as masked auto-encoders (MAEs) [43] train an encoder–decoder architecture to reconstruct the original RGB values from the mask latent representation.,1,neutral
"Following MAE, we concatenate the mask token and rest token and then forward it to the online decoder.",2,positive
"We sample random patches without replacement
following MAE.",2,positive
"Loss Function: Consistent with MAE and bidirectional encoder representation from transformers (BERT) [42], we use the mean square error on the reconstructed feature maps and the original image space.",2,positive
This article uses l2 regression loss following MAE [43].,1,neutral
"At a high level, our method simply substitutes MAE [72] for the self-supervised part of TTT [171].",2,positive
We intentionally avoid modifying them to make clean comparison with [72].,2,positive
"In this chapter, we show that masked autoencoders (MAE) [72] is well-suited for test-time training.",2,positive
"object recognition) head h, [72] uses a linear projection from the dimension of the encoder features to the number of classes.",1,neutral
The most successful work is masked autoencoders (MAE) [72]: it splits each image into many (e.,1,neutral
"Recent works [72, 17, 196] combine spatial autoencoding with Vision Transformers (ViT) [45] to achieve state-of-the-art performance.",1,neutral
"Spatial autoencoding – removing parts of the data, then predicting the removed content – forms the basis of some of the most successful self-supervised tasks [186, 136, 72, 17, 196].",1,neutral
"Fine-tuning in [72] (and [17, 196]), however, adds aggressive augmentations on top of the two above, including random changes in brightness, contrast, color and sharpness(1).",1,neutral
"Following standard practice, we start from the MAE model provided by the authors of [72], with a ViT-Large encoder, pre-trained for reconstruction on ImageNet-1k [40].",2,positive
"Our default setup, during training-time training, only uses image cropping and horizontal flips for augmentations, following the protocol in [72] for pre-training and linear probing.",2,positive
"Consequently, the largest magnitude of gradients is significantly reduced with the MAE-initialization as shown in Table.",1,neutral
"As a tentative exploration, the stability of training ViT models under the supervision of modern face recognition losses is improved with better parameter initialization via MAE, which has been verified on various training factors, including the scale of data, the ratio of masked faces, and the choice of supervision heads.",2,positive
"The baseline model, without prompt tuning and prompt pooling, is a PreNorm ViT-B model with MAE-initialization.",2,positive
"Empirically, Masked AutoEncoder (MAE) [65] provides a smooth optimization objective to initialize ViT parameters and allows the pre-trained model to be fine-tuned by modern face recognition losses.",1,neutral
"Finally, the ablation study is conducted to verify the effectiveness of the MAE-initialized DeepNorm backbone, prompt tuning, and prompt pooling.",2,positive
"Notably, Masked AutoEncoder (MAE) [65] masks out 75% of patches to reconstruct and makes the plain, nonhierarchical ViT model back state-of-the-art via fine-tuning.",2,positive
The model parameter is initialized by MAE for 10 epochs and fine-tuned for another 10 epochs using the AdamW optimizer [76].,2,positive
"When pre-trained by MAE, ViT drops the [cls] token and uses global average pooling (GAP) to predict image features, assuming all the tokens contribute equally to the classification task.",2,positive
"PIXEL (Rust et al., 2023) uses masked autoencoding with vision transformers (He et al., 2021) to pre-train a masked language model on rendered texts.",2,positive
", 2023) avoids this challenge altogether by casting language modeling as a vision problem: text is rendered as images and the model uses masked autoencoding (He et al., 2021), which is similar to BERT (Devlin et al.",2,positive
"…this challenge altogether by casting language modeling as a vision problem: text is rendered as images and the model uses masked autoencoding (He et al., 2021), which is similar to BERT (Devlin et al., 2019) and other masked language models but for continuous domains like images, to learn a…",1,neutral
", 2023) uses masked autoencoding with vision transformers (He et al., 2021) to pre-train a masked language model on rendered texts.",2,positive
"Based on this dataset, we design the RingMo training method to apply the MIM method to train an RS foundation model.",2,positive
"Masked and visible tokens are fed together as input to predict the original pixel values of masked patches by the encoder–decoder, which allows SimMIM to be applied to Swin Transformer as well.",2,positive
"Similar to other MIM methods [32], we only compute the loss for masked regions.",1,neutral
"Currently, most MIM methods are trained based on natural images [31], [32].",1,neutral
SimMIM [32] designs a simple MIM method and replaces masked patches with learnable mask token vectors.,1,neutral
"Masked image modeling (MIM) [31], [32] is a kind of SSL method applying the",1,neutral
"However, to our best knowledge, there is little exploration of MIM-based
SSL for the RS foundation model.",2,positive
"In addition, for deriving general RS feature representation, we further propose the RingMo MIM method, which enhances the issue of dense and small objects being easily disregarded in complicated RS scenes.",1,neutral
RingMo applies MIM to improve the performance of pixel-level feature encoding and decoding tasks.,2,positive
"Index Terms— Foundation model, masked image modeling (MIM), pretraining, remote sensing (RS), self-supervised, Vision Transformer (ViT).",1,neutral
"MIM can be regarded as a DAE model, which learns to reconstruct the original uncorrupted image from the masked input image.",1,neutral
"1) PIMask Strategy: The masking strategy commonly used in most of MIM methods is random masking, as shown in Fig.",1,neutral
"Masked image modeling (MIM) [31], [32] is a kind of SSL method applying the generative model.",1,neutral
"SimMIM (Xie et al., 2021d) and MAE (He et al., 2021) are the first two methods applying mask modeling in the visual domain.",1,neutral
"(4) where e = 10000 is the pre-defined parameter, which is also commonly used in MAE (He et al., 2021).",1,neutral
", 2009) training set with 1,000 classes, as used in SSL for both MIM (He et al., 2021) and contrastive learning (Chen et al.",2,positive
"We use standard self-supervised learning protocols, including learning a linear classifier on frozen features (Chen et al., 2020a; He et al., 2020) and
finetune on downstream tasks (He et al., 2021; Chen et al., 2022).",2,positive
"Based on the reconstruction objective, they can be divided into: pixel-wise reconstruction (He et al., 2021) and auxiliary feature/tokens prediction (Dong et al., 2021; Zhou et al., 2022).",2,positive
"…(Vaswani et al., 2017) to generate positional embedding:
Wh,w = [ sin ( w e2∗1/d ) , cos ( w e2∗2/d ) , · · · , sin (w e ) ,
sin
( h
e2∗1/d
) , cos ( h
e2∗2/d
) , · · · , sin ( h
e )] (4)
where e = 10000 is the pre-defined parameter, which is also commonly used in MAE (He et al., 2021).",1,neutral
"We conduct self-supervised pre-training on the ImageNet-1K (Deng et al., 2009) training set with 1,000 classes, as used in SSL for both MIM (He et al., 2021) and contrastive learning (Chen et al., 2020a).",2,positive
", 2021d) and MAE (He et al., 2021) are the first two methods applying mask modeling in the visual domain.",1,neutral
"Based on the reconstruction objective, they can be divided into: pixel-wise reconstruction (He et al., 2021) and auxiliary feature/tokens prediction (Dong et al.",2,positive
", 2009) training set, as commonly used in SSL methods for both MIM (He et al., 2021) and contrastive learning (Chen et al.",1,neutral
"are MIM-based methods MAE (He et al., 2021), BEiT (Bao et al.",1,neutral
"Standard SSL protocols is to either learn a linear classifier on frozen features (Chen et al., 2020a; He et al., 2020) or to finetune on downstream tasks (He et al., 2021; Chen et al., 2022).",2,positive
"The compared baselines
are MIM-based methods MAE (He et al., 2021), BEiT (Bao et al., 2021), CIM (Fang et al., 2022) as well as contrastive methods (Caron et al., 2021; Zhou et al., 2022; Chen et al., 2021) and the combination of the two techniques: CAE (Chen et al., 2022) which is emerging.",2,positive
"We conduct self-supervised pre-training on the ImageNet-1K (Deng et al., 2009) training set, as commonly used in SSL methods for both MIM (He et al., 2021) and contrastive learning (Chen et al., 2020a).",2,positive
"Differences. iBOT learns the local information by adding MIM objective, which is now widely developed in MAE (He et al., 2021), SimMIM (Xie et al., 2021d).",2,positive
", 2020) or to finetune on downstream tasks (He et al., 2021; Chen et al., 2022).",2,positive
"iBOT learns the local information by adding MIM objective, which is now widely developed in MAE (He et al., 2021), SimMIM (Xie et al.",2,positive
The training of the model uses MAE [34] to initialize the model backbone.,2,positive
"To accelerate the training and reduce the difficulty of fitting, we use the pre-trained MAE [34] model for model initialization.",2,positive
"Compared with OSTrack’s and MAE blocks, our MNM block has a more significant advantage in performance.",2,positive
Both models are initialized with the MAE [34] pre-trained model.,0,negative
• Masked autoencoders (MAE).,2,positive
The MAE is proven effective in image-related tasks.,1,neutral
"[96] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Following MAE [96], the uncorrupted atoms are used as input of the encoder, while corrupted atoms are not.",1,neutral
"Following (He et al., 2022), during linear probing we include BatchNorm without the affine transformation before the final classifier.",1,neutral
", 2021) or masked reconstruction (He et al., 2022) objectives.",2,positive
"This can be operationalized using contrastive (Radford et al., 2021; Jia et al., 2021) or masked reconstruction (He et al., 2022) objectives.",2,positive
", 2018), images (He et al., 2022), videos (Feichtenhofer et al.",2,positive
"For audio, we use a fixed 2D sinusoidal positional encoding as described in (Huang et al., 2022) and (He et al., 2022).",2,positive
"Most previous approaches have focused on single modalities, such as text (Devlin et al., 2018), images (He et al., 2022), videos (Feichtenhofer et al., 2022), and audio (Baade et al., 2022; Chong et al., 2022).",2,positive
", 2022) and the retained natural information in adversarial samples, we leveraged a masking-related generative network (He et al., 2022) as the denoising model to restore missing natural representation from preserved neighboring pixels.",2,positive
"…in view of the spatial redundancy of images, the missing spatial semantics can be recovered and denoised by the global position of each patch in the complete image using a deep neural network with the pixel information of the patch itself., e.g., the masked autoencoder (He et al., 2022).",1,neutral
"The purpose of pre-training was only to facilitate the ability of the denoising model to extract the effective objective semantics from the remaining neighboring pixels (as was done in (He et al., 2022)).",2,positive
"Based on the spatial redundancy of images (Bastani et al., 2010; Hu et al., 2020; He et al., 2022) and the retained natural information in adversarial samples, we leveraged a masking-related generative network (He et al., 2022) as the denoising model to restore missing natural representation from…",2,positive
"…of images (Bastani et al., 2010; Hu et al., 2020; He et al., 2022) and the retained natural information in adversarial samples, we leveraged a masking-related generative network (He et al., 2022) as the denoising model to restore missing natural representation from preserved neighboring
pixels.",2,positive
"Based on the spatial redundancy of images (Bastani et al., 2010; Hu et al., 2020; He et al., 2022) and the retained natural information in adversarial samples, we leveraged a masking-related generative network (He et al.",2,positive
"For the experiments in Section B of the Appendix, we train all models using the Vanilla training strategy to perform fair comparisons with other methods such as Masked Autoencoders [6] and our Inpainting-based augmentation training strategy.",2,positive
"To compare with Masked Autoencoders (MAE) [6], we pretrain using the masked image modelling technique proposed by them [6].",2,positive
MAE-based pretraining strategy performs better (75.8) compared to the baseline (75.2).,2,positive
"We compared the differences among IBOT [17], DINO [2] and MAE [5].",2,positive
"We replaced iBOT ImageNet 1K pre-trained weights with that of MAE [5], DINO [2].",2,positive
Results show that MAE and DINO also perform well with 61.2% and 54.0% of PCK@0.1 respectively.,0,negative
While MAE utilizes masked image modeling that can extract consistent features despite severe occlusion. iBOT exploits both contrastive learning and masked image modeling and thus gets better performance.,2,positive
The majority of the experiments are conducted on top of the reconstruction-pretrained Masked Autoencoders (MAE) model proposed in [15] finetuned on a small segmentation dataset on a simple binary-segmentation task.,2,positive
"1: Masked Autoencoders (MAE) architecture of [15] (reprinted): In training, only 25 % of all patches are fed into the large-scale encoder, facilitating efficient training.",2,positive
"Self-supervised representation learning has allowed for training on large amounts of training data without the extra annotation cost, achieving impressive performance and generalization capabilities [15], [34], [35] Before being adopted by the computer vision community, self-supervised pre-training first gained popularity in the natural language processing (NLP) domains.",1,neutral
Masked Autoencoders (MAE) by [15] have shown to be particularly strong for selfsupervised pre-training.,1,neutral
"Given the prevalence of self-supervised pretraining, how powerful it has been shown to be for dense-prediction tasks [14], [15] and the success of TTT with Masked Autoencoders (MAE) [16] on classification, continuing training on these tasks alongside the target task is often a straightforward extension and generally, the area of TTT for image segmentation is a direction worth exploring.",1,neutral
The focus will be on masked-autoencoders of [15] since these are the basis of the models used in the experiments and optimizing the reconstruction loss of the masked autoencoder is one of the proposed test-time training methods.,2,positive
1 Masked Autoencoders (MAE) architecture of [15].,1,neutral
This part describes the work done by the authors of [15].,0,negative
This work builds on top of the large-scale pretrained MAE model of [15].,2,positive
"It was first pre-trained in the same fashion as MAE from [15] but the architecture is not a plain Vision Transformer (ViT), instead, it is adapted for efficiency on images of higher resolution required for high-quality masks by using a window attention mechanism, rather than global attention.",2,positive
", the same as fine-tuning datasets), we pretrain a ResNet50 with several typical SOTA SSL methods in computer vision including MoCo v2 [8], SimCLR [11], Simsiam [12], and DenseCL [10] while a ViT-Base [46] is pretrained with MAE [9],",2,positive
"In recent years, studies on self-supervised learning (SSL) have paved the way to alleviate reliance on mass labeled data [7], [8], [9], [10], [11], [12].",1,neutral
"It is worth noting that the ImageNet pretraining method which is a strong baseline in the Potsdam dataset and Vaihingen dataset, performs dramatically worse, being exceeded by several SSL methods including SimCLR and MAE.",2,positive
"At present, another promising generative SSL paradigm is masked image modeling (MIM), such as MAE [9], which reconstructs the masked image patches.",1,neutral
"Despite this, the downsampled ScoreSeg still provides superior overall performance compared to those full-resolution segmentation models which are also pretrained using SSL methods such as the UperNet with MAE.",2,positive
"We also ﬁnd that in our experimental settings, even though other SSL methods such as MoCo v2 and MAE pretrained directly on the training and testing sets of the downstream datasets, they did not work well here, which we attributed to the fact that the insufﬁcient size of the Vaihingen dataset resulted in these pretrained backbones did not learn good representations.",0,negative
"(MIM), such as MAE [9], which reconstructs the masked image patches.",1,neutral
"For comparison of performance, using RSIs in target segmentation datasets (i.e., the same as ﬁne-tuning datasets), we pretrain a ResNet50 with several typical SOTA SSL methods in computer vision including MoCo v2 [8], SimCLR [11], Simsiam [12], and DenseCL [10] while a ViT-Base [46] is pretrained with MAE [9], whichisaninﬂuentialrepresentativeofmaskedimagemodeling.",2,positive
each masked patch can further improve the representation quality [44].,1,neutral
The ViT is powerful but hard to train to achieve high generalization and robustness because it lacks inductive bias and it depends heavily on massive data for largescale training [44].,2,positive
ing with masked autoencoders (MAEs) [44] for learning significant features representations in seismic volumes.,1,neutral
be discarded in downstream fine-tuning tasks [44].,1,neutral
"4, we visualize the MAE [27, 31] reconstruction results on a few Ego4D [29] examples with a ViT-B [24] trained for 200 epochs without per-patch normalization.",2,positive
", the recently introduced Ego4D dataset [29]) as well as the development of data-efficient training methods, such as masked autoencoders [27,31,60].",1,neutral
Our pipeline applies the original MAE [31] and video MAE [27] algorithms.,2,positive
"Recently, masked autoencoders [4,27,31,67,76] demonstrate training efficiency [31], model scalability [31], data efficiency [60], and effectiveness on videos [27, 60, 67].",1,neutral
"MAE [27, 31] reconstruction results on Ego4D [29] MQ val set.",1,neutral
"…several recent works on self-supervised learning have reported that the ViTbased model can benefit more from the various self-supervised learning schemes like learning semantic meaning with knowledge distillation (Caron et al., 2021) or masked patch prediction (Bao et al., 2021; He et al., 2022).",1,neutral
"Instead of predicting discrete tokens, He et al. (2022) proposed a rather simple strategy of learning directly from predicting pixels within the masked patches called masked autoencoder (MAE), by adopting computation efficient encoderdecoder design.",1,neutral
", 2021) or masked patch prediction (Bao et al., 2021; He et al., 2022).",2,positive
"For the downstream OAR segmentation task, the foundation ViT model and the UperNet (Xiao et al., 2018) decoder were used as the encoder and the decoder, respectively, following the implementation in Bao et al. (2021); He et al. (2022).",2,positive
"This random masking strategy is also consistent with pioneering self-supervised learning approaches that conjugate masked image modeling with the ViT models in a patch-wise manner (Bao et al., 2021; He et al., 2022; Xie et al., 2022).",1,neutral
DINO and MAE are pre-trained and fine-tuned (or linear probed) in ImageNet.,2,positive
"Self-supervised learning: ERM vs. DINO vs. MAE
Table 18 reports the bias keywords from ImageNet-R using DINO and MAE models (ERM is in the table above).",1,neutral
"We use the zero-shot classifier for CLIP, linear probing for DINO, and fine-tuned classifier for MAE.",2,positive
"We compare ViT-B trained by different methods: supervised model (ERM), multimodal learning - CLIP (Radford et al., 2021), and self-supervised learning - DINO (Caron et al., 2021) and MAE (He et al., 2022).",2,positive
", 2020), computer vision (Dosovitskiy et al., 2020; He et al., 2022) and related fields (Lu et al.",1,neutral
"…between vision and language using a pre-trained multimodal transformer (Geng et al., 2022), which is a large masked autoencoding transformer (Vaswani et al., 2017; He et al., 2022) jointly trained on image-text (Changpinyo et al., 2021; Thomee et al., 2016) and text-only data (Devlin et al., 2018).",2,positive
"It consists of two modules: a pre-trained multimodal masked autoencoder (He et al., 2022; Geng et al., 2022) to encode instructions and visual observations, and a transformer-based (Vaswani et al., 2017) policy that predicts actions.",2,positive
", 2022), which is a large masked autoencoding transformer (Vaswani et al., 2017; He et al., 2022) jointly trained on image-text (Changpinyo et al.",2,positive
"It consists of two modules: a pre-trained multimodal masked autoencoder (He et al., 2022; Geng et al., 2022) to encode instructions and visual observations, and a transformer-based (Vaswani et al.",2,positive
"Transformers (Vaswani et al., 2017) have led to significant gains in natural language processing (Devlin et al., 2018; Brown et al., 2020), computer vision (Dosovitskiy et al., 2020; He et al., 2022) and related fields (Lu et al., 2019; Radford et al., 2021; Geng et al., 2022).",1,neutral
"For the downstream OAR segmentation task, the foundation ViT model and the UperNet decoder were used as the encoder and the decoder, respectively, following the implementation in [12].",2,positive
"Instead of predicting discrete tokens, [12] proposed a rather simple strategy of learning directly from predicting pixels within the masked patches called masked autoencoder, with computation efficient encoder-decoder design.",1,neutral
"In addition, several recent works on self-supervised learning have reported that the ViT-based model can benefit more from the various self-supervised learning schemes like learning semantic meaning with knowledge distillation [10] or masked patch prediction [11], [12].",1,neutral
"This random masking strategy is also consistent with pioneering selfsupervised learning approaches that conjugate masked image modeling with the ViT models in a patch-wise manner [11], [12].",1,neutral
"24 Use MAE (He et al., 2022) to pretrain the plain ViTAE",2,positive
"(Yang et al., 2022)
Oriented RCNN ViTAE 81.24 Use MAE (He et al., 2022) to pretrain the plain ViTAE transformer.",2,positive
"[25] Z. Hou, X. Liu, Y. Cen, Y. Dong, H. Yang, C. Wang, and J. Tang, ‘‘GraphMAE: Self-supervised masked graph autoencoders,’’ 2022, arXiv:2205.10803.",1,neutral
"To further emphasize that c2i is a feature-specific embedding, we align c 2 i with the original node feature embedding v0i , shown as following:
LAL = 1 N N∑ i=1 (v0i − c 2 i ) 2 (9)
The overall disentangled loss LD is expressed as follow:
LD = LRD + LAL (10)
VOLUME 11, 2023 23983
C. UNIFORMITY AUTOENCODER Inspired by the MAE [26] and BERT [31], We designed an Uniformity Autoencoder model based on the masked mechanism, to learn the internal structure of equipment alignment.",2,positive
"Inspired by the MAE [26] and BERT [31], We designed an Uniformity Autoencoder model based on the masked mechanism, to learn the internal structure of equipment alignment.",2,positive
"However, the encoder for MAE is a vision transformer [56], which is not suitable for processing high-resolution SAR images due to its huge memory usage.",2,positive
"In particular, SSL has established a breakthrough with the birth of masked autoencoders (MAE) [55].",1,neutral
"In practice, previous studies have demonstrated that MAE performs better than CNNs for large enough datasets [55], [56].",1,neutral
"Besides, MAE will experience a serious overfitting problem if it is trained on small-scale datasets; the performance of MAE is 10% lower than that of general CNNs.",2,positive
"…of fast-evolving and well-studied computer vision techniques in the time series domain, such as better model architecture (Liu et al., 2022), data augmentation (Shorten & Khoshgoftaar, 2019), interpretability (Chefer et al., 2021), self-supervised learning (He et al., 2022), to name a few.",2,positive
", 2021), self-supervised learning (He et al., 2022), to name a few.",1,neutral
"Masked language modeling (MLM) (Devlin et al., 2018) and masked image modeling (MIM) (Xie et al., 2022; He et al., 2022) have been dominant self-supervised approaches in NLP and CV domains.",1,neutral
", 2018) and masked image modeling (MIM) (Xie et al., 2022; He et al., 2022) have been dominant self-supervised approaches in NLP and CV domains.",1,neutral
"…into two categories: discriminative (Noroozi & Favaro, 2016; Gidaris et al., 2018; Chen et al., 2020b; Grill et al., 2020; He et al., 2020; Chen et al., 2020d; 2021; Zbontar et al., 2021; Caron et al., 2021) and generative methods (Pathak et al., 2016; Larsson et al., 2016; 2017; He et al., 2021).",1,neutral
"We use a MAE (He et al., 2021) unsupervised pre-training model in ImageNet for 1600 epochs to ensure labels are not available during the whole pretraining process.",2,positive
"Furthermore, we find that CycleMAE gets higher performance gains on finetuning evaluation protocol, which is consistent with other unsupervised learning researches (He et al., 2021; Xie et al., 2022).",2,positive
"One of the most representative methods is DiMAE (Yang et al., 2022), which establishes an MAE-style (He et al., 2021) generative framework for UDG task.",2,positive
"MAE (He et al., 2021), a recent state-of-the-art method, recovers the input images based on a few patches of the images for pre-training the autoencoder, which captures semantic representations in this way.",2,positive
"However, without the supervision from other modalities, such pretrained models require task-specific finetuning (Bao et al., 2021; He et al., 2022; O Pinheiro et al., 2020; Wang et al., 2022a; Lin et al., 2022a) or linear probing He et al. (2020); Chen et al. (2020) for reasonably domain-adapted…",2,positive
"Generally, pretraining a vision encoder by generative objectives (Bao et al., 2021; He et al., 2022) or discriminative objectives (He et al.",1,neutral
"However, without the supervision from other modalities, such pretrained models require task-specific finetuning (Bao et al., 2021; He et al., 2022; O Pinheiro et al., 2020; Wang et al., 2022a; Lin et al., 2022a) or linear probing He et al.",2,positive
"Generally, pretraining a vision encoder by generative objectives (Bao et al., 2021; He et al., 2022) or discriminative objectives (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al., 2021) at the scale of one to ten million images (Deng et al., 2009) is sufficient to yield good…",2,positive
The experimental settings are the same as “endto-end fine-tuning” in the original MAE [15].,1,neutral
"To tackle input images with different resolutions, prior studies [15, 26, 29, 25] often randomly scale up or crop input images to a fixed resolution (e.",1,neutral
"TiT-ViT [55] aggregates structure information by recursively merging neighboring tokens into one token, and MAE [15] introduces self-supervised learning to the vision domain built upon ViT backbones.",1,neutral
"Second, thank to vision transformer (ViT) [12], the gap between natural language processing (NLP) and computer vision (CV) has been significantly mitigated [15].",1,neutral
"Meanwhile, deep learning (DL) has enjoyed impressive advances in various applications [46, 9, 22, 17, 7, 12, 34, 38, 31, 15, 49, 3, 27, 18], including those [42, 41, 1, 40, 33, 23, 59] for weather forecasting.",2,positive
"To tackle the aforementioned obstacles, we endeavor to develop a comprehensive model architecture able to flexibly admit the satellite and radar images for real-world storm prediction, resorting to the vision transformer (ViT) [12] and masked autoencoders (MAE) [15].",2,positive
"Following [2, 37, 15, 14, 24], we employ the layer-wise learning rate decay [6] of 0.",1,neutral
"Despite the current situation where Masked AutoEncoders (MAE) [33] are becoming the State-of-the-arts solution for action recognition tasks, like",1,neutral
"Despite the current situation where Masked AutoEncoders (MAE) [33] are becoming the State-of-the-arts solution for action recognition tasks, like
CHAPTER 2.",2,positive
"RELATED WORK 23
in this case, VideoMAE V2-g is able to achieve 99.6 3-fold accuracy in UCF101 dataset, 95.9 Top-5 accuracy in Something-Something V2 dataset, and 88.1 accuracy in HMDB-51 dataset [93], CNN still is a very fundamental approach that inspired the emergence of masked autoencoder.",2,positive
"In addition, based on the characteristics of the image itself, [23] adopts random mask operation to reduce the computation load without damaging the accuracy.",1,neutral
The next model is ViT-MAE [28] which uses a simple MLM-like architecture.,2,positive
"What makes this operation feasible and effective, besides the small number of words in the sentence, is that each word itself contains rich semantic information [42].",1,neutral
"The transformer architecture has been applied to many vision tasks, which can be found in [55], [66], [69], [96], [328]–[330].",1,neutral
"To be consistent with [66], BYOL and SimSiam belong to CL in this paper.",1,neutral
"7), such as bidirectional encoder representation from image transformers (BEiT) [96], masked AEs (MAEs) [66], context AE (CAE) [97], and a simple framework for MIM (SimMIM) [98], have recently become are very popular and are challenging the dominance of CL.",1,neutral
"Historically, the strongest self-supervised visual-encoder baselines (when compared to supervised methods) include methods such as: SimCLR [11], SimCLRV2 [12] and masked auto encoders [37], which all excel on ImageNet—a somewhat diverse but a relatively “clean” and object-centric dataset with 1000 disjoint classes.",1,neutral
"Self Supervised techniques that learn visual representations from the image data alone [37, 11]",1,neutral
"In Figure 2, we provide relation maps of three samples from ImageNet, using 10 checkpoints of MAE-Large (He et al., 2022).",2,positive
"Specifically, we measure the detection performance with MAE, BEIT, and ConvNeXt (He et al., 2022; Bao et al., 2022; Liu et al., 2022).",2,positive
"ImageNet We measure the label error detection performance on ImageNet with the synthetic label noise by train-
ing an MAE-Large model (He et al., 2022).",2,positive
[22] demonstrates the great promise of Vision Transformer by using ViT as an encoder to improve training speed and recognition performance.,2,positive
"A variety of pretext tasks have been proposed for self-supervised learning, including but not limited to autoencoder (AE)-based image reconstruction [30], [31], [32], generative adversarial network (GAN)-based image generation and image inpainting [33], [34], [35], deep clustering [36], [37], and content-based contrastive learning (CL) [38], [39].",1,neutral
Fu et al. (2022) and He et al. (2021a) (see also references therein) attempt to build a unified approach of PETL and suggest improved methods.,2,positive
"In recent years, it became increasingly popular to finetune large pretrained models (Devlin et al., 2018; Radford et al., 2021; He et al., 2021b).",2,positive
"(2022) study the performance of masked autoencoder (He et al., 2022), a pre-trained vision foundation model, on few-shot and zero-shot image AD using the reconstruction error as the anomaly score.",2,positive
"Schwartz et al. (2022) study the performance of masked autoencoder (He et al., 2022), a pre-trained vision foundation model, on few-shot and zero-shot image AD using the reconstruction error as the anomaly score.",2,positive
"To date, zero-shot AD has primarily been dealt with by using foundation models—large neural networks trained on massive unlabeled data at scale by self-supervised learning (Radford et al., 2021; He et al., 2022).",2,positive
"BERT [13], computer vision model MAE [15] and cross modal retrieval model CLIP [16], pre-trained language model (PLM) and fine-tuning have become one of the important research fields in all kinds of research fields.",1,neutral
"B. PRE-TRAINING With the advance of natural language processing model BERT [13], computer vision model MAE [15] and cross modal retrieval model CLIP [16], pre-trained language model (PLM) and fine-tuning have become one of the important research fields in all kinds of research fields.",1,neutral
"To better understand the BPBA setting, we give an analysis and comparison for BPBA, and self-supervised learning [6,9,11,12,32].",1,neutral
"detection, and instance segmentation [22], [23].",1,neutral
"First of all, we train a Masked Autoencoder (MAE) [5, 18] on our private large-scale face dataset in a self-supervised manner.",2,positive
"For the first challenge, we follow the successful practices in language [12, 29, 88] and vision [7, 35] modeling to construct the supervision signals, i.",2,positive
"into the field of computer vision [37], [38], [39], [40] and",1,neutral
Prior work [17] showed that MAE is both efficient at reducing redundancy in feature reprear X iv :2 30 3.,2,positive
"This paper presents DRAM, a test-time defense using masked autoencoder (MAE) [17], one of the strongest SSL tasks, to detect and repair adversarial attacks, including unforeseen ones.",2,positive
"The concatenation of unmasked patches’s embedding and masked tokens are processed by the decoder to predict the unmasked patches [17, 43].",1,neutral
"Kaiming He recently proposed a patch-level occlusion and reconstruction model called MAE [18], which is based on the ViT [13] autoencoder, dif-",2,positive
"However, since the introduction of the MAE[18] method, autoencoders based on pure transformers[33] have gained attention and have been applied to a variety of downstream tasks[9, 15, 35].",1,neutral
"MAE, a recently developed unsupervised learning model, has been widely recognized for its powerful data reconstruction ability and application prospect [76], [77].",2,positive
"Its extensive training and zero-shot learning allowing it to respond appropriately to any prompt at inference time [17, 18].",2,positive
"tion papers [15], [25], [48] have shown that state-of-the-art performance can be achieved by eliminating downsampling layers.",1,neutral
"Even with certain monotonicity, the tendency by CAE/VAE/MAE is not noticeable enough, while the volatility is rather significant.",1,neutral
"Although DAE is capable of unsupervised feature extraction, most existing works, such as contractive autoencoder (CAE) [29], VAE [30], and masked autoencoder (MAE) [31], concentrate on feature representation and scalable network structure.",2,positive
"autoencoder (CAE) [29], VAE [30], and masked autoencoder (MAE) [31], concentrate on feature representation and scalable network structure.",2,positive
"Recently, with the popularity of the new neural network architecture ViT [47], some works, such as FourCastNet [30], MaskViT [48], Rainformer [49], and Earthformer [50], try to apply various Transformer [51] variants to the field of precipitation nowcasting.",2,positive
"architecture ViT [47], some works, such as FourCastNet [30], MaskViT [48], Rainformer [49], and Earthformer [50], try",1,neutral
"MAE [18], MoCo v3 [11]) pre-trained weights are all utilized for a comprehensive comparison.",0,negative
"Since the pre-trained weights of ViT [15] are relatively easy to acquire, supervised learning-based, multimodal learning based (i.e. CLIP [38]) and self-supervised learning based (i.e. MAE [18], MoCo v3 [11]) pre-trained weights are all utilized for a comprehensive comparison.",1,neutral
"The vision transformer in the image encoder is pretrained with masked autoencoder modeling [17], which can process high-resolution images (i.",1,neutral
"Convolutional neural networks (CNNs) [23, 35, 60, 47], Vision Transformers [11, 46, 21, 9, 64], and all-MLP architectures [62, 41, 5] have brought massive empirical success in computer vision.",1,neutral
"Since Vision Transformers pretrained with and without SAM on ImageNet-1k [58] are available as public checkpoints, we evaluate the efficacy of token compression techniques on the larger ImageNet dataset.",2,positive
"Specification: only fine-tuning specified or adaptive limited number of layers, modules, weights and biases of the model (Lee et al., 2019; Zhao et al., 2020; Guo et al., 2021; He et al., 2022; Ben Zaken et al., 2022), this can be most memory-efficient method for the training phase because no additional parameter is involved.",2,positive
"…specified or adaptive limited number of layers, modules, weights and biases of the model (Lee et al., 2019; Zhao et al., 2020; Guo et al., 2021; He et al., 2022; Ben Zaken et al., 2022), this can be most memory-efficient method for the training phase because no additional parameter is involved.",2,positive
"Motivated by the success of the self-attention technique in tasks of natural language processing [47], [48] and computer vision [49], [50], recent studies [24], [51], [52] related to audio enhancement/denoising have benefited from the self-attention mechanism.",1,neutral
"Xiao et al. (2022) demonstrates that masked-autoencoder (He et al., 2022) trained on diverse video data (Goyal et al., 2017; Shan et al., 2020) can be an effective visual embedding for online RL.",2,positive
"(2022) demonstrates that masked-autoencoder (He et al., 2022) trained on diverse video data (Goyal et al.",2,positive
"Inspired by pre-training successes in computer vision (CV) (He et al., 2020; 2022) and natural language processing (NLP) (Devlin et al., 2018; Radford et al., 2019; 2021), pre-training visual representations on out-of-domain natural and human data (Deng et al., 2009; Grauman et al., 2022) has…",2,positive
"However, recent work on self-supervised learning has argued that fine-tuning performance should be the primary evaluation metric (He et al., 2022; Balestriero et al., 2023).",1,neutral
"There are numerous pretext tasks in the literature, including Autoencoding [4], [40], [41], patch context prediction [42], [43], solving jigsaw puzzles [44], [45], predicting rotations [46], adversarial training [47], [48], and so on.",1,neutral
"But how can we use these large self-supervised models in visual representation come at use for robotics tasks? In this paper, we study Masked Autoencoder (MAE) [12] for helping with predictions for top-down images.",1,neutral
Masked autoencoders (MAE) [34] are an effective and,1,neutral
"However, there is no one-size-fits-all augmentation strategy in various types of time-series (Yue et al. 2022) except for dropout (Srivastava et al. 2014), or random masking (Devlin et al. 2018; He et al. 2022).",1,neutral
[62] masked the random part of the input image and reconstructed the image by predicting the pixel value of mask blocks.,1,neutral
"…(Chen et al., 2022), in vision transformers trained with a self-supervised objective (e.g., distillation (Caron et al., 2021) or masked autoencoding (He et al., 2022)) begins to represent object-centric information (Wang et al., 2022), meaning the patches that have the highest affinity to a given…",1,neutral
", 2021) or masked autoencoding (He et al., 2022)) begins to represent object-centric information (Wang et al.",1,neutral
"target, recent studies [49] have shown that random masking can reduce network parameters and increase the robustness of target samples.",1,neutral
"…are able to encode generalized contextual representations for texts (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), images (Bao et al., 2021a; He et al., 2022), and image-text pairs (Chen et al., 2020; Li et al., 2020, 2021a), further facilitating the downstream tasks and research.",2,positive
", 2020), images (Bao et al., 2021a; He et al., 2022), and image-text pairs (Chen et al.",1,neutral
"Inspired by MAE (He et al., 2022), we observe a different information density between sentences and documents — for an event, most parts of the document are irrelevant, leading to a low information density.",1,neutral
"The pre-trained model, nicknamed PIXEL, is a Vision Transformer Masked Auto Encoder (ViT-MAE (He et al., 2022)) with 112M parameters.",2,positive
"◦ Smoothed ViT [29], ECViT [4], and ViP [19] leveraged the advancement in Vision Transformer (ViT) research [7], [13], [32], and significantly improved the performance of DeRS [17], in terms of performance, training, and efficiency.",2,positive
"This agreement-checking masking strategy for attack detection has been adapted and improved by PatchGuard++ [38] (more efficient feature space masking), ScaleCert [11] (more efficient superficial neuron analysis), MR-v2 [36] (better backbone and mask design), PatchVeto [15] and ViP [19] (using ViT [7], [13] as better backbones), and Yatsura et al.",2,positive
"This has been boosted by a very comprehensive body of research [4, 5, 6, 7, 8, 9, 10] that has proposed ways to pre-train deep learning models, such as ResNet [11] and Vision Transformers [12], employing different types of SSL methods, primarily contrastive [13, 14], clustering [15, 16], distillation [2, 17], information maximisation [18, 10], and masking [19, 20].",1,neutral
"SSL learns without labels by the construction of proxy tasks that are informed by the data itself such as predicting an occluded region of the image [27, 19].",1,neutral
"Contrastive learning and reconstruction are two common techniques to achieve these goals (He et al., 2020; Grill et al., 2020; He et al., 2022).",1,neutral
"Instead of discretizing the visual information, MAE (He et al., 2022) and SimMIM (Xie et al., 2022) propose to directly predict the pixel-level value as the reconstruction target.",2,positive
"Motivated by the great success of BERT, masked image modeling (MIM) (Xie et al., 2022; Bao et al., 2021; He et al., 2022) becomes a new trend in self-supervised visual pre-training, which randomly masks parts of images and reconstructs them based on the corrupted image.",2,positive
"For any network architecture with fixed training distributions, such as pretrained models (Iandola et al., 2014; He et al., 2016; 2021a), it is always possible to find a beneficial or detrimental test distribution with a small or large generalization error.",1,neutral
"In contrast, for robust representation learning, some work aims to separate domain-invariant variables from the specific ones of the learned features (Niu et al., 2015; Ilse et al., 2020; He et al., 2021b).",1,neutral
"1 (2) (Iandola et al., 2016), and MAE-ViTBase/Large/Huge (3) (He et al., 2021a).",1,neutral
"Currently, the most prevalent SSL method is the Masked Autoencoder (MAE) (He et al., 2022), which constructs supervision signals from raw image data by masking random input patches and then reconstructing the missing pixels.",1,neutral
"MAE (He et al., 2022) proposes an asymmetric encoder-decoder architecture for better training efficiency.",2,positive
"Recently, SimMIM (Xie et al. (2022)), MAE (He et al. (2022)) simplify the MIM designs and improves transformers.",2,positive
"This operation is the same as in other MIM methods (e.g., MAE (He et al. (2022)) ).",1,neutral
", 2022) is a simple extension of image-based Masked Autoencoders (MAE) (He et al., 2022) for SSL from audio spectrograms.",2,positive
"Due to the fact that audio spectrograms and images are continuous signals with significant redundancy, and thus SSL models still could reconstruct results given most tokens dropped, which is consistent with the masked autoencoders (He et al., 2022) in the visual domain.",2,positive
"Audio-MAE (Xu et al., 2022) is a simple extension of image-based Masked Autoencoders (MAE) (He et al., 2022) for SSL from audio spectrograms.",2,positive
"The success in NLP has also been replicated in vision tasks by masking patches of pixels (He et al., 2022) or masking tokens generated by a pretrained dVAE (Bao et al.",1,neutral
"The success in NLP has also been replicated in vision tasks by masking patches of pixels (He et al., 2022) or masking tokens generated by a pretrained dVAE (Bao et al., 2021; Xie et al., 2022).",2,positive
"So called masked auto-encoders, where additionally the input contains masked patches, where found to be similarly well-performing for transfer learning on downstream tasks ([46]).",1,neutral
"Vision Transformers (ViT) allowed computer vision models to leverage the same transformer-based architecture, which improved the success of this approach as seen in BERT Pre-Training of Image Transformers (BEiT) [8] and Masked Autoencoders (MAE) [30].",1,neutral
"has become prevalent for text [25], image [26], and speech and audio [27], [28], [29], [30], [31], [32], [33], and the derived rep-",1,neutral
"Recently, the computer vision community [3] has also begun to leverage a similar mask method.",1,neutral
"Note that the results are also aligned to the experimental results in previous approaches [3, 1].",1,neutral
"In addition, some researchers[10] have replaced the ViT[12] used by MAE[13] with Swin Transformer to adapt to small medical datasets.",1,neutral
"Since previous research[13] has shown that a 75% masking rate produces optimal representations in autoencoders, this paper will use a 75% masking rate by default.",1,neutral
that used a 60% masking [32] of the input image.,1,neutral
The encoder model architecture follows closely that of the MAE paper [1].,2,positive
"It is further MAE fine-tuned (He et al., 2021), using the same in-domain data as for the Mask R-CNN object detector.",2,positive
"To prevent catastrophic forgetting, VIMA only fine-tunes the last two layers of the language encoder with layer-wise learning rate decay (He et al., 2021) but freezes all other layers.",2,positive
We build our VideoTrack model on top of the vanilla ViT-Base [5] model pre-trained with MAE [8].,2,positive
"For example, CNN (LeCun et al., 1989; Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016), ViT (Dosovitskiy et al., 2021), and MAE (He et al., 2022) are designed for computer vision tasks, while RNN (Rumelhart et al., 1986), LSTM (Hochreiter & Schmidhuber, 1997), Seq2Seq (Sutskever et al., 2014), BERT (Devlin et al., 2019), and GPT-3 (Brown et al., 2020) are designed for natural language processing tasks.",1,neutral
", 2021), and MAE (He et al., 2022) are designed for computer vision tasks, while RNN (Rumelhart et al.",2,positive
"Machine learning, especially supervised learning, has achieved significant success in many fields, such as computer vision (Russakovsky et al., 2015; He et al., 2016; Carion et al., 2020; He et al., 2022), speech recognition (Sainath et al.",1,neutral
"It is worth noting that most common network architectures [37], [38], [39] in image restoration can be used as backbones to replace the ImgEst and ResEst blocks.",1,neutral
"Inspired by these advances, for vision transformer pre-training, the model receives incomplete images with a large portion of the patches removed and learns to reconstruct the missing contents on low-level image pixels (He et al., 2022; Li et al., 2021c; Dosovitskiy et al., 2021; Chen et al., 2020), high-level semantics (Bao et al.",2,positive
"Inspired by this, MLM-style pre-training task has been extended to many other domains (Hu et al., 2020; He et al., 2022).",2,positive
"As pointed out in [45], the reconstruction of the original image information can extract and retain the low-dimensional and high-dimensional information of the image.",1,neutral
"Inspired by recent progress in high-fidelity efficient restoration through an efficient MLP [42] or a small transformer [22], we adopt an asymmetric encoder-decoder framework, which enables real-time reconstruction.",2,positive
"Pre-training plays a crucial role in visual discriminative tasks [42, 32, 22, 8] and has been widely studied.",1,neutral
"It is feasible to obtain a task-specific model by relying on a pre-trained model that is optimized with a surrogate objective, such as contrastive learning [20, 12, 23] or masked image modeling [22], which facilitates exceptional generalization capabilities.",1,neutral
"Similar strategies have been successful in methods such as [22], [23].",1,neutral
"Specifically, we use the ViT-B/16 weights from the Masked AutoEncoding pre-training [10].",2,positive
Simply generating a random mask such as in MAE [42] fails to isolate information within the intended partitioning due to multi-hop message passing caused by repeated application of the same mask.,1,neutral
We propose to use a generalized form of masking based self-supervision [42] based on predicting shuffled future features.,2,positive
"Masking based self-supervision has proven to be a new frontier for both image [7, 42, 23, 83, 45] and video [40, 105] representation learning.",1,neutral
"learning algorithms [31], [32], this article aims to bridge",1,neutral
[35] proposed MAE that masked the image in a pretty high ratio and forced the model to reconstruct the masked patches in,1,neutral
The MAE represents a different idea to learn the image representation by masking the origin image and forces the model to reconstruct the image [35].,1,neutral
VideoMAE [2] is a simple extension of Masked Autoencoders [3] for video representation learning.,1,neutral
"For example, masked reconstruction loss [10] has proved successful in image pretraining and can be smoothly applied mel-spectrograms.",1,neutral
"Inspired by the recent success of selfsupervised representation learning in the vision and language communities [78, 30, 8], an alternative approach involves pretraining a neural network on robotics data for improved downstream adaptability.",1,neutral
"For example, while β-VAE learned a sufficient representation for the domains we considered, a version of LS(3) learned on top of masked autoencoders [17] would likely be applicable to a wider variety of situations.",1,neutral
"Recently, the masked autoencoder (MAE) [23] has made a breakthrough in auto-encoding pretraining of visual transformers by masking random patches of the input image and reconstructing them from potential representations and masked tokens, which has attracted a great deal of attention from the computer vision community.",1,neutral
"This can partially be motivated by the reduced time needed for pre-training, but we also find the encoder to achieve higher downstream task performance when trained in conjunction with a smaller decoder, similar to the results in [18].",2,positive
"Specifically, masked language modeling [11] and masked image modeling [2, 18, 40] have proven themselves as simple, yet effective, pre-training strategies.",1,neutral
"Inspired by their success, multiple methods have applied similar techniques to the image domain [2, 7, 12, 18, 36].",1,neutral
Figure 1: MAE [18] (left) divides images into nonoverlapping patches of fixed size.,1,neutral
"cess in fields such as Natural Language Processing (NLP) [3, 11, 28] and computer vision [5, 8, 18].",1,neutral
"For the pre-training, we follow the training paradigm of MAE [18] and equip the model with a lightweight decoder used for reconstructing masked input.",2,positive
This work aims to extend the MAE-style pre-training [18] to voxelized point clouds.,2,positive
"Recently, the authors of [18] proposed MAE, a simple approach where random image patches are masked and their pixel values are used as reconstruction targets.",1,neutral
"In computer vision, self-supervised learning approaches such as contrastive learning (Chen et al., 2020c; He et al., 2020), bootstrapping (Grill et al., 2020) and masking (He et al., 2022) are shown to obtain competitive performance on widely-used benchmarks like ImageNet.",1,neutral
", 2020) and masking (He et al., 2022) are shown to obtain competitive performance on widely-used benchmarks like ImageNet.",2,positive
"The correlation between the two sets of score rankings is 0.25, which questions the conventional approach’s rationale for evaluating the quality of learned embedding with linear models (He et al., 2022).",1,neutral
", 2019), computer vision (Alain & Bengio, 2017; Caron et al., 2021b; Chen et al., 2020b; 2021; He et al., 2022; Li et al., 2021; Resnick et al., 2019; Wang et al., 2021), and biomedical science (Dohan et al.",2,positive
"…Liu et al., 2019; Tenney et al., 2019; Wang et al., 2019), computer vision (Alain & Bengio, 2017; Caron et al., 2021b; Chen et al., 2020b; 2021; He et al., 2022; Li et al., 2021; Resnick et al., 2019; Wang et al., 2021), and biomedical science (Dohan et al., 2021; Elnaggar et al., 2021; Rao…",2,positive
"25, which questions the conventional approach’s rationale for evaluating the quality of learned embedding with linear models (He et al., 2022).",1,neutral
"In this paper, we demonstrate that a vanilla ViT-based backbone pretrained by MAE method (He et al. 2022) without extra data is comparable to the SOTA work.",2,positive
The masked autoencoder (He et al. 2022) employs an asymmetric encoder-decoder architecture to implement the self-supervised task by randomly masking the patches of input images and reconstructing missing pixels.,2,positive
The pretraining length is 300 epochs and the other settings of pretraining and fine-tuning are same as MAE (He et al. 2022).,0,negative
"In this paper, we demonstrate that a vanilla ViT-base backbone pretrained by MAE method (He et al. 2022) without extra data is comparable to the SOTA work.",2,positive
"We initialize the original ViT as a Masked Autoencoder (MAE) (He et al. 2022) on AffectNet (Mollahosseini, Hasani, and Mahoor 2017) dataset such that it can extract a stronger representation of expressive semantic information.",2,positive
"182 Following the MAE SSL algorithm [9], we mask 75% of 183 patches and then encode the unmasked patches with the SatViT 184 encoder.",2,positive
"For each314 downstream dataset, we sweep mask ratios {50%, 75%, 90%}315 and fine-tune our models starting from seven checkpoints to316 investigate the role of TAPT steps on downstream accuracy.317 Performing TAPT with a mask ratio of 75% for 50k–100k318 steps results in our most accurate models after fine-tuning.319
Self-pretraining [37] found that MAE pretraining on a small 320 target dataset for 10k epochs was optimal.",2,positive
Our work differs by: 1) pretraining on 143 a dataset with 7× more images; 2) pretraining with a higher 144 performing SSL algorithm in MAE; and 3) using a higher 145 capacity model (ViT-Base versus Swin-Tiny).,2,positive
"In this research, we select ViT as our model128 architecture and masked autoencoding (MAE) [9] as our129 SSL algorithm.130 With four recent exceptions, all transformer-based CV mod-131 els used in RS have either initialized their parameters ran-132 domly [12], [13], [14], [15], [16] or from models pretrained on133 ImageNet [17], [18], [19], [20], [21], [22].",2,positive
ViT-RGB was pretrained for 1600 epochs on IN1K284 (1.3 million images) using MAE.,2,positive
"273 To compare SatViT with an SOTA RGB model, we fine-tune 274 a ViT-Base model pretrained using MAE on ImageNet-1k 275 (IN1K).",2,positive
", pixels [9], features [10], and tokens [11]) are SOTA 126 on the ImageNet benchmark, even outperforming supervised 127 pretraining.",2,positive
ViTs are optimally pretrained using reconstructive SSL [9].,1,neutral
"For309 us, this amounts to continued MAE pretraining on the target310 datasets.",0,negative
"165
B. SatViT’s Architecture and MAE Pretraining 166
SatViT’s encoder has a depth of 12 (transformer lay- 167 ers) and a width of 768 (features per patch), corresponding 168 to 86 M learnable parameters.",0,negative
"182
Following the MAE SSL algorithm [9], we mask 75% of 183 patches and then encode the unmasked patches with the SatViT 184 encoder.",2,positive
", a large encoder and small decoder) reduces pretraining 201 FLOPs by approximately 3× [9].",1,neutral
"In this research, we select ViT as our model 128 architecture and masked autoencoding (MAE) [9] as our 129 SSL algorithm.",2,positive
"Meanwhile, with the vast development of MIM-based methods (Li et al., 2021a; He et al., 2021), people find that fine-tuned MIM models produce surprising results, despite poor linear performance.",1,neutral
"For the masking strategy, we follow the random mask sampling of a 75% ratio as in MAE (He et al., 2021).",1,neutral
"On the right, we provide two simplified computational graphs of MAE (He et al., 2021) and our method to illustrate the difference.",1,neutral
"In parallel with contrastive-based methods such as MoCov3 (Chen et al., 2021) and DINO (Caron et al., 2021), another line of research in masked image modeling (MIM) rencently caught attention (Bao et al., 2021; He et al., 2021).",2,positive
"During pre-training, we follow MAE (He et al., 2021) to use the Xavier initialization (Glorot & Bengio, 2010) and choose not to adopt color jittering and drop path.",0,negative
"Although current results are not competitive with transformer-based heads, it out-performs the baseline without mask-then-draw such as MAE (He et al., 2021) process by 0.5%.",2,positive
"Our pre-training setup generally follows the configurations in MAE (He et al., 2021) with AdamW (Loshchilov & Hutter, 2017) and cosine learning rate decay applied.",2,positive
"MAE (He et al., 2021) and SimMIM (Xie et al., 2022) further demonstrate the viability of reconstructing the missing pixels.",2,positive
"For instance, while MAE (He et al., 2021) reports state-of-the-art image classification accuracy after the backbone being fine-tuned, exploiting frozen features from its pre-trained model performs significantly worse than previous contrastive counterparts such as DINO (Caron et al., 2021).",2,positive
"However, MIM is currently treated deterministically, as either a regression (He et al., 2021; Xie et al., 2022) or classification (Bao et al., 2021; Dong et al., 2021) question given target inputs.",2,positive
"Following the sampling strategy (He et al., 2021), we randomly sample a subset without replacement and mask the remaining tokens.",1,neutral
"This incorporated module only appears during pre-training and incurs little computation overhead, as it is much smaller than the backbone or the asymmetric decoder in MAE (He et al., 2021).",2,positive
"Among them, masked image modeling (MIM)-based methods (He et al., 2021; Bao et al., 2021; Xie et al., 2022; Chen et al., 2022; El-Nouby et al., 2021; Dong et al., 2021) exhibit potentials on learning transferable features for downstream tasks.",1,neutral
"…(Touvron et al., 2021) - 45.6 BEiT w Inter FT† - 47.7 without labeled data: BEiT (Bao et al., 2021) 800 43.2 PeCo (Dong et al., 2021) 300 46.7 MAE (He et al., 2021) 1600 48.1 CAE (Chen et al., 2022) 800 48.8 CrossMAE (Ours) 300 50.4
Table 3: Results of semantic segmentation on ADE20K dataset,…",0,negative
"The decoder strategy in recent MIM frameworks (Xie et al., 2022; He et al., 2021) can also be regarded as an example of using predictors.",1,neutral
"Compared with mask-then-predict baselines like MAE (He et al., 2021), the results of fine-tuning and linear probing on ImageNet-1k are improved by 1% and 8.5%, with 300 pre-training epochs.",2,positive
"Moreover, masked inputs incur a discrepancy between pre-training and fine-tuning, as the fine-tuning tasks rarely see mask tokens (Bao et al., 2021) or incomplete image patches (He et al., 2021).",1,neutral
"To encode multi-scale information into the detection pipeline, we integrate FPN (Lin et al., 2017) into the backbone following the setup in (Zhou et al., 2021; He et al., 2021).",2,positive
"To further allow a consistent comparison, we report the wall-clock pre-training time on the same platforms to better illustrate this advantage over the baseline MAE (He et al., 2021).",2,positive
", 2021) or incomplete image patches (He et al., 2021).",0,negative
"MAE (He et al., 2021) and SimMIM (Xie et al., 2022) further investigated encoder-decoder design and showed the potentials of pixel reconstruction in representation learning.",1,neutral
"For linear probing, the protocols in MoCov3 (Chen et al., 2021) and MAE (He et al., 2021) are adopted.",2,positive
"Note that our proposed CrossMAE is general and could be compatible with various MIM prediction targets, such as HOG (Wei et al., 2021), discrete tokens (Bao et al., 2021) and
even the pixel reconstruction as in (Xie et al., 2022; He et al., 2021).",2,positive
"The backbone transformer fnet extracts useful semantics from given xdraw and a follow-up predictor fpred maps extracted features to the target modality, such as pixels (He et al., 2021; Xie et al., 2022) and HOG (Wei et al.",2,positive
"The backbone transformer fnet extracts useful semantics from given xdraw and a follow-up predictor fpred maps extracted features to the target modality, such as pixels (He et al., 2021; Xie et al., 2022) and HOG (Wei et al., 2021).",2,positive
"Compared with mask-then-predict baselines like MAE (He et al., 2021), the results of fine-tuning and linear probing on ImageNet-1k are improved by 1% and 8.",2,positive
"Existing MIM methods majorly map the observed signal xM to a latent representation using the backbone, and then reconstruct the tokenized (Bao et al., 2021) or original signal (He et al., 2021).",1,neutral
"For instance, while MAE (He et al., 2021) reports state-of-the-art image classification accuracy after the backbone being fine-tuned, exploiting frozen features from its pre-trained model performs significantly worse than previous contrastive counterparts such as DINO (Caron et al.",1,neutral
"The end-to-end fine-tuning and linear probing protocols are also kept consistent with those in (He et al., 2021; Bao et al., 2021).",1,neutral
"However, MIM is currently treated deterministically, as either a regression (He et al., 2021; Xie et al., 2022) or classification (Bao et al.",2,positive
"ImageNet Experiments For the experiments on ImageNet, we follow the most standard settings and hyper-parameters in MAE (He et al., 2021) when using ViT (Dosovitskiy et al., 2020) as the backbone.",2,positive
"The accuracy of the mixed multi-modality with DeseNet (RGB-D) and MAE (RGB-D) was 93.40%, which outperformed all single models, multiCNNs, and multi-ViTs.",2,positive
"To further verify the superiority of the mixed multi-modality approach with ViT(MAE) and DenseNet for FGVC, we performed a new round of experiments based on the fine-grained shoe object dataset.",2,positive
"In particular, the accuracy of the mixed multi-modality with ViT(MAE) and DenseNet reached 93.13%.",2,positive
"We also ran experiments by using two-CNNs (MnasNet and DenseNet), and two-ViTs (MAE and MAE-L), mixed CNNViT approach (ViTMAE and DenseNet).",2,positive
"Following the above procedure, we first constructed car object embeddings based on ViT(MAE), DenseNet, and mixed multi-model, and used k-NN classifier.",2,positive
"Subsequently, Swin [33], DeiT [34], and MAE [35] are introduced respectively for computer vision tasks.",1,neutral
"5(toprow)shows that, in general, our approach with both CNN and ViT obtained better classification accuracy than ViT(MAE)only and DenseNet-only models.",2,positive
"To better analyze the effect of multi-modality representations on fine-grained instance accuracy, we first conduct a t-SNE analysis with mix-network representations based on
DenseNet [44] and ViTMAE [45], followed by recognition results on the fine-grained car and shoe data.",2,positive
"2 (top-row), we extract the RGB, depth views of the object, and then fed the RGB view into ViTMAE and the depth view into DenseNet, respectively.",2,positive
"For a fair comparison, we compare our DMAE ViT-B model with the MAE ViT-B checkpoint released by He et al. (2022) in the linear probing setting on ImageNet.",2,positive
"The other class is the generative learning approach, which randomly masks patches in an image and learns to generate the original one (Bao et al., 2021; He et al., 2022).",1,neutral
"Following He et al. (2022); Xie et al. (2022), we use ImageNet-1k as the pre-training corpus which contains 1.28 million images.",2,positive
"Like MAE (He et al., 2022), we adopt the asymmetric encoder-decoder design for DMAE.",2,positive
"To enforce the encoder (but not the decoder) to learn robust semantic features, we control the capacity of the decoder by setting a smaller value of the hidden dimension and depth following He et al. (2022).",2,positive
"For the pre-training of the two DMAE models, we set the masking ratio to 0.75 following He et al. (2022).",2,positive
"…auto-encoder (Vincent et al., 2008; 2010) or denoising diffusion model (Ho et al., 2020; Nichol & Dhariwal, 2021) to pre-train θdenoiser, and leverage contrastive learning (Chen et al., 2020; He et al., 2020) or masked image modelling (He et al., 2022; Xie et al., 2022) to pre-train θencoder.",2,positive
", 2020) or masked image modelling (He et al., 2022; Xie et al., 2022) to pre-train θencoder.",2,positive
"Motivated by the Masked AutoEncoder (MAE) (He et al., 2022), which learns latent representations by reconstructing missing pixels from masked images, we design a new self-supervised task called Denoising Masked AutoEncoder (DMAE).",2,positive
Traditional MIM methods often use a random masking strategy for ordinary images [9].,1,neutral
"For RGMIM and MAE, we used the same settings in all experiments, except for the masking strategy.",2,positive
We also conducted hyperparameter studies on the masking ratio for RGMIM and MAE.,2,positive
"And we can verify that RGMIM is more robust than MAE, especially when the masking ratio is relatively low.",2,positive
"Furthermore,
Table 2 shows the COVID-19 detection accuracy of RGMIM and MAE when using different masking ratios.",2,positive
"When using a lightweight ViT decoder with fewer layers, the representations learned by the MIM are also well, and it can significantly reduce the network parameters and pretraining time [9].",1,neutral
"Method Architecture 1% 5% 10% 50% 100% RGMIM ViT-Base 0.771 0.893 0.919 0.957 0.962 MAE [9] ViT-Base 0.754 0.875 0.903 0.948 0.956 SKD [24] ResNet-50 0.742 0.812 0.896 0.947 0.957 Cross [25] ResNet-50 0.747 0.795 0.817 0.934 0.953 BYOL [26] ResNet-50 0.683 0.754 0.790 0.933 0.954 SimSiam [27] ResNet-50 0.623 0.700 0.781 0.929 0.949 Transfer ViT-Base 0.689 0.861 0.893 0.940 0.953 Transfer ResNet-50 0.539 0.619 0.665 0.913 0.936 From Scratch ViT-Base 0.413 0.580 0.645 0.810 0.848 From Scratch ResNet-50 0.284 0.496 0.532 0.619 0.774
Fig.",0,negative
"We used five SOTA self-supervised learning methods as comparative methods, including masked autoencoder (MAE) [9], selfknowledge distillation based self-supervised learning (SKD) [24], cross-view self-supervised learning (Cross) [25], bootstrap your own latent (BYOL) [26], and simple siamese self-supervised learning (SimSiam) [27].",2,positive
"4, after only ten epochs of learning, the detection accuracy of RGMIM has already started to converge, which shows RGMIM has higher accuracy and convergence speed than MAE.",2,positive
We compared our complementary masking strategy with the random masking strategy proposed in MAE [9] in Table 2d.,2,positive
"To address this challenge, we further design a spatiotemporal autoencoder (STAR) inspired by the recent masked autoencoders (MAE) [9].",2,positive
"Different from MAE [9], the STAR encoder uses a vision transformer (ViT) [34] backbone, which operates on all patches yet only sends out a subset (spatial sub-sampling).",2,positive
We compared our complementary masking strategy with the random masking strategy proposed in MAE [9] in Table 2d. Results show that switching from complementary to random masking leads to a degradation in the completion performance.,2,positive
"SSRL is generally composed of: (1) task-agnostic pre-training via carefully-designed self-supervised pretext tasks such as contrastive learning [24, 25] or autoencoding [26, 27, 9], and (2) task-specific adaptation to finetune the pre-trained model on the downstream tasks such as object detection or image classification.",1,neutral
"Different from MAE which uses mask tokens to replace the missed patch embeddings, robot i as a receiver aggregates the historic tokens Fj,t−1 and the current tokens Fj,t from robot j (temporal mixing), which approximately form a complete observation towards the entire spatial range.",2,positive
"Recent works extend MAE into multimodal representation learning [28, 29], video [30, 31], and 2D image completion [32].",1,neutral
"Unlike MAE mainly for object-level recognition, we aim at large-scale dynamic scene modeling.",2,positive
"Inspired by the idea of ”masking” in MAE [9], we employ a similar asymmetric design as MAE yet with different purposes: MAE is to design a nontrivial self-supervisory task for pre-training via randomly masking, while the goal of STAR is to reduce the communication volume in multi-robot systems via partial broadcasting.",2,positive
Masked autoencoder (MAE) achieves great performance with a simple reconstruction objective [9].,2,positive
"Second, unlike the high mask ratio [11] adopted in natural images, the two segmentation",1,neutral
"In practice [11], normalized pixel/voxel values within each patch are better reconstruction targets than raw pixel/voxel values.",1,neutral
"As the reconstruction loss is only applied to the masked patches, the restored visible patches look blurrier, which is also observed in the natural images [11].",1,neutral
"Recent advancements in self-supervised representation learning show masked image modeling (MIM) [3,24,8,11] as an effective pre-training strategy for the Vision Transformer (ViT) [7], which is powerful yet hard to train because of lack of inductive bias.",1,neutral
"Among the different MIM frameworks, Masked Autoencoder (MAE) [11] is one of the most simple and effective.",2,positive
"Similar to MAE [7], we find that the occlusion ratio of 75% performs the best on both the linear accuracy and supervised fine-tuning accuracy.",2,positive
"Similar to BERT [38] and MAE [7], we don’t pay much attention to the reconstruction ability of visible parts, and only compute loss between the points of predicted patches Po and the ground truth point cloud of these occluded patches denoted as Pt.
LCD(Po,Pt) = 1 |Po| ∑
po∈Po min
pt∈Pt ‖po − pt‖2
+ 1 |Pt| ∑
pt∈Pt min
po∈Po ‖pt − po‖2.",1,neutral
"Partly inspired by MAE, we design a new self-supervised learning framework to recover the complete shapes from the highly occluded shapes.",2,positive
[7] propose a new form of auto-encoders named MAE by masking regular patches of images and learning to recover the masked parts.,1,neutral
"Similar to BERT [38] and MAE [7], we don’t pay much attention to the reconstruction ability of visible parts, and only compute loss between the points of predicted patches P and the ground truth point cloud of these occluded patches denoted as P.",2,positive
The recent improvements of mask-based 2D auto-encoders [7] have proved that masked auto-encoders are effective in image representation learning through the inference of the overall image information based on the visible local patches.,1,neutral
"Recently, in 2D vision, He et al. [7] propose a new form of auto-encoders named MAE by masking regular patches of images and learning to recover the masked parts.",1,neutral
"Compared with MAE [15], our model learns competitive multi-modal representations from vision-language pretraining while retains high-quality image representations.",2,positive
"In contrast, we follow MAE [15] to randomly mask image patches with a probability of 0.",1,neutral
"To further show the generalization ability of our pre-trained model, we examine our model on ImageNet-1K classification task following common practice [11, 15].",2,positive
We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders [15] that does not require this supervision.,2,positive
An unsupervised visual semantics is learned via Masked Auto-Encoders [15] before language is integrated.,1,neutral
"To accelerate training, we follow MAE [15] and skip the mask token [MASK] in the encoder and only apply it in the lightweight decoder.",2,positive
We use a 12-layer ViT [11] initialized from MAE [15] (ImageNet-1K without labels) as our backbone; (3) Task-specific decoder.,2,positive
SimMIM [55] and MAE [15] predict RGB values of raw pixels by direct regression.,1,neutral
"For supervised learning models (i.e., ViT), we train base scale models for 300 epochs and large scale models for 200 epochs following He, Chen, Xie, Li, Dollár, and Girshick [13].",2,positive
"For training ViT from scratch, we report the original results [9] and the results with strong data augmentation [13].",0,negative
"The first one is from original ViT paper [9], and the second one is from He, Chen, Xie, Li, Dollár, and Girshick [13]’s re-implementation with strong data augmentation.",2,positive
We use supervised vision transformer (ViT) [9] and vision masked autoencoder (MAE) [13] as two representative platforms to show our insights in both theoretical and experimental analysis.,2,positive
"With masked autoencoder, recent studies [1, 13] successfully train large scale transformers, even without using additional training data compared with supervised learning.",1,neutral
"For large and huge models, we fine-tune them for 50 epochs following existing work [1, 13].",2,positive
"We use the per-patch normalization following He, Chen, Xie, Li, Dollár, and Girshick [13] for better representations.",1,neutral
", ViT), we train base scale models for 300 epochs and large scale models for 200 epochs following He, Chen, Xie, Li, Dollár, and Girshick [13].",2,positive
", DINO [2], MoCo v3 [4], BEiT [1] and MAE [13].",1,neutral
"In MAE [9], the Transformer network is selected to perform self-attention on the unmasked portions of the training data, which are not influenced by the masked portions.",1,neutral
"In particular, the simple masked autoencoders (MAE) [9] have been proved effective in learning representative features with the masked data reconstruction task.",1,neutral
"Inspired by this, we adopt the 3D Spatially Sparse Convolution proposed in SECOND [25] to build the encoder network to aggregate information from only the unmasked voxels that contain point clouds with the positional encoding module, thus our voxel masking strategy can reduce the memory complexity for training, similar to Transformer network in MAE [9].",2,positive
"Unlike MAE [9], the reconstruction target for large-scale point clouds is set to whether the voxel contains the point clouds, and the Encoder is built with the 3D Spatially Sparse Convolutions to handle the large-scale unmasked voxels.",2,positive
"Masked Autoencoders for image [9] (a), video [21] (b), (c) synthetic point clouds [13] (d) and large-scale point clouds of our Voxel-MAE.",2,positive
"Recently, MAE [9] first masks random patches of the input image and reconstructs the missing pixels with the simple autoencoder framework, showing promising results in self-supervised learning.",2,positive
The Transformer network used in MAE [9] can not aggregate information from all the unmasked input data.,1,neutral
"Inspired by the great self-supervised learning performance of MAE [9] in 2D images, we design the pretraining network of masked autoencoders for LiDAR-based 3D object detectors to learn representative features.",2,positive
"Self-supervised learning has been proven to be effective for learning representative features in 2D images [9], videos [5, 21] and synthetic point clouds [13], as shown in Figure 1.",1,neutral
It is worth noting that our Voxel-MAE applies the 3D Spatially Sparse Convolutions to aggregate information from the unmasked data as the Transformers used in MAE [9] can not handle the largescale unmasked voxels.,2,positive
"Recent Vision Transformers (ViTs) have achieved impressive performance on various image understanding tasks [16, 7, 50, 37, 11, 25].",1,neutral
"Specifically, MAE and DINO are based on the vanilla plain-ViT, while Twins is based on the hierarchical-ViT.",1,neutral
"ViTs achieve state-of-the-art performance in many vision tasks, which include image classification [16, 37, 11, 25], detection and segmentation [11, 66, 34], optical flow [27], point cloud processing [23, 65] and so on.",1,neutral
"In particular, 1) we utilize the recent ViTs pre-trained with self-supervised (MAE [25], DINO [7]) and supervised (Twins [11]) tasks, while it is an interesting future work of exploring the influence of different pre-training tasks on MVS.",2,positive
"For different pre-training tasks, ViTs can be categorized into self-supervised ones [2, 25, 7] and supervised ones [16, 50, 37, 11].",1,neutral
"The involved methods – MAE, DINO, and Twins are relatively representative.",0,negative
"Specifically, by considering two typical ViT backbones – plain-ViT [16, 7, 25] and hierarchical-ViT [50, 37, 11], we implement MVSFormer-P and MVSFormer-H as in Fig.",1,neutral
"Inspired by these achievements, transformers are also introduced into the computer vision [16, 7, 50, 37, 11, 25], which may include CNNs are all simplified as ViTs here.",1,neutral
"For the comparisons among pre-trained ViTs with the multi-scale training strategy, both DINO-small and MAE-base are fixed during the training.",0,negative
"3, which include ResNet50 [26], DINO [7], MAE [25], and Twins [11].",1,neutral
"Supervised ViTs are usually pre-trained for the classification task, while self-supervised ones are implemented with masked prediction [25, 2] or contrastive learning [7, 9].",1,neutral
"The learning rates of MAE and DINO are 1e-5; the learning rate of Twins is 3e-5, and the ones of all CNNs are 1e-3.",0,negative
"Furthermore, to reduce notorious computations and memory costs of the vanilla attention, many ViTs use multi-scale architectures [37, 50, 11] instead of single-scale backbones [16, 25].",1,neutral
"Besides, Twins leverages Conditional Position Encoding (CPE) [12] instead of absolute positional encodings in other ViTs [16, 25, 37].",1,neutral
"To explore the learning ability of ViTs, some quantitative results about ViTs (DINO-small [7], MAEbase [25], Twins-small [11]) trained without FPNs for MVS are shown in Tab.",1,neutral
"Recent Vision Transformers (ViTs) have achieved impressive performance on various image understanding tasks [16, 7, 50, 37, 11, 25].",1,neutral
"Specifically, MAE and DINO are based on the vanilla plain-ViT, while Twins is based on the hierarchical-ViT.",1,neutral
"In particular, 1) we utilize the recent ViTs pre-trained with self-supervised (MAE [25], DINO [7]) and supervised (Twins [11]) tasks, while it is an interesting future work of exploring the influence of different pre-training tasks on MVS.",2,positive
"ViTs achieve state-of-the-art performance in many vision tasks, which include image classification [16, 37, 11, 25], detection and segmentation [11, 66, 34], and so on.",1,neutral
"For different pre-training tasks, ViTs can be categorized into self-supervised ones [2, 25, 7] and supervised ones [16, 50, 37, 11].",1,neutral
"The involved methods – MAE, DINO, and Twins are relatively representative.",0,negative
"Inspired by these achievements, transformers are also introduced into the computer vision [16, 7, 50, 37, 11, 25].",1,neutral
"Twins also leverages Conditional Position Encoding (CPE) [12] with 2D depthwise convolutions instead of the absolute positional encoding in other ViTs [16, 25, 37].",1,neutral
"3, which include ResNet50 [26], DINO [7], MAE [25], and Twins [11].",1,neutral
"Supervised ViTs are usually pre-trained for the classification task, while self-supervised ones are implemented with masked prediction [25, 2] or contrastive learning [7, 9].",1,neutral
"Specifically, by considering two typical ViT backbones – plainViT [16, 7, 25] and hierarchical-ViT [50, 37, 11], we implement MVSFormer-P and MVSFormer-H as in Fig.",2,positive
"The learning rates of MAE and DINO are 1e-5; the learning rate of Twins is 3e-5, and the ones of all CNNs are 1e-3.",0,negative
"Furthermore, to reduce notorious computations and memory costs of the vanilla attention, many ViTs use multi-scale architectures [37, 50, 11] instead of single-scale backbones [16, 25].",1,neutral
"For the comparisons among pre-trained ViTs with the multi-scale training strategy, both DINO-small and MAE-base are freezed during the training.",0,negative
"Particularly, many works tried to finetune pre-trained ViTs with specific models, such as FPNs or CNNs, for various downstream tasks [11, 25, 34, 27, 8].",1,neutral
"To explore the learning ability of ViTs, some quantitative results about ViTs (DINO-small [7], MAEbase [25], Twins-small [11]) trained without FPNs for MVS are shown in Tab.",1,neutral
The effective learning rate is obtained following MAE: lr= base_lr×globalbatchsize / 256.,1,neutral
The fine-tuning codes and checkpoints refer to the MAE repository3.,2,positive
"Based on the MAE pretrained models, we finetune 50 epochs on the SnakeCLEF 2022 dataset, and the default setting is depicted in Table 1.",2,positive
"In this paper, we use the Masked autoencoder (MAE) [27] pretrained ViT [28] models conducted on ImageNet-1K [29] training set for 800 epochs.",2,positive
We borrow the ViT-large MAE model pretrained in ImageNet1k dataset.,2,positive
"To be more specific, a ViT-based masked autoencoder (MAE) pretrained in a self-supervised manner in ImageNet is finetuned and tested in the PlantCLEF2022 dataset.",2,positive
Masked autoencoder (MAE) [22] is chose to achieve the challenge as a self-supervised vision transformer because of its high performance and stable training process.,2,positive
Figure 5: The high-level architecture of MAE [22].,1,neutral
"Another one is our model in this paper, finetuning the MAE model in PlantCLEF2022 dataset.",2,positive
"To pretrain the model, MAE employs a reconstruction loss and the original input image as the ground truth.",2,positive
"In MAE, an image is firstly split into patches that are then randomly blocked.",1,neutral
Figure 7 displays the complete performance of our submissions via fine-tuning a MAE model.,0,negative
We report seven official submissions by fine-tuning the MAE model with different epochs.,0,negative
"As finetuning the ViT-large MAE model with only four RTX 3090 GPUs, we set the actual batch size 512 and train the model 100 epochs.",2,positive
"One of the pretrained model is the model from MAE [22], self-supervised trained in ImageNet1k dataset.",2,positive
"Third, we only leveraged the large model of MAE, other types of model with different training strategies and loss function are also possible, such as contrast learning-based [25] [26] and text-image pair-based [27].",1,neutral
"Benefited by the recent progress in self-supervised vision transformers, especially the powerful masked autoencoder (MAE) [12], we achieve the state-of-the-art performance on all related tasks.",2,positive
"This also illustrates the effect of MIM-based pretraining methods; when the adversarial patch sizes are bigger, the detection rate is lower however the MAE-based model is consistently better.",1,neutral
Our method using MAE base all achieve a better performance with about 2% improvement over all different width and stride.,2,positive
Our methods with masked autoencoder (MAE) achieves state-of-the-art performance on all related tasks.,2,positive
"Additionally, MAE makes further improvement, about 6 ∼ 16% compared to [14].",0,negative
"Also, we test MAE large with 30-epoch finetuning.",2,positive
Both DeiT and MAE surpass results in [14] a lot.,0,negative
"Compared with about 2% improvement over MAE base and 5% improvement over DeiT on clean accuracy, we find that certified robustness benefits more from larger model.",2,positive
"This greatly helps self-supervision, especially using masked-image-modeling strategy [1, 12, 24, 31].",1,neutral
"This can further give about 5% improvement compared with MAE base, and nearly 10% improvement compared with DeiT base.",2,positive
MAE-based model improves about 20% under two 2×2 adversarial patches.,0,negative
"This can be proven by the fact that the mask-and-predict task can be well solved in the image domain (He et al., 2022; Wei et al., 2022), where the model can reconstruct the masked region depending on the unmasked information in a single image.",1,neutral
"This static information can be easily inferred from other unmasked patches in the same single frames (He et al., 2022).",2,positive
"To help the model better model the trajectory, we adopt the patch norm method to normalize the trajectory values in each patch into a standard normal distribution, as has also been done in He et al. (2022).",2,positive
"The reconstruction targets vary in different works, including raw RGB pixels (He et al., 2022), hand-crafted local patterns (Wei et al., 2022) and discrete VQVAE embedding (Bao et al., 2022).",2,positive
"For computation efficiency, we follow He et al. (2022) to only feed the unmasked patches (and their positions) to the encoder.",2,positive
"The reconstruction targets vary in different works, including raw RGB pixels (He et al., 2022), hand-crafted local patterns (Wei et al.",1,neutral
"Taking the advantage of mask-and-predict tasks in NLP (Brown et al., 2020; Devlin et al., 2018), prior works (Bao et al., 2022; He et al., 2022; Zhou et al., 2022) have successfully introduced this task to pre-train an image transformer.",2,positive
He et al. (2022) and Bao et al. (2022) show two different mask image modeling paradigms and both achieve state-of-the-art results.,2,positive
", 2018), prior works (Bao et al., 2022; He et al., 2022; Zhou et al., 2022) have successfully introduced this task to pre-train an image transformer.",2,positive
"2) Pixel: predicting all the pixels of each 3D patch (He et al., 2022; Tong et al., 2022; Feichtenhofer et al., 2022).",2,positive
MAE [18] explored how Masked Autoencoders can be used for the problem of self-supervised representation learning in vision domain.,1,neutral
"Moreover, such independent patch-wise processing of an image allows to simply drop masked patches for decreasing the computational cost [18].",1,neutral
This is quite different from the mechanism of Masked Autoencoders (MAE) that predict masked areas to learn good representations.,1,neutral
"Different from Masked Image Modeling (MIM), which is to reconstruct the masked contents for learning good representation, the Masked Siamese
Networks will not predict the information in removed areas, so erasing will only lose information and is not desired in the learning procedure.",1,neutral
MAE [18] proposed a simple transformer-based masked autoencoder architecture that tries to reconstruct the original image using MSE loss.,2,positive
Masked Image Modeling (MIM) is a task whose goal is to learn useful representations by trying to reconstruct a masked image to its original view.,1,neutral
"This is different from transformer-based MIM methods [3, 18] that will recover the masked regions using reconstruction loss.",1,neutral
"This observation is different from MIM-based approaches [18,35], where a discrete mask achieves better performance and highlights the importance of maintaining global features when generating an image mixture as opposed to the case when erasing parts of the image by performing a vanilla masking operation.",1,neutral
"Recently, masked image modeling (MIM) [1, 3, 18] has emerged and proven to be an effective approach to learn useful representation.",1,neutral
Masked Autoencoder is a common model which can be used in a MIM framework.,2,positive
MAE for spatiotemporal representation learning was proposed by [13].,1,neutral
"Our observation on Masked Siamese ConvNets is opposite to that in MIM methods, which found discrete/random masking is better [18, 35].",1,neutral
It eliminates information redundancy in images [21].,1,neutral
"Recently, inspired by natural language processing’s great success in masked pre-training methods [14,33,13,39], like Bert [14], a similar implementation for image patches have been studied [21,17,2,52].",1,neutral
MAE [21] simply masks random patches and reconstructs the missing pixels.,1,neutral
"Self-supervised learning [10,22,8,11,21,2] often designs surrogate supervision signal with image intrinsic properties.",1,neutral
This inspires us to use such an approach called masked autoencoders (MAE) [21] to pre-train transformer-based models on the target dataset.,2,positive
"A recent study shows a new paradigm of self-supervised learning based on the idea of masking image modeling (MIM) [2,21,52] is robust to the type and size of the pre-training data [17], and even pre-training on target task data can still obtain comparable performance.",1,neutral
"We follow the fine-tuning setting almost the same as MAE [9] to use layer-wise learning rate decay, weight decay, and AdamW.",2,positive
"S-1 such as BeiT [2], PeCo [7], MAE [9] and CAE [5], we also provide the comparison of several recently proposed works in Fig.",2,positive
"For a fair comparison, we directly follow most of the hyperparameters of MAE [9] in our fine-tuning experiments.",2,positive
The settings are almost the same as MAE [9].,1,neutral
"However, in MAE [9], the decoder reconstructs the image pixels, which reconstructs the latent representations into the low-level pixel space.",1,neutral
"In vision tasks, Masked Image Modeling [18,42] aims to learn representations of the input images by solving the regression problem in which a model predicts RGB pixel values in randomly zeroed patch regions in images.",1,neutral
"Masked Video Modeling Self-supervised learning with Masked Image Modeling (MIM) [18, 20, 42] has recently become a popular alternative to contrastive learning for its ability to learn rich representations without having to define negative examples.",1,neutral
"They find that spatiotemporal inductive bias in video clips helps a decoder predict input pixels in masked regions, allowing a higher masking ratio (∼ 90%) than MIM (∼ 60% [42] and ∼ 75% [18]) on image self-supervised learning.",1,neutral
"Masked Video Modeling
In this section, we first introduce Masked Image Modeling (MIM) for image representation learning, and then extend it to our video representation learning scheme, Masked Video Modeling (MVM) (Sections 3.1 and 3.2).",2,positive
"Recently, a nascent self-supervised learning paradigm, Masked Image Modeling (MIM) [18, 42], significantly outperforms the previous representation learning methods on various downstream tasks.",1,neutral
"MIM aims to learn image representations by solving pixel regression problems in regions of an image that are zeroed out through random [18, 42] or attentionbased [20] masking strategies.",1,neutral
He et al. proposed Masked AutoEncoder (MAE) inspired by the success of masked-language modeling [15].,1,neutral
"2) Enriching structural diversity via knowledge-based masked reconstruction: Inspired by MAE [15], TOWER introduces randomly masked reconstruction as a proxy task to augment the translated images x′n.",1,neutral
by the success of masked-language modeling [15].,1,neutral
"masked reconstruction: Inspired by MAE [15], TOWER introduces randomly masked reconstruction as a proxy task",1,neutral
"MAE adopted block-wise masks in model training to reconstruct the randomly-masked input images, thus the structural semantics in the target images might be better captured.",1,neutral
"[35] proposed Masked Autoencoders (MAE), in which a transformer-based encoder learns the latent representation of a small subset of visible patches and a lightweight decoder reconstructs the original input from mask tokens and latent representation.",1,neutral
"In the original MAE [35], a lightweight decoder is designed to reconstruct patches in the masked position.",1,neutral
"To obtain the patch weight α, we generate the binary mask based on the random sampling strategy [35].",1,neutral
"Before fine-tuning for downstream tasks, the encoder of the network is initialized by performing a pretext task such as solving jigsaw puzzles [33], masked pixel prediction [34], or masked image reconstruction [35].",1,neutral
"For the low-level target, ViT (Dosovitskiy et al., 2020), MAE (He et al., 2022),
SimMIM (Liu et al., 2022b), ConvMAE (Gao et al., 2022), HiViT (Zhang et al., 2022) and GreenMIM (Huang et al., 2022) utilize the original or normalized pixels as the MIM target.",2,positive
"Following MAE (He et al., 2022) and BEiT v2 (Peng et al.",1,neutral
"Following MAE (He et al., 2022) and BEiT v2 (Peng et al., 2022), we test the robustness of MaskDistill on three ImageNet validation sets, i.e., ImageNet-Adversarial (Hendrycks et al., 2021b), ImageNet-Rendition (Hendrycks et al., 2021a) and ImageNet-Sketch (Wang et al., 2019).",2,positive
", 2020) Pixel ViT FC / N/A MAE (He et al., 2022) Pixel ViT Decoder LayerNorm l2 SimMIM (Liu et al.",2,positive
"After that, the main difference between previous work lies in the architecture design (He et al., 2022; Chen et al., 2022a) and reconstruction targets (He et al.",2,positive
"MAE (He et al., 2022) also introduces per-patch normalized pixels (i.e., layer normalization without affine transformation) as the target to boost local pixels contrast, resulting in better performance.",2,positive
"After that, various target supervision has been explored under the masked image modeling framework, such as original or normalized pixels (He et al., 2022; Dong et al., 2021; Liu et al., 2022b; Gao et al., 2022; Liu et al., 2022a; Zhang et al., 2022; Huang et al., 2022), high-level features (Wei et al.",2,positive
", 2022a) and reconstruction targets (He et al., 2022; Liu et al., 2022b; Wei et al., 2021; 2022a; Baevski et al., 2022).",0,negative
"After that, the main difference between previous work lies in the architecture design (He et al., 2022; Chen et al., 2022a) and reconstruction targets (He et al., 2022; Liu et al., 2022b; Wei et al., 2021; 2022a; Baevski et al., 2022).",0,negative
"MAE (He et al., 2022) introduces a decoder to decouple the masked prediction task from the encoder.",2,positive
"After that, various target supervision has been explored under the masked image modeling framework, such as original or normalized pixels (He et al., 2022; Dong et al., 2021; Liu et al., 2022b; Gao et al., 2022; Liu et al., 2022a; Zhang et al., 2022; Huang et al., 2022), high-level features (Wei et…",2,positive
"MAE (He et al., 2022) also introduces per-patch normalized pixels (i.",1,neutral
"We evaluate our proposed K-way disjoint masking as augmentation in comparison to different MIM methods, including SimMIM [74] and MAE [32], and another data augmentation method, CutOut [18] in either fully supervised or semi-supervised settings with a base-
line, which is trained in a supervised manner and without any augmentation.",2,positive
"To address this issue, we present a novel data augmentation technique tailored to monocular depth estimation, inspired by recent masked image modeling techniques [4, 32, 74], which allows for generating geometrically consistent pseudo depth maps while applying sufficient perturbations to the inputs.",2,positive
"After BERT [17] proposed the masked language modeling tasks, which is one of the successful methods for pre-training in NLP, related works explored a variety of masked image predictions strategies suited for Transformers [6, 32].",1,neutral
"To apply enough perturbations to an input image in a consistency regularization framework, we propose to adopt a token masking technique as data augmentation, inspired by recent Masked Image Modeling (MIM) strategies for Transformers [6,32,74].",1,neutral
MAE [32] only utilizes unmasked tokens to encode meaningful representations.,1,neutral
"Note that similar techniques were also used to increase the robustness of Transformers for image-level or pixel-level classification such as segmentation [32, 74].",1,neutral
"Masked image modeling is the process of learning representations by reconstructing images that are corrupted by masking [6, 32, 74].",1,neutral
"Recent literature [32, 74] introduce an extremely simple yet effective approach, called masked autoencoder (MAE).",1,neutral
"Our key ingredient is inspired by recent MIM techniques for Transformers [6,32,74].",2,positive
"We evaluate our proposed K-way disjoint masking as augmentation in comparison to different MIM methods, including SimMIM [74] and MAE [32], and another data augmentation method, CutOut [18] in either fully supervised or semi-supervised settings with a base-",2,positive
(Left) masked image (Middle) reconstructed image by MAE (Right) ground-truth image [7],1,neutral
"While there are several methods for self-supervision, masked autoencoders are one of the leading selfsupervised methods in research [7].",1,neutral
"Output patches (in orange) are reshaped into the original image shape, which masked regions are predicted by the model [7]",1,neutral
"Corruption by masking is currently known as the best method of corruption for autoencoders in computer vision, compare to the other methods including adding noise or changing brightness [7].",1,neutral
"Among them, Mask Autoencoders (MAE) [28] demonstrate superior learning ability and scalability.",1,neutral
"The masking strategy adopted in current maskautoencoding frameworks, such as BEiT [2], MAE [28], SimMIM [59], cannot be naively used for MCMAE as all tokens need to be kept in the later transformer stages.",2,positive
1 A Brief Revisit of MAE Masked Autoencoders (MAE) [28] is a self-supervised method for pretraining ViT by reconstructing masked RGB patches from visible patches.,1,neutral
discriminative multi-scale visual representations and to prevent pretraining-finetuning discrepancy when applies MAE [28] on convolution-transformer networks.,1,neutral
"Different from MAE [28], the encoder of MCMAE progressively abstracts the input image into multi-scale token embedding, while the decoder reconstructs the pixels corresponding to masked tokens.",2,positive
"Masked auto-encoding [2, 1, 28, 55] for feature pretraining and multiscale hybrid convolution-transformer architectures [12, 21, 49, 34, 57] can further unleash the potentials of ViT, leading to state-of-the-art performances on image classification, detection and semantic segmentation.",1,neutral
"Self-supervised learning frameworks, such as DINO [6], MOCO-V3 [10], MAE [28], unleash the potential of Vision Transformers (ViT) and achieve high performance on various downstream vision tasks [33, 30, 58].",1,neutral
"A natural question is whether multi-scale backbone with local and global operations, which show promising performance on supervised learning can be exploited to enhance the masked auto-encoding paradigm [28, 15, 2, 65].",1,neutral
2 MCMAE MCMAE is a simple and effective derivative of the popular MAE [28] with minimal but effective modifications on the encoder design and the masking strategy.,2,positive
"In contrast to MAE [28], well-performing multi-scale backbones built upon local and global operations are mainly trained in supervised manner.",2,positive
"This success has recently started to percolate into specific perception domains: computer vision (He et al., 2021; Bao et al., 2022), point-cloud understanding (Yan et al.",2,positive
", 2019) of replacing the masked inputs with a learned token, we choose the more efficient alternative of dropping the masked inputs (He et al., 2021), which we refer as groupwise-masking.",1,neutral
"This success has recently started to percolate into specific perception domains: computer vision (He et al., 2021; Bao et al., 2022), point-cloud understanding (Yan et al., 2022), and speech recognition (Hsu et al., 2021).",2,positive
For masked auto-encoding pre-training we use the optimization hyperparameters from He et al. 2021 and pre-train for 500 epochs with 85% masking for most experiments because that seems to do slightly better than 75% on ImageNet in preliminary experiments.,2,positive
"Instead of using the classic masking approach (Devlin et al., 2019) of replacing the masked inputs with a learned token, we choose the more efficient alternative of dropping the masked inputs (He et al., 2021), which we refer as groupwise-masking.",1,neutral
"In all the linear probing experiments, we use the embedding of the CLS token of the last layer (unlike in DINO [7], which uses the CLS token of the last four attention layers of the network and concatenates them) and perform a coarse grid search over learning rates, batch sizes and whether to normalize the data before feeding them to the linear layer or not (similarly to the added BatchNorm layer [41] in MAE [36]).",2,positive
"Successful pre-training strategies from NLP like masked token prediction [19] have recently also been adapted to the image domain [2, 81, 36, 81].",1,neutral
"A technique that has proven very beneficial to improve the training efficiency of vision transformers is token dropping [1, 36, 25, 10].",1,neutral
"One practical technique to reduce the computational load, which we adopt, is to sparsify the input tokens as in VATT [1] and MAE [36].",1,neutral
We adopt ViT-B pretrained by MAE on the ImageNet dataset Deng et al. (2009) as the baseline in the ablation study.,2,positive
"To alleviate the data-hungry issue of vision transformers, we use MAE pretrained vision transformers to initialize the backbone networks for human pose estimation.",2,positive
"Surprisingly, using the unlabelled pose data only for MAE pretraining, the simple ViTPose baselines get similar performance compared with using backbones pretrained on ImageNet-1K.",2,positive
"To this end, we train a ViT-B model from scratch using MAE on the unlabelled person instances from COCO and AI Challenger human pose datasets for 1600 epochs following the same setting in He et al. (2021). Then, the pretrained model is used to initialize the backbone for pose estimation and finetuned with training images from COCO for 210 epochs.",2,positive
"To this end, we train a ViT-B model from scratch using MAE on the unlabelled person instances from COCO and AI Challenger human pose datasets for 1600 epochs following the same setting in He et al. (2021).",2,positive
"In this paper, we initialize the vision transformer backbones with MAE He et al. (2021) pretrained weights on ImageNet1K Deng et al. (2009), which contains 1M image data.",2,positive
We demonstrate that a plain vision transformer with MAE pretraining can obtain superior performance after finetuning on human pose estimation datasets.,2,positive
"Recently, self-supervised learning (SSL) has exhibited a very successful approach in computer vision [7,15,17,6,16,4].",1,neutral
"We, however, take a fundamentally different approach by not targeting to optimize the contrastive function [30,19] but focusing on maximizing the mutual information between masked inputs and self-supervised signals at pixel-level reconstruction [16,4,34] and features-level regression [2].",1,neutral
"Recent works built upon Vision Transformer (ViT) [13] framework, such as BeiT [4], MAE [16], SimMIM [34] have shown potential of MIM in learning representations.",1,neutral
"In addition, while most existing works [16,4,34,2] utilize a random image masking strategy, our adaptive sampling method could further minimize the conditional entropy H(S|ZX) and learn better representations.",1,neutral
"For this reason, S is usually the augmented images [7,17,15,6,35,30,2] or random masked images [16,34,4,2].",1,neutral
"S can be the augmented image [7,17,6] or the target of image reconstruction [16,34,4].",1,neutral
"x = {xi : i / ∈M}i=1 ∪ {ei : i ∈M}i=1, where e is the learnable embedding replacing for masked patches, we feed only the unmasked patches xU = {xi : i / ∈M}i=1 which similar to MAE [16].",1,neutral
"We propose a novel SSL framework that optimizes both pixel [16,4,34], and feature-level losses [2,19,30] for brain cell image analysis as shown in Fig.",2,positive
"Our method maximizes the mutual information between masked inputs [16,4,2] and self-supervised signals.",1,neutral
"Alternatively, based on ViT [13] framework, we optimize the objective function on both pixel-level reconstruction [16,34,4] and features-level regression [2] to predict the content of masked regions.",2,positive
"Masked image modeling [5, 6, 7], which trains the model to predict missing information from masked image patches, has recently emerged as the superior self-supervised learning method for vision transformers (ViT) [8].",1,neutral
ImageNet-C [47] CLIP ST MUST MAE+Supervised [6] mCE ↓ 70.,1,neutral
"Following [5, 6], we use a layer-wise learning rate decay [35] of 0.",1,neutral
The mCE of MUST is only slightly higher than a model that is first trained with self-supervised MAE [6] followed by supervised finetuning on ImageNet.,2,positive
"While some methods train the model to predict discrete tokens [5] or contextualized representations [32] for masked image patches, MAE [6] and SimMIM [7] achieve competitive performance by simply predicting the pixel values.",1,neutral
All networks are trainable expect that we keep frozen the parameters of pretrained BERT and MAE.,0,negative
"In comparison, vision transformers [6] can establish long range attention, but are for now less efficient in local pattern recognition.",1,neutral
"For image semantic analyzer and text analyzer,
we respectively use a pretrained MAE [6] and BERT [7] model to generate ris ∈ R768×196 and rt ∈ R768×196, where 196 and 768 are respectively the amount of tokens and the feature dimensionality.",1,neutral
"Similarly, we respectively preserve the image semantics branch and the text branch, and find that both of the partial settings provide decent classifications result that can defeat MVAE [8], which verifies the power of transfer learning using pretrained BERT and MAE model.",2,positive
"we respectively use a pretrained MAE [6] and BERT [7] model to generate ris ∈ R768×196 and rt ∈ R768×196, where 196 and 768 are respectively the amount of tokens and the feature dimensionality.",1,neutral
"Most recently, MAE [16] proposes to apply Autoencoders to computer vision by using ViT [12] as an encoder on visible (unmasked) patches and a decoder to predict the missing (masked) patches.",2,positive
Our work will leverage the encoder of a pre-trained model of MAE [16] and use it as a starting point for transformer branch.,2,positive
"It is observed that different pre-trained models of CNN and ViT have different heads, ViT [12] uses cls tokens as the final output while MAE [16] uses the average pool of the tokens other than cls token.",1,neutral
The experiment is done with pre-trained model released by Conformer [26] to initialize the Conv Branch and pre-trained model released by MAE [16] to initialize the Model Epochs Top-1(%),2,positive
"Following MAE [16]’s encoder ViT architecture, this branch contains N repeated transformer blocks.",1,neutral
"Self-supervised pre-training (Grill et al. 2020; Chen and He 2021; He et al. 2021; Yang, Zhang, and Cui 2022; Ohri and Kumar 2021; Zhao and Dong 2020; Larsson, Maire, and Shakhnarovich 2017; He et al. 2020) is attracting growing attentions as it can transfer knowledge learned from large scale…",2,positive
"…works had been proposed in both natural language processing (Devlin et al. 2018) and computer vision (Grill et al. 2020; Chen and He 2021; He et al. 2021; Yang, Zhang, and Cui 2022; Ohri and Kumar 2021; Zhao and Dong 2020; Larsson, Maire, and Shakhnarovich 2017; He et al. 2020), which…",2,positive
"Self-supervised pre-training (Grill et al. 2020; Chen and He 2021; He et al. 2021; Yang, Zhang, and Cui 2022; Ohri and Kumar 2021; Zhao and Dong 2020; Larsson, Maire, and Shakhnarovich 2017; He et al. 2020) is attracting growing attentions as it can transfer knowledge learned from large scale unlabeled dataset to boost the performance on downstream tasks.",2,positive
"2018) and computer vision (Grill et al. 2020; Chen and He 2021; He et al. 2021; Yang, Zhang, and Cui 2022; Ohri and Kumar 2021; Zhao and Dong 2020; Larsson, Maire, and Shakhnarovich 2017; He et al. 2020), which motivated works in point cloud (Yu et al.",2,positive
"For this reason, S is usually the augmented images [2,6,7,16,18,31,37] or random masked images [2,4,17,36].",1,neutral
"We, however, take a fundamentally different approach by not targeting to optimize the contrastive function [21, 31] but focusing on maximizing the mutual information between masked inputs and self-supervised signals at pixel-level reconstruction [4, 17, 36] and features-level regression [2].",1,neutral
"We propose a novel SSL framework that optimizes both pixel [4, 17, 36], and feature-level losses [2, 21, 31] for brain cell image analysis, called DAMA, see Fig.",2,positive
"In addition, while most existing works [2, 4, 17, 36] utilize a random image masking strategy, our method uses adaptive sampling to more effectively minimize the conditional entropyH(S|ZX) and learn better representations.",1,neutral
MAE [17] and SimMIM [36] learn to reconstruct missing image patches from uncorrupted patches.,1,neutral
"The method maximizes the mutual information between masked inputs [2, 4, 17] and self-supervised signals.",1,neutral
"Recent works built upon Vision Transformer (ViT) [13] framework, such as BeiT [4], MAE [17], SimMIM [36] have shown potential of MIM in learning representations.",1,neutral
"Our DAMA employs the image masked autoencoder modeling approach similar to [2,4,17,36] instead of augmenting the input.",2,positive
"Alternatively, based on ViT [13] framework, we optimize the objective function on both pixel-level reconstruction [4, 17, 36] and features-level regression [2] to predict the content of masked regions.",2,positive
"x = {xi : i / ∈ M}i=1 ∪ {ei : i ∈ M}i=1, where e is the learnable embedding replacing for masked patches, we feed only the unmasked patches xU = {xi : i / ∈ M}i=1 which similar to MAE [17].",1,neutral
"He et al. (2021a), also motivated by the label-deficiency issue in federated learning, developed a series of self-supervised FL algorithms that incorporated the advances of supervised FL, especially those algorithms with personalization, to handle the heterogeneity in data.",1,neutral
"…of unlabeled data and achieved tremendous successes for a wide range of downstream tasks in computer vision (He et al., 2020; Chen et al., 2020; He et al., 2021b), natural language processing (Devlin et al., 2018; Sarzynska-Wawer et al., 2021), and embodied intelligence (Sermanet et al., 2018;…",2,positive
"Very recently, there have been a few contemporaneous/concurrent attempts (He et al., 2021a; Zhuang et al., 2021; 2022; Lu et al., 2022; Makhija et al., 2022) that bridged unsupervised/self-supervised learning and decentralized learning, with focuses on designing better algorithms that mitigate the…",2,positive
"…al. (2022) requires access to an unlabeled public dataset for all models to measure the distance, and requires the communication of some datasets; He et al. (2021a) experiments with multiple methods to do personalization in decentralized learning, and our clustering-based approach can be viewed…",2,positive
"For masked autoencoder (He et al., 2021b) experiment on CIFAR-10, we use Vit-Tiny (Dosovitskiy et al., 2020) with AdamW optimizer for 1000 epochs with batch size 256.",2,positive
"In particular, the works He et al. (2021a); Zhuang et al. (2021; 2022); Lu et al. (2022); Makhija et al. (2022) are closest to ours.",2,positive
"To the best of our knowledge, there have only been a few contemporaneous/concurrent attempts (Zhang et al., 2020a; He et al., 2021a; Zhuang et al., 2021; 2022; Lu et al., 2022; Makhija et al., 2022) that bridged unsupervised/self-supervised learning with unlabeled data and decentralized learning,…",2,positive
"Other significant examples of SSL include masked auto-encoding in language (Devlin et al., 2018) and vision (He et al., 2021b).",1,neutral
", 2020), computer vision (Dosovitskiy et al., 2020; He et al., 2022) and related fields (Lu et al.",1,neutral
"It consists of two modules: a pre-trained multimodal masked autoencoder (He et al., 2022; Geng et al., 2022) to encode instructions and visual observations, and a transformer-based (Vaswani et al., 2017) policy that predicts actions.",2,positive
"It consists of two modules: a pre-trained multimodal masked autoencoder (He et al., 2022; Geng et al., 2022) to encode instructions and visual observations, and a transformer-based (Vaswani et al.",2,positive
"…cross-modal alignment between vision and language using a pre-trained multimodal encoder (Geng et al., 2022), which is a large transformer (Vaswani et al., 2017; He et al., 2022) jointly trained on image-text (Changpinyo et al., 2021; Thomee et al., 2016) and text-only data (Devlin et al., 2018).",2,positive
"Transformers (Vaswani et al., 2017) have led to significant gains in natural language processing (Devlin et al., 2018; Brown et al., 2020), computer vision (Dosovitskiy et al., 2020; He et al., 2022) and related fields (Lu et al., 2019; Radford et al., 2021; Geng et al., 2022).",1,neutral
", 2022), which is a large transformer (Vaswani et al., 2017; He et al., 2022) jointly trained on image-text (Changpinyo et al.",2,positive
", 75% [29] This challenging task generated by these two vital design ideasmakes theMAE model more effective and efficient in training large models process.",2,positive
"Masked autoencoders (MAE) are scalable self-supervised learners for CV, and the approach is clear: we just need to mask random patches of the input image and reconstruct the missing pixels [29].",1,neutral
"Moreover, mask tokens would have no information about their location in the image [29].",1,neutral
"Different from SwinIR [21], Uformer [29], and MAE [13], our method generates saturated colorized images, e.",2,positive
14% SwinIR [21] Uformer [29] ColTran [17] MAE [13] Ours 6.,0,negative
"We provide a grayscale image and colorized images from 10 different methods: CIC [32], DeOldify[1], ChromaGAN [28], InstColor [26], GCP [31], SwinIR [21], Uformer [29], MAE [13], ColTran [17] and ours.",2,positive
Input SwinIR [21] Uformer [29] MAE [13] ColTran [17] Ours Ground Truth,0,negative
"We also compare our method with 4 advanced transformer-based methods, including: (i) two state-of-the-art image restoration approaches, SwinIR [21] and Uformer [29], by retraining models on the colorization task; (ii) the stateof-the-art self-supervised learner MAE [13], by finetuning its pretrained weights to colorization as a downstream task; and (iii) the state-of-the-art colorization methods ColTran [17] with same experiment settings.",2,positive
", supervisedlearning-based pretraining [3], generative-model-based methods (VAE) [39], pretext-task-based algorithm (Jigsaw) [14], contrastive-learning-based method (MoCo) [23], and transformer (MAEs) [28].",1,neutral
"We compare our methods with five representative methods, i.e., supervisedlearning-based pretraining [3], generative-model-based methods (VAE) [39], pretext-task-based algorithm (Jigsaw) [14], contrastive-learning-based method (MoCo) [23], and transformer (MAEs) [28].",2,positive
[28] design the masked autoencoder (MAE) to reconstruct the masked image patches based on the ViT.,2,positive
"As shown in Table IV, compared with the pretext task-based method (Jigsaw [14]), generative model-based method (VAE [39]), contrastive learning-based algorithm (MoCo [23]), and transformer-based model (MAE [28]), our methods get superior performance.",2,positive
"In addition, we compare our pretraining method with representative transformer-based methods, such as ViT [25], Swin [26], BEiTs [27], and MAEs [28].",2,positive
"Recently, MAE [13] first masks random patches of the input image and reconstructs the missing pixels with the simple autoencoder framework, showing promising results in selfsupervised learning.",2,positive
"Inspired by the excellent performance of masked autoencoding in NLP [7] and 2D vision [13], we design the masked voxel autoencoding network for 3D perception.",2,positive
"ing, similar to Transformer network in NLP [7] and 2D vision [13].",2,positive
"These observations are consistent with those in self-supervised pre-training in NLP [7] and 2D vision [13], and we hope they will promote the development of self-supervised learning for autonomous driving.",2,positive
"In 2D vision, masked autoencoding has outperformed the supervised pre-training counterparts [13].",1,neutral
"In NLP [7] and 2D vision [13], the goal of masked autoencoding is to reconstruct the masked patches as a regression task.",1,neutral
"In particular, the simple masked autoencoding has proved effective in learning representative features, whose task is to reconstruct the masked data from the unmasked input [7, 13, 5, 32].",1,neutral
"Masked autoencoding in NLP [7] and 2D vision [13] adopts the Transformer network to perform self-attention on the unmasked portions of the training data, which are not influenced by the masked portions.",1,neutral
"…to leverage large, diverse datasets via representation learning (Mandlekar et al., 2020; Yang & Nachum, 2021; Yang et al., 2021; Nair et al., 2022; He et al., 2021), as well as other methods for learning from human demonstrations, such as behavioral cloning methods with expressive policy…",2,positive
", 2022), Auto-regressive), representation learning methods (Nair et al., 2022; He et al., 2021) and offline RL with no pre-training (“Target data only”) and joint training (Singh et al.",2,positive
"In our experiments, we compare our approach to other approaches that attempt to leverage large, diverse datasets via representation learning (Mandlekar et al., 2020; Yang & Nachum, 2021; Yang et al., 2021; Nair et al., 2022; He et al., 2021), as well as other methods for learning from human demonstrations, such as behavioral cloning methods with expressive policy architectures (Shafiullah et al.",2,positive
"Pre-trained MAE initialization (He et al., 2021): We took a similar training procedure to R3M for our MAE representation.",2,positive
MoCo DenseCL MAE GLIP CLIP MDETR [22] [59] [21] [30] [46] [27] 22.,1,neutral
"Visual and language encoder We explore different pre-trained foundation models including MoCo [22], DenseCL [59], MAE [21], GLIP [30], CLIP [46] and MDETR [27] as the visual and language encoder to train a policy, and evaluate their performances without self-play and self-describe on the compositional generalization experiment with two evaluation protocols.",2,positive
"[47] propose to train a masked autoencoder [21] on large scale data such as Ego4D [16], and perform real robot learning with behavior cloning using this pre-trained representation.",1,neutral
"First of all, Vit [9] based MAE achieves the worst result.",0,negative
"Among visual SSL, two broad categories of training have emerged in contrastive learning [1, 2, 3] and reconstructive learning [4].",1,neutral
"In this paper, we study these questions by comparing the representations of a standard ViT-Base model [5] trained with 16x16 image patches (ViT-B/16) on the ImageNet [6] dataset across popular contrastive (MoCo-V3 [2] and DINO [3]) and reconstructive (MAE [4]) methods using Centred",2,positive
"Are the pre-trained representations learned by contrastive and reconstructive methods similar? How does supervised fine-tuning affect theirs representations? Are the similarities and differences in the representations learned by these methods affected by depth and layer-types? Understanding the answers to these questions is important to address several theoretical and practical questions about visual SSL; like why frozen contrastive representations perform better for transfer learning with a linear probe classifier, while reconstructive learning representations transfer better when the ViT is fine-tuned end-to-end [4].",1,neutral
We tackle the challenging task of training GAN with limited data from a perspective of image masking training [21].,2,positive
"Different from most existing work that explores image masking in MAE training for learning better unsupervised representations, we explore the idea of image masking for GAN training and aim to develop robust image generation learners with limited training data.",2,positive
"Recently, [18, 2, 21] mask random image patches for MAE training.",1,neutral
"The shift operation allows to conduct spatial masking continuously along spatial dimensions: a masked patch could appear on any possible locations instead of on fixed grid locations [21, 2].",1,neutral
"Trivial solutions are ubiquitous in unsupervised learning [3, 6, 7, 10], and so in Masked Autoencoders (MAE) [48, 42, 8, 18, 2, 21, 51] as one prevalent unsupervised representation method.",1,neutral
Relations to the spatial masking in MAE .,1,neutral
"For instance, [21] presents a high-ratio masking strategy for MAE: 1) it can impede shortcuts in image reconstruction, e.g. , local spatial interpolation that recovers a masked patch from adjacent patches without high-level understanding [21, 5]; 2) it encourages global spatial interpolation that requires modelling inter-patch context information [54, 25] globally, ultimately learning nontrivial and meaningful representations [21, 5].",1,neutral
"Most existing work can be viewed as a generalized DAE under different types of corruptions, such as masking image pixels, patches, and regions [48, 42, 8, 18, 2, 21], or masking image color channels [57].",1,neutral
"In particular, it is demonstrated in [21] that various image masking strategies can help suppress trivial solutions in MAE training while large mask ratios help learn better representations.",1,neutral
"For instance, [21] presents a highratio masking strategy for MAE: 1) it can impede shortcuts in image reconstruction, e.",1,neutral
"MAE works by masking and reconstructing the image, where image masking is the core design that suppresses trivial solutions and enables learning high-level semantic features [21, 5].",2,positive
"We therefore introduce an additional “mask shift” to allow the masked patches to appear at any image locations (instead of fixed grid locations only as in the spatial masking in MAE [21, 2]), which helps generate images with high-fidelity along spatial dimensions.",1,neutral
", local spatial interpolation that recovers a masked patch from adjacent patches without high-level understanding [21, 5]; 2) it encourages global spatial interpolation that requires modelling inter-patch context information [54, 25] globally, ultimately learning nontrivial and meaningful representations [21, 5].",1,neutral
"MAE aims to encode an image patch into a feature vector, which is a downsampling process and does not require high precision along spatial dimensions ( i.e. , pixels within a patch are considered as the same).",2,positive
"This work benchmarks the following algorithms: SimCLR [10], MoCo v2 [12], BYOL [22], SimSiam [11], Barlow-Twins [58], SwAV [8], DINO [9], and MAE [28].",2,positive
"Additionally, a masked autoencoding objective has been proposed and proven efficient in training large-scale ViTs [28], which opens space for an entirely different route for unsupervised DNN training than contrastive learning algorithms.",1,neutral
"Recently, unsupervised learning models have made significant progress in closing the gap to supervised models in performance on visual recognition tasks without the need for labeled data [56, 62, 52, 27, 12, 10, 22, 11, 58, 7, 9, 28].",1,neutral
"Surprisingly, we find that several of the more recently proposed self-supervised algorithms, including BYOL [22], SimSiam [11], SwAV [7] and MAE [28], largely fail to match human learning in the real-time benchmark and show lower performance in the life-long benchmark, compared to an earlier generation of algorithms like SimCLR [10] and MoCo v2 [27, 12].",2,positive
"Since 2020, ViT becomes a hot topics in the compute vision area (Dosovitskiy et al., 2021; Radford et al., 2021; Bao et al., 2021; Zhou et al., 2021; He et al., 2021a).",2,positive
"Recently, pre-training models have achieved significant success across various areas, including computer vision (e.g., ViT (Dosovitskiy et al., 2021), CLIP-ViT (Radford et al., 2021) and BEiT (Bao et al., 2021)), natural language processing (e.g., BERT (Devlin et al., 2019), GPT2 (Radford et al., 2019) and DeBERTa (He et al., 2021b)) and crossmodal vision-and-language (e.g., VisualBERT (Li et al., 2019b), LXMERT (Tan and Bansal, 2019) and SimVLM (Wang et al., 2021b)).",2,positive
"…et al., 2021) and BEiT (Bao et al., 2021)), natural language processing (e.g., BERT (Devlin et al., 2019), GPT2 (Radford et al., 2019) and DeBERTa (He et al., 2021b)) and crossmodal vision-and-language (e.g., VisualBERT (Li et al., 2019b), LXMERT (Tan and Bansal, 2019) and SimVLM (Wang et al.,…",2,positive
"Inspired by BERT [10] and MAE [20], we propose a self-supervised graph transformer model named Graph Masked Autoencoders (GMAE).",2,positive
"Recently, recovering missing part from incomplete input as the pre-text task has been proved remarkably successful in NLP [5], [6] and 2D vision [10] while few works have been explored in point clouds unsupervised learning.",1,neutral
"On the contrary, generation-based methods for unsupervised learning have shown great successes in both NLP [6], [32] and 2D vision [10].",1,neutral
"URL can thus help reduce training data and annotations which has demonstrated great effectiveness in the areas of natural language processing (NLP) [5], [6], 2D computer vision [7], [8], [9], [10], etc.",1,neutral
"Similarly as [30], we employ random masking of 75% of the patches.",1,neutral
"More recently, generative approaches like Masked Auto-encoders (MAE) [30] have been introduced to predict a masked latent representation of patches.",1,neutral
"More recently, generative self-supervised approaches [5, 22, 30] using auto-encoders have been used to learn representations in the feature space through image patches and visual tokens.",1,neutral
"Afterwards, Transformers spring up and make splendid breakthroughs on various vision tasks [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27].",1,neutral
"reconstruction- [23, 24], or non-negative (positive pairs only)based learning [25–27].",1,neutral
"Secondly, inspired by the success of Masked Autoencoder (MAE) [13], we design a mask-and-replace strategy to intentionally alter the true label to analyze the influence of the accuracy of the annotation.",2,positive
"Inspired by the success of pre-training in NLP, many researchers have proposed pre-training methods suitable for computer vision (CV), such as SimCLR [3], MAE [7], and CLIP [26], bringing us powerful representation models pre-trained on a large-scale dataset.",2,positive
"Different from the natural language, which is humangenerated signals that are highly semantic and informationdense [37], images are generally natural signals with quite objective descriptions of things.",1,neutral
"Training with masks shows no merit Patch-wise augmentation has been adopted in recent works [12, 29] and masked image modeling [15, 50] becomes a new trend on ViTs.",1,neutral
"Recently, pre-training models have achieved great successes in NLP [2, 13] and CV [5].",2,positive
"Supervised pretraining [27, 45, 73] casts representation learning as a multi-class/label classification problem, while un/self-supervised learning learns representation via proxy tasks like instance classification [88] and reconstruction [29, 56].",1,neutral
"Inspired by this, some works [28, 46, 31, 35, 20] showed complex application scenarios benefit from the usage of suitable position embeddings.",1,neutral
"In practice, image masking methods are commonly applied in recent vision task [2, 57, 20, 58, 7], which is a useful and off-the-shelf strategy for self-supervised image reconstruction.",1,neutral
Recent efforts [He et al. 2021] on robust decoders can help in this goal but we need to further account for the unique geometric context available in our setup.,2,positive
Recent works in unsupervised learning have largely focused on removing inductive biases from the training process: transformer-based methods have successfully removed the scale-and-shift invariance from CNNs [17] and autoencoders have successfully removed the hardcoded augmentation-based invariances from contrastive learning methods [23].,1,neutral
"Other very recent works which use a “masked-patch” pretext task are [33, 74, 72, 23, 39, 17, 3, 26, 81, 44].",1,neutral
"Due to architectural constraints, Image Transformer based methods naturally use a patch-based representation [He et al., 2021, Bao et al., 2021].",1,neutral
"For example, MAE [He et al., 2021] generates masked image patches conditioned on other image patches, and ImageBERT [Bao et al., 2021] predicts vector-quantized tokens based on nearby tokens in a context.",1,neutral
"For example, MAE [He et al., 2021] generates masked image patches conditioned on other image patches, and ImageBERT [Bao et al.",1,neutral
Similar observations on the spatial redundancy of images are discussed in a concurrent paper MAE [6].,1,neutral
"They employ a masked autoencoder (MAE) with vision Transformer (ViT) architecture [159] in the encoding process, and adopt an asymmetric encoder-decoder architecture.",2,positive
[32] also showed the potential ability of masked learning.,1,neutral
"[32] summarized prior works and proposed a simple Siamese network that did not need a large batch size, momentum encoder or negatives.",1,neutral
"A masked auto encoder as proposed by [9] attempts to build suitable latent representation by being trained on cropped/masked portions of the images, forcing it to learn and identify classes based on partial images.",1,neutral
"We’re specifically looking at [9], a masked autoencoder that we hope may be able to provide suitable latent representations.",2,positive
"For example, masked autoencoders [10] based for pretext task followed by CNN based architecture for a downstream task could also provide improved performance.",1,neutral
"Successful imputation methods in this literature have often been computationally prohibitive [9], but less intensive approaches like denoising autoencoders have been proven to show decent imputation results.",1,neutral
"Self-supervised learning (SSL) is one of the most popular paradigms in the unsupervised scenario, which can learn transferable representations without depending on manual labeling (He et al. 2022; Chen and He 2021).",1,neutral
MAE Based Features The first type of features are based on two pre-trained MAE models[6].,2,positive
[16] proposed an decoder-encoder based transformer which name masked autoencoder.,1,neutral
"Our network is inspired by [16] and [9], which is the simplification of [9] due to memory cost and forward speed considerations.",2,positive
The ViT models were pretrained as Masked autoencoders [28] on ImageNet-1K [29].,2,positive
MAE is similar to BEIT [24] where the self supervised task consists in training a backbone vision transformer to predict missing tokens from partially masked images.,1,neutral
"The Masked Auto-Encoder (MAE) [22] used by the team is an other way to perform a self-supervised learning inspired by the successful idea of masked language modeling in Natural Language Processing, especially since BERT [23].",1,neutral
"On the other hand, vision transformers (ViT) [3] have shown promise in various vision tasks [1, 4] including low-level ones [17, 19].",1,neutral
"There exist many variations of VAEs [24] that are useful, among others in image processing [25].",1,neutral
"Self-supervised learning techniques have been successful in learning powerful representations for vision tasks such as classification and object detection (Chen et al., 2020b; Grill et al., 2020; He et al., 2022).",1,neutral
"255 As prior work has demonstrated that fine-tuning outper- 256 forms linear classification on most datasets [29], at the second 257 stage, the backbone from the online encoder was frozen for 258 five epochs and then unfroze, and we then trained the whole 259 model with 100 epochs on a single A100 GPU card.",2,positive
"We compare GPF with other tuning methods described as follows:
• PARTIAL-k: We finetune the last k layers of the model with the classfication head and freeze other parts, which is utilized in Zhang et al. (2016); He et al. (2021); Jia et al. (2022).",2,positive
"In addition to augmentation invariance, generative pre-training (Ramesh et al., 2021; Bao et al., 2022; He et al., 2022) and visual-language pre-training (Radford et al., 2021) are promising ways to learn transferable representations.",2,positive
"For transformer, we leverage pre-trained models on ImageNet (Deng et al., 2009) from ViT (Dosovitskiy et al., 2021), DeiT (Touvron et al., 2021), DINO (Caron et al., 2021), MoCo-v3 (Chen et al., 2021b), and MAE (He et al., 2022).",2,positive
"Inspired by the form of the training sample, we propose the center-masked pre-training task, which is similar to MAE [18] but our method is easier to implement.",2,positive
"The MQP differs from the standard self-supervised approaches of using unsupervised data (He et al., 2021; Hendrycks et al., 2019; You et al., 2021) in that the supervised signal is used to select relevant passage as the context of the query.",2,positive
"Besides learning representations for recognition tasks (Vincent et al., 2008; Rasmus et al., 2015; He et al., 2021), it has also been widely utilize to learn disentangled representations (Kulkarni et al.",1,neutral
"Besides learning representations for recognition tasks (Vincent et al., 2008; Rasmus et al., 2015; He et al., 2021), it has also been widely utilize to learn disentangled representations (Kulkarni et al., 2015; Park et al., 2020).",1,neutral
"It consists of a unified pretraining stage and a task-specific finetuning stage, motivated by the recent successful practices in natural language processing [11, 26, 78] and computer vision [7, 32].",1,neutral
"For the first challenge, we follow the pretrained models in language [11,26,78] and vision [7,32] modeling to construct the supervision signals, i.",2,positive
"In recent years, neural networks represented by convolutional neural networks [11], recurrent neural networks [12], and autoencoders [13] have made great progress in the fields of image recognition, speech recognition, image reconstruction, speech denoising, and so on.",1,neutral
"SSL has achieved a lot of remarkable results in the field of computer vision, and even some studies can be comparable to supervised learning [26], [27], which makes many scholars introduce SSL into the field of remote sensing [28], [29].",1,neutral
"[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"We also hope to incorporate self-supervised methods 499 that learn representations by training on a union of unlabeled data from source and target via proxy 500 tasks like reconstruction [24, 28] and contrastive learning [12, 14].",2,positive
"More recently, Transformer models have been successfully applied for selfsupervised learning in natural language (using next-token prediction in GPT [9] or masked token prediction in BERT [4]), computer vision (using masked image patch prediction [14]) as well as tabular data (using masked token prediction or replaced token detection in TabTransformer [15] and TabNet [10]) domains.",1,neutral
"Following common practices [76, 77], we design two transfer strategies with different parameters Θf to optimize: Fixed Feature Extractor Fixed Feature Extractor",2,positive
"Unsupervised representation learning The labeled data bottleneck can also be approached from the perspective of learning useful representations from few labeled samples using semi-supervised learning [33, 31, 64, 53, 46] methods, or with only unlabeled images using self-supervised learning [17, 56, 40, 63, 20, 42, 26, 65, 5].",1,neutral
"‘*’ denotes that we end-to-end finetune RegionCL-M pretrained models for 50 epochs [8,1].",2,positive
"‘*’ denotes that we end-to-end finetune RegionCLM pretrained models for 50 epochs [8,1].",2,positive
"text tasks, the effectiveness of MAE has been validated in many applications [13,23,35], including point cloud understanding [23, 38, 46].",1,neutral
"To this end, self-supervised learning (SSL) techniques have been proposed to generate supervision signals from unlabeled data themselves via carefully designed pretext tasks, such as jigsaw puzzles [22, 30], instance contrastive discrimination [4, 14, 29], and masked autoencoder (MAE)1 [13,23,46].",1,neutral
"1We denote by MAE the general autoencoder with masked inputs of different modalities, not only masked 2D images [13].",1,neutral
"We believe that the obtained decomposed feature embeddings can make the image fusion easier, so that the fused images are generated by a simple convolutional layer called projector, which just likes the last linear layer for classification in general self-supervised learning [10].",1,neutral
", 2021), and easily combined with state-of-the-art visual self-supervised learning (He et al., 2021).",2,positive
"…including out-ofdistribution generalization (Naseer et al., 2021), natural extension to video domains (Bertasius et al., 2021), integration with other domains like language or speech (Radford et al., 2021), and easily combined with state-of-the-art visual self-supervised learning (He et al., 2021).",2,positive
"Just as found in [4], although Autoencoder (reconstruction) works well, the Masked Autoencoder (masking) is the key factor to learning better features.",1,neutral
"In addition, we implement a patched masking like [2].",2,positive
[55] used a self-supervised asymmetric transformer MAE to reconstruct masked patches from a limited amount of unmasked data.,1,neutral
"In response, inspired by the MAE [55] and the SF [31], in this work we propose the MAEST, a two-branched model with a self-supervised spectral adapted transformer with a masked reconstruction AE to learn generalizing noise-free representations from HS imagery to later classify them.",2,positive
"[55], in contrast to other masked transformer-based models as bidirectional encoder representations from transformers (BERT) [44], whose typical masking ratio is 15% and other related computer ViTs as [30], [54] (from 20% to 50% masking ratio).",1,neutral
We utilize ViT-base backbone pretrained by MAE [21].,2,positive
"Other recent research shows that on the premise of having expanded training data, the new model transformer [13], [14] can surpass the convolution residual network architecture of the classic state-of-the-art in image processing [15], and the latest image restoration and technology based on Transformer’s attention mechanism image restoration can restore the image in the case of almost complete loss of the original image [16], [17].",2,positive
"Deep learning methods have achieved great success on a wide variety of tasks, and show surprising performances even without labels [1,2,3].",1,neutral
This autoencoder learns to predict masked (unknown) dimensions of an observation based on the recorded (known) ones [14].,1,neutral
"We also compare Model Assembling with the recently proposed improved E2E baselines in [17], where a systematically hyper-parameter search is performed on training configurations.",2,positive
E2E-ViT [12] E2E-DeiT [36] E2E [17] DeiT + Ours E2E-ViT/DeiT E2E [17] DeiT + Ours,1,neutral
"One can observe that our method shows a better efficiency-accuracy trade-off than all E2E baselines, including the recently proposed improved E2E baselines [17].",2,positive
"Following [63], we apply the regularization in a denoising autoencoding manner.",1,neutral
"However, the improving performance of the deep learning model is always achieved by the increasing size of the model, which makes it inconvenient for us to train or deploy our models further [21; 17].",2,positive
This observation is similar to spatial masking in MAE [35] where an optimal masking ratio is found.,1,neutral
"As a result, models learned from the masking supervision demonstrate strong capability for capturing correlations between sequential tokens [35].",1,neutral
"Another important line of work generalizes masked modeling [22], which was initially proposed for language modeling, to other data modalities and domains [35, 65, 77].",1,neutral
"More recently, as an efficient sequence parallel computing model, vision transformers enable to overcome the limit of local convolutions and effectively apply the self-attention mechanism to achieve a global information modeling for reducing the loss of visual information [16], [17], achieving great advancement in various computer vision tasks.",1,neutral
"MAE [14] and data2vec [18], has surpassed that of “end-to-end supervised learning"" by a significant margin on classification, object detection and segmentation tasks, and its success is increasingly scaled to other tasks thanks to its compatibility and effectiveness.",2,positive
"Then we randomly mask the latent patch tokens z by replacing the selected masked tokens with a shared and learned mask token [14, 27].",1,neutral
"[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"M3AE (Geng et al., 2022) extends the MAE approach to both image and language by combining image patches and language tokens as a single sequence.",2,positive
"Recently, MAE (He et al., 2022) further removes the need of a discrete image tokenizer by directly predicting the patch pixel output using an encoder-decoder architecture.",2,positive
"We expect methods like self-supervised learning[13, 14, 15] to bring more improvements.",2,positive
MVP 126 [3] learn a policy with PPO and use a pre-trained visual encoder for feature extraction in addition to 127 proprioceptive state information; the pre-trained representation is an MAE [30] trained on frames 128 from diverse human videos.,0,negative
MVP126 [3] learn a policy with PPO and use a pre-trained visual encoder for feature extraction in addition to127 proprioceptive state information; the pre-trained representation is an MAE [30] trained on frames128 from diverse human videos.,0,negative
"Visual transformer [8, 25] has gradually become a new backbone for computer vision tasks, including classification [8, 25], detection [2], segmentation [55] and representation learning [5, 16].",1,neutral
"Current methods in the field [32, 31, 27, 8, 9] have demonstrated remarkable downstream transfer performance with state-of-the-art methods [69] outperforming supervised ImageNet baselines.",1,neutral
"Among this line, autoencoder is one of the most classical methods [4, 21, 39, 52, 53].",1,neutral
"Actually, there are many inherent variations between vision and language data, such as the information density (He et al., 2022).",1,neutral
"We have also tried learnable prototypes similar to learnable tokens in the Transformer-based models [10, 14, 9], but do not observe performance gain.",2,positive
", 2022), Auto-regressive), representation learning methods (Nair et al., 2022; He et al., 2021) and offline RL with no pre-training (“Target data only”) and joint training (Singh et al.",2,positive
"In our experiments, we compare our approach to other approaches that attempt to leverage large, diverse datasets via representation learning (Mandlekar et al., 2020; Yang & Nachum, 2021; Yang et al., 2021; Nair et al., 2022; He et al., 2021), as well as other methods for learning from human demonstrations, such as behavioral cloning methods with expressive policy architectures (Shafiullah et al.",2,positive
"Especially the reconstruction of a masked input gained huge traction across various domains of application, among others like natural language processing (NLP) [10] and 2D imagery [17] also on point clouds.",1,neutral
They have also been successfully applied for self-supervised image representation learning [39] by training a denoising autoencoder to predict masked image-patches of its input.,1,neutral
", 2020) where the model learns via simple masking of tokens/patches (He et al., 2022).",2,positive
"Masked Auto-Encoders (MAE) are an interesting recent class of selfsupervised methods based on the Vision Transformer architecture (Dosovitskiy et al., 2020) where the model learns via simple masking of tokens/patches (He et al., 2022).",2,positive
"Current methods in SSL can be divided into two categories: contrastive learning [32, 18, 52] and non-contrastive learning [7, 25, 14, 73, 17, 9, 24, 68, 33].",1,neutral
