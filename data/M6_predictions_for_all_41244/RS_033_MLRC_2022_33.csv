text,target_M6_predict,target_predict_M6_label
"Recent studies [20, 35] about the verbalizer have proposed one-to-many mapping with similar words from the external knowledge base, e.",1,neutral
"Flan-T5 (Chung et al., 2022) is an improved upstream model scaled to thousands of tasks (Wang et al., 2022b).",2,positive
Wang et al. (2022a) additionally ensure that label tokens belong only to a single class.,1,neutral
"For example, using top-level predictions to refine prompts of bottom levels can surpass soft prompts and hard prompts (Wang et al., 2022b).",1,neutral
"Compared with fine-tuning, prompt tuning may have a better generalization on various tasks due to the aligned nature of language descriptions and answer semantics, e.g., classification problems (Gao et al., 2021; Wang et al., 2022a).",1,neutral
"AutoL (Gao et al., 2021) and AMuLaP (Wang et al., 2022) are both automatic label words searching methods utilizing few-shot examples.",1,neutral
", 2021) and AMuLaP (Wang et al., 2022) are both automatic label words searching methods utilizing few-shot examples.",1,neutral
", 2021); AMuLaP: method in (Wang et al., 2022); Majority: majority class.",2,positive
"However, most previous works focus injecting knowledge into prompt on single-label multi-class classification task (Hu et al., 2022; Wang et al., 2022a; Ye et al., 2022).",1,neutral
