text,target_M6_predict,target_predict_M6_label
Some methods of this type require multiple pruning and retraining cycles [20] and thus prolong the required training time.,1,neutral
"We apply an incremental fine-tuning algorithm based on learning rate rewinding (Renda et al., 2020) to the baseline model to obtain the accuracy values corresonding to the following sparsity levels: 50%, 75%, 87.",2,positive
"A number of algorithms have been proposed in the literature for accuracy recovery of pruned models (Deng et al., 2021; Renda et al., 2020; Hoefler et al., 2021).",1,neutral
", 2022), learning rate rewinding (Renda et al., 2020), and knowledge distillation (Hinton et al.",2,positive
Both the IMP and LTH language-specific pruning meth-ods achieve matching performance to the original dense multilingual model and surpass the dense monolingual models.,2,positive
The iterative magnitude pruning (IMP) method [6] involves fine-tuning a dense model for a specified number of steps denoted as T while making pruning decisions based on the magnitude of weights.,1,neutral
"The multilingual pathway model undergoes training for 200K and 100K steps for IMP and LTH methods, respectively.",2,positive
"The ASR Pathways [12] provides a method to fine-tune a multi-lingual ASR model using the language-specific sub-networks (or pathways) identified through IMP, LTH, or other pruning methods.",2,positive
"To initiate the IMP procedure, we initialize model parameters θ with pre-trained dense weights θ 0 and set the binary pruning mask m to all ones 1 , where m ∈ { 0 , 1 } | θ | .",1,neutral
"For the pruning step, we simply raise the sparsity level and prune from all weights in θ n as opposed to pruning from weights in m ⊙ θ T in the IMP procedure.",1,neutral
"When monolingual data is used in IMP, this procedure yields a language-specific pruning mask m l for a language l .",1,neutral
"Within the framework of the IMP procedure, we introduce a mask adaptation step denoted as n (where n   T ).",1,neutral
Our proposed Stage (2) modified the IMP and the LTH language-specific pruning methods in Stage (2) and achieved a consistent 5.3% relative WER reduction averaged across languages.,2,positive
"However, the pruning process, such as Iterative Magnitude Pruning (IMP) [6, 11] and Lottery Ticket Hypothesis (LTH) [8], involves multiple iterations of pruning and re-training to achieve the best performance.",1,neutral
The IMP procedure is illustrated as follows: Repeat 2.,1,neutral
"The lottery ticket hypothesis (LTH) method [8] modifies the Step 3 of the IMP procedure by assigning the pre-trained dense weights θ 0 to θ instead of θ T , referred to as a re-winding step.",1,neutral
"In addition, unstructured pruning [14] is further used to remove unimportant weights and reduce computational costs.",1,neutral
"In 2019, lottery ticket hypothesis (LTH) [15] was proposed by combining existing pruning and training modes.",2,positive
It leverages upon both channel-level and LTH pruning.,2,positive
"*corresponding author was originally a one-shot pattern but has since been improved to an iterative pattern, which is further guided by the learning rate rewinding strategy [18] in the case of CPLR.",2,positive
"Researchers have compared various strategies based on LTH [16, 17, 18], and one effective scheme is to use learning rate rewinding [18].",1,neutral
"• Learning Rate Rewinding: Learning rate rewinding, proposed in [40], trains the remained weights from the final values using the learning rate schedule for the specified number of epochs.",1,neutral
"The results in [40, 53] show that weight rewinding can achieve higher accuracy than fine-tuning.",1,neutral
"• Compression Ratio: Compression ratio in [39, 40] is defined as the ratio of the original weight numbers to the preserved weight numbers, but in [41] it is defined as the ratio of the preserved weight numbers to the original weight numbers.",1,neutral
"recover the performance [40], where w ′′ T and m ′′ are the final results of weights and masks after the overall pruning process, respectively.",1,neutral
"For example, if 10% of weights are preserved, then the compression ratio in [40] is 10, but it is 10% in [41].",1,neutral
"Furthermore, the work [24] extends the training of the subnetwork from initialization to the early stage of pretraining, which improves the accuracy of the subnetwork in more challenging tasks.",2,positive
"[24] Alex Renda, Jonathan Frankle, and Michael Carbin, ‘Comparing rewinding and fine-tuning in neural network pruning’, in International Conference on Learning Representations, (2020).",1,neutral
"…magnitude pruning (IMP) framework [Han et al., 2015], a simple and successful approach for pruning deep neural networks [Blalock et al., 2020, Renda et al., 2020] which is pivotal to finding “lottery tickets” [Frankle and Carbin, 2019, Frankle et al., 2019, 2020], i.e. sparse subnetworks…",2,positive
"…(lottery ticket rewinding was used when required, see appendix B) was used for our experiments as it provides an effective procedure to find subnetworks with nontrivial sparsities that have low test error [Frankle and Carbin, 2019, Frankle et al., 2020, Blalock et al., 2020, Renda et al., 2020].",2,positive
"A quick review of existing pruning methods reveals a possible reason: they typically require retraining [3, 25], training from scratch [8, 32, 44, 54] or even an extensive iterative process [23, 37, 77].",0,negative
"Existing pruning methods usually require either modifications to the training procedure [23, 33, 44, 56], retraining the pruned networks to regain accuracy [25, 26, 41], or an even more computationally intensive iterative retraining process [18, 32, 54, 77].",1,neutral
"Recent studies [30, 21, 39] have highlighted the importance of the learning rate schedule in the retraining phase and have proposed specific guidelines for selecting an appropriate schedule for iterative magnitude pruning (IMP) [13] and provide valuable insights into the impact of learning rate schedules.",1,neutral
"Additionally, weight values located around 0 have minimal impact on the final accuracy outcome, which is consistent with the principles of weight magnitude pruning [22].",0,negative
"Usually, extra fine-tuning is conducted for the pruned network to help maintain the performance [20, 28, 37].",1,neutral
Fine-tuning [41] is adopted to adjust the potential performance of the pruned model to the optimal level.,2,positive
"In the follow-up work [22, 55], the authors introduce LTH with rewinding to enable LTH for deeper models and larger datasets.",2,positive
"Some literatures discuss the effectiveness of pruning, fine-tuning [24], and rewinding weights [29] for LTH.",1,neutral
"LTH has been widely applied in image classification [9– 12, 30, 32, 33], natural language processing [5, 28, 29, 34], and so on.",1,neutral
"While [53], [54] scaled up LTH by rewinding [55], [56].",0,negative
"A considerable amount of work has focused on improving the sub-network performance in the (second) pruning stage (Blalock et al., 2020) and the (third) retraining stage (Renda et al., 2020; Le & Hua, 2021).",2,positive
", 2020) and the (third) retraining stage (Renda et al., 2020; Le & Hua, 2021).",2,positive
"[26] Alex Renda, Jonathan Frankle, and Michael Carbin.",0,negative
"Recently, iterative magnitude pruning [25, 26] has emerged as a natural solution to this problem.",1,neutral
"To address this issue, the concept of rewinding is introduced [Frankle et al., 2020a, Renda et al., 2020].",2,positive
"Empirical studies have demonstrated that iterative magnitude pruning (IMP) - applying magnitude-based pruning multiple times - can effectively remove a large proportion of weights in neural networks [Frankle and Carbin, 2019, Renda et al., 2020, Frankle et al., 2020a].",1,neutral
"Although simple to implement, IMP shows state-of-the-art results for network pruning, especially for extreme sparsity regimes [Renda et al., 2020].",2,positive
"Prevalent importance criteria are based on the parameter’s magnitude (Zhu and Gupta, 2017; Renda et al., 2020) or sensitivity (Louizos et al.",0,negative
"Prevalent importance criteria are based on the parameter’s magnitude (Zhu and Gupta, 2017; Renda et al., 2020) or sensitivity (Louizos et al., 2018; Sanh et al., 2020; Liang et al., 2021; Zhang et al., 2022).",0,negative
"Another alternative is weight pruning, which reduces the number of parameters in neural networks (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b).",1,neutral
"Other efforts have sought to eliminate the need for model weight rewinding (Renda et al., 2020) and to identify winning tickets at initialization (Wang et al.",2,positive
"Other efforts have sought to eliminate the need for model weight rewinding (Renda et al., 2020) and to identify winning tickets at initialization (Wang et al., 2020a; Sreenivasan et al., 2022b).",0,negative
"However, some pruning techniques may result in more training efforts during model building stage, such as methods that are based on the procedures of training, pruning, and fine-tuning/re-training [36], [37].",1,neutral
"The strategies in [12], [13], [14], [15] try to accelerate com-",1,neutral
"Moreover, it is demonstrated that in the case of CVNN the rewinding process of both weights and learning rate has a positive effect, and as a consequence WR is a more promising pruning technique than LR and FT, which is not the case in real-valued NN [4].",1,neutral
"In LR only the learning rate is reset to its pre-trained value, leaving the unpruned weights to be re-trained from their values at the end of the initial training phase [4].",0,negative
", 2020); and (ii) recovering weights to previous values is compatible with the finding that tuning pruned networks with earlier weight values or learning rate schedule is beneficial (Renda et al., 2020).",1,neutral
"…allowing the growth of connections has been shown to yield better sparse networks (Evci et al., 2020); and (ii) recovering weights to previous values is compatible with the finding that tuning pruned networks with earlier weight values or learning rate schedule is beneficial (Renda et al., 2020).",1,neutral
This rejuvenated the field of sparse deep learning Renda et al. (2020); Chen et al. (2020) and more recently the interest spilled over into sparse reinforcement learning (RL) as well Arnob et al. (2021); Sokar et al. (2021).,1,neutral
"There are mainly two types of pruning methods: post-hoc pruning (Han et al., 2015; Renda et al., 2020; Molchanov et al., 2019; LeCun et al., 1989; Hassibi & Stork, 1992) and foresight pruning (Lee et al.",1,neutral
"Most post-hoc pruning methods intend to reduce the inference time, with the exception of lottery ticket pruning (Frankle & Carbin, 2018; Renda et al., 2020) which prunes the networks for the purpose of finding a trainable sub-network.",2,positive
"There are mainly two types of pruning methods: post-hoc pruning (Han et al., 2015; Renda et al., 2020; Molchanov et al., 2019; LeCun et al., 1989; Hassibi & Stork, 1992) and foresight pruning (Lee et al., 2018; Wang et al., 2020; Alizadeh et al., 2022; Tanaka et al., 2020; de Jorge et al., 2020b;…",1,neutral
"Lottery ticket experiments (Frankle & Carbin, 2018; Renda et al., 2020; Frankle et al., 2020a) use the same architectures.",2,positive
[17] found that weight rewinding and learning rate rewinding outperformed fine-tuning.,1,neutral
"[17] proposed a third retraining technique, learning rate rewinding, where the unpruned weights are trained from their final values with the learning rate schedule from weight rewinding.",1,neutral
One such retraining technique is fine-tuning where a small fixed learning rate is used used to train the unpruned weights from their final trained values [17].,1,neutral
"…et al., 2018a; Li et al., 2019; Zhang et al., 2018), reinforcement learning (He et al., 2018b; Chen et al., 2019), lottery ticket (Frankle & Carbin, 2018; Frankle et al., 2019; Renda et al., 2020), etc.; (iii) (iteratively) retrain the pruned model to regain the accuracy regression during pruning.",2,positive
"This is in stark contrast with the behavior of SNNs on the image classification task, where LTH can gracefully preserve the matching performance even at very extreme sparsities (>95% on CIFAR-10/100 (Yin et al., 2022) and >80% on ImageNet (Renda et al., 2020)).",2,positive
"For instance, even though ImageNet has been considered a rather challenging task over years, very high accuracy (>90",1,neutral
"Following a mainstream convention in many sparse training papers (Frankle & Carbin, 2019; Gale et al., 2019; Evci et al., 2020; Lee et al., 2019; Liu et al., 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al., 2020) and Knowledge Distillation (Hinton et al., 2015) in our main evaluations, even if we observe that they help to alleviate accuracy drops as in Appendix C.
• Lottery Ticket Hypothesis (LTH) (Frankle & Carbin, 2019) is a strong post-training pruning baseline that iteratively adopts magnitude pruning after training to produce binary masks and re-train together with weights from step t.",2,positive
"Nevertheless, such a well-tuned pruning recipe is both time- and resource-intensive (9×more fine-tuning time, besides Hessian matrix
approximation); and even so, the strongest SNNs still fall short of their dense counterpart by around 10% accuracy at sparsities between 60%− 80%, in contrast to “normal” SNNs that easily match their dense models on CIFAR, ImageNet, or GLUE.",1,neutral
"Secondly, people are obsessed with evaluating SNNs on well-understood datasets, including but not limited to MNIST (LeCun, 1998) (26 papers), CIFAR-10/100 (Krizhevsky et al., 2009) (59 and 37 papers, respectively), ImageNet (Deng et al., 2009) (62 papers), and GLUE (Wang et al., 2018) (9 papers), where deep neural networks have already exceeded the human-equivalent performance (refer to Appendix D for more details).",1,neutral
", 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al., 2020) and Knowledge Distillation (Hinton et al.",2,positive
"…in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al., 2020) and Knowledge Distillation (Hinton et al., 2015) in our main evaluations, even if we observe that they help to alleviate accuracy…",2,positive
"Specifically, we follow Kurtic et al. (2022) and replace the magnitude pruning criterion of the LTH framework with the second-order oBERT criterion; adopt Learning Rate Rewinding (LRR) (Renda et al., 2020) and Knowledge Distillation (KD) (Hinton et al., 2015) during each pruning iteration; and keep the embeddings and classification heads dense.",2,positive
"Renda et al. (2020) further found that instead of re-training with the initial weights, re-training with the final weights achieves better performance.",0,negative
"…(2022) and replace the magnitude pruning criterion of the LTH framework with the second-order oBERT criterion; adopt Learning Rate Rewinding (LRR) (Renda et al., 2020) and Knowledge Distillation (KD) (Hinton et al., 2015) during each pruning iteration; and keep the embeddings and classification…",2,positive
"(2022) and replace the magnitude pruning criterion of the LTH framework with the second-order oBERT criterion; adopt Learning Rate Rewinding (LRR) (Renda et al., 2020) and Knowledge Distillation (KD) (Hinton et al.",2,positive
[272] propose to rewind the learning rate schedule but not the weight value (learning rate rewinding).,1,neutral
[30] and the L-norm based filter pruning approach proposed by Li et al.,1,neutral
"Post-hoc pruning prunes weights of a fully-trained neural network, and they usually have high computation cost due to the multiple rounds of train-prune-retrain procedure (Han et al., 2015; Renda et al., 2020).",1,neutral
"Post-hoc pruning was initially proposed to reduce the inference time, while later work on lottery ticket works (Frankle & Carbin, 2018; Renda et al., 2020) aimed to mine trainable sub-networks.",0,negative
[35] compares the impact of finetuning and rewinding and introduces the aspect of learning rate rewinding.,1,neutral
Pruning is a powerful compression approach which removes redundant parameters without significantly deteriorating the full model performance Han et al. (2015b;a); Paganini & Forde (2020); Zhu & Gupta (2017); Renda et al. (2020); Zafrir et al. (2021); Liang et al. (2021).,2,positive
"Unfortunately, coarse-grained sparsity suffers severe performance drops at sparsity levels higher than 50% due to the flexibility constraint on network sparsity (Renda et al., 2021).",2,positive
"LTH has since been empowered by weight/learning rate rewinding (Frankle et al. 2020; Renda et al. 2020), and the existence of LTH has been verified in various applications, showing the almost universal intrinsic sparsity in overparameterized networks (Chen et al. 2020, 2021c,b).",2,positive
"LTH has since been empowered by weight/learning rate rewinding (Frankle et al. 2020; Renda et al. 2020), and the existence of LTH has been verified in various applications, showing the almost universal intrinsic sparsity in overparameterized networks (Chen et al.",1,neutral
"In the literature, The design space of pruning algorithms encompasses a range of aspects, including pruning schemes [21, 39], parameter selection [20, 43, 44], layer sparsity [27, 49] and training techniques [47, 58].",1,neutral
"In the literature, there have also been several studies, for example investigating whether rewinding (training from scratch with a fixed mask) can perform just as well as the fine-tuning on top of the original unpruned network (Renda et al., 2020).",1,neutral
"However, empirical studies have shown that unstructured pruning often yields much better results than structured [35].",1,neutral
"In structured pruning, weights are pruned in groups by removing whole neurons or entire channels [35, 23].",1,neutral
The LTH has been well applied in different domains [12]– [15].,1,neutral
"Several state-of-the-art pruning methods (Renda et al., 2019; Frankle & Carbin, 2019) have demonstrated that a significant quantity of parameters can be removed without sacrificing accuracy.",1,neutral
"reasons: (i) Iterative pruning tends to provide better pruning performance than one-shot pruning as reported in the literature (Frankle & Carbin, 2019; Renda et al., 2019).",0,negative
"Lastly, we highlight that all LR schedules used, including SILO, are rewound to the initial state at the beginning of each pruning cycle, which is the same as the LR rewinding in (Renda et al., 2019).",2,positive
"Several recent works (Renda et al., 2019; Frankle & Carbin, 2019) have noticed the important role of LR in network pruning.",1,neutral
"Some follow-on works (Zhou et al., 2019; Renda et al., 2019; Malach et al., 2020) investigated this phenomenon more precisely and applied this method in other fields (e.",1,neutral
"9 and a weight decay of 1e-4 (same as (Renda et al., 2019; Frankle & Carbin, 2019)).",1,neutral
"Some follow-on works (Zhou et al. 2019; Renda, Frankle, and Carbin 2019; Malach et al. 2020) investigated this phenomenon more precisely and applied this method in other fields (e.g., transfer learning (Mehta 2019), reinforcement learning and natural language processing (Yu et al. 2020)).",1,neutral
"We refer to the standard implementation reported in (Renda, Frankle, and Carbin 2019; Frankle and Carbin 2019) (i.e., SGD optimizer (Ruder 2016), 100 training epochs and batch size of 128, learning rate warmup to 0.03 and drop by a factor of 10 at 55 and 70 epochs) and compute the static DNR and…",2,positive
"A recent work (Renda, Frankle, and Carbin 2019) proposed learning rate rewinding which used the same learning rate schedule to retrain the pruned network, leading to a better
pruning performance.",2,positive
"21: end if
We train the network using SGD with momentum = 0.9 and a weight decay of 1e-4 (same as (Renda, Frankle, and Carbin 2019; Frankle and Carbin 2019)).",2,positive
"Specifically, the implementations for Tables 1 - 6 are from (Frankle and Carbin 2019), (Zhao et al. 2019), (Chin et al. 2020), (Renda, Frankle, and Carbin 2019) and (Dosovitskiy et al. 2020).",2,positive
"The implementation for Table 10 - 13 are from (Zhao et al. 2019), (Chin et al. 2020), (Renda, Frankle, and Carbin 2019) and (Dosovitskiy et al. 2020), respectively.",2,positive
"Most authors [6, 5, 32, 18] agree on the fact that: (i) the pruning should not start at the beginning of training, due to the heavy changes a model undergoes in the early stage of training; and (ii) the increase of sparsity should be progressive, to limit the interference between weight update and weight zeroing.",0,negative
"To limit these computations, sparse networks have been thoroughly investigated in the past few years [43, 4, 27, 41, 21, 38, 3, 6, 32, 22, 42, 25], and significant efforts have been devoted to their efficient hardware implementation [8, 29].",1,neutral
"This is in contrast with the iterative post-training pruning solutions [6, 32], which achieve SoA accuracy/sparsity trade-off, but suffer from an extremely large computation cost due to the multiple rounds of training required to progressively increase the pruning ratio.",2,positive
"As a recognized SoA upper bound in terms of accuracy/sparsity trade-off, the Learning-Rate-Rewind (LRR) recursive pruning method is considered [32].",1,neutral
"Doing so, SoA accuracy are obtained, but the training cost largely grows with the sparsity ratio [6, 32].",0,negative
"Another convincing experimental argument in favor of our method lies in the fact that, when combined with iterative and thus quite complex (due to multiple rounds of training cycles) post-training pruning solutions proposed in [6, 32], our ST-3 defines a novel SoA accuracy/sparsity trade-off.",2,positive
"This aspect is generally overlooked in previous work [6, 4, 32, 20], where only the model compression is considered, disregarding the number of operations affected by the zeroed weights.",1,neutral
"…may reduce memory footprint and improve computational efficiency of model training or inference (e.g., [Han et al., 2015, Frankle and Carbin, 2019, Renda et al., 2020, Blalock et al., 2020, Lebedev and Lempitsky, 2016, Molchanov et al., 2017, Dong et al., 2017, Yu et al., 2018, Baykal et al.,…",1,neutral
"Second, the NN-based learning permits the application of complexity reduction through network pruning [41], which is more fine-grained than discarding triplets.",1,neutral
"Pre-trained language models (PLMs) have achieved great success in NLP (Devlin et al., 2019a), but they are vulnerable to adversarial examples crafted by performing subtle perturbations on normal examples (Ren et al., 2019; Garg and Ramakrishnan, 2020).",1,neutral
"In NLP, previous work has found that matching subnetworks exist in Transformers, LSTMs, and PLMs (Yu et al., 2020; Renda et al., 2020; Chen et al., 2020).",2,positive
"RobusT (Zheng et al., 2022b): An approach that identifies robust tickets from the original PLMs through learning binary masks.",1,neutral
"In previous work, robust tickets with unstructured sparsity in PLMs are extracted after a tedious training process.",1,neutral
"When the preset pruning-rate is too small, the weights of the network change slightly, and the similar structure may be utilized by using the same learning rate to retrain the network, makes the network converge faster to the early-stop point with the similar performance as the original network [26].",1,neutral
Some researchers have compared various strategies based on LTH and one effective scheme is based on learning rate rewinding [26].,1,neutral
"Apart from the image classification tasks, the LTH is also imported in many other research areas [Chen et al., 2020; Mallya et al., 2018; Gale et al., 2019; Yu et al., 2019; Renda et al., 2020; Chen et al., 2020; Prasanna et al., 2020; Girish et al., 2021].",2,positive
"The works [Gale et al., 2019; Yu et al., 2019; Renda et al., 2020] show that the subnetworks exist early in the training instead of initialization on Transformers.",2,positive
"The following work [Renda et al., 2020] extends (rewinds) the training of the subnetwork from initialization to the early stage of pretraining, which improves the accuracy of the subnetwork in more challenging tasks.",2,positive
"A recent, very promising line of research [10, 31] suggests that only a subnetwork of the topology is responsible for carrying out accurately a particular task, and thus if we can detect which is this subnetwork and then train only this, pruning the rest of the network, then we can gain significant speedups in training.",2,positive
"We note that this learning rate schedule is different from prior work on pruning, which typically uses a single decay cycle [Kusupati et al., 2020, Singh and Alistarh, 2020, Peste et al., 2021], or dynamic learning rate rewinding, e.g. [Frankle et al., 2019, Renda et al., 2020].",2,positive
"In line with LTH, weight rewinding (Renda et al., 2020) is adopted to retrain the identified positive soft prompts.",0,negative
"Following the idea in LTH, we adopt the weight rewinding technique (Renda et al., 2020) to re-train the soft prompts after the two-level hierarchical structured pruning.",2,positive
"Among the iterative schemes, the IMP (Iterative Magnitude Pruning scheme) [17, 20, 56– 65, 36] has played a significant role in identifying high-quality ‘winning tickets’, as postulated by LTH (Lottery Ticket Hypothesis) [18, 19].",1,neutral
", the use of early-epoch rewinding for model re-initialization [18] and the no-rewinding (i.",1,neutral
"In this work, if a matching subnetwork is found better than the winning ticket obtained by the same method that follows the original LTH setup [18, 19], we will also call such a matching subnetwork a winning ticket throughout the paper.",1,neutral
"[18] Alex Renda, Jonathan Frankle, and Michael Carbin, “Comparing rewinding and fine-tuning in neural network pruning,” in 8th International Conference on Learning Representations, 2020.",1,neutral
"For (ii), the unpruned weights in each pruning iteration are re-set to the weights at initialization or at an early-training epoch [18], and re-trained till convergence.",1,neutral
"As illustrated by the Lottery Ticket Hypothesis (LTH), pruning also has the potential of improving their generalization ability.",1,neutral
"L G
] 2
1 A
Among various proposed model pruning algorithms [5, 9, 11, 14–27], the heuristics-based Iterative Magnitude Pruning (IMP) is the current dominant approach to achieving model sparsity without suffering performance loss, as suggested and empirically justified by the Lottery Ticket Hypothesis (LTH) [17].",1,neutral
", specified by different learning rates and model initialization or ‘rewinding’ strategies [18].",1,neutral
"Let τ denote a training step from which IMP-WR works (i.e., the onset of linear mode connectivity).",1,neutral
Renda et al. (2020) find that both IMP-WR and IMP-LRR find matching subnetworks while standard finetuning (with fixed small learning rates) does not.,2,positive
"As shown in (Renda et al., 2020), this produces networks that perform equivalently to IMP-WR.",1,neutral
"However, IMP-WR has a special property: it can be retrained from an iteration early in training.",2,positive
"IMP with Weight Rewinding (IMP-WR) is described in Algorithm 1 (Frankle et al., 2020).",1,neutral
"D IMP-LRR SUBNETWORKS CAN BE RETRAINED FROM AN EARLY REWIND POINT
IMP with learning rate rewinding (IMP-LRR) has been shown to exceed the performance of standard fine tuning, and match the performance of IMP-WR (Renda et al., 2020).",2,positive
Both IMP-LRR and IMP-WR can be used to find matching subnetworks.,1,neutral
"We also study two variants of IMP with different retraining strategies in Section 3.4: IMP with LR rewinding (IMP-LRR) and IMP with finetuning (IMP-FT) (Renda et al., 2020).",2,positive
"IMP-FT is similar to IMP-LRR but instead of repeating the entire LR schedule, we continue training at the final low LR for the same number of steps: w(L) = AT (m(L) w(L−1), T ).",2,positive
"7 demonstrates that both IMP-WR and IMP-LRR reequilibriate the weight distribution after pruning, i.e. a retrained network once again contains a substantial fraction of small magnitude weights that are amenable to pruning.",1,neutral
Renda et al. (2020) show that this finetuning underperforms IMP-WR in final test accuracy.,2,positive
Renda et al. (2020) show that fine tuning underperforms IMP-WR in final test accuracy.,2,positive
"Alternative methods that attain matching performance at the sparsity levels as IMP also feature iterative pruning and retraining (Renda et al., 2020; Savarese et al., 2020).",2,positive
"To alleviate this, Renda et al. (2020) propose a middle ground between IMP-WR and FINE TUNING called IMP with LEARNING RATE REWINDING (IMP-LRR).",2,positive
"4: IMP with LR rewinding (IMP-LRR) and IMP with finetuning (IMP-FT) (Renda et al., 2020).",2,positive
"In the IMP-WR framework proposed by Frankle et al. (2020) after each pruning step the network is rewound to an early rewind point wτ , and from that point on the network is retrained with the new sparsity pattern.",2,positive
"Why does pruning a larger fraction in one iteration destroy the actionable information in the mask? Fourth, why does retraining allow us to prune more weights? A variant of IMP that uses a different retraining strategy (learning rate rewinding) also successfully identifies matching subnetworks while another variant (finetuning) fails (Renda et al., 2020).",1,neutral
"The axial subspace associated with mask m(L) is a colored subspace, the pruned rewind point m(L) wτ is the circle in this subspace, the level L solution w(L) obtained from
Algorithm 1: Iterative Magnitude Pruning-Weight Rewinding (IMP-WR) (Frankle et al., 2020)
1: Initialize a dense network w0 ∈ Rd and a pruning mask m(0) = 1d. 2: Train w0 for τ steps to wτ .",1,neutral
"As shown in Renda et al. (2020), this produces networks that perform equivalently to IMP-WR.",1,neutral
"To alleviate this, Renda et al. (2020) propose a middle ground between IMP-WR and IMP-FT called learning rate rewinding (IMP-LRR).",2,positive
"Successful retraining strategies such as weight and learning rate (LR) rewinding (Renda et al., 2020) both do this while finetuning (FT), an unsuccessful retraining strategy, does not.",1,neutral
"A variant of IMP that uses a different retraining strategy (learning rate rewinding) also successfully identifies matching subnetworks while another variant (finetuning) fails (Renda et al., 2020).",2,positive
", 2020) or follow a computationally expensive cycle of pruning and retraining for multiple iterations (Gale et al., 2019; Savarese et al., 2020; You et al., 2019; Frankle & Carbin, 2019; Renda et al., 2019).",2,positive
LTH based pruning is able to further reduce DCRNN-PMD complexity by almost 50% with negligible performance loss.,2,positive
"have varying potential for complexity reduction, and we apply an iterative pruning and fine-tuning approach based on the lottery ticket hypothesis (LTH)[8] to operate the learned NLC methods at their optimal complexity.",1,neutral
LTH-based weight pruning is then applied to reduce complexity of each model and obtain performance results at each complexity level.,1,neutral
"For this, we account for the fact that different NN models have varying potential for complexity reduction, and we apply an iterative pruning and fine-tuning approach based on the lottery ticket hypothesis (LTH)[8] to operate the learned NLC methods at their optimal complexity.",1,neutral
"Then, to effectively retrain the pruned model to compensate for the removed weights, we apply LTH[8], wherein we rewind the learning rate schedule before fine-tuning.",2,positive
"We use a pruning approach similar to[5], but rather than applying an absolute threshold, we gradually prune the relatively small weights and rewind the learning rate schedule before fine tuning[17].",2,positive
"Unstructured Sparsity prunes the model without any sparsity pattern constraint [43, 17, 28, 12, 14, 60, 18, 31, 51, 6, 14, 26, 9, 4, 34].",1,neutral
"Magnitude pruning, which selects the pruning elements by their absolute values, is the most widely used method [43, 17, 28, 12, 14, 60, 18, 31, 40, 33].",1,neutral
"The sparsified models often ends up with similar or worse performance (because of the extra complexity to compress and decompress the parameters) than their dense counterparts [2, 32, 43, 21, 30, 15, 59, 50, 10].",1,neutral
"Also, methods such as network pruning (Han et al., 2015b; Renda et al., 2020; Hu et al., 2016; Iandola et al., 2016; Goetschalckx et al., 2018), weight sharing (Chen et al.",1,neutral
"Later, weight/learning rate rewinding techniques (Frankle et al. 2020; Renda, Frankle, and Carbin 2020) was proposed to scale up LTs to larger networks and datasets.",2,positive
"We use unstructured weight pruning, which 140 can achieve higher sparsities than structured prun- 141 ing (Renda et al., 2020), and has comparatively 142 standard implementations.",2,positive
"We use unstructured weight pruning, which can achieve higher sparsities than structured pruning (Renda et al., 2020), and has comparatively",2,positive
"We introduce these settings because previous works (Renda et al., 2020; Le & Hua, 2021; Wang et al., 2021a; 2023) have showed that retraining LR has a great impact on the final performance.",2,positive
"Recent papers (Renda et al., 2020; Le & Hua, 2021) report an interesting phenomenon: During retraining, a larger learning rate (LR) helps achieve a significantly better final performance, empowering the two baseline methods, random pruning and magnitude pruning, to match or beat many more complex…",2,positive
"Recent papers (Renda et al., 2020; Le & Hua, 2021) report an interesting phenomenon: During retraining, a larger learning rate (LR) helps achieve a significantly better final performance, empowering the two baseline methods, random pruning and magnitude pruning, to match or beat many more complex pruning algorithms.",2,positive
"Previous work discovers winning tickets in LMs via either unstructured (Frankle and Carbin, 2019; Renda et al., 2020; Chen et al., 2020) or structured pruning techniques (Michel et al., 2019; Prasanna et al., 2020; Chen et al., 2020).",2,positive
"Previous work discovers winning tickets in LMs via either unstructured (Frankle and Carbin, 2019; Renda et al., 2020; Chen et al., 2020) or structured pruning techniques (Michel et al.",2,positive
"[7]: traditional (fine-tuning [11], gradual magnitude pruning [37]), rewinding lottery-tickets (weight-rewinding [8], learning-rate rewinding [30]), and initialization lottery-tickets (edgepopup [27], biprop [6]).",1,neutral
"Namely, lottery-ticket style pruning methods [8, 30, 27, 6] were able to provide robustness gains on CIFAR-10-C [7].",1,neutral
"Parameter magnitude is an effective importance metric for model pruning (Han et al., 2015b;a; Paganini & Forde, 2020; Zhu & Gupta, 2018; Renda et al., 2020; Zafrir et al., 2021).",1,neutral
"When the context is clear, we simply write S.
Parameter magnitude is an effective importance metric for model pruning (Han et al., 2015b;a; Paganini & Forde, 2020; Zhu & Gupta, 2018; Renda et al., 2020; Zafrir et al., 2021).",1,neutral
"…covers multiple ways to make use of sparsity during and after model training including static and dynamic sparsity (e.g., βLasso (Neyshabur, 2020)), iterative hard thresholding (e.g., Lottery Ticket Hypothesis with various pruning strategies (Frankle & Carbin, 2018; Renda et al., 2020)) and others.",2,positive
", Lottery Ticket Hypothesis with various pruning strategies (Frankle & Carbin, 2018; Renda et al., 2020)) and others.",2,positive
"• Learning rate rewinding: retrains the pruned model for T − t epochs from the final parameters, but reuse the learning rate schedule from the iteration t at the early training phase (Renda et al., 2020).",2,positive
", 2020), it will still be too hard to distinguish from the irregular fluctuations of test accuracy in pruning cases (Frankle & Carbin, 2019; Liu et al., 2019; Renda et al., 2020).",0,negative
"…in test error around the interpolation point of dense models is reported on the noiseless CIFAR dataset (Nakkiran et al., 2020), it will still be too hard to distinguish from the irregular fluctuations of test accuracy in pruning cases (Frankle & Carbin, 2019; Liu et al., 2019; Renda et al., 2020).",2,positive
"This observation is aligned to previous works [18], where the authors observe the performance improvement when pruning.",2,positive
Related work [18] shows that iterative pruning achieves better performance than one-shot pruning.,1,neutral
", 2019), weight rewinding, and fine-tuning (Renda et al., 2020).",2,positive
"After that, LR-Rewinding is applied.",1,neutral
"Once the final pruning rate is reached, the network is re-trained following a warm-restart schedule, which can be called LR-Rewinding [17].",0,negative
"(Frankle et al., 2019; Renda et al., 2020) further scale up LTH to larger datasets and networks by weight rewinding techniques that re-initialize the subnetworks to the weight from the early training stage instead of scratch.",2,positive
"Fine-Tuned Deep Model Fine-tuning is a process that takes a model that has already been trained (pre-trained) for one task and returns it or tweaks the same model to perform a classification task [43,44].",1,neutral
"Fine-tuning is a process that takes a model that has already been trained (pre-trained) for one task and returns it or tweaks the same model to perform a classification task [43,44].",1,neutral
"Previous work (Renda et al.,
2020) presents evidence that rewinding remaining weights to earlier learned values may be beneficial for compressibility.",1,neutral
"In the standard pruning scenario (Renda et al., 2020; Han et al., 2015a), training simply resumes with the remaining weights after each iteration of pruning.",2,positive
"Previous work (Renda et al., 2020) presents evidence that rewinding remaining weights to earlier learned values may be beneficial for compressibility.",1,neutral
"Later investigations point out [19, 67] that the original LTH can not scale up to larger networks and datasets unless leveraging the weight rewinding techniques [19, 67].",0,negative
"[159], Le and Hua [160] show that fine-tuning a pruned network with a learning rate schedule rewound to earlier stages in training outperforms classical fine-tuning of sparse networks with small learning rates.",1,neutral
"[13], nöral ağların budanmasında en yaygın kullanılan “ince ayar” yöntemi yerine “geri sarma” yöntemini önermiştir.",0,negative
"However, some pruning techniques may result in more training efforts during model building stage, such as methods that are based on the procedures of train, prune and fine-tune/re-train [34], [35].",1,neutral
", 2021; Fischer & Burkholz, 2022) and whether LTs are identifiable by contemporary pruning algorithms to solve complex problems with large scale architectures (Frankle et al., 2020; Renda et al., 2020).",1,neutral
"Most LT experiments are conducted in the context of image classification and thus rely heavily on pruning convolutional and residual neural network architectures to reduce the number of trainable parameters of a neural network (LeCun et al., 1990a; Mozer & Smolensky, 1989; Han et al., 2015; Frankle & Carbin, 2019; Srinivas & Babu, 2016; Lee et al., 2020; You et al., 2020; Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a; Liu et al.; Weigend et al., 1991; Savarese et al., 2020a; Chen et al., 2021c; Savarese et al., 2020b; LeCun et al., 1990b; Hassibi & Stork, 1992; Dong et al., 2017; Li et al., 2017; Molchanov et al., 2017; Zhang et al., 2021c).",1,neutral
"…in finding task specific computational neural network structures (Su et al., 2020; Ma et al., 2021; Fischer & Burkholz, 2022) and whether LTs are identifiable by contemporary pruning algorithms to solve complex problems with large scale architectures (Frankle et al., 2020; Renda et al., 2020).",1,neutral
"…Mozer & Smolensky, 1989; Han et al., 2015; Frankle & Carbin, 2019; Srinivas & Babu, 2016; Lee et al., 2020; You et al., 2020; Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a; Liu et al.; Weigend et al., 1991; Savarese et al., 2020a; Chen et al., 2021c; Savarese et al., 2020b; LeCun et…",1,neutral
"Many pruning methods have been proposed to reduce the number of neural network parameters during training (LeCun et al., 1990a; Mozer & Smolensky, 1989; Han et al., 2015; Frankle & Carbin, 2019; Srinivas & Babu, 2016; Lee et al., 2020; You et al., 2020; Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a; Liu et al.; Weigend et al., 1991; Savarese et al., 2020a; Chen et al., 2021c) or thereafter (Savarese et al.",1,neutral
"While these methods can achieve superior theoretical compression rates (Renda et al., 2020), their use remains impractical without specialised hardware that can take advantage of sparsity (Han et al., 2016).",2,positive
"While these methods can achieve superior theoretical compression rates (Renda et al., 2020), their use remains impractical without specialised hardware that can take advantage of sparsity (Han et al.",2,positive
"The third class of methods, perform the pruning while training [22, 23, 33, 55, 61, 64].",1,neutral
"Besides a few papers that discuss post-training pruning [74, 40], most existing studies such as [37, 75, 8, 71, 76] are implemented as in-training pruning.",1,neutral
"Unstructured pruning is another major research direction, especially gaining popularity in the theory of Lottery Ticket Hypothesis (Frankle and Carbin, 2019; Zhou et al., 2019; Renda et al., 2020; Frankle et al., 2020; Chen et al., 2020a).",1,neutral
"Un- 535 structured pruning is another major research direc- 536 tion, especially gaining popularity in the theory 537 of Lottery Ticket Hypothesis (Frankle and Carbin, 538 2019; Zhou et al., 2019; Renda et al., 2020; Frankle 539 et al., 2020).",2,positive
We put the emphasis on the efficiency of BN-folding for DNN acceleration by comparing the cost in terms of accuracy to reach similar acceleration with pruning and quantization methods.,2,positive
There are two main approaches to DNN inference acceleration: pruning and quantization.,1,neutral
"Pruning consists in removing elements of the graph defined by the DNN [Renda et al., 2020].",1,neutral
"To tackle this problem, many solutions for DNN acceleration have been developed including, but not limited to, pruning [Frankle and Carbin, 2018; Lin and others, 2020; Yvinec et al., 2021] and quantization [Zhao and others, 2019; Meller et al., 2019;
∗Contact Author
Finkelstein et al., 2019].",2,positive
"The naive approach [Jacob and others, 2018] is widely documented and applied in DNN acceleration.",1,neutral
"Deep Neural Networks (DNNs) are ubiquitous in various subdomains of computer vision, e.g. in Image Classification [He et al., 2016], Object Detection [He et al., 2017] or Semantic Segmentation [Chen and others, 2017].",1,neutral
"On the other hand, the ""rewinding late"" rule is found by [15, 49] to scale up LTH to larger networks and datasets.",1,neutral
"• Rewinding has minor impact: Unlike [15, 49], we find that ”late rewinding” technique does not have a notable effect on style transfer subnetworks.",2,positive
"Then, we prune individual weights with the lowest-magnitudes globally throughout the network [22, 49].",2,positive
"Unlike the feature global transformation strategy employed by AdaIN, SANet is able to flexibly match the local semantically nearest style features onto the content features, i.e., local transformation based, thanks to the introduction of the attention mechanism.",2,positive
"Evidence of the existence of LTH has been shown great success in various fields [5, 17, 49, 57], and its property has been studied widely [6, 15, 31, 44].",1,neutral
Considering that rewinding paradigm is found to be necessary to identify winning tickets [49] for large networks nowadays.,1,neutral
"Once the procedure is finished, we identify the least important graph frequencies and remove them from the model, then we retrain the remaining parameters using the same scheduler as for baseline architectures, what is referred to as LR-rewinding [11] in the literature.",2,positive
"Once the procedure is finished, we identify the least important graph frequencies and remove them from the model, then we retrain the remaining parameters using the same scheduler as for baseline architectures, what is referred to as LR-rewinding [12] in the literature.",2,positive
"However, there is an growing body of work on pruning during training or at initialization (Frankle & Carbin, 2019; Lee et al., 2019; Liu et al., 2019; Lee et al., 2020; Wang et al., 2020; Renda et al., 2020; Tanaka et al.,
2020; Frankle et al., 2021; Zhang et al., 2021).",2,positive
"However, there is an growing body of work on pruning during training or at initialization (Frankle & Carbin, 2019; Lee et al., 2019; Liu et al., 2019; Lee et al., 2020; Wang et al., 2020; Renda et al., 2020; Tanaka et al., 2020; Frankle et al., 2021; Zhang et al., 2021).",2,positive
"in the latest methods [16, 20, 21], making it the best in this field .",1,neutral
"This shows that given equal number of epochs, GEM-MINER, which prunes at initialization, can close the gap to Learning rate rewinding [34] which is a prune-after-training method.",1,neutral
[34] is a pruning after training algorithm and just outputs a high accuracy subnetwork and hence the sanity checks do not apply to it.,2,positive
[34] show that rewinding the learning rate as opposed to weights(like in IMP) leads to the best performing,1,neutral
IMP [9] ✗ ✓ ✓ 2850 epochs SNIP [24] ✓ ✓ ✗ 1 epoch GraSP [43] ✓ ✓ ✗ 1 epoch SynFlow [42] ✓ ✓ ✗ 1 epoch Edge-popup [33] ✓ ✗ ✗ 150 epochs Smart Ratio [41] ✓ ✓ – O(1) Learning Rate Rewinding [34] ✗ – – 3000 epochs GEM-MINER ✓ ✓ ✓ 150 epochs,0,negative
"We tested our method against the following baselines: dense weight training and four pruning algorithms: (i) IMP [10], (ii) Learning rate rewinding [34], denoted by Renda et al.",2,positive
"[34] tested on ResNet-20, CIFAR-10 at 98.",0,negative
"In [27], the authors find that rewinding the parameter to the early training stage of the neural network, rather than the initial value, can bring profits to winning tickets in complicated datasets.",1,neutral
"[27] Alex Renda, Jonathan Frankle, and Michael Carbin.",0,negative
"Although sparsity is beneficial, the current methods (Frankle & Carbin, 2019; Frankle et al., 2020; Renda et al., 2020) often empirically locate sparse critical subnetworks by Iterative Magnitude Pruning (IMP).",2,positive
"Although LTH-based low-complexity neural models have proven competitive prediction performance on several image classification tasks, machine translation [28, 29] and acoustic scene classification [30], and recently have been supported with some theoretical findings [31] related to overparameterization, the effect of LTH on our multimodal task of audio-visual WWS is not unknown.",1,neutral
"They report the best achievable results by these methods and highlight the gap between their performance and two pruning-at-convergence methods, weight rewinding and magnitude pruning (Renda et al., 2020; Frankle et al., 2020).",2,positive
", 90%, 95%) can be achieved without sacrificing the test accuracy; (ii) the located winning ticket maintains undamaged expressive power as its dense counterpart, and can be easily trained from scratch or early-epoch weights (Renda et al., 2020; Frankle et al., 2020a) to recover the full performance.",0,negative
"%) can be achieved without sacrificing the test accuracy; (ii) the located winning ticket maintains undamaged expressive power as its dense counterpart, and can be easily trained from scratch or early-epoch weights (Renda et al., 2020; Frankle et al., 2020a) to recover the full performance.",0,negative
"With the assistance of weight rewinding techniques (Renda et al., 2020; Frankle et al., 2020a), the original LTH can be scaled up to larger networks and datasets.",2,positive
"Initialization (Frankle & Carbin, 2019; Renda et al., 2020) as another
key factor in LTH, also contributes significantly to the existence of winning tickets.",2,positive
"In this paper, we mainly follow the routine notations in (Frankle & Carbin, 2019; Renda et al., 2020).",2,positive
", 2020a) and weight rewinding (Frankle et al., 2020; Renda et al., 2020) could serve as good starting points.",0,negative
"Our baseline is iterative L1-norm weight-based pruning technique (Li et al. 2016; Renda, Frankle, and Carbin 2020) applied iteratively with rewinding.",2,positive
"Among these efforts, DNN pruning is a promising approach (Li et al. 2016; Han, Mao, and Dally 2015; Molchanov et al. 2016; Theis et al. 2018; Renda, Frankle, and Carbin 2020), which identifies the parameters (or weight elements) that do not contribute significantly to the accuracy, and prunes them…",2,positive
"There are two types of rewinding—weight rewinding and learning rate rewinding (Renda, Frankle, and Carbin 2020).",1,neutral
"Model pruning adversaries first prune the victim model using some pruning methods, then finetune the model using a small set of data [26], [35].",1,neutral
"…and perform structure learning, including dynamic sparse training (Evci et al., 2020; Liu et al., 2021b), adaptations (Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a; Liu et al.) of Iterative Magnitude Pruning (IMP) Han et al. (2015); Frankle & Carbin (2019) and sparse…",1,neutral
"Pruning algorithms that search for strong LTs achieve sparsity levels of around 0.5 but not substantially smaller if the resulting models should be able to compete with the accuracy of the entire, trained mother network (Ramanujan et al., 2020a).",1,neutral
"As we have discussed, the LTs that exist with high probability rarely fulfill criteria of interest, such as low sparsity, favorable generalization properties, or adversarial robustness.",1,neutral
"The existence of strong LTs has also been proven formally for networks without (Malach et al., 2020; Pensia et al., 2020; Orseau et al., 2020) and with potentially nonzero biases (Fischer & Burkholz, 2021).",1,neutral
"Our results indicate that state-of-the-art LT pruning methods achieve in general sub-optimal sparsity levels, and are not able to recover LTs that are competitive with a planted ground truth.",2,positive
", 2020) nor LT fine-tuning techniques (Liu et al., 2021a; Renda et al., 2020) find architectures that are competitive with our constructed ground truth tickets.",2,positive
"We utilize our planting framework to answer the question whether LT pruning algorithms that identify subnetworks of randomly initialized neural networks are able to identify highly sparse LTs, ideally in a strong sense but we also analyze weak LTs. Hypothetically, it could be possible that pruning algorithms for weak LTs only have to resort to training the identified LT because a highly sparse strong LT does not exist with high probability.",2,positive
"Strong tickets based on trained neural networks Even though we cannot expect to construct sparse baseline solutions for benchmark image classification tasks, we can leverage the fact that weak LTs can currently be identified at lower sparsity levels than strong LTs (see our experiments).",2,positive
"As many relevant targets have known representations of lower sparsity than what is covered by this bound, we will afterwards propose a planting algorithm to design experiments that can distinguish between algorithmic and fundamental limitations of pruning for strong LTs.",1,neutral
"As these approaches do not identify LTs as subnetworks of randomly initialized NNs, they do not rely on the existence of planted tickets and are therefore beyond the scope of our experimental analysis.",1,neutral
"Similarly, if we insist on finding extremely sparse architectures, it might be necessary to give up the search for initial LTs (Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a).",2,positive
"To answer the question whether state-of-the-art pruning algorithms can find sparse strong LTs in the setting of standard benchmark data, we plant a trained weak LT in a randomly initialized (VGG
like) neural network.",1,neutral
"If this were true, we should be able to find highly sparse LTs with the original pruning algorithm if we ensure the existence of a solution by planting.",1,neutral
We analyze the performance of tickets before training to assess whether they qualify as strong LTs and after training to evaluate whether at least pruning for weak LTs is feasible and can identify LT of sparsities that can compete with our planted ground truth.,2,positive
"In addition, we identify an opportunity to improve state-of-the-art pruning algorithms in order to find strong LTs of better sparsity.",2,positive
"Usually, these methods try to find LTs in a ‘weak’ (but powerful) sense, that is to identify a sparse neural network architecture that is well trainable starting from its initial parameters.",1,neutral
"L G
] 2
2 N
ov 2
02 1
ing LT pruning algorithms solely on standard benchmark datasets (Frankle et al., 2021), but demand the comparison with known ground truth LTs.",2,positive
"We show in Appendix B that neither dynamic sparse training (Evci et al., 2020) nor LT fine-tuning techniques (Liu et al., 2021a; Renda et al., 2020) find architectures that are competitive with our constructed ground truth tickets.",2,positive
"Strong LTs are sparse sub-networks that perform well with the initial parameters, hence do not need to be trained any further (Zhou et al., 2019; Ramanujan et al., 2020b).",1,neutral
"[43] only reports ResNet results (see the paragraph below) on image classification, we implement one-shot and iterative unstructured magnitude pruning baselines [11,43] using PyTorch’s pruning API4.",2,positive
* Renda et al. 2020.,1,neutral
"AQCompress consistently outperforms or achieves competitive performances compared to recently proposed pruning and quantization methods [11, 16, 36, 43, 52].",2,positive
"Many of our experiments compare AQCompress to unstructured pruning [43], which is a well-studied and widely adopted DNN compression method.",2,positive
"We compare AQCompress with state-of-the-art unstructured pruning [11,36,43] and quantization [16,52] approaches that are designed to compress DNNs.",2,positive
"In this paper, we compare AQCompress with iterative unstructured pruning [43] to demonstrate the high compression ratios attained by AQCompress.",2,positive
(iterative) * Renda et al. 2020.,1,neutral
"The key idea of existing methods [11, 10, 49, 22, 29, 15, 51, 43, 46, 35, 18] is to develop effective criteria (e.",1,neutral
"Renda et al. (2020) noted that the learning rate schedule during retraining can have a dramatic impact on the predictive performance of the pruned network and proposed LEARNING RATE REWINDING (LRW), where one retrains the pruned network for Trt epochs using the last T − Trt learning rates ηT−Trt+1,…",1,neutral
"As a baseline performance for a pruned network, we will use the approach suggested by Renda et al. (2020) as it serves as a good benchmark for the current potential of IMP.",2,positive
"While many details of this procedure are not specified or elaborated on in the original paper, Renda et al. (2020) suggested the following complete approach: train a network for T epochs and then iteratively prune 20% percent of the remaining weights and retrain for Trt = T epochs until the desired…",2,positive
"More concretely, for all three levels of sparsity, 90%, 95%, and 98%, IMP meets the baseline laid out by Renda et al. (2020) after only around 100 epochs of retraining, instead of requiring the respectively 2000, 2800, and 3600 epochs used to establish that baseline.",2,positive
"We then study to what degree the retraining phase of IMP can be shortened in the iterative setting compared to the recommendations of Renda et al. (2020) when using an
appropriate learning rate schedule in Section 3.2.",2,positive
"In its full iterative form, for example formulated by Renda et al. (2020), IMP can require the original train time several times over to produce a pruned network, resulting in hundreds of retraining epochs on top of the original training procedure and leading to its reputation for being…",2,positive
This further demonstrates the importance of the learning rate scheme during the retraining phase previously already noted by Renda et al. (2020) and Le & Hua (2021).,2,positive
This approach effectively interpolates between the recommendations of Renda et al. (2020) and Le & Hua (2021) based on a computationally cheap proxy.,2,positive
We have relied on the simple exponential pruning schedule suggested by Renda et al. (2020) for BIMP while GMP relies on a particular schedule defined by a cubic polynomial that effectively leads to pruning larger amounts initially and progressively smaller amounts later in training when compared to…,2,positive
"We empirically find that the results of Li et al. (2020) regarding the Budgeted Training of Neural Networks apply to the retraining phase of IMP, providing further context for the results of Renda et al. (2020) and Le & Hua (2021).",2,positive
"Neural network compression (Renda et al., 2020; Xu et al., 2020) has been proposed to accelerate the deep CNNs computation.",1,neutral
"The following works aim to: (1) study LTs’ properties [36, 37, 38, 39], (2) improve LTs’ performance [40, 41, 42, 43], and (3) extend LTH to various networks/tasks/trainingpipelines [44, 45, 46, 47, 48, 49, 50].",2,positive
"Several state-of-the-art pruning methods (Renda, Frankle, and Carbin 2020; Frankle and Carbin 2019; Frankle et al. 2020) have demonstrated that a large amount of parameters can be removed without sacrificing accuracy.",2,positive
"However, as the network is iteratively pruned, LR rewinding leads to a much higher accuracy (see Figs.1 & 2 in (Renda, Frankle, and Carbin 2020)).",2,positive
"The inspiring performance of pruning methods hinges on a key factor - Learning Rate (LR) - as mentioned in prior works (Renda, Frankle, and Carbin 2020; Frankle and Carbin 2019).",2,positive
"…hyperparameters for ResNet-20 are from (Frankle and Carbin 2019; Frankle et al. 2020), hyperparameters for VGG-19 are from (Frankle and Carbin 2019; Frankle et al. 2020; Liu et al. 2019), and hyperparameters for ResNet-50 are adapted from (Frankle et al. 2020; Renda, Frankle, and Carbin 2020).",2,positive
"This agrees with the results stated in prior works (Renda, Frankle, and Carbin 2020).",0,negative
"In a followup work, Renda, Frankle, and Carbin (2020) propose LR rewinding which rewinds the LR schedule to its initial state during iterative pruning and demonstrate that it can outperform standard fine-tuning.",2,positive
"Similarly, Renda, Frankle, and Carbin (2020) propose LR rewinding and demonstrate it outperforms standard fine-tuning.",2,positive
"One follow-up work (Renda, Frankle, and Carbin 2020) further investigates this phenomenon and proposes a retraining technique called LR rewinding.",2,positive
"Several recent works (Renda, Frankle, and Carbin 2020; Frankle and Carbin 2019) have noticed the important role of LR in network pruning.",2,positive
"We chose to focus on iterative pruning of ReLU-based networks for two reasons: (i) Iterative pruning tends to provide better pruning performance than one-shot pruning as reported in the literature (Frankle and Carbin 2019; Renda, Frankle, and Carbin 2020).",2,positive
"Some follow-on
works (Zhou et al. 2019; Frankle et al. 2019; Renda, Frankle, and Carbin 2020; Malach et al. 2020) investigate this phenomenon more precisely and apply this method in other fields (e.g., transfer learning (Mehta 2019), reinforcement learning and natural language processing (Yu et al.…",1,neutral
"For the proposed S-
Cyc, we evaluate its performance using SGD with momentum = 0.9 and a weight decay of 1e-4 (same as (Renda, Frankle, and Carbin 2020; Frankle and Carbin 2019)).",2,positive
"…to make the original algorithm to find winning tickets (Frankle and Carbin, 2019) more stable: after fine-tuning, Frankle et al. (2019) rewind the parameters to their values after a few iterations rather than their values before training, whereas Renda et al. (2020) also rewind the learning rate.",2,positive
"IMP and its variations [22, 46] succeed in deeper networks like Residual Networks (Resnet)-50 and Bidirectional Encoder Representations from Transformers (BERT) network [11].",1,neutral
"Similarly, F is defined via the refinement procedure used, making it dependent on the choice of optimizer and whether or not the DNN parameters are left alone or “rewound” to their value at a previous point during training (Frankle et al., 2020a; Renda et al., 2020).",2,positive
"As studied in [49], unstructured pruning can easily achieve far better pruning ratios than structured pruning, which is usually more constrained.",1,neutral
"As thoroughly studied in [49], pruning methods are divided into either unstructured or structured approaches.",1,neutral
"Recently, IMP with learning rate rewinding, which repeats the learning rate schedule, shows better results in bigger networks (Renda, Frankle, and Carbin 2020).",2,positive
"We apply unstructured pruning that removes more weights, more precisely LR rewinding [39], to prune the teacher model.",1,neutral
"Unstructured Pruning We use the standard iterative magnitude pruning method for unstructured pruning in our framework, specifically learning rate rewinding (LR rewinding) (Renda, Frankle, and Carbin 2020).",2,positive
The right plot shows the student’s accuracies when different pruning algorithms (LR rewinding [39] and SynFlow [48]) are applied to the teacher.,1,neutral
"Recently, IMP with learning rate (LR) rewinding, which repeats the learning rate schedule, shows better results in bigger networks [39].",1,neutral
"In Section 5, we apply LR rewinding [39] to prune the model, and apply the vanilla KD [20] to distill the pruned teacher.",2,positive
"Recently, interesting solutions for the third stage have been suggested that involve weight rewinding [10] and learning rate rewinding [17].",1,neutral
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets (Zagoruyko and Komodakis [2016]). The new experiments led us to discover the limitations of learning rate rewinding which can worsen pruning results on large architectures. Methodology We implemented the code ourselves in Python with TensorFlow 2, basing our implementation of the paper alone and without consulting the source code provided by the authors. We ran two sets of experiments. In the reproduction set, we have striven to exactly reproduce the experimental conditions of Renda et al. [2020]. We have also conducted additional experiments, which use other network architectures, effectively showing results previously unreported by the authors.",2,positive
"[2019], Renda et al. [2020]). Lottery Ticket Hypothesis from Frankle and Carbin [2019] formulates a hypothesis that for every dense neural network, there exists a smaller subnetwork that matches or exceeds results of the original.",1,neutral
"2 Scope of reproducibility Renda et al. [2020] formulated the following claims: Claim 1: Widely used method of training after pruning: finetuning yields worse results than rewinding based methods (supported by figures 1, 2, 3, 4 and Table 5) Claim 2: Newly introduced learning rate rewinding works as good or better as weight rewinding in all scenarios (supported by figures 1, 2, 3, 4 and Table 5, but not supported by Figure 5) Claim 3: Iterative pruning with learning rate rewinding matches state-of-the-art pruning methods (supported by figures 1, 2, 3, 4 and Table 5, but not supported by Figure 5)",0,negative
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods.",2,positive
"For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones.",2,positive
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019].",2,positive
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets (Zagoruyko and Komodakis [2016]).",2,positive
"For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in He et al. [2016a]. Additionally, to verify the general usefulness of pruning and retraining methods proposed in Renda et al.",2,positive
"For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in He et al. [2016a]. Additionally, to verify the general usefulness of pruning and retraining methods proposed in Renda et al. [2020] we extend the list of tested network architectures to much larger wide residual networks from Zagoruyko and Komodakis [2016].",2,positive
"…to modern neural networks (Han et al. 2015), many works have shown success through various pruning criteria in attaining a much sparse network that performs on par with or even better than the original unpruned network (Guo, Yao, and Chen 2016; Xiao and Wang 2019; Renda, Frankle, and Carbin 2020).",1,neutral
"Since then, many methods have been proposed in both unstructured pruning (Renda, Frankle, and Carbin 2020; Xiao and Wang 2019) and structured pruning (He, Zhang, and Sun 2017; Li et al. 2017).",1,neutral
"We compare our proposed approach with the latest state-ofthe-art in trojan mitigation techniques, including fine-tuning, bridge mode connectivity (BMC), Neural Attention Distillation (NAD), Maxup and Cutmix augmentation [18], [19], and our own version of fine-pruning based on learning rate rewinding [20], which we refer to as Learning-Rate rewinding and Compression, or LRComp.",2,positive
"LSTMs (Renda et al., 2020), and fully-weighted per-trained BERT (Chen et al.",2,positive
"In NLP, previous works find that matching subnetworks exist early in training with Transformers (Yu et al., 2019), LSTMs (Renda et al., 2020), and fully-weighted per-trained BERT (Chen et al., 2020; Prasanna et al., 2020) or Vison-and-Language model (Gan et al., 2021), but not at initialization.",2,positive
"…in large neural networks, the overhead of the pruning-reset-retraining cycle is unaffordable, and Frankle et al. then tried using weights after trained for iterations to reset the pruned network (Frankle et al. 2019; Renda, Frankle, and Carbin 2020) instead of the totally original initial weights.",1,neutral
Renda et al.[42] provide experimental results suggesting that the training strategy of re-training is more significant to the final result.,0,negative
"However, some recent analyses [39], [40], [42] give experimental evidence to show that retraining the model from pre-trained parameters makes no significant difference from retraining from randomly initialized weights.",1,neutral
"Such a finding offers a theoretical insight into the early-bird ticket phenomenon and provides intuition for why discovering high-performing subnetworks is more difficult in large-scale experiments [63, 52, 38].",1,neutral
"To design the progressive pruning schedule, we develop a straightforward heuristic design, following the commonly used schedule in most pruning works [17, 26, 35].",2,positive
"For CNN compression, the general procedure can be largely summarized as: (i) train a full model; (ii) identify and prune the redundant structures to build a slimmer model based on various criteria, including (structured) sparsity (58; 85; 14; 56; 102; 27; 102; 62; 91), Bayesian pruning (101; 65; 59; 81), ranking importance (54; 60; 41; 36; 57; 100), reinforcement learning (37; 7), adversarial robustness (76), scientific control (79), lottery ticket (23; 24; 72), joint quantization learning (80; 90), etc.",1,neutral
"We distinguish our definition of the lottery ticket hypothesis from the weight rewinding technique [5, 12].",1,neutral
"In this paper, we follow the notations from [1, 5].",1,neutral
"Unfortunately, among the various researches on the lottery ticket hypothesis [2, 3, 4, 5, 6, 7, 8], there are many inconsistencies regarding the settings of training recipe, and they further lead to the controversies over the conditions for identifying winning tickets.",0,negative
"In the lottery ticket hypothesis studies, it is a standard setting to use the same learning rate in pretraining (for finding the mask by pruning thereafter) and subnetwork training (for training the sparse model) [1, 5, 12].",1,neutral
IMP(·) prunes 20% of remaining weights per iteration until arriving at target sparsity s [5].,1,neutral
We find the weight rewinding technique [5] consistently improves the subnetwork accuracy.,2,positive
"For IMP(·), we follow the settings in [1, 5] that 20% of the weights are pruned in each iteration.",1,neutral
"The following works [5, 12] extend the subnetwork training from initial weights to the weights at early stage of pretraining (rewinding), and improve the accuracy in more challenging tasks at nontrivial sparsity.",1,neutral
"This finding helps to explain several observations (1) for gradual magnitude pruning (GMP), it is always optimal to end pruning before the second learning rate drop [77, 13]; (2) dynamic sparse training (DST) benefits from a monotonically decreasing pruning rate with cosine or linear update schedule [8, 9]; (3) rewinding techniques [12, 54] outperform fine-tuning as rewinding retrains subnetworks with the original learning rate schedule whereas fine-tuning often retrains with the smallest learning rate.",1,neutral
"The process of post-training pruning typically involves fully pre-training a dense network as well as many cycles of retraining (either fine-tuning [18, 17, 39] or rewinding [12, 54]).",1,neutral
"Recently, posttraining pruning [49, 29, 18, 47, 10, 54, 74, 5, 57, 75] and before-training pruning [31, 30, 67, 63, 6, 11] have been two fast-rising fields, boosted by lottery tickets hypothesis (LTH) [10] and singleshot network pruning (SNIP) [31].",0,negative
"Later on, learning rate rewinding (LRR) [54] was proposed further to improve the re-training performance by only rewinding the learning rate.",2,positive
"This finding makes a connection to the success of the iterative magnitude pruning [10, 54, 5, 6, 65], where usually a pruning process with a small pruning rate (e.",1,neutral
"[32] reported similar findings for different pruning approaches [43, 2] applied to a ResNet-20 model [19] tested on an analogously corrupted dataset, CIFAR-10-C.",0,negative
"We experiment with four models of increasing size (ResNeXt-29, ResNet-18, ResNet-50, WideResNet-18-2), three data augmentation methods (clean, AugMix, Gaussian), two sparsity levels (90%, 95%), and six compression methods (LTH, LRR, EP (layerwise and global), BP (layerwise and global)).",2,positive
"To test the CARD hypothesis, we use: five models (VGG [12, 46] and ResNet [19] style architectures of varying size), five sparsity levels (50%, 60%, 80%, 90%, 95%), and six model compression methods (FT, GMP, LTH, LRR, EP, BP).",2,positive
"Our best performing 6-CARD-Deck using LRR WideResNet-18 models (53.58 MB) sets a new state-of-the-art for CIFAR-10 and CIFAR-10-C accuracies of 96.8% and 92.75%, respectively.",0,negative
"Notably, we found a single LRR CARD (a WideResNet-18 at 96% sparsity) trained with AugMix can attain 91.24% CIFAR-10-C accuracy, outperforming dense ResNeXt-29 trained with AugMix (a state-of-the-art among methods that do not require non-CIFAR-10 training data) by more than 2 percentage points simply by pruning a larger model, i.e., WideResNet-18.",0,negative
", fine tuning [18] and gradual magnitude pruning [60]), “lottery ticket-style” compression approaches [12, 43, 41, 9] can surprisingly be used to create CARDs.",1,neutral
"Similar experiments are performed for CIFAR-100 and CIFAR-100-C, however only WideResNet-18-2 and four model compression methods (LTH, LRR, EP (global), BP (global)) are used.",2,positive
The difference heatmaps show that rewinding methods offer mild to moderate improvements across much of the frequency spectrum with LRR outperforming LTH in a few regions of the heatmap.,1,neutral
"Weight rewinding (LTH) [12, 13] is iterative like GMP but fully trains the network, prunes, rewinds the unpruned weights (and learning rate schedule) to their values early in training, then fully trains the subnetwork before pruning again; learning rate rewinding (LRR) [43] is identical to LTH, except only the learning rate schedule is rewound, not the unpruned weights.",0,negative
"For a comprehensive analysis of existing pruning methods, we introduce a framework inspired by those in [43, 51] that covers traditional-through-emerging pruning methodologies.",2,positive
"To summarize, CARD-Decks can maintain compactness while leveraging additional robustness improvement techniques, LRR CARD-Decks set a new SOTA on CIFAR-10-C and CIFAR-100-C in terms of accuracy and robustness, binary-weight CARD-Decks can provide up to ∼105x reduction in memory while providing comparable accuracy and robustness, and the domain-adaptive CARD-Decks used here are ∼2x faster than the domain-agnostic CARD-Decks as only half of the CARDs are used at inference.",2,positive
"For the best results, this can be iterated, alternatingly pruning weights and retraining the network (Renda et al., 2019).",1,neutral
"For example, Renda et al. (2019) show that complete retraining is superior to just fine-tuning when pruning iteratively.",1,neutral
"We also examined the effectiveness of IMP with different rewinding starting points as studied in [29, 69], and found rewinding initializations bear minimal effect on downstream ASR.",2,positive
"This step is referred to as subnetwork finetuning in the pruning literature [53, 69, 9].",1,neutral
"Pruningmethods [7, 22, 28, 46, 47, 52] require training of a dense network as well as iterative cycles of pruning and retraining.",1,neutral
"Later on, (Frankle et al., 2019a; Renda et al., 2020) scaled up LTH to larger models by early weight rewinding that relaxes the use of original random initialization.",2,positive
"For example, a recent study [11] shows that a 5:96 parameter reduction of ResNet-50 can well retain the accuracy performance of the original network by weight pruning, however, it is only a 1 reduction in filter pruning.",1,neutral
"The hypothesis has successfully shown its success in various fields (Yu et al., 2020; Renda et al., 2020; Chen et al., 2020b), and its property has been studied widely (Malach et al., 2020; Pensia et al., 2020; Elesedy et al., 2020).",2,positive
"It was pointed out by Renda et al. (2020) that subnetworks found by IMP and rewound early in training can be trained to achieve the same accuracy at the same sparsity as subnetworks found by the standard pruning, providing a possibility that rewinding can also help GAN subnetworks.",2,positive
"The hypothesis has successfully shown its success in various fields (Yu et al., 2020; Renda et al., 2020; Chen et al., 2020b), and its property has been studied widely (Malach et al.",2,positive
"In order to scale up LTH to larger networks and datasets, the “late rewinding” technique is proposed by Frankle et al. (2019); Renda et al. (2020).",2,positive
"…labelled data, such as LDI (Lee et al., 2020), DPF (Lin et al., 2020b), PFP (Liebenwein et al., 2020), FT (Li et al., 2017), SoftNet (He et al., 2018), Lottery (Frankle & Carbin, 2018), PoReg (Zhuang et al., 2020), PFF (Meng et al., 2020), OS (Renda et al., 2020) and SCOP (Tang et al., 2020).",2,positive
"Most architecture compression methods rely on an underlying approximation of the predictive function to later perform pruning, wether it can is unstructured or structured (as stated in Renda et al. (2020)), data-driven or data-free, magnitude-based or similarity-based.",1,neutral
"Interestingly, neural models are capable of performing tasks on sizes smaller than they were trained for [9, 10].",1,neutral
"Most of existing results focus on finding unstructured winning tickets via iterative magnitude pruning and rewinding in randomly initialized networks (Frankle et al., 2019; Renda et al., 2020), where each ticket is a single neuron.",2,positive
"We adopt the weight rewinding technique in Renda et al. (2020): We reset the parameters of the winning tickets to their values in the pre-trained weights, and subsequently fine-tune the subnetwork with the original learning rate schedule.",2,positive
"• With the insights from dynamical isometry and OrthP, we unveil two mysteries in pruning: why a larger finetuning LR can improve the final performance significantly [40, 24] (Sec.",2,positive
"Several recent works [40, 24] observed an interesting phenomenon in neural network pruning: A larger finetuning learning rate can improve the final performance significantly.",1,neutral
"Specifically, [40, 24] found that the learning rate (LR) in finetuning holds a critical role in the final performance.",1,neutral
"3 answers why a larger finetuning LR can improve the final performance in pruning [40, 24]; Sec.",1,neutral
"Concretely, we propose the following plausible explanation to the effect of a larger LR in finetuning [40, 24]: A larger LR helps the network converge faster, thus the dynamical isometry (measured by mean JSV) recovers faster.",1,neutral
LEARNING RATE REWINDING (LRW) Renda et al. (2020) propose to reuse the learning rate schedule of the original training when retraining pruned networks.,2,positive
"Although in the previous work, Renda et al. (2020) demonstrated the efficacy of learning rate rewinding across datasets and pruning criteria, there is a lack of understanding of the actual reason behind the success of this technique.",2,positive
"As the performance of larger learning rate schedules such as LRW, SLR, and CLR are rather similar, we select CLR for use in this experiment.",2,positive
"To verify this conjecture empirically, we conduct experiments with different learning rate schedules including learning rate rewinding (Renda et al., 2020) while varying pruning algorithms, network architectures and datasets.",2,positive
"To analyze the effect of retraining a pruned network, we based on learning rate rewinding (Renda et al., 2020) and experiment with different retraining settings.",2,positive
"We selected these works because they do not utilize a similar learning rate value as LRW in the original implementation, i.e., the authors applied a smaller value then the heuristic of LRW.",2,positive
"Renda et al. (2020) found that learning rate rewinding usually saturate at half of original training, thus, we perform on retraining for 80 epochs on CIFAR-10 and 45 epochs on ImageNet.",2,positive
"The retraining step is a critical part in implementing network pruning, but it has been largely overlooked and tend to vary in each implementation including differences in learning rate schedules, retraining budget, hyperparameter choices, etc.
Recently, Renda et al. (2020) proposed a state-of-the-art technique for retraining pruned networks namely learning rate rewinding (LRW).",2,positive
"We first experiment with iterative `1-norm filters pruning on CIFAR-10 and report the results in Figure 4(a,b), we can observe that SLR and CLR also perform comparable or better than LRW in this setting.",2,positive
"…been largely overlooked and tend to vary in each implementation including differences in learning rate schedules, retraining budget, hyperparameter choices, etc.
Recently, Renda et al. (2020) proposed a state-of-the-art technique for retraining pruned networks namely learning rate rewinding (LRW).",2,positive
"In our experiments, CLR usually reaches slightly higher accuracy than LRW and SLR.",2,positive
"This seemingly subtle change in learning rate schedule led to an important result: LRW was shown to achieve comparable performance to more complex and computationally expensive
ar X
iv :2
10 5.",2,positive
"Thus, the value of LRW can be a good heuristic to choose the learning rate for retraining after pruning.",1,neutral
"In our results, cyclic learning rate restarting (CLR) is slightly more efficient than scaled learning rate restarting (SLR) and learning rate rewinding (LRW).",2,positive
"Based on the Lottery Winning Ticket hypothesis (Frankle and Carbin, 2018), (Renda et al., 2020) suggests a weight-rewinding method to explore sub-networks from full-trained models.",2,positive
"Based on the Lottery Winning Ticket hypothesis (Frankle & Carbin, 2018), (Renda et al., 2020) suggests a weight-rewinding method to explore sub-networks from full-trained models.",2,positive
Pruning ResNet32 is more challenging since VGG19 has about 10 times parameters than ResNet32.,2,positive
"Foresight Pruning
Lottery Ticket Hypothesis was proposed in [6], which conjectures and verifies that there exists sparse subnetworks which can be trained directly to achieve even better performance than dense counterparts with less training time.",1,neutral
"[9, 41, 6, 30] follows the idea of using weight magnitude as the criterion.",1,neutral
"Network Pruning [10, 8, 39, 21, 24, 14, 41, 17, 34, 30, 38] has been extensively studied in recent years to reduce the model size and improve the inference efficiency of deep neural networks.",1,neutral
[Gradually Increasing Pruning Rate] We increase the pruning rate gradually to make a smooth transformation from dense to sparse status.,1,neutral
"Finally we review another line of research on Lottery Tickets Hypothesis, SuperMask and Foresight Pruning.",2,positive
"Dense-to-sparse training starts with a dense network and obtains a sparse network at the end of the training [10, 41, 27, 6, 30, 36, 32, 23, 35].",1,neutral
"The results in the literature [8, 22, 39, 21, 30, 18, 5, 35] demonstrate that pruning methods can significantly improve the inference efficiency of DNNs with minimal performance degradation, making the deployment of modern neural networks on resource-limited devices possible.",1,neutral
[30] achieves strong results but needs multiple rounds of pruning,0,negative
"We choose six representative methods PBW (Pruning by Weight, [10]), MLPrune [39], RIGL [5], STR [18], DNW[25], GMP [41]) as baselines.",2,positive
": Best of (Frankle & Carbin, 2019; Renda et al., 2020; Su et al., 2020), obtained from Wang et al.",2,positive
"Other studies concerned hyperparameters modifications [24, 25], and concentrated on the transferability [16, 26] of the pruned networks.",1,neutral
"If, instead, structured pruning [35], [40] is applied to improve regularity, model quality will then drop quickly.",1,neutral
"Lottery Ticket Hypothesis (LTH) [13] suggests the existence of sparse subnetworks in overparameterized neural networks at their random initialization, early training stage, or pre-trained initialization [35, 44, 5, 3, 2].",1,neutral
"That implies ETTs might be able to transfer pruned solutions in general [35] – this is out of the current work’s focus, but would definitely be our future work.",2,positive
", [7] reported iterative GMP to outperform the single-shot variant, [33] reported occasional near-catastrophic performance hits while pruning, [22] reported weight and learning-rate rewinding methods to outperform fine-tuning and [1, 12] showed that the keep-ratio time-traces of their schemes resembled an exponential-decay, despite their methods not imposing any explicit pruning schedule.",0,negative
"The cyclic learning-rate schedule is similar to the learning-rate rewinding scheme of [22], albeit with a much shorter cycle length.",1,neutral
Another application of GMP was by [22] where weight and learningrate rewinding schemes were used to achieve competitive pruning performances.,1,neutral
• We shed light on weight and learning-rate rewinding methods of re-training [22].,0,negative
"For example, [22] reports that fine-tuning a ResNet50 with a momentum of 0.",1,neutral
"…parameters for removal; we focus on randomly selected parameters (random pruning) and removing the smallestmagnitude parameters (magnitude pruning), though other approaches exist in the literature (Han et al., 2015; Renda et al., 2020; Lee et al., 2019; Wang et al., 2020; Blalock et al., 2020).",1,neutral
"There are many approaches to selecting parameters for removal; we focus on randomly selected parameters (random pruning) and removing the smallestmagnitude parameters (magnitude pruning), though other approaches exist in the literature (Han et al., 2015; Renda et al., 2020; Lee et al., 2019; Wang et al., 2020; Blalock et al., 2020).",1,neutral
[5] proposes learning rate rewinding in addition to weight rewinding to more efficiently find the winning lottery tickets.,1,neutral
"Here we focus specifically on parameter pruning: the selective removal of weights based on a particular ranking [4, 5, 7, 45, 57, 58].",1,neutral
"Many works have been proposed to investigate the behaviors on weight pruning (Tanaka et al., 2020; Ye et al., 2020; Renda et al., 2020; Malach et al., 2020).",1,neutral
"The following work (Frankle et al., 2019; Renda et al., 2020) also confirms that, for deeper networks and using relatively large learning rates, the winning property can hardly be observed.",1,neutral
"We perform preliminary experiments to investigate the potential for incorporating compression techniques such as pruning [22, 41, 52] as part of our tuning framework.",2,positive
"We create pruned models (using the approach in [52]) for MobileNet, VGG16, and ResNet18 on CIFAR-10.",2,positive
"…on pruning neural networks at initialization instead of after training (Lee et al., 2019; 2020; Wang et al., 2020; Tanaka et al., 2020; Frankle et al., 2021) as well as on what parameters to use when these networks are retrained (Frankle & Carbin, 2019; Liu et al., 2019b; Renda et al., 2020).",2,positive
", 2021) as well as on what parameters to use when these networks are retrained (Frankle & Carbin, 2019; Liu et al., 2019b; Renda et al., 2020).",2,positive
Frankle et al. (2019); Renda et al. (2020) introduced the “late rewinding” techniques to scale up LTH.,2,positive
"…ResNet-50 network generated by unstructured pruning can achieve a 5.96× compression ratio, with the same accuracy as the original network, but it can only achieve 1× com-
∗The first two authors equally contribute to this paper.
pression in the case of structured sparsity (Renda et al., 2020).",2,positive
"pression in the case of structured sparsity (Renda et al., 2020).",1,neutral
"Specifically, we employ the magnitude-based pruning method (Renda et al., 2020; Gale et al., 2019) during the forward process.",2,positive
"Therefore, we tested the elementwise pruning on the relatively high compression rates (×2, ×3, ×4) compared to the filter pruning [39].",2,positive
"However, the sparsity that the filter pruning can achieve is often lower than that of element-wise pruning [39].",1,neutral
"In most (if not all) LTH literature (Frankle & Carbin, 2019; Frankle et al., 2019; Renda et al., 2020), the re-training step takes care of the masked subnetwork, which is re-trained with the same initialization (or rewinding) and same training recipe as its dense network.",2,positive
"For NLP models, previous work has also found that matching subnetworks exist in transformers and LSTMs (Yu et al., 2019; Renda et al., 2020).",2,positive
"The evaluation metrics also follow the standards [69, 16, 7, 3].",0,negative
"LTH has been widely explored in image classification [31, 55, 76, 28, 34, 72, 80, 81, 56, 15], natural language processing [35, 83, 69, 67, 9], generative adversarial networks [17, 8], graph neural networks [14], and reinforcement learning [83].",1,neutral
"Subnetworks (mTi , θ0) and (mTi , θ5%) are found on the task Ti with the random initialization θ0 [31] and an early rewinding weights θ5% [69].",2,positive
"Followup investigations [55, 35] scale up LTH by rewinding approaches [33, 69], that re-initializes the subnetwork from",2,positive
"6Early weight rewinding [69, 32] improves the quality of found matching subnetworks.",2,positive
"In the previous larger-scale settings of LTH for CV [31, 69], the matching subnetworks are found at an early point in training.",1,neutral
"We use the default implementations and hyperparameters [69, 10, 40, 16, 7, 51, 3].",2,positive
", applying At ) and then removing a portion of weights with the globally smallest magnitudes [38, 69].",1,neutral
"We also investigate the efficacy of methods introduced by [12, 38, 13, 42] such as iterative magnitude pruning, late resetting, early bird training, and layerwise pruning in the context of object recognition.",1,neutral
"More recently, the Rewind [57] and Group-Sparsity [49] algorithms have been demonstrated to be state-of-the-art compression algorithms.",1,neutral
[57] proposed the rewind algorithm which is similar to simple fine-tuning of the network to regain the loss in accuracy incurred during the pruning step.,1,neutral
We prune the model using unstructured pruning at a sparsity of 81% and the rewind algorithm [57] (see Section 2 for a more detailed description of rewinding).,2,positive
"[61], while studying the lottery ticket hypothesis, came up with a method called learning rate rewinding, which proposes replacing the fine-tuning step with a full retraining stage that uses the weights of the trained and pruned network as a new initialization.",1,neutral
"Inspired by this finding, we rewound the unimportant component to the initialization values Frankle and Carbin (2019; Renda et al. (2020) and fine-tune
them together with the other trained components for a few more steps.",2,positive
"Network Pruning On the other hand, our work is also related to the studies on network pruning, including but not limited to [39, 40, 41, 42, 43, 44, 45, 46].",1,neutral
"In light of the aforementioned related works, we observed that most claims and conclusions are based on experiments with unstructured pruning, with the exception of Renda et al. (2020) [20] who present preliminary experiments with structured pruning.",2,positive
"More recently, Renda et al. (2020) [20] introduced the Learning Rate Rewinding method, a novel retraining approach, as a variation of weight rewinding.",1,neutral
"…iteratively pruned networks found by weight rewinding, learning rate rewinding and
2The percentage of remaining parameters was calculated from the reported compression ratio values of Renda et al. (2020) [20].
their randomly initialized counterparts and considering local and global pruning.",2,positive
"2) Learning Rate Rewinding: In Figure 5, we present the results from applying the learning rate (LR) rewinding retraining technique as proposed by Renda et al. (2020) [20].",2,positive
"For comparison, Renda et al. (2020) [20] apply structured pruning with learning rate Rewinding for a small range of compression levels, from approximately 86.96% to 58.82% of the remaining parameters on ResNet-56 and 92.60% to 79.36% of the remaining parameters on ResNet-34.",2,positive
"In addition, despite we considered an architecture (VGG16 [6]) different from those investigated by Renda et al. (2020) [20] (ResNet-56 and ResNet-34 [35]), they were all convolutional neural networks.",2,positive
"2) Global pruning: In the global pruning, as applied by Frankle et al. (2019) [18], Salama et al. (2019) [25], Renda et al. (2020) [20], the elements are removed without taking into account their location.",2,positive
"On the other hand, our work is also related to the studies on network pruning, including but not limited to [40], [41], [42], [43], [44], [45], [46], [47].",1,neutral
"As to our choice of LC, recent work [35], [36] has shown that for high-sparsity regimes, LC performs really well, especially when the optimization becomes difficult with stringent resource constraints imposed.",1,neutral
"Recent works show that their parameters can be reduced by more than 90% without accuracy drop [3], [4].",1,neutral
"One of popular frameworks for compressing a neural network consists of three steps: pre-training the network, removing unimportant components, and re-training the remaining structure [3], [7], [4].",1,neutral
"However, previous studies do not consider time constraints for the pruning phase [12, 19], which differs from human sleep that lasts for a fixed amount of time.",1,neutral
"The model without fine-tuning (dotted) outperforms some of the related works [1, 2, 4, 7, 8, 10, 15, 21, 22, 23].",1,neutral
"parameter count against state-ofthe-art pruning approaches including [1, 2, 4, 7, 8, 10, 15, 21, 22, 23].",1,neutral
"Note that models outperforming the proposed approach for stronger compression levels [6, 10], see Fig.",1,neutral
"Iterative pruning [4, 5, 6] and access to substantial fine-tuning [7, 8, 9] or even re-training from scratch [5, 10] allows many techniques to arrive at the (nearly) original levels of performance at the cost of additional 30% to 300% of the original amount of training.",2,positive
"On the other hand, a very clever method was recently presented by Renda et al. (2020), with much success on a variety of neural networks.",1,neutral
"Simple yet powerful algorithms, such as the one by Renda et al. (2020) have already supplied an empirical confirmation of this hypothesis, by providing a procedure through which to obtain high-performing pruned versions of a variety of models.",1,neutral
"This is partially confirmed by Renda, Frankle, and Carbin (2020) which shows that restarting the learning rate schedule from the pruning solution performs better than rewinding the weights.",2,positive
"Another line of research loosely connected to our work is to reduce inference time via pruning less significant weights and/or converting the model to low-precision (aka quantization) (Han et al., 2016; Howard et al., 2017; Iandola et al., 2016; Renda et al., 2020; Frankle and Carbin, 2019).",2,positive
"Another line of research loosely connected to our work is to reduce inference time via pruning less significant weights and/or converting the model to low-precision (aka quantization) (Han et al., 2016; Howard et al., 2017; Iandola et al., 2016; Renda
et al., 2020; Frankle and Carbin, 2019).",2,positive
"And as pointed out in [37], learning rate rewinding usually surpasses weights rewinding, so we mostly focus on learning rate rewinding.",1,neutral
"iterative pruning cannot consistently outperform one shot pruning, and even when iterative pruning is better, the gap is small, as shown in [27] and [37].",1,neutral
"Different from initial tickets, [37] propose a learning rate rewinding method that improves beyond weights rewinding [10].",1,neutral
"In addition, we also find a very recent pruning method in ICLR 2020 [37] for obtaining “partiallytrained tickets” can pass our sanity checks.",0,negative
"• Partially-trained tickets: Different from initial tickets, partially-trained tickets are constructed by first training the network, pruning it, and then rewinding the weights to some middle stage [37].",0,negative
"In this section, we study pruning methods in a very recent ICLR 2020 paper [37], which is classified as partially-trained tickets (Section 2.",0,negative
"We then apply our sanity checks on the pruning methods in four recent papers from ICLR 2019 and 2020 [23, 9, 41, 37].",0,negative
The methods used in [37] include weights rewinding and learning rate rewinding.,1,neutral
We then combine our insights of random tickets with these partially-trained tickets and propose a method called “hybrid tickets” (Figure 2) which further improves upon [37].,2,positive
"Magnitude pruning is a state-of-the-art method for one-shot pruning after training (Renda et al., 2020).",2,positive
"After pruning at step t of training, we subsequently train the network further by repeating the entire learning rate schedule from the start (Renda et al., 2020).",2,positive
"These subnetworks are as small as those found by inference-focused pruning methods after training (Appendix B, Renda et al., 2020), meaning it may be possible to maintain this level of sparsity for much or all of training.",1,neutral
"We
∗Equal contribution.
prune in an iterative, lottery-ticket fashion to identify Transformers at competitive sparsities with no drop in task performance (Renda et al., 2020; Yu et al., 2020; Brix et al., 2020).",2,positive
"Renda et al. (2020) propose learning rate (LR) rewinding, where the learning rate is rewound to a value earlier in training, but the weights remain unchanged.",2,positive
"prune in an iterative, lottery-ticket fashion to identify Transformers at competitive sparsities with no drop in task performance (Renda et al., 2020; Yu et al., 2020; Brix et al., 2020).",2,positive
"Note that, in addition to the polynomial terms, the relationship between accuracy and model sparsity is further modeled through an additional exponential term – a reasonable modeling assumption supported by prior knowledge of accuracy-sparsity curves in the pruning literature [9, 12, 25, 38, 31, 1, 10].",1,neutral
"The intuition behind mapping specific ranges of weights per layer to approximate multiplication derives from the weight pruning based on magnitude [42], [43].",1,neutral
"In order to find an efficient weight-to-approximate mode mapping and reduce the number of evaluated solutions, we employ a four-step methodology based on the concepts of layer significance and weight magnitude [42], [43] (Figure 5).",1,neutral
"[18] show that, in other settings, IMP subnetworks rewound early in training reach the same accuracies at the same sparsities as subnetworks found by this standard pruning procedure.",2,positive
"Winning tickets have not been found in larger-scale settings, including transformers [23, 17] and LSTMs [17, 18] for NLP tasks.",1,neutral
"Although the lottery ticket hypothesis has been evaluated in the context of NLP [17, 18] and transformers [17, 23], it remains poorly understood in the context of pre-trained BERT models.",1,neutral
"The lottery ticket hypothesis in NLP. Previous work has found that matching subnetworks exist early in training on Transformers and LSTMs [17, 18] but not at initialization [23].",1,neutral
"Practically speaking, this would allow us to replace a pre-trained BERT with a smaller subnetwork while retaining the capabilities that make it so popular for NLP work.",2,positive
We use standard hyperparameters for several downstream NLP tasks as shown in Table 1.,2,positive
"• Unlike previous work in NLP, we find these subnetworks at (pre-trained) initialization rather after some amount of training.",2,positive
We conclude that the lottery ticket observations from other computer vision and NLP settings extend to BERT models with a pre-trained initialization.,2,positive
"Although many ideas from this literature have been applied to Transformer models for NLP, compression ratios are typically lower than in computer vision (e.g., 2x vs. 5x in [23]).",1,neutral
"These trends have been especially pronounced in natural language processing (NLP), where massive BERT models—built on the Transformer architecture [5] and pre-trained in a selfsupervised fashion—have become the standard starting point for a variety of downstream tasks [6].",1,neutral
", using At ) and pruning individual weights with the lowest-magnitudes globally throughout the network [41, 18].",1,neutral
"To do so, we adopt a strategy in which we iteratively prune the 10% of lowestmagnitude weights and train the network for a further t iterations from there (without any rewinding) until we have reached the target sparsity [41, 40, 18].",2,positive
"Although the lottery ticket hypothesis has been evaluated in the context of NLP [17, 18] and transformers [17, 23], it remains poorly understood in the context of pre-trained BERT models.2 To address this gap in the literature, we investigate how the transformer architecture and the initialization resulting from the lengthy BERT pre-training regime behave in comparison to existing lottery ticket results.",2,positive
The existence of winning tickets here implies that the BERT pre-trained initialization has different properties than other NLP settings with random initializations.,1,neutral
"Previous work has found that matching subnetworks exist early in training on Transformers and LSTMs [17, 18] but not at initialization [23].",2,positive
"Claim 4: When matching subnetworks are found, they reach the same accuracies at the same sparsities as subnetworks found using standard pruning [18].",2,positive
"As pre-training becomes increasingly central in NLP and other areas of deep learning [7, 8], our results demonstrate that the lottery ticket observations—and the tantalizing possibility that we can train smaller networks from the beginning—hold for the exemplar of this class of learning algorithms.",2,positive
"In a similar setting, as demonstrated in the model pruning literature [7, 10, 26, 33, 38], having different pruning ratios for different layers of the network can further improve results over a single ratio across layers.",1,neutral
"Following works try to extend [39], theoretically prove [34], understand [53], and improve the training process [40] of LTH.",1,neutral
"However, state-of-the-art unstructured pruning methods typically rely on Magnitude Pruning (MP) (Han et al., 2015), a simple and computationally cheap criterion based on weight magnitude, that works extremely well in practice (Renda et al., 2020).",1,neutral
", 2015), a simple and computationally cheap criterion based on weight magnitude, that works extremely well in practice (Renda et al., 2020).",1,neutral
"While the fine tuning-phase would require hyper-parameters optimisation, Renda et al. (2020) showed that using the same ones as for the original training usually leads to good results.",1,neutral
"The later is typically used in the literature (Zeng & Urtasun, 2019; Wang et al., 2019; Frankle & Carbin, 2018; Renda et al., 2020).",1,neutral
", 2019), and is used in current state-of-the-art methods (Renda et al., 2020).",2,positive
", 2019), or in an iterative training/fine-tuning fashion (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015; Frankle & Carbin, 2018; Renda et al., 2020).",1,neutral
"…Pruning (MP) (Han et al., 2015), is a popular pruning criterion in which the saliency is simply based on the norm of the parameter:
sMPk = θ 2 k (4)
Despite its simplicity, MP works extremely well in practice (Gale et al., 2019), and is used in current state-of-the-art methods (Renda et al., 2020).",1,neutral
"We hypothesize that this could be one of the reasons behind the success of the Lottery Ticket and Rewinding experiments (Frankle & Carbin, 2018; Frankle et al., 2019; Renda et al., 2020).",2,positive
"…before training (Lee et al., 2019b; Wang et al., 2020), during training (Louizos et al., 2017; Molchanov et al., 2017; Ding et al., 2019), or in an iterative training/fine-tuning fashion (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015; Frankle & Carbin, 2018; Renda et al., 2020).",1,neutral
", 2015) that gets stateof-the-art tradeoffs between error and unstructured density (Gale et al., 2019; Renda et al., 2020).",2,positive
"This method is a standard way to prune (Han et al., 2015) that gets stateof-the-art tradeoffs between error and unstructured density (Gale et al., 2019; Renda et al., 2020).",1,neutral
"For IMP, we use a practice called weight rewinding (Frankle et al., 2020; Renda et al., 2020), in which the values of unpruned weights are rewound to their values earlier in training (in our case, epoch 10) and the training process is repeated from there to completion.",2,positive
", 2020) to 5x (Renda et al., 2020) with no increase in error.",0,negative
"In practice, pruning can reduce the parameter-counts of contemporary models by 2x (Gordon et al., 2020) to 5x (Renda et al., 2020) with no increase in error.",1,neutral
"We use per-weight magnitude pruning because it is generic, well-studied (Han et al., 2015), and produces stateof-the-art tradeoffs between density and error (Gale et al., 2019; Blalock et al., 2020; Renda et al., 2020).",2,positive
", 2015), and produces stateof-the-art tradeoffs between density and error (Gale et al., 2019; Blalock et al., 2020; Renda et al., 2020).",2,positive
"Recent works have focused on pruning methods that include the pruning process in the training phase [28, 32, 25, 22].",1,neutral
"Recent works have focused on pruning methods that include the pruning process in the training phase [35, 42, 30, 27].",1,neutral
"They also argue that using the initial weights values is fundamental to achieve competitive performance, which is degraded when starting from a random initialization [18].",1,neutral
"[11, 19, 27] pruned the least important weights in the network.",1,neutral
"This resulted in a renewed interest in sparse deep learning and model pruning (Renda et al., 2020; Chen et al., 2020; 2021) and more recently in the area of sparse reinforcement learning (Arnob et al., 2021; Sokar et al., 2021).",1,neutral
"This resulted in a renewed interest in sparse deep learning and model pruning (Renda et al., 2020; Chen et al., 2020; 2021) and more recently in the area of sparse reinforcement learning (Arnob et al.",1,neutral
"The comparisons in Table 2 further show that inheriting weights from the EB tickets favor the generalization of retraining as compared to both the random initialization and “over-cooked” weights, aligning well with the recent discussion between rewinding and fine-tuning (Renda et al., 2020).",2,positive
"We implemented structured pruning and LR factorization based on the original papers – (Renda et al., 2020) for pruning and (Tai et al., 2016) for LR factorization.",2,positive
"ApproxCaliper currently supports two existing approximation techniques (which can be applied in combination): (1) structured pruning based on Learning Rate Rewinding (LRR) (Renda et al., 2020) and (2) low-rank factorization (or LR factorization) based on (Tai et al., 2016).",2,positive
"We implemented structured pruning and LR factorization based on the original papers – (Renda et al., 2020) for pruning and (Tai et al.",2,positive
"As baselines for comparison, we apply pruning using LRR (Renda et al., 2020) or LR factorization using the technique in (Tai et al.",2,positive
"As baselines for comparison, we apply pruning using LRR (Renda et al., 2020) or LR factorization using the technique in (Tai et al., 2016) to all the candidate neural network architectures with the constraint to retain the same accuracy as the original model, and pick the most efficient pruned…",2,positive
"ApproxCaliper currently supports two existing approximation techniques (which can be applied in combination): (1) structured pruning based on Learning Rate Rewinding (LRR) (Renda et al., 2020) and (2) low-rank factorization (or LR factorization) based on (Tai et al.",2,positive
", 2020), heads (Renda et al., 2019; Wang et al., 2020), and layers (Fan et al.",1,neutral
"…methods (He et al., 2017; Molchanov et al., 2019; Guo et al., 2020) aim to search a sub-model for large-size models by pruning unimportant dimensions (McCarley et al., 2019; Prasanna et al., 2020), heads (Renda et al., 2019; Wang et al., 2020), and layers (Fan et al., 2019; Sajjad et al., 2020).",2,positive
Recent works also introduce gradual magnitude pruning which can outperform iterative pruning algorithms Gale et al. (2019); Renda et al. (2020).,2,positive
"…on ImageNet dataset and VGG-19 on Tiny-ImageNet dataset, comparing our method with PFB (Liebenwein et al. (2019)), HRank (Lin et al. (2020)), ILP (Renda et al. (2020)), Sparse Structure Selection (SSS) (Huang & Wang (2018)), NN Slimming (Liu et al. (2017)), and Eigendamage (Wang et al. (2019)).",2,positive
"For example, on ResNet-56, with 1% of accuracy loss, the amount of parameter reduction achieved by our method is higher than that of ILP (Renda et al. (2020)) by 34.",2,positive
"We compare our method with the state-of-the-art works, including SCOP (Tang et al. (2020)), HRank (Lin et al. (2020)), ILP (Renda et al. (2020)), PPR (Zhuang et al. (2020)), RFR (He et al. (2017)), GAL (Lin et al. (2019)), DCP (Zhuang et al. (2018)), GBN (You et al. (2019)), CP (He et al. (2017)),…",2,positive
"For ResNet-50 on ImageNet, the learning rate increases to 0.256 in a warmup mechanism during the first 5 epochs, and decays with a factor of 0.1 at epochs 30, 60, 80 (Renda et al. (2020); Frankle et al. (2019)).",2,positive
"For example, on ResNet-56, with 1% of accuracy loss, the amount of parameter reduction achieved by our method is higher than that of ILP (Renda et al. (2020)) by 34.51%; and with 75% FLOPs reduction, the accuracy achieved by our method is higher than PPR (Zhuang et al. (2020)) by 0.87%.",2,positive
"The widely used structured pruning method—L1-norm based structured pruning Renda et al. (2020) removes filters with the lowest L1-norm values, whereas the most recent method—Polarization-based structured pruning Zhuang et al. (2020) improves it by using a regularizer on scaling factors of filters and pruning filters whose scaling factors are below than a threshold.",1,neutral
"For example, on ResNet-56, with 1% of accuracy loss, the amount of parameter reduction achieved by our method is higher than that of ILP (Renda et al. (2020)) by 34.51%; and with 75% FLOPs reduction, the accuracy achieved by our method is higher than PPR (Zhuang et al. (2020)) by 0.",2,positive
"The widely used structured pruning method—L1-norm based structured pruning Renda et al. (2020) removes filters with the lowest L1-norm values, whereas the most recent method—Polarization-based structured pruning Zhuang et al. (2020) improves it by using a regularizer on scaling factors of filters and pruning filters whose scaling factors are below than a threshold. These two methods both assume that the weight values of a filter can be used as an indicator about the importance of that filter, much like how LTH uses weight values in unstructured pruning. However, we observe that weight-based structured pruning methods cannot produce accurate pruned models. For example, to prune ResNet-56 on CIFAR-10 with no loss in top-1 accuracy, L1-norm based structured pruning can achieve at most only 1.15ˆ model compression, and Polarization-norm based structured pruning can achieve at most only 1.89ˆ inference speedup. The reason is that, some filters, even though their weight values are small, can still produce useful non-zero activation values that are important for learning features during backpropagation. That is, filters with small values may have large activations. We propose that the activation values of filters are more effective in finding unimportant filters to prune. Activations like ReLu enable non-linear operations, and enable convolutional layers to act as feature detectors. If an activation value is small, then its corresponding feature detector is not important for prediction tasks. So activation values, i.e., the intermediate output tensors after the non-linear activation, not only detect features of training dataset, but also contain the information of convolution layers that act as feature detectors for prediction tasks. We present a visual motivation in Figure 1(a). The figure shows the activation output of 16 filters of a convolution layer on one input image. The first image on the left is the original image, and the second image is the input features after data augmentation. We observe that some filters extract image features with high activation patterns, e.g., the 6th and 12th filters. In comparison, the activation outputs of some filters are close to zero, such as the 2nd, 14th, and 16th. Therefore, from visual inspection, removing filters with weak activation patterns is likely to have low impact on the final accuracy of the pruned model. There is a natural connection between our activation-based pruning approach and the related attention-based knowledge transfer works (Zagoruyko & Komodakis (2016)).",1,neutral
"DNN pruning is a promising approach (Li et al. (2016); Han et al. (2015); Molchanov et al. (2016); Theis et al. (2018); Renda et al. (2020)), which identifies the parameters (or weight elements) that do not contribute significantly to the accuracy and prunes them from the network.",2,positive
"The widely used structured pruning method—L1-norm based structured pruning Renda et al. (2020) removes filters with the lowest L1-norm values, whereas the most recent method—Polarization-based structured pruning Zhuang et al. (2020) improves it by using a regularizer on scaling factors of filters…",1,neutral
"The widely used structured pruning method—L1-norm based structured pruning Renda et al. (2020) removes filters with the lowest L1-norm values, whereas the most recent method—Polarization-based structured pruning Zhuang et al.",1,neutral
"To make matters worse, since we know that there are often multiple winning tickets available from the training process (Renda et al., 2020), picking one ticket but not the other will potentially yield a different set of preferred clustering schemes for the network.",0,negative
"This technique is referred as weights rewinding and we denote the range of [k1, k2] as the tickets window (Renda et al., 2020).",1,neutral
"…on the lottery ticket hypothesis, scholars have their disagreements on whether to reinitialize the weights of W ′
t to their initial values (namely, k = 0 for k in W ′
k) (Frankle & Carbin, 2019), to near initialization (0   k t) (Renda et al., 2020), or just to reset randomly (Liu et al., 2019).",1,neutral
"Yet some simple tricks like dynamic pruning rate, soft-pruning (keep the pruned components updatable until very end), and weight reinitialization before fine-tuning may often provide most algorithms another performance boost (Li et al., 2017; He et al., 2018; Renda et al., 2020; Liu et al., 2019).",1,neutral
"This is because, for early or even concurrent arts on the lottery ticket hypothesis, scholars have their disagreements on whether to reinitialize the weights of W ′ t to their initial values (namely, k = 0 for k in W ′ k) (Frankle & Carbin, 2019), to near initialization (0 < k t) (Renda et al., 2020), or just to reset randomly (Liu et al.",1,neutral
"We address the first challenge by consulting model-generated information — in this case, the empirical findings on Lottery Ticket Hypothesis (LTH) and related literature on weights shifting — to develop a scoring system that identifies the optimal clustering scheme among options per each convolutional layer (Frankle & Carbin, 2019; Renda et al., 2020).",2,positive
Our proposed method can also be applied to experiments outside of Renda et al. (2020).,2,positive
"In addition, our method consults empirical findings on the lottery ticket hypothesis and its derived literature regarding weights shifting (Frankle & Carbin, 2019; Renda et al., 2020; Zhou et al., 2019), and we propose a novel greedy kernel pruning algorithm that is again simple, efficient, yet effective — more on this in Section 3.",2,positive
"…information — in this case, the empirical findings on Lottery Ticket Hypothesis (LTH) and related literature on weights shifting — to develop a scoring system that identifies the optimal clustering scheme among options per each convolutional layer (Frankle & Carbin, 2019; Renda et al., 2020).",1,neutral
"Thus, we can decide which model we will prune on, identify its tickets window by consulting experiment results from Renda et al. (2020), and truncate some epochs in such window to conduct multiple evaluations.",2,positive
"…method consults empirical findings on the lottery ticket hypothesis and its derived literature regarding weights shifting (Frankle & Carbin, 2019; Renda et al., 2020; Zhou et al., 2019), and we propose a novel greedy kernel pruning algorithm that is again simple, efficient, yet effective — more…",2,positive
"We address both challenges by relying on the same finding from Renda et al. (2020), which demonstrates that for any k where 0 < k1 ≤ k ≤ k2 < t, W ′
k can be a winning ticket.",1,neutral
"A vast amount of research has been done to demonstrate the existence of winning tickets across different networks and datasets, making the lottery ticket hypothesis one of the most tested inductive biases among neural networks (Renda et al., 2020).",1,neutral
"However, unstructured pruning is widely considered in academia (LeCun, Denker, and Solla 1989; Renda, Frankle, and Carbin 2019) and with improvements in sparse tensor support on embedded hardware might become the predominant method for practical applications in the future.",2,positive
"[30] — retraining for 25% of the original training time (within the rewinding safe zone [30]) and, notably, rewinding the learning rate schedule to its value before the last 25% of the training schedule.",0,negative
"We introduce these settings because previous works [47,23,56] showed that finetuning LR has a great impact on the final performance.",2,positive
"Recent structured pruning works [47,23] showed an interesting phenomenon: During finetuning, a larger learning rate (LR) helps achieve a significantly better final performance (e.",1,neutral
", applyingAt ); (2) eliminating a portion of insignificant weights with the globally smallest magnitudes (Han et al., 2015; Renda et al., 2020) so that the model only has si% of weights remaining (i.",2,positive
"Follow-up efforts (Renda et al., 2020; Frankle et al., 2020b) introduce the effective weight rewinding techniques to scale up LTH to large networks on large-scale datasets.",2,positive
"…on a datasetD (i.e., applyingADt ); (2) eliminating a portion of insignificant weights with the globally smallest magnitudes (Han et al., 2015; Renda et al., 2020) so that the model only has si% of weights remaining (i.e., the sparsity); (3) rewinding model weights to θ (θ = θ0, the original…",2,positive
", 2022) or follow a computationally expensive cycle of pruning and retraining for multiple iterations (Savarese et al., 2020; You et al., 2019; Frankle & Carbin, 2019; Renda et al., 2019; Dettmers & Zettlemoyer, 2019).",2,positive
"…(Lee et al., 2018; Wang et al., 2020; Tanaka et al., 2020; Sreenivasan et al., 2022) or follow a computationally expensive cycle of pruning and retraining for multiple iterations (Savarese et al., 2020; You et al., 2019; Frankle & Carbin, 2019; Renda et al., 2019; Dettmers & Zettlemoyer, 2019).",2,positive
"We note that this learning rate schedule is different from prior work on pruning, which typically uses a single decay cycle [Kusupati et al., 2020, Singh and Alistarh, 2020, Peste et al., 2021], or dynamic learning rate rewinding, e.g. [Frankle et al., 2019, Renda et al., 2020].",2,positive
"The works (Gale, Elsen, and Hooker 2019; Yu et al. 2019; Renda, Frankle, and Carbin 2020) show that the subnetworks exist early in the training instead of initialization on Transformers.",2,positive
"The following work (Renda, Frankle, and Carbin 2020) extends (rewinds) the training of the subnetwork from initialization to the early stage of pretraining, which improves the accuracy of the subnetwork in more challenging tasks.",2,positive
"…from the image classification tasks, the LTH is also imported in many other research areas (Chen et al. 2020; Mallya, Davis, and Lazebnik 2018; Gale, Elsen, and Hooker 2019; Yu et al. 2019; Renda, Frankle, and Carbin 2020; Chen et al. 2020; Prasanna, Rogers, and Rumshisky 2020; Girish et al. 2021).",2,positive
"Other studies concerned hyperparameters modifications (Frankle et al., 2020b; Renda et al., 2020), and concentrated on the transferability (Morcos et al., 2019; Sabatelli et al., 2020) of the pruned networks.",1,neutral
"Other studies concerned hyperparameters modifications (Frankle et al., 2020b; Renda et al., 2020), and concentrated on the transferability (Morcos et al.",1,neutral
", 2019), fine tuning the learning rates (Renda et al., 2020), more efficient training (You et al.",2,positive
"However, as training continues (e.g., at the 2,000th iteration), these non-active weights with small gradients will have large magnitude and hence are important to model accuracy (Renda, Frankle, and Carbin 2020; Zafrir et al. 2021).",2,positive
"Many works prune the model in the training phase [33], [34], [35], [36], [37].",1,neutral
It was first noticed by Renda et al. (2020) that the precise learning rate schedule during retraining can have a dramatic impact on the predictive performance of the pruned network.,2,positive
"Clearly, the choice of the weight decay affects both the accuracy of the dense model as well as the accuracy of the pruned model when using the approach by Renda et al. (2020).",1,neutral
"Clearly, we see that SLR benefits from a larger weight decay, while the approach by Renda et al. (2020) is suffering from an increased penalty on the weights.",2,positive
"As a baseline performance for a pruned network, we will use the approach suggested by Renda et al. (2020) as it serves as a good benchmark for the current potential of IMP.",2,positive
"-sparsity tradeoffs (Han et al., 2015; Renda et al., 2020).",2,positive
"For the respective results of the algorithm by Renda et al. (2020) we observe test accuracies of 92.91% (±0.39%), 93.08% (±0.54",0,negative
"Our interest lies in exploring these claimed disadvantages of IMP through rigorous and consistent computational experimentation with a focus on recent advancements concerning the retraining phase, see the results of Renda et al. (2020) and Le and Hua (2021).",2,positive
"…is repeatedly removing only a small fraction of the parameters followed by extensive retraining, is said to achieve results on the Pareto frontier (Renda et al., 2020), its iterative nature is also considered to be computationally tedious, if not impractical: “iterative pruning is computationally…",2,positive
This is in fact how it appears to be used by Han et al. (2015) and how it is for example presented by Renda et al. (2020).,1,neutral
"While only the iterative approach, that is repeatedly removing only a small fraction of the parameters followed by extensive retraining, is said to achieve results on the Pareto frontier (Renda et al., 2020), its iterative nature is also considered to be computationally tedious, if not impractical: “iterative pruning is computationally intensive, requiring training a network 15 or more times consecutively for multiple trials” (Frankle and Carbin, 2018), leading Liu et al.",2,positive
"Renda et al. (2020) for example suggested the following approach: train a network for T epochs and then iteratively prune 20% percent of the weights and retrain for Trt = T epochs using LRW, i.e., use the same learning rate scheme as during training, until the desired sparsity is reached.",2,positive
"…baseline in the case of a 1e-4 weight decay within the given retraining time frame, we note that SLR easily outperforms the LRW-based proposal by Renda et al. (2020) when considering the weight decays that also lead to the best performing dense model, which is a strong indicator that it is…",2,positive
"For a fair comparison we also include LRR (Renda et al., 2020) which uses a pre-trained network and multiple rounds of pruning and retraining by leveraging learning rate rewinding.",2,positive
"…N-BEATS
WER FLOPs PPL FLOPs SMAPE FLOPs
Dense 12.2 4.53G 18.6 927.73G 8.3 41.26M
SNIP (Lee et al., 2019) 14.3 2.74G 24.6 398.92G 10.1 21.45M LRR (Renda et al., 2020) 13.7 2.61G 23.1 339.21G 9.3 14.47M RigL (Evci et al., 2020) 13.9 2.69G 22.4 326.56G 10.2 15.13M SIS (Ours) 13.1 2.34G 21.1…",0,negative
"(Renda et al., 2020) proposed weight rewinding technique instead of vanilla fine-tuning post-pruning.",1,neutral
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets (Zagoruyko and Komodakis [2016]). The new experiments led us to discover the limitations of learning rate rewinding which can worsen pruning results on large architectures. Methodology We implemented the code ourselves in Python with TensorFlow 2, basing our implementation of the paper alone and without consulting the source code provided by the authors. We ran two sets of experiments. In the reproduction set, we have striven to exactly reproduce the experimental conditions of Renda et al. [2020]. We have also conducted additional experiments, which use other network architectures, effectively showing results previously unreported by the authors.",2,positive
"[2019], Renda et al. [2020]). Lottery Ticket Hypothesis from Frankle and Carbin [2019] formulates a hypothesis that for every dense neural network, there exists a smaller subnetwork that matches or exceeds results of the original.",1,neutral
"2 Scope of reproducibility Renda et al. [2020] formulated the following claims: Claim 1: Widely used method of training after pruning: finetuning yields worse results than rewinding based methods (supported by figures 1, 2, 3, 4 and table 5) Claim 2: Newly introduced learning rate rewinding works as good or better as weight rewinding in all scenarios (supported by figures 1, 2, 3, 4 and table 5, but not supported by figure 5) Claim 3: Iterative pruning with learning rate rewinding matches state-of-the-art pruning methods (supported by figures 1, 2, 3, 4 and table 5, but not supported by figure 5)",0,negative
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods.",2,positive
"For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones.",2,positive
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019].",2,positive
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets (Zagoruyko and Komodakis [2016]).",2,positive
"For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in He et al. [2016a]. Additionally, to verify the general usefulness of pruning and retraining methods proposed in Renda et al.",2,positive
"For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in He et al. [2016a]. Additionally, to verify the general usefulness of pruning and retraining methods proposed in Renda et al. [2020] we extend the list of tested network architectures to much larger wide residual networks from Zagoruyko and Komodakis [2016].",2,positive
"As the specific rewind points, we follow the setting in [3].",1,neutral
"As a remedy, recent studies [2, 3] demonstrate that a sparse sub-network can still be obtained by using the rewinding technique, which is to re-train it from early-phase training weights or learning rates of the dense model, rather than from random initialization.",1,neutral
We use Pytorch[18] to implement all experiments and follow hyperparameters identically in [3].,2,positive
The recent work [3] demonstrates that an early training point at 18]% of the dense training process is a good choice for ResNet[c]-56 on CIFAR-10.,2,positive
"[3] presented a simplified version called learning rate rewinding, i.",2,positive
"The rewinding studies [2, 3] find that directly inheriting late stage model weights cannot yield any competitive lottery ticket.",0,negative
"The follow-up works [2, 3] prove that for deeper networks, either rewinding the weights to the ones at early training iteration, or rewinding the learning rate schedule to the early phase, can achieve better performance than rewinding to iteration 0, when seeking lottery tickets in larger networks.",2,positive
"Scaling up LTH to larger networks calls on the “late rewinding"" technique [19, 3].",1,neutral
"Similar to previous studies [1, 3], we perform experiments on CIFAR-10, Tiny-ImageNet and ImageNet.",2,positive
"Liu et al. (2019); Gale et al. (2019) further scale up LTH by rewinding (Frankle et al., 2020a; Renda et al., 2020).",2,positive
", 2020b), natural language processing (Gale et al., 2019; Yu et al., 2020; Renda et al., 2020; Chen et al., 2020c; Desai et al., 2019; Chen et al., 2020e), graph neural network (Chen et al.",1,neutral
"(2019) further scale up LTH by rewinding (Frankle et al., 2020a; Renda et al., 2020).",2,positive
"…2020; You et al., 2020; Chen et al., 2021c; Ma et al., 2021; Chen et al., 2020b), natural language processing (Gale et al., 2019; Yu et al., 2020; Renda et al., 2020; Chen et al., 2020c; Desai et al., 2019; Chen et al., 2020e), graph neural network (Chen et al., 2021b), and reinforcement…",2,positive
"Many works have been proposed to investigate the behaviors on weight pruning (Tanaka et al., 2020; Ye et al., 2020; Renda et al., 2020; Malach et al., 2020).",1,neutral
"The following work (Frankle et al., 2019; Renda et al., 2020) also confirms that, for deeper networks and using relatively large learning rates, the winning property can hardly be observed.",1,neutral
": Best of (Frankle & Carbin, 2019; Renda et al., 2020; Su et al., 2020), obtained from Wang et al.",2,positive
"For example, a recent study [30] showed that a 5.",1,neutral
"Although subsequent studies of LTH [8], [25], [29] dramatically reduced the training time,",0,negative
"Later on, [20, 61] scaled up LTH to larger models on large-scale datasets via weight rewinding techniques.",2,positive
", either from their initial values or some later point) [9, 23, 58].",1,neutral
"[58] Alex Renda, Jonathan Frankle, and Michael Carbin.",0,negative
Hybrid Ticket: the idea of smart-ratios can be applied to the recently proposed learning rate rewinding method [3] with further performance improvement.,2,positive
"The recently proposed learning rate rewinding method [3] passes the sanity-checking, meaning the data information is important to keep the performance the same.",1,neutral
"In the algorithm, we prune the language model using gradual pruning (Liu et al., 2018b; Renda et al., 2019).",2,positive
"from neural networks that do not contribute significantly to the model performance, and remove them for compressing original dense neural networks [49, 68, 52, 57, 3, 30, 35, 64, 39, 27, 29, 4, 40, 61, 33, 56, 16, 21].",1,neutral
"However, the majority of neural network pruning methods require to train a dense neural network to generate a pruned sparse network, as it is difficult to train a pruned network from scratch [20, 62, 34, 23, 41, 67, 49, 68, 52, 57, 3, 30, 74, 24, 25, 63, 35, 64, 39, 27, 29, 4, 40], although there are few pruning before training approaches [32, 61, 33, 56, 16, 21].",1,neutral
"A workflow to generate networks that satisfy this 2:4 constraint, using a form of Learning Rate Rewinding [28], has been empirically verified to maintain accuracy across a wide range of networks and tasks (with the exception of the aforementioned small, parameter-efficient networks).",1,neutral
"This is not only resource-intensive but there is also an on-going discussion on whether and how we should fine-tune [36] or rewind to initial weights [23] or train from scratch [29], making it difficult to prefer one approach over another.",0,negative
"[29] and Gale, Elsen, and Hooker [36] show that none of the OSP techniques dominate one another, and IMP outperforms them all [41].",1,neutral
"However, state-of-the-art applications of magnitude pruning now employ iterative magnitude pruning (IMP) [3, 40], where a fraction of the smallest weights are pruned on each round, followed by rewinding training back to an earlier iteration [40] or simply rewinding the learning rates [41].",1,neutral
(1) Unstructured pruning : Rewind Renda et al. (2020).,2,positive
"This is not only resource-intensive but there is also an on-going discussion on whether and how we should fine-tune [36] or rewind to initial weights [23] or train from scratch [29], making it difficult to prefer one approach over another.",0,negative
", 2019a), fine tuning the learning rates (Renda et al., 2020), more efficient training (You et al.",2,positive
"In the subsequent studies, the focus goes on finding this lottery ticket for more competitive tasks by pruning with weight rewinding(Frankle et al., 2019a), fine tuning the learning rates (Renda et al., 2020), more efficient training (You et al., 2019; Brix et al., 2020; Girish et al., 2021).",2,positive
"For some experiments, we also modified line 6 for techniques such as late rewinding and learning rate rewinding (Renda et al., 2020).",2,positive
"We ran late rewinding experiments from Renda et al. (2020), where instead of rewinding the weights to θ0 in line 6 of Algorithm A1, we set the weights to their values at epoch 2.",2,positive
"Our main experiments are mainly based on the lottery ticket procedure from Frankle & Carbin (2018) along with follow up work (Renda et al., 2020) for unstructured pruning.",2,positive
"Our learning rate rewinding experiments also encapsulate the fine-tuning experiments (Renda et al., 2020).",2,positive
"Methods that fine-tune weights after pruning typically train at a smaller learning rate than the training phase to find the mask (Han et al., 2015; Renda et al., 2020), but other hyperparameters are held constant.",1,neutral
"1 MODIFIED PRUNING PROCEDURES Our main experiments are mainly based on the lottery ticket procedure from Frankle & Carbin (2018) along with follow up work (Renda et al., 2020) for unstructured pruning.",2,positive
"This also explains why fine-tuning in Renda et al. (2020) did worse than learning rate rewinding: a larger LReval is better, and fine-tuning is effectively the same as learning rate rewinding but with a smaller and worse LReval.",1,neutral
"Moreover, the introduction of weight rewinding is spurred by the so-called rewinding ticket (Renda et al., 2020; Frankle et al., 2020).",2,positive
"Following (Frankle & Carbin, 2019; Renda et al., 2020), IMP iteratively prunes p 1 n(i) (%) non-zero weights of m(i−1) θ(i−1) over n rounds at T .",1,neutral
"Following (Frankle & Carbin, 2019; Renda et al., 2020), IMP iteratively prunes p 1
n(i) (",1,neutral
"More Technical Details of Top-down Pruning In our implementation, we set p 1
n(i) = 20% as (Frankle & Carbin, 2019; Renda et al., 2020) and adjust {n(i)} to control the pruning schedule of IMP over sequential tasks.",2,positive
"(Renda et al., 2020) further compares different retraining techniques and endorses the effectiveness of rewinding.",2,positive
"The early phase of the project will consist of implementing a range of established methods for network sparsification [3, 8, 11, 16], in order to put in place a solid testbed for our projects.",2,positive
"A great body of work [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16] has been dedicated to this task.",1,neutral
"Recent works show that their parameters can be reduced by more than 90% without accuracy drop [4], [5].",1,neutral
"One of popular frameworks for compressing a neural network consists of three steps: pre-training the network, removing unimportant components, and re-training the remaining structure [4], [8], [5].",1,neutral
"In a similar setting, as demonstrated in the model pruning literature (Gordon et al., 2018; Liu et al., 2019b; Morcos et al., 2019; Renda et al., 2020), having different pruning ratios for different layers of",1,neutral
"In a similar setting, as demonstrated in the model pruning literature (Gordon et al., 2018; Liu et al., 2019b; Morcos et al., 2019; Renda et al., 2020), having different pruning ratios for different layers of
the network can further improve results over a single ratio across layers.",1,neutral
"…rate schedules other than compressing the original learning rate schedule from the fist phase of training; it is plausible that other learning rate schedules better tuned to the technique or network could train even faster (Renda et al., 2020), but we do not evaluate these alternative schedules.",0,negative
"We also do not evaluate learning rate schedules other than compressing the original learning rate schedule from the fist phase of training; it is plausible that other learning rate schedules better tuned to the technique or network could train even faster (Renda et al., 2020), but we do not evaluate these alternative schedules.",0,negative
"Ex157 amples include weight pruning (dropping/skipping weights or filters) [34, 35, 10, 11], perforated 158 convolutions [36, 37] (skipping and interpolating some output computations), integer quantiza159 tion [35] (INT8, INT4) and others.",1,neutral
"neural network models [10, 11, 12] (used for visual perception) can be used to trade off accuracy 43 for computation time improvements in closed-loop visual navigation tasks, even making it possible 44 to run both navigation and a real-time object detection task concurrently (three CNNs in total) on a 45 single Raspberry Pi 4, without significantly hurting system robustness.",1,neutral
"To scale up LTH for large networks and large-scale datasets, weight rewinding techniques (Renda et al., 2020; Frankle et al., 2020a) is proposed.",2,positive
"For a fair comparison, we follow the standard implementations and hyperparameters in (Renda et al., 2020) for OMP, LTH, RP, and PI experiments, as shown in Table 1.",2,positive
"This phenomenon has been extensively discussed in the literature, as evidenced by studies such as [17,21].",1,neutral
