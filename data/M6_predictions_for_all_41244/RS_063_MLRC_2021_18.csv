text,target_M6_predict,target_predict_M6_label
"However, due to the existence of hard samples, the pursuit of invariant features and indiscriminate elimination of domain-specific information can lead to adverse effects on within-class variations and amplify the empirical source risk, ultimately undermining the model’s generalization performance [44], [45].",2,positive
"There have been many works on the unsupervised domain adaptation (UDA) [10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21], the multi-target domain adaptation (MTDA) [22; 23; 23; 23; 24; 25] and the domain generalization (DG) [26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37] for semantic segmentation.",1,neutral
"mismatch is to take the semantic labels into account [114], [115].",1,neutral
"In particular, they are devoted to either acquiring invariant causal mechanisms [17] or recovering causal features [27, 2] for image classification problem.",1,neutral
This is reasonable because the causal factors for decision-making are often stable patterns that persist across samples and domains [44].,1,neutral
Deep neural networks (DNNs) have shown impressive performance in computer vision tasks over the past few years.,1,neutral
"greatly impaired the applications of DNNs [33, 61], as training and test data often come from different distributions in reality.",1,neutral
"The work is supported by NSFC Program (62222604, 62206052, 62192783), China Postdoctoral Science Foundation Project (2023T160100), Jiangsu Natural Science Foundation Project (BK20210224), and CCF-Lenovo Bule Ocean Research Fund.
greatly impaired the applications of DNNs [33, 61], as training and test data often come from different distributions in reality.",2,positive
"Following this line, some works [34, 38, 39, 63, 74] achieve the goal by the pre-defined causal graph to learn the key features involving class labels.",1,neutral
"In addition, many past works also utilize causality to find stable features across domains [1, 28].",1,neutral
"• Fundamental of Causal Robustness [20] • Causal Data Augmentation [2] • Invariant Causal Representation Learning [4, 14] • Invariant Causal Mechanism Learning [1] (5) Conclusion and Future Trends Finally, we conclude our tutorial with a discussion on open challenges and future directions with respect to the following: • Future directions and Promises: Causality, security, and privacy • Challenges: Limitation of data and unidentifiability",2,positive
"The extent of overlap between these representations indicates the model’s ability to extract invariant features, which are crucial for generalization [17].",1,neutral
"Causality is recently explored to capture the invariance across different platforms [16, 17].",1,neutral
[42] CLS 2021 generalizability Causal inference Domain (I),1,neutral
"Mainstream Domain Generalization studies [1,23,28,32, 33, 33, 42] primarily focus on extracting invariant representations from source domains that can be effectively generalized to the target domain, which remains inaccessible during training.",1,neutral
"Hence, with reference to [5, 33], we make the following assumption for components of the Fourier transform:",1,neutral
"Existing methods mainly fall into three categories: domain generalization [15]– [17], [41], [42], causal learning [43]–[46] and stable learning [47]–[50].",1,neutral
"Besides, several studies [23, 45] have leveraged domain generalization [17, 20, 29], mainly focusing on pretraining a recommender in one domain and applying it in downstream domains.",1,neutral
"Along this line, there are mainly four types of methods: kernel-based methods [3, 11], domain adversarial learning [8], explicit feature alignment [17, 18], and invariant risk minimization [1, 13].",1,neutral
"Invariant risk minimization (IRM) [3] and its variants [2, 11, 21, 25, 29, 34] are originally proposed for out-of-distribution (OOD) generalization [29, 43].",1,neutral
"The concept of causal-invariant prediction (CIP), as defined above, is not a new one and frequently appears in the DG literature in various forms [34, 32].",1,neutral
"Theorems 2 and 3 share a similar essence with the theoretical findings of several previous works [32, 34, 58].",1,neutral
"[32] propose an iterative algorithm that uses contrastive learning to map images to a latent space, and then match up images from different domains that have the same class label and are close to each other in the latent space.",1,neutral
"When contrastive pairs are available, we can directly apply CIP constraints to the feature extractor f as in MatchDG, or apply CIP constraints to the whole model g◦f as in ReLIC and CoRE.",2,positive
"Probability matching is utilized in ReLIC (Representation Learning via Invariant Causal Mechanisms)[34], logit matching is employed in CoRE (Conditional Variance Regularization) [15], and feature matching is used in MatchDG [32].",1,neutral
This is done in CoRE and MatchDG.,2,positive
"casual matching [29], extrinsic-intrinsic interaction [53], balance invariance [2], batch normalization embeddings [45] and multiple latent domain modeling [30].",1,neutral
"Despite the remarkable success, DNNs tend to take shortcuts to learn spurious features [24, 27].",1,neutral
"To address the above problem, the widely concerned Domain Generalization (DG) methods provide us with some inspiration [19, 32, 38, 42].",1,neutral
"Following this line, some works [34, 38, 39, 63, 74] achieve the goal by the pre-defined causal graph to learn the key features involving class labels.",1,neutral
Mahajan et al. (2021) introduces a novel regularizer to match the representation of the same object in different environments.,1,neutral
", CoRe [24] and MatchDG [41], are closely related to LAM since they also based on data pairs.",1,neutral
"data pair MBDG [52], RICE [63] CoRe [24], MatchDG [41]",1,neutral
"As for existing DG approaches, there are only several attempts to utilize data pairs that are either discovered from real data [24, 41] or generated by cross-domain image translation [52, 63].",1,neutral
"Similar models are also proposed in [38, 24, 41, 45], where X/X are referred to as semantic/variation factors, causal/noncausal factors, core/non-core factors, and content/style.",1,neutral
"For example, MatchDG [41] pushes the feature representations of a data pair (x, x̃; y) close to each other regardless of the class label y.",1,neutral
These methods include MatchDG [41] and CoRe [24].,1,neutral
"Existing methods that are most related to LAM are CoRe [24], MatchDG [41], MBDG [52], and RICE [63].",1,neutral
"Moreover, all representation invariance approaches su er from their neglect of di erences in intra-class variations between groups due to, for example, di erent disease subtypes [17, 19, 23].",1,neutral
"A key drawback of any notion of representation invariance concerns their neglect of di erences in the within-class distributions between groups [17, 19].",1,neutral
"methods [23], [24], domain adversarial learning [25], [26]",1,neutral
"…made in many directions, such as Invariant Representation (Chuang et al., 2020; Nguyen et al., 2021; Xiao et al., 2021; Shi et al., 2022), Causal (Mahajan et al., 2021; Mouli & Ribeiro, 2021; Lv et al., 2022), and Optimization (Krueger et al., 2021; Zhang et al., 2021; Lei et al., 2021;…",2,positive
"Specifically, following prior works (Suter et al., 2019; Zhang et al., 2020a; Mitrovic et al., 2021; Mahajan et al., 2021; Zhang et al., 2022b; Veitch et al., 2021; Lv et al., 2022), we assume that observed data X are generated by an causal mechanism G with two causes: semantic factor C and…",2,positive
"Over the years, great efforts have been made in many directions, such as Invariant Representation (Chuang et al., 2020; Nguyen et al., 2021; Xiao et al., 2021; Shi et al., 2022), Causal (Mahajan et al., 2021; Mouli & Ribeiro, 2021; Lv et al., 2022), and Optimization (Krueger et al., 2021; Zhang et al., 2021; Lei et al., 2021; Gulrajani & Lopez-Paz, 2021).",2,positive
", 2022), Causal (Mahajan et al., 2021; Mouli & Ribeiro, 2021; Lv et al., 2022), and Optimization (Krueger et al.",2,positive
"Following prior works (Mitrovic et al., 2021; Zhang et al., 2020a; Suter et al., 2019; Mahajan et al., 2021; Zhang et al., 2022b; Lv et al., 2022; Nguyen et al., 2022; Chen et al., 2022), we assume that the feature random variables are generated by the following causal mechanism.",2,positive
"Specifically, following prior works (Suter et al., 2019; Zhang et al., 2020a; Mitrovic et al., 2021; Mahajan et al., 2021; Zhang et al., 2022b; Veitch et al., 2021; Lv et al., 2022), we assume that observed data X are generated by an causal mechanism G with two causes: semantic factor C and nonsemantic factor S, i.",2,positive
Causal Assumption.,1,neutral
"Therefore, following Mitrovic et al. (2021); Zhang et al. (2020a); Suter et al. (2019); Mahajan et al. (2021); Lv et al. (2022); Nguyen et al. (2022), we further assume that for any l = 1, ..., N ,
Y ← C and Yl = Yt = Y. (2)
Uncertainty Set and Non-semantic Space.",1,neutral
"The domain distance measures [68] include moments [71], contrastive loss [72], [73], Kullback–Leibler divergence [74], Maximum Mean Discrepancy (MMD) distance [75] and adversarial learning [70].",1,neutral
[72] selected images of the same objects for domain generalization by a causal matching algorithm.,1,neutral
"Specifically, MatchDG narrowed the difference between different machines and working conditions, which was similar to the purpose of DAN.",2,positive
"[15] proposed an iterative algorithm named matching-based algorithms (MatchDG), which minimized the contrast loss of multiple domains to realize crossdomain image recognition.",1,neutral
"However, some methods, such as MatchDG in TD and ADIG in TF, do not explicitly deal with the individualized biases in machine data, which causes overfitting of the classification space and limits the generalization ability of the network.",1,neutral
"To verify the effectiveness and rationality of the designed method, traditional domain generalization methods, domain adversarial network (DAN) [12], ADIG [13], and IEDGNet [14], and causal learning methods, MatchDG [15], ANDMask [16], and IRM [17], were adopted for comparison.",2,positive
"Mahajan et al. [15] proposed an iterative algorithm named matching-based algorithms (MatchDG), which minimized the contrast loss of multiple domains to realize crossdomain image recognition.",1,neutral
"The problem of uncertainty in the feature vector has been addressed in the literature and deals with issues such as generalization of the output hypothesis once a new data set is available or concerns with noisy input [25, 26, 27, 28, 29, 30].",1,neutral
"A broad spectrum of methodologies based on data augmentation [78, 84], meta-learning [14, 40], or domain alignment [50,52] has made great progress.",1,neutral
"On the other hand, many existing OOD generalization methods, such as domain alignment [28, 32, 43, 44, 51, 55, 89], meta-learning [15–17, 41, 75], and disentangled representation learning [10, 31, 33, 42, 61, 76], require class/domain-related supervision, which are inapplicable for the AD task.",1,neutral
"Our investigated problem is closely related to OOD generalization which is a broad field that contains many popular research topics, such as Domain Generalization [3, 18, 36, 38, 41, 43, 47, 66], Causal Invariant Learning [1, 25, 27, 46, 62, 63].",1,neutral
"Other DG methods also employ self-supervised learning [16, 17, 27], ensemble learning [1, 50, 67] and dropout regularization [13, 14, 38].",1,neutral
"A primary strategy of DG is domain-invariant feature learning which aims to reduce domain gaps, including aligning distributions among multiple domains with contrastive learning [22, 41, 68, 70], learning useful representations with self supervise learning [6], matching statistics of feature distributions across domains [54,59], domain adversarial learning [14,31] and causality inference [38,39].",1,neutral
"Previous works [36,37,54,64,65] have incorporated causal inference in high-level tasks by instantiating the back-door criterion [17] with attention intervention [64], feature interventions [66], etc, which are arduous to be exploited in the low-level task of image restoration.",1,neutral
"To improve the generalization capability, [29, 36, 37, 65] introduce the causal learning to domain adaptation/generalization.",1,neutral
"This perspective has motivated several invariancelearning methods that make causal assumptions on the datagenerating process [1, 48, 49].",1,neutral
"• Evaluating more generalization techniques on Spawrious, including different robustness penalties [44, 3, 37, 5, 48, 27, 57], meta-learning [77, 9, 32, 70, 28], unsupervised domain adaptation [14, 46, 75], dropout [39], flat minima [5, 30], weight averaging [58, 73, 29], (counterfactual) data augmentation [31, 19, 76], finetuning of only specific layers [35, 40], diversity [65, 58], etc.",2,positive
"Structural equation models are often assumed for theoretical analysis (von Kügelgen et al., 2021; Liu et al., 2020; Mahajan et al., 2021).",1,neutral
"Most works aim to learn a representation that performs well on different source domains simultaneously (Rojas-Carulla et al., 2018; Mahajan et al., 2021; Jin et al., 2020), following the idea of causal invariance (Peters et al.",1,neutral
"Structural equation models are often assumed for theoretical analysis (von Kügelgen et al., 2021; Liu et al., 2020; Mahajan et al., 2021).",1,neutral
"It aims to find features that capture some invariance across different distributions, and assume that such invariance also applies to test distributions (Peters et al., 2016; Rojas-Carulla et al., 2018; Arjovsky et al., 2019; Mahajan et al., 2021; Jin et al., 2020; Ye et al., 2021).",1,neutral
"Most works aim to learn a representation that performs well on different source domains simultaneously (Rojas-Carulla et al., 2018; Mahajan et al., 2021; Jin et al., 2020), following the idea of causal invariance (Peters et al., 2016; Arjovsky et al., 2019).",1,neutral
"Mahajan et al. (2021) focus on cases where the distribution of causal features vary across domains; we additionally allow for xd:robust to be non-causal, such as habitat features in iWildCam and BirdCalls.",1,neutral
"Recently, Mahajan et al. (2021) introduced causal matching to model within-class variations for generalization.",1,neutral
"One of the predominant methods is domain invariant learning (Muandet et al., 2013; Ghifary et al., 2016; Motiian et al., 2017; Seo et al., 2020; Zhao et al., 2020; Xiao et al., 2021a; Mahajan et al., 2021; Nguyen et al., 2021; Phung et al., 2021; Shi et al., 2022).",1,neutral
"Other methods include using adversarial training [53–55] inspired from [17], meta learning [56], domain mixup [57–59], distributionally robust optimization [60], causal matching [61] and invariant risk minimization [62].",1,neutral
"In feature-related algorithms, domain-invariant representation learning including kernel-based methods[4], adversarial learning[5][6] and invariant risk minimization[7][8] are proposed.",1,neutral
[31] assume that domains are generated by mixing causal and non-causal features and that the same object from different domains should have similar representations.,1,neutral
"One typical approach to domain generalization is to learn domain-invariant representations across domains [18, 30, 42, 3, 11, 14, 45, 31, 35].",1,neutral
"We compare our framework with previous domain generalization works including domain-invariant based methods [30, 41, 11, 45, 14, 31, 35, 8] and other state-of-the-art methods [15, 4, 9, 28, 13, 46, 34, 22, 7, 44, 10] including data augmentation based methods [34, 46, 7], meta-learning based methods [4, 28, 13], etc.",2,positive
48 ResNet50 Target Baseline Metareg DSON DMG ER RSC MatchDG SWAD Fishr mDSDI LRDG [4] [38] [11] [45] [22] [31] [10] [35] [8] (ours) A 82.,0,negative
"The matching algorithm to produce features invariant to adversarial perturbations has been shown to produce robust models [33, 51].",1,neutral
(4) Causal domain adaptation/generalization on graphs: Recent studies have revealed the importance of incorporating causal perspective to remove the spurious correlations (Arjovsky et al. 2019; Mahajan et al. 2021) and enhance the performance of domain adaptation/generalization.,1,neutral
"It can be observed that FAST surpasses MatchDG [31], a quite state-of-the-art (SOTA) causality-based method that aims to learn domain-invariant representation, by about 5.0% and 1.3% in ArtPainting and Sketch on PACS respectively.",2,positive
One drawback of MatchDG is its reliance on the availability of domain labels.,2,positive
"On the other hand, MatchDG [31] assumes a causal graph where the content feature is independent of the domain given the hidden object and proposes a method that enforces this constraint to the classifier by alternately performing contrastive learning and finding good positive matches.",1,neutral
"Many causality-inspired approaches [17, 29, 31, 55] learn the domain-invariant representation or content feature that can be seen as a direct cause of the label.",1,neutral
"Such variable Z can be captured by several learning methods, such as style transfer models [15, 20] or contrastive learning [31, 53].",1,neutral
"Following previous works [29, 31, 69], we evaluate our proposed methods on three standard OOD generalisation benchmark datasets described below.",2,positive
"The baselines are divided into two groups: non-causality-based methods (from MMD-AAE [25] to FACT [58]), and causality-based methods (from MatchDG [31] to CIRL [29]).",2,positive
"Many recent state-of-the-art causality-based methods in OOD generalisation enforce regularisation with different data transformations on the learnt feature to achieve the domain-invariant feature [29, 31, 55].",1,neutral
"Causal inference has been applied to many deep learning research problems, especially in domain adaptation (DA) [30, 42, 62, 64] and domain generalisation (DG) [31, 35, 55].",1,neutral
"It can be observed that FAST surpasses MatchDG [31], a quite state-of-the-art (SOTA) causality-based method that aims to learn domain-invariant representation, by about 5.",1,neutral
"The baselines are divided into two groups: non-causality-based methods (from DeepAll [68] to FACT [58]), and causality-based methods (from MatchDG [31] to CIRL [29]).",2,positive
[42] proposed an unsupervised matching algorithm and [76] introduced the propensity score matching method to balance the mini-batch.,1,neutral
"Representative methods include incorporating invariance constraints by designing new loss functions [5, 8, 28, 37], learning latent semantic features in causal graphs by VAE [38,40], and eliminating selection bias by matching [42,76].",1,neutral
"Benefiting from the recent advances in self-supervised learning [9,19], contrastive-based learning has been proposed as a promising solution [10, 16, 26, 33, 37, 38, 52, 64, 71].",1,neutral
"Domain generalization (DG) [5, 6, 7, 8], as well as unsupervised domain adaptation (UDA) methods [9, 10, 11], aims to solve this hard and significant problem.",1,neutral
"Many studies have focused on how causality can help deep learning address long-standing issues such as interpretability [44-45, 47, 170-171], generalization [172-174], robustness [175-177], and fairness [178-180].",1,neutral
"Similar to Okapi, MatchDG [51] draws upon causal matching to tackle DG.",2,positive
"There are also other methods for domain-invariant learning on DG [28, 29].",1,neutral
"The justification is that the topological structure of these prototypes is unexplored, making them being sensitive to the change of environments [44, 9, 79].",1,neutral
We follow the same settings as in [44] to create spurious correlation.,1,neutral
"Prevailing approaches [54, 53, 16, 44, 61, 40] resort to causal graphs [50] to explicitly identify causal and non-causal factors with theoretical guarantees.",1,neutral
", ERM [64], IRM [3], CSD [53] and MatchDG [44]) are cited from [44].",1,neutral
"However, we argue that these operations may still be constrained by the spurious correlation as style-agnostic representation is not sufficient to ensure semantic invariance [44].",1,neutral
The robustness of ASTR is further evaluated on a challenging Chest X-ray benchmark [44] that has explicit spurious correlation.,0,negative
"(2) To statistically avoid the spurious correlations, a body of research [54, 53, 16, 44, 61, 40] proposes to identify the common component through latent factor disentanglement.",1,neutral
", meta learning [16], casual matching [24], disentangled representation [15] and adaptive methods [6].",1,neutral
"Amplifying the domain gap in image decomposition: Domain gaps between training and testing data have been a long-standing issue in Computer Vision [2, 29, 30, 46].",1,neutral
"…literature on images Zhou et al. (2021b); Wang et al. (2022), the goal then is use one of these methods to learn Xc/Zc from observed data: regularization Mahajan et al. (2021); Lee et al. (2022), weighting Sagawa et al. (2019); Yao et al. (2022) or data augmentation Zhou et al. (2021b).",2,positive
"With the idea of invariant causal mechanism, increasing attention has been paid to causality-inspired generalization learning (Wald et al. 2021; Liu et al. 2021; Sun et al. 2021; Mahajan, Tople, and Sharma 2021).",1,neutral
"There is a series of prior works (Christiansen et al. 2021; Yuan et al. 2021; Mahajan, Tople, and Sharma 2021; Chen et al. 2021; Sun et al. 2021; Li et al. 2021a) investigate domain generalization task and provide their own approaches in causal view.",2,positive
"In addition, several recent works(Mahajan, Tople, and Sharma 2021; Chen et al. 2021; Sun et al. 2021; Li et al. 2021a) have further identified other hidden causal relationship.",1,neutral
"In order to deal with domain shift problems, Domain Generalization (DG) (Zhang et al. 2021; Chen et al. 2021; Liu et al.
2021; Sun et al. 2021; Mahajan, Tople, and Sharma 2021; Wald et al. 2021) is introduced, which aims to learn stable knowledge from multiple source domains and train a…",2,positive
"Especially in works(Li et al. 2021a; Mahajan, Tople, and Sharma 2021; Chen et al. 2021), these SCMs introduce variables that are only relevant to objects.",1,neutral
"Mahajan et al. (Mahajan, Tople, and Sharma 2021) consider high-level causal features and domain-dependent features while the labels are only determined by the former.",1,neutral
"(Mahajan, Tople, and Sharma 2021) proposes a causal graph with the domainD and objectO which is similar to ours, as shown in Figure 1 (d).",1,neutral
"The authors in [13, 23, 24] aim to utilize auxiliary variables to separate causal from non-causal features, and learn the representations accordingly.",1,neutral
"Thus, causality can be a useful tool in capturing the invariance present in the data, justifying the range of methods that have leveraged different causal theories for improving the generalization capabilities of models [4, 13].",1,neutral
"For instance, the leave-one-domain-out rule is one of the most prominent when dealing with multi-source domain generalization [13, 23, 31, 118].",1,neutral
"Thus, to improve the generalization performance, a range of works [13, 19, 49] leveraged real-world medical imaging datasets such as Alzheimer’s Disease Neuroimaging Initiative (ADNI) [82].",2,positive
"[13] argue that for representations, the class-conditional domain invariant objective is inadequate.",1,neutral
"To reduce environmentspecific overfitting, techniques including invariant risk minimization [35, 36], self-training [37, 38], dropout [39] and feature selection [40–42] are proposed to focus more on features with causal relationships to the outcome.",1,neutral
[25] proposed Exact Pareto Optimal (EPO) Search to find a preference-specific Pareto optimal solution.,1,neutral
[25] proposed MatchDG and tried to deal with domain generalization from the view of a structural causal model.,1,neutral
"Therefore, assuming zc → zs is more persuasive and consistent with previous works (Gong et al., 2016; Stojanov et al., 2019; Mahajan et al., 2021).",1,neutral
"This assumption has been employed by some recent works (Mahajan et al., 2021; Liu et al., 2021; Sun et al., 2021).",1,neutral
"Particularly, these two different labels, ŷ and y, has been simultaneously considered in Mahajan et al. (2021).
zc causes zs: Here we consider having the object essence zc first, from which a latent style zs springs to render x.",1,neutral
"Recently, contrastive learning with multi-view data augmentation has shown their efficacy in self-supervised representation learning [45,19,4,31] and domain generalization [28].",1,neutral
"And many other alternatives could also be found [19, 29, 6].",1,neutral
"[29] Divyat Mahajan, Shruti Tople, and Amit Sharma.",0,negative
"We also compare our proposed method with a variety of state-of-the-art domain generalization methods, including DeepAll [64], Jigen [3], CCSA [26], MMD-AAE [19] , CrossGrad [38], DDAIG [65], L2AOT [64], ATSRL [55], MetaReg [2] , Epi-FCR [18], MMLD [24], CSD [32], InfoDrop [40], MASF [9], Mixstyle [66], EISNet [48], MDGH [23], RSC [14] and FACT [53].",2,positive
"Compared with the state-of-the-art methods, our method clearly beats the methods based on the GAN-based data augmentation and meta-learning methods, such as the latest L2A-OT [64], MDGH [23] and ATSRL [55].",2,positive
"We also compare our proposed method with a variety of state-of-the-art domain generalization methods, including DeepAll [58], Jigen [3], CCSA [23], MMD-AAE [16] , CrossGrad [35], DDAIG [59], L2A-OT [58], ATSRL [50], MetaReg [2] , Epi-FCR [15], MMLD [21], CSD [29], InfoDrop [37], MASF [7], Mixstyle [60], EISNet [44], MDGH [20], RSC [11] and FACT [48].",2,positive
Mahajan et al. (2021) have proposed that correlations independent of domain conditional on class (Φ(x) ⫫ D∣Y ) are not necessarily causal correlations if P (ẋ∣Y ) changes across domains.,1,neutral
"For instance, the computational complexity of causal matching in MatchDG (Mahajan et al., 2021) and gradient matching in FISH (Shi et al.",1,neutral
Mahajan et al. (2021) state that learning representations independent of the domain after conditioning on the class label is insufficient for training a robust model.,1,neutral
"Backbones We take MatchDG (Mahajan et al., 2021), FISH (Shi et al., 2021b) and DANN (Ganin et al., 2016) as backbone algorithms.",2,positive
"Backbones We take MatchDG (Mahajan et al., 2021), FISH (Shi et al.",2,positive
"For instance, the computational complexity of causal matching in MatchDG (Mahajan et al., 2021) and gradient matching in FISH (Shi et al., 2021b) is O(n2) with n training domains.",1,neutral
"If the spurious attributes are domain labels, the conditional independence in Theorem 1 becomes the ones in (Liu et al., 2015; Hu et al., 2020; Mahajan et al., 2021), while they do not explore its correlation with the OOD generalization.",1,neutral
", (Hu et al., 2020; Mahajan et al., 2021; Ben-David et al., 2007; 2010; Muandet et al., 2013; Ganin et al., 2016), but none of them directly control the OOD generalization error.",2,positive
"Besides, the counterexample in Mahajan et al. (2021) violates our conditional invariant assumption in (1) and hence is not contrary to our theorem.",1,neutral
"Notably, in contrast to the existing metrics related to OOD generalization (Hu et al., 2020; Mahajan et al., 2021), our CSV can control the OOD generalization error.",2,positive
"…perspectives, e.g., distributional robust optimization (Sinha et al., 2018; Volpi et al., 2018; Sagawa et al., 2019; Yi et al., 2021b; Levy et al., 2020) or causal inference (Arjovsky et al., 2019; He et al., 2021; Liu et al., 2021b; Mahajan et al., 2021; Wang et al., 2022; Ye et al., 2021).",1,neutral
"To this end, Arjovsky et al. (2019); Hu et al. (2020); Li et al. (2018); Mahajan et al. (2021); Heinze-Deml & Meinshausen (2021); Krueger et al. (2021); Wald et al. (2021); Seo et al. (2022) propose plenty of invariant metrics as training regularizer.",2,positive
"Our definition is different from the ones in (Mahajan et al., 2021; Makar & D’Amour, 2022), as they rely on a causal directed acyclic graph and the existence of a sufficient statistic such that Y only affects X through it.",2,positive
"The OOD generalization error is also connected to many other metrics e.g., (Hu et al., 2020; Mahajan et al., 2021; Ben-David et al., 2007; 2010; Muandet et al., 2013; Ganin et al., 2016), but none of them directly control the OOD generalization error.",1,neutral
"Then the main classifier is regularized to have the same representation for such pairs of inputs ([3, 26]).",1,neutral
"[26] Divyat Mahajan, Shruti Tople, and Amit Sharma.",0,negative
"Existing DG methods handle domain shift from various perspectives, including domain alignment [35]–[40], training strategy [41]– [43], data augmentation [28], [32], [44]–[46], and the causal mechanism [27], [47].",1,neutral
"By focusing on observed co-occurrence patterns, they can be easily misled by such spurious correlations and hard to identify the real causal correlations [16].",1,neutral
"[25] Divyat Mahajan, Shruti Tople, and Amit Sharma.",0,negative
"Meanwhile, the ML model is also trained to perform consistently across all environments, which is achieved by adding a penalty term to the loss function (Arjovsky et al., 2019; Koyama and Yamaguchi, 2020; Krueger et al., 2021; Mahajan et al., 2021).",2,positive
"Given a dataset with auxiliary attributes and their relationship with the target label, CACM constrains the model’s representation to obey the conditional independence constraints satisfied by causal features of the label, generalizing past work on causality-based regularization [10, 14, 15] to multi-attribute shifts.",1,neutral
"[10] Divyat Mahajan, Shruti Tople, and Amit Sharma.",0,negative
"Following [10], the two training domains have p as 0.",1,neutral
"Here, we compare the performance of two popular independence constraints in the literature [10]: unconditional Xc ⊥⊥ A|E, and conditional on label Xc ⊥⊥ A|Y,E (we condition on E for fully generality) on Synthetic Causal, Confounded and Selected shift datasets (Table 6).",1,neutral
"Our synthetic dataset is constructed based on the data-generating processes of the slab dataset [10, 30].",2,positive
"Partly because advances in representation learning for DG [8, 9, 10, 7, 11, 12] have focused on either one of the shifts, these studies find that performance of state-of-the-art DG algorithms are not consistent across different shifts: algorithms performing well on datasets with one kind of shift fail on datasets with another kind of shift.",1,neutral
Our canonical multi-attribute graph generalizes the DG graph from [10] that considered an Independent domain/environment as the only attribute.,2,positive
"We evaluate two constraints motivated by DG literature [10]: unconditional Xc ⊥⊥ A|E, and conditional on label Xc ⊥⊥ A|Y,E.",1,neutral
"Our extended slab dataset, adds to the setting from [10] by using non-binary attributes and class labels to create a more challenging task and allows us to study DG algorithms in the presence of linear spurious features.",2,positive
"We utilize a strategy from past work [10, 14] to use graph structure of the underlying data-generating process (DGP).",2,positive
"It further inspires plentiful invariant learning works (Parascandolo et al., 2021; Mahajan et al., 2021; Creager et al., 2021; Wald et al., 2021; Ahuja et al., 2021a; Chen et al., 2022b; Lin et al., 2022b).",2,positive
"These works can broadly be divided into three categories—learning invariant representations [39, 40, 41, 42], causal representation learning to embed priors in the learning strategy [43, 44, 45], and custom optimization methods to enable generalization [46, 47, 48, 49].",1,neutral
"Such methods often aim to find invariant data representations using new loss function designs that incorporate the invariance conditions across different domains into the training process (Arjovsky et al., 2020; Mahajan et al., 2021; Liu et al., 2021; Lu et al., 2022; Wald et al., 2021).",1,neutral
"Learning from multi-source data has a long history in machine lerning (Cortes et al. 2021, Hoffman et al. 2018, Zhao et al. 2018, Blanchard et al. 2011,Muandet et al. 2013,Mahajan et al. 2021, Wang et al. 2021, Zhou et al. 2021, Zhang and Yang 2021, Sener and Koltun 2018).",1,neutral
"…essential part in multi-task learning (Caruana 1997, Zhang and Yang 2021), domain generalization (DG) (Blanchard et al. 2011, Muandet et al. 2013, Mahajan et al. 2021, Wang et al. 2021, Zhou et al. 2021), and out-of-distribution (OOD) generalization (Arjovsky 2019, Wald et al. 2021) that…",2,positive
"Mahajan et al. (2021) show that class-conditional invariance can fail if P (zc|y) does not remain the same across domains, which also applies to our method.",1,neutral
"…invariance of Ee[y|f(x)] (Arjovsky et al., 2019) with information bottleneck constraint (Ahuja et al. (2021)), imposing object-invariant condition (Mahajan et al. (2021)), using domain inference (Creager et al., 2021), model calibration (Wald et al., 2021), and others (Krueger et al., 2021; Li et…",1,neutral
Mahajan et al. (2021) argues learning invariant representations for inputs derived from the same object.,1,neutral
"To that end, Domain Generalization (DG) [52, 53, 30], which aims at learning a domain-agnostic model, has drawn increasing attention in the ReID community.",1,neutral
"The main idea is to identify and extract the transferable components that are invariant across different domains under certain causal models (Gong et al., 2016; Magliacane et al., 2017; Mahajan et al., 2021; Rojas-Carulla et al., 2018).",1,neutral
"A few image generation works have modeled a causal connection between images and their labels, often assuming the labels are generating the images [24, 48], and some prior work studied the connection between causality and specific types of generalizations [4,37,38,58].",1,neutral
Existing works show that it is indeed possible to approach this computationally via sub-sampling methods [52] or by learning elaborate matching functions to identify image or object pairs across sites that are “similar” [28] or have the same value for C.,1,neutral
"2a, similar to existing works [28,52] with minimal changes.",0,negative
"Lastly, (v) RM [30]: Also used in [28], RandMatch (RM) learns invariant representations on samples across sites that ”match” in terms of the class label (we match based on both Y and C values) .",1,neutral
"Specially, recent work [28] has shown the value of integrating structural causal models for domain generalization, which is related to dataset pooling.",1,neutral
"IRM and its variants (Krueger et al., 2021; Xie et al., 2020; Mahajan et al., 2021) posit the existence of a feature embedder such that the optimal classifier on top of these features is the same for every environment from which data can be drawn.",2,positive
"then, there have been many works on improving the IRM objective (Xie et al., 2020; Chang et al., 2020; Ahuja et al., 2020; Krueger et al., 2021; Mahajan et al., 2021) and comparing ERM and IRM from theoretical (Ahuja et al.",2,positive
"…there have been many works on improving the IRM objective (Xie et al., 2020; Chang et al., 2020; Ahuja et al., 2020; Krueger et al., 2021; Mahajan et al., 2021) and comparing ERM and IRM from theoretical (Ahuja et al., 2021; Rosenfeld et al., 2021; 2022) and empirical perspectives…",2,positive
"Very recently, MatchDG [32] introduces causality into DG literature by enforcing the inputs across domains have the same representation via contrastive learning if they are derived from the same object.",1,neutral
"Specifically, compared with MatchDG [32], which also introduces causality into DG problem, CIRL outperforms MatchDG by a large margin of 1.",1,neutral
It can be observed that MatchDG [32] and CIRL present higher representation importance since they embed the causal information that truly affects the classification into representations.,1,neutral
"To capture the invariant causal mechanism, existing works have assumed a particular form of the causal diagram [25,46,48,57,67], which may be restrictive in practice and is untestable from the observed data.",1,neutral
"To utilize the invariant causal mechanism and hence improve OOD generalization, some works impose restrictive assumptions on the causal diagram or structural equations [25, 46, 57, 67].",1,neutral
"Self-supervision as an emerging technique has been employed to train neural networks for more generalizable predictions on the image field [110, 111, 112].",1,neutral
"Although using contrastive learning to improve OOD generalization is not new in the literature [27, 61, 124], previous methods cannot yield OOD guarantees in graph circumstances due to the highly non-linearity and the unavailability of domain labels E.",1,neutral
"Moreover, most existing methods require environment labels that are however expensive to obtain in graphs, which limits their applications to graphs [4, 49, 2, 81, 31, 93, 27, 61].",1,neutral
IRM [4] Yes R Yes PIIF IB-IRM [2] Yes R Yes PIIF&FIIF EIIL [23] Yes R No PIIF DANN [31] N/A R Yes N/A MatchDG [61] N/A R Yes FIIF GroupDro [81] N/A R Yes N/A CNC [124] N/A R No N/A GIB [120] Yes G No FIIF DIR [104] No G No FIIF CIGA (Ours) Yes G No PIIF&FIIF,0,negative
"On Euclidean data, Invariant Learning [4, 23, 2], Group Distributionally Robust Optimization [49, 81, 124], Domain Adaption and Domain Generalization [31, 93, 52, 27, 61, 100] are three widely adopted approaches to enable OOD generalization.",1,neutral
"Besides, traditional approaches to tackle OOD generalization also include Domain Adaption, Transfer Learning and Domain Generalization[79, 21, 31, 93, 52, 27, 61, 100], which aim to learn the class conditional invariant representation shared across source domain and target domain.",1,neutral
Algorithm OODGuarantee Regime E Known SCMSupport IRM[4] Yes Yes PIIF IB-IRM[2] Yes Yes PIIF&FIIF EIIL[23] Yes No PIIF DANN[31] N/A Yes N/A MatchDG[61] N/A Yes FIIF GroupDro[81] N/A Yes N/A CNC[124] N/A No N/A GIB[120] Yes G No FIIF DIR[104] No Figure 1: (a) Illustration of C ausality Inspired I nvariant G raph Le A rning (CIGA): GNNs need to classify graphs based on the specific motif (“House” or “Cycle”).,1,neutral
"For instance, Ahmed et al. (2021) proposes to match the output distribution spaces from different domains via some divergence, while a recent work (Mahajan et al., 2021) also leverages a matching-based algorithm that resorts to shared representations of cross-domain inputs from the same object.",1,neutral
"(2021) proposes to match the output distribution spaces from different domains via some divergence, while a recent work (Mahajan et al., 2021) also leverages a matching-based algorithm that resorts to shared representations of cross-domain inputs from the same object.",1,neutral
"Notably, domain-invariant representation learning has emerged as one of the most common and efficient approaches in Domain Generalization and provided many promising results [1] [2] [7] [9] [10] [21] [22] [23] [24].",1,neutral
This assumption can be verified upon the setting of causal graphical models that illustrate the relationship between the data and its label [22] [32] [33] [34].,1,neutral
"The first class, domain generalization methods [2, 23, 26], aim at learning representations invariant to a predefined set of extraneous attributes or groups.",1,neutral
"While the IRM framework assumes only the invariance of the conditional expectation of the label given the representation, some follow-up works rely on stronger invariance assumptions [23, 31].",1,neutral
"In general, causal learning has inspired MDG approaches such as IRM Arjovsky et al. (2019), MatchDG Mahajan et al. (2021) and Deep CAMA Zhang et al. (2020a).",1,neutral
Torralba and Efros (2011). Such drops in performance indicate poor generalization capabilities of the models.,2,positive
"Generalization via assuming a causal structure has also been explored in [9, 28].",1,neutral
"[19] Divyat Mahajan, Shruti Tople, and Amit Sharma.",0,negative
"Recently, several works [18, 19, 20] utilize causal structures to learn invariance factors, which have shown better model generalizability on unseen data, especially on data from different distributions than the train distribution.",1,neutral
"Inspired from Theorem 2 in [19], we are able to derive the following theorem under our problem setting.",1,neutral
"Many researchers have been working on developing DG algorithms [12, 13, 14, 15, 16, 17, 18, 19, 20, 21], but most of these models are designed for tasks such as object recognition, semantic segmentation, image classification, and disease detection.",1,neutral
"A commonly-used strategy is to extract task-specific but domaininvariant features [12, 13, 17, 18, 19, 20].",1,neutral
"Inspired from [19], for this unsupervised contrastive learning process, we initialize Ω with a random match based on classes and keep updating Ω by minimizing the contrastive loss (3) until convergence.",1,neutral
"The first category intends to learn domain-invariant features that follow the same distributions [17,24,25,28,30,31,47].",1,neutral
[53] improve domain robustness using contrastive losses.,1,neutral
"Causal ideas have been used for discovering image features that are semantically essential and robust [52], [53], [54], [55].",1,neutral
"Variants of such a regularization include the minimization across the source domains of the maximum mean discrepancy criteria (MMD) [21, 35], the minimization of a distance metric between the domain-specific means [71] or covariance matrices [69], the minimization of a contrastive loss [50, 83, 44, 29], or the maximization of loss gradient alignment [65, 63].",1,neutral
"[44] provide a causal interpretation of domain generalization, and show the importance of learning within-class variations for generalization.",1,neutral
"However, previous methods may either perform experiments on low-dimensional toy data [11, 16, 78] or lack simulation results to show its causality learning performance [44, 70].",1,neutral
"To learn distribution-irrelevant features and models for stable generalization, numerous causality-based distribution generalization methods [11, 13, 16, 23, 44, 48, 67, 70, 74, 76, 78] have been introduced recently.",1,neutral
"1 Domain generalization Unlike domain adaptation, domain generalization [8, 24, 25] cannot use any sample of the target domain, but it still has to capture transferable information across domains.",1,neutral
"the difference between the means [16] or covariance matrices (CORAL) [17] in the embedding space across different domains, or minimizing a contrastive loss [18]–[20], e.",1,neutral
"2) [2], [3], [56], [111], [112], [113], [114], [115], [116], [117](Section 4.",1,neutral
[114] introduce a contrastive regularizer which matches the representation of same objects across environments.,1,neutral
"[11] Divyat Mahajan, Shruti Tople, and Amit Sharma.",0,negative
"Domain generalization and domain-invariant learning methods assume the training data consists of multiple sufficiently different environments to generalize to unseen test data that is related to the given environments or subgroups [2, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28].",1,neutral
"Recent work uses multiple sufficiently different environments to generalize to unseen test data that lies in the support of the given environments or subgroups [2, 10, 11, 17, 18, 19, 20, 21, 22].",1,neutral
"Methods for invariant representation learning [2, 10, 11, 12] typically require data from multiple different environments.",1,neutral
"In table 1, we summarize key differences between NURD and the related work: invariant learning [2, 10], distribution matching [11, 12], shift-stable prediction [13], group-DRO [14], and causal regularization [15, 16].",2,positive
"Methods based on conditional distribution matching [11, 12] build representations that are conditionally independent of the environment variable given the label.",1,neutral
", multiple source datasets) to learn domain-invariant features using methods like adversarial losses [19], [85], [86], [87], or moment-matching [88], [89].",1,neutral
"Rosenfeld et al. [2021b] prove that even for a simple generative model and linear classifiers, the environment complexity of IRM—and other objectives based on the same principle of invariance—is at least as large as the dimension of the spurious latent features, ds.",1,neutral
"Arjovsky et al. [2019] assume invariant E[y | Φ(x)], and follow-up works assume invariance of higher moments [Xie et al., 2020, Jin et al., 2020, Mahajan et al., 2020, Krueger et al., 2020, Bellot and van der Schaar, 2020].",1,neutral
"Like other casual related works (Chang et al. 2020; Mahajan, Tople, and Sharma 2020), we begin with a structural causal model, shown in Figure 2.",2,positive
"Over the past several years, the progress in DG has stemmed from areas such as invariant representation learning [69, 36, 45, 115], causality [7, 53, 106, 64], meta-learning [57, 8, 27], and feature disentanglement [27, 46, 75].",1,neutral
", 2019), there have been several interesting works — (Teney et al., 2020; Krueger et al., 2020; Ahuja et al., 2020; Jin et al., 2020; Chang et al., 2020; Ahuja et al., 2021a; Mahajan et al., 2020; Koyama and Yamaguchi, 2020; Müller et al., 2020; Parascandolo et al., 2021; Ahmed et al., 2021; Robey et al., 2021; Zhang et al., 2021) is an incomplete representative list — that build new methods inspired from IRM to address the OOD generalization problem.",2,positive
"One of the major issues behind this observed behavior is a lack of clarity and applicability for the underlying assumptions regarding the problem statement and the data-generating process (Zhao et al., 2019; Rosenfeld et al., 2020; Mahajan et al., 2020).",2,positive
"According to [40], T = T ∗ holds when the training data are large enough.",1,neutral
Other methods are also matching the feature covariance across source domains [150] or take a causal interpretation to match representations of features [122].,1,neutral
"Other approaches include meta learning [24], invariant risk minimization [2], distributionally robust optimization [38], mixup [46, 47, 44], and causal matching [32].",1,neutral
"Amplifying the domain gap in image decomposition: Traditionally, domain gaps between training and testing data have been a long-standing issue in Computer Vision [2, 28, 29, 46].",1,neutral
"is another option for reducing distribution mismatch [169], [170], [171], which takes into",1,neutral
"When such object label is unavailable, [76] further learned an object feature based on labels in a separate stage.",1,neutral
"activity recognition [65], chest X-ray recognition [76], [90], and EEG-based seizure detection [88].",1,neutral
", 2019] and MatchDG [Mahajan et al., 2020], and use the same hyper-parameters suggested by the original paper.",2,positive
"We use a learning rate of 1e-4 for DIVA and HDUVA, a learning rate of 1e-5 (better than 1e-4) for Deep-All and the suggested learning rate for MatchDG.",2,positive
", 2019] and Match-DG [Mahajan et al., 2020], while Deep-All is used as baseline by pooling all training domain s together.",2,positive
"For the second domain nominal domain, we rotate the same
ColorMnist (Figure 5 ) Test Domain 1 Test Domain 3
DIVA 0.53 ± 0.05 0.63 ± 0.05 HDUVA 0.56 ± 0.05 0.68 ± 0.05 Deep-All 0.53 ± 0.06 0.61 ± 0.06 Match-DG 0.44 ± 0.04 0.67 ± 0.10
subset of MNIST, by 30, 45 and 60 degrees respectively.",2,positive
", 2019] and Match-DG [Mahajan et al., 2020], while Deep-All is used as baseline by pooling all training domains together.",2,positive
"For comparing algorithms, we implemented DIVA [Ilse et al., 2019] and MatchDG [Mahajan et al., 2020], and use the same hyper-parameters suggested by the original paper.",2,positive
"Causality based Method Recently, Mahajan et al. [2020] proposed MatchDG with that approximates base object similarity by using a contrastive loss formulation adapted for multiple domains.",1,neutral
"We use a learning rate of 1e-5 for HDUVA, DIVA, Deep-All and use default learning rate of MatchDG.",2,positive
", 2019] and Match-DG [Mahajan et al., 2020], while Deep-All by pooling all training domains together is used as baseline.",2,positive
"Our approach is able to implicitly learn the unobserved domain substructure of the data, resulting is substantially better accuracy on unseen test domains (i.e. a new hospital) compared to state-of-the-art approaches DIVA and MatchDG.",2,positive
"Many papers have suggested similar alternatives (Jin et al., 2020; Mahajan et al., 2020; Bellot & van der Schaar, 2020).",1,neutral
"(2019) only assume invariance of E[y | Φ(x)]; follow-up works rely on a stronger assumption of invariance of higher conditional moments (Krueger et al., 2020; Xie et al., 2020; Jin et al., 2020; Mahajan et al., 2020; Bellot & van der Schaar, 2020).",1,neutral
"2 Related work A number of contributions under the domain generalization setting borrowed tools from causal inference to enforce the learned representations to be invariant across the different domains presented to the model at training time [11, 12, 13].",1,neutral
"…algorithms (Scutari, 2009) or recent methods for learning invariant relationships from training datasets from different distributions (Peters et al., 2016; Arjovsky et al., 2019; Bengio et al., 2019; Mahajan et al., 2020), or learn based on a combination of randomized experiments and observed data.",1,neutral
"Alternatively, one may exploit the strong relevance property from (Pellet & Elisseeff, 2008), use score-based learning algorithms (Scutari, 2009) or recent methods for learning invariant relationships from training datasets from different distributions (Peters et al., 2016; Arjovsky et al., 2019; Bengio et al., 2019; Mahajan et al., 2020), or learn based on a combination of randomized experiments and observed data.",1,neutral
"In these challenging tasks, our method not only beats all popular domain generalization methods but also surpasses the methods of introducing causal inference (MatchDG [14], CSG-ind [12], CIRL [13]), clearly showing the advantages of analyzing the causes of the domain shift by causal inference.",2,positive
"We compare our method with most related methods that introduces causal inference into generalization (MatchDG [14], CSGind [12], CIRL [13]), and existing popular domain generalization methods (MetaReg [1], GUD [18], Epi-FCR [10], MASF [5], JiGen [2], DMG [3], DDAIG [24], CSD [16], L2A-OT [25], EISNet [19], RSC [8], ME-ADA [23], MMLD [15], L2D [20], FACT [21]).",2,positive
"Mainstream Domain Generalization (DG) studies [1, 20, 25, 29, 30, 30, 38] primarily focus on extracting invariant representations from source domains that can be effectively generalized to the target domain, which remains inaccessible during training.",1,neutral
"MATCHDG(Mahajan et al., 2021) constructs a representation using contrastive learning, where representations of the same object across environments are invariant.",1,neutral
"Domain generalization (DG) (Muandet et al., 2013; Ghifary et al., 2016; Mahajan et al., 2021; Li et al., 2020) aims to solve this hard and significant problem.",2,positive
"The goal is to ""learn representations independent of the domain after conditioning on the class label"" [21].",1,neutral
"The authors [21] propose new methods RandMatch, MatchDG, and MDGHybrid to increase performance over the previous state-of-the-art methods for various ML problems.",2,positive
Figure 2: Slab Dataset (Slab (y-axis) is the stable feature) [21],1,neutral
"Figure 3: Causal graph proposed by the original authors [21] depicting the relation between Ytrue, the causal features which are domain independent Xc and Y",1,neutral
"On the one hand, many causality-inspired approaches [25, 27, 49] learn domain-invariant representations based on the theory of invariant causal mechanism [30, 39].",1,neutral
"It can be observed that FAST surpasses MatchDG [27], a quite state-of-the-art (SOTA) causality-based method that aims at learning domain-invariant representation, by about 5.",1,neutral
"The baselines are divided into two groups: non-causality-based methods (from DeepAll [61] to FACT [52]), and causality-based methods (from MatchDG [27] to CIRL [25]).",2,positive
"The baselines are divided into two groups: non-causality-based methods (from
DeepAll [61] to FACT [52]), and causality-based methods (from MatchDG [27] to CIRL [25]).",2,positive
"It can be observed that FAST surpasses MatchDG [27], a quite state-of-the-art (SOTA) causality-based method that aims at learning domain-invariant representation, by about 5.0% and 1.3% in Art-Painting and Sketch on PACS respectively.",1,neutral
"Following previous works [25, 27, 62], we evaluate our proposed methods on three standard OOD generalisation benchmark datasets described below.",2,positive
"Literature has shown a common research streamline, that is to learn domain-invariant features so that models could maintain good performance on unseen target domains [27, 42, 49].",1,neutral
"Causal inference has been applied to many deep learning research problems, especially in domain adaptation [26, 40, 58] and DG [27, 30, 49].",1,neutral
"Methods for invariant representation learning (Arjovsky et al., 2019; Krueger et al., 2020; Mahajan et al., 2020; Guo et al., 2021) typically require data from multiple different environments.",1,neutral
"Recent work uses multiple sufficiently different environments to generalize to unseen test data that lies in the support of the given environments or subgroups (Li et al., 2018a; Arjovsky et al., 2019; Akuzawa et al., 2019; Mahajan et al., 2020; Krueger et al., 2020; Bellot and van der Schaar, 2020; Li et al., 2018b; Goel et al., 2020; Wald et al., 2021).",2,positive
"Domain generalization and domain-invariant learning methods assume the training data consists of multiple sufficiently different environments to generalize to unseen test data that is related to the given environments or subgroups (Li et al., 2018a; Arjovsky et al., 2019; Akuzawa et al., 2019; Mahajan et al., 2020; Krueger et al., 2020; Bellot and van der Schaar, 2020; Li et al., 2018b; Goel et al., 2020; Wald et al., 2021; Ganin et al., 2016; Xie et al., 2017; Zhang et al., 2018; Ghimire et al., 2020; Adeli et al., 2021; Zhang et al., 2021).",2,positive
"Methods based on conditional distribution matching (Mahajan et al., 2020; Guo et al., 2021) build representations that are conditionally independent of the environment variable given the label.",1,neutral
"…key differences between NURD and the related work: invariant learning (Arjovsky et al., 2019; Krueger et al., 2020), distribution matching (Mahajan et al., 2020; Guo et al., 2021), shift-stable prediction (Subbaswamy et al., 2019a), group-DRO (Sagawa et al., 2019), and causal…",2,positive
"…to generalize to unseen test data that lies in the support of the given environments or subgroups (Li et al., 2018a; Arjovsky et al., 2019; Akuzawa et al., 2019; Mahajan et al., 2020; Krueger et al., 2020; Bellot and van der Schaar, 2020; Li et al., 2018b; Goel et al., 2020; Wald et al., 2021).",2,positive
"…to unseen test data that is related to the given environments or subgroups (Li et al., 2018a; Arjovsky et al., 2019; Akuzawa et al., 2019; Mahajan et al., 2020; Krueger et al., 2020; Bellot and van der Schaar, 2020; Li et al., 2018b; Goel et al., 2020; Wald et al., 2021; Ganin et al.,…",1,neutral
", 2020), distribution matching (Mahajan et al., 2020; Guo et al., 2021), shift-stable prediction (Subbaswamy et al.",1,neutral
"Besides that, the counterexample in (Mahajan et al., 2021) violates our invariant conditional distribution assumption in (1) and hence is not contrary to our theorem.",1,neutral
"…perspectives, e.g., distributional robust optimization (Sinha et al., 2018; Volpi et al., 2018; Sagawa et al., 2019; Yi et al., 2021b; Levy et al., 2020) or causal inference (Arjovsky et al., 2019; He et al., 2021; Liu et al., 2021b; Mahajan et al., 2021; Wang et al., 2022; Ye et al., 2021a).",1,neutral
"But none of them directly control the OOD generalization error as our CSV (Seo et al., 2022; Hu et al., 2020; Liu et al., 2021b; Heinze-Deml and Meinshausen, 2021; Mahajan et al., 2021; Ben-David et al., 2007, 2010; Muandet et al., 2013; Ganin et al., 2016).",2,positive
"To the best of our knowledge, this property has not been explored by existing metrics related to OOD generalization (Hu et al., 2020; Seo et al., 2022; Mahajan et al., 2021; Heinze-Deml and Meinshausen, 2021; Krueger et al., 2021).",2,positive
"To this end, Arjovsky et al. (2019); Hu et al. (2020); Li et al. (2018); Mahajan et al. (2021); Heinze-Deml and Meinshausen (2021); Krueger et al. (2021); Wald et al. (2021); Seo et al. (2022) regularize the training process with plenty of invariant metrics.",2,positive
"If the spurious attributes are domain labels, the conditional independence in Theorem 1 reduces to the conditional independence discussed in (Liu et al., 2015; Hu et al., 2020; Mahajan et al., 2021), while they do not explore its correlation with the OOD generalization.",1,neutral
"Although using contrastive learning to improve OOD generalization is not new in the literature (Dou et al., 2019; Mahajan et al., 2021; Zhang et al., 2022), previous methods cannot yield OOD guarantees in graph circumstances due to the highly non-linearity and the unavailability of domain labels E.",1,neutral
"Besides, traditional approaches to tackle OOD generalization also include Domain Adaption, Transfer Learning and Domain Generalization(Rojas-Carulla et al., 2018; Chuang et al., 2020; Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2018b; Dou et al., 2019; Mahajan et al., 2021; Wang et al., 2021), which aim to learn the class conditional invariant representation shared across source domain and target domain.",1,neutral
"Moreover, the unavailability of E prevents the direct usage of E in enforcing the independence that is often adopted by previous objectives (Arjovsky et al., 2019; Krueger et al., 2021; Ahuja et al., 2021; Sagawa* et al., 2020; Ganin et al., 2016; Sun & Saenko, 2016; Dou et al., 2019; Mahajan et al., 2021), making the identification of Gc more challenging.",2,positive
"…methods require environment labels that are however expensive to obtain in graphs, which limits their applications to graphs (Arjovsky et al., 2019; Krueger et al., 2021; Ahuja et al., 2021; Sagawa* et al., 2020; Ganin et al., 2016; Sun & Saenko, 2016; Dou et al., 2019; Mahajan et al., 2021).",1,neutral
"Moreover, most existing methods require environment labels that are however expensive to obtain in graphs, which limits their applications to graphs (Arjovsky et al., 2019; Krueger et al., 2021; Ahuja et al., 2021; Sagawa* et al., 2020; Ganin et al., 2016; Sun & Saenko, 2016; Dou et al., 2019; Mahajan et al., 2021).",1,neutral
"On Euclidean data, Invariant Learning (Arjovsky et al., 2019; Creager et al., 2021; Ahuja et al., 2021), Group Distributionally Robust Optimization (GroupDro) (Krueger et al., 2021; Sagawa* et al., 2020; Zhang et al., 2022), Domain Adaption (DA) and Domain Generalization (DG) (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2018b; Dou et al., 2019; Mahajan et al., 2021; Wang et al., 2021) are three widely adopted approaches to enable OOD generalization.",2,positive
", 2022), Domain Adaption (DA) and Domain Generalization (DG) (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2018b; Dou et al., 2019; Mahajan et al., 2021; Wang et al., 2021) are three widely adopted approaches to enable OOD generalization.",2,positive
"…of E in enforcing the independence that is often adopted by previous objectives (Arjovsky et al., 2019; Krueger et al., 2021; Ahuja et al., 2021; Sagawa* et al., 2020; Ganin et al., 2016; Sun & Saenko, 2016; Dou et al., 2019; Mahajan et al., 2021), making the identification of Gc more challenging.",2,positive
"…Domain Generalization(Rojas-Carulla et al., 2018; Chuang et al., 2020; Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2018b; Dou et al., 2019; Mahajan et al., 2021; Wang et al., 2021), which aim to learn the class conditional invariant representation shared across source domain and target…",2,positive
", 2016) N/A R Yes N/A MatchDG (Mahajan et al., 2021) N/A R Yes FIIF GroupDro (Sagawa* et al.",1,neutral
"…et al., 2021; Sagawa* et al., 2020; Zhang et al., 2022), Domain Adaption (DA) and Domain Generalization (DG) (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2018b; Dou et al., 2019; Mahajan et al., 2021; Wang et al., 2021) are three widely adopted approaches to enable OOD generalization.",2,positive
"Given a dataset with auxiliary attributes and their relationship with the target label, CACM constrains the model’s representation to obey the conditional independence constraints satisfied by causal features of the label, generalizing past work on causality-based regularization [10, 14, 15] to multi-attribute shifts.",1,neutral
"Partly because advances in representation learning for DG [8, 9, 10, 7, 11, 12] have focused on either one of the shifts, these studies find that performance of state-of-the-art DG algorithms are not consistent across different shifts: algorithms performing well on datasets with one kind of shift fail on datasets with another kind of shift.",1,neutral
Our canonical multi-attribute graph generalizes the DG graph from [10] that considered an Independent domain/environment as the only attribute.,2,positive
"We evaluate two constraints motivated by DG literature [10]: unconditional Xc ⊥⊥ A|E, and conditional on label Xc ⊥⊥ A|Y,E.",1,neutral
"We utilize a strategy from past work [10, 14] to use graph structure of the underlying data-generating process (DGP).",2,positive
"For instance, Ahmed et al. (2021) proposes to match the output distribution spaces from different domains via some divergence, while a recent work (Mahajan et al., 2021) also leverages a matching-based algorithm that resorts to shared representations of cross-domain inputs from the same object.",1,neutral
"(2021) proposes to match the output distribution spaces from different domains via some divergence, while a recent work (Mahajan et al., 2021) also leverages a matching-based algorithm that resorts to shared representations of cross-domain inputs from the same object.",1,neutral
"We also compare our proposed method with a variety of state-of-the-art domain generalization methods, including DeepAll [59], Jigen [3], CCSA [23], MMD-AAE [16] , CrossGrad [35], DDAIG [60], L2A-OT [59], ATSRL [51], MetaReg [2] , Epi-FCR [15], MMLD [21], CSD [29], InfoDrop [37], MASF [7], Mixstyle [61], EISNet [44], MDGH [20], RSC [11] and FACT [49].",2,positive
"[51], we introduce the causal domain features in this work to learn the domain-independent representation and reduce the domain gap, as shown in Fig.",1,neutral
"[15] Divyat Mahajan, Shruti Tople, and Amit Sharma.",0,negative
"From OOD generalization literature on images [32, 24], we may use one of these 192 methods to learn Xc, Zc from observed data: regularization [15, 13], weighting [19, 29], or data 193 augmentation [32].",1,neutral
"110 Benchmarking OOD algorithms, especially those that proposed to view distribution shift from a 111 causal perspective [6, 1, 3, 10, 11, 8] since we have mimicked many of their assumptions about the 112 underlying data generating process in our dataset.",2,positive
"Another line of work tries to learn stable/causal features across domains by learning disentangled features (Mahajan et al., 2021; Zhang et al., 2021b; Li et al., 2021).",1,neutral
This set may include both spurious and robust features if these distributions vary across domains [18].,1,neutral
"A related decomposition has been used in the context of causal approaches to robust learning [16, 17, 18], where prior knowledge is used to map all non-causal features to spurious features, treating only causal features as predictive.",1,neutral
"[14] Divyat Mahajan, Shruti Tople, and Amit Sharma.",0,negative
"Recent work uses multiple sufficiently different environments to generalize to unseen test data that lies in the support of the given environments or subgroups [2, 12, 13, 14, 15, 16, 17, 18, 19].",1,neutral
"Methods for invariant representation learning [2, 14, 15, 29] typically require data from multiple different environments.",1,neutral
"Methods based on conditional distribution matching [14, 29] build representations that are conditionally independent of the environment variable given the label.",1,neutral
"Another line of work [7, 8, 9, 14, 15] considers domain generalization for unstructured data using specifically designed causal graphs as a tool to model the distribution shift and guide the learning method, which is more task targeted.",1,neutral
give a causal interpretation to DG and generalize based on invariant representations [13].,1,neutral
"(2019), there have been several interesting works — (Teney et al., 2020; Krueger et al., 2020; Ahuja et al., 2020; Chang et al., 2020; Mahajan et al., 2020) is an incomplete representative list — that build new methods inpired from IRM to address the OOD generalization problem.",2,positive
"…from Arjovsky et al. (2019), there have been several interesting works — (Teney et al., 2020; Krueger et al., 2020; Ahuja et al., 2020; Chang et al., 2020; Mahajan et al., 2020) is an incomplete representative list — that build new methods inpired from IRM to address the OOD generalization problem.",2,positive
"[35] provide a causal interpretation of domain generalization, which is similar to ours in letting input features be generated by domain-specific and domaininvariant factors/features.",1,neutral
"To overcome the inequality between training and test estimation, many works in domain generalization and adaptation have been proposed (Arjovsky et al., 2019; Zhao et al., 2020; Mahajan et al., 2021; Zhang et al., 2020; Wang et al., 2020b), but casting theses methods into FL is not trivial.",2,positive
"It learns two match functions, one on the different source domains as per MatchDG, and the other on the augmented domains using Perfect-Match.",2,positive
"Note that Perfect-Match is an ideal training algorithm that assumes knowledge of ground-truth matches across domains, and therefore cannot be applied in real-world settings.",1,neutral
We follow the same procedure as in [33] for the OOD training and evaluation of methods.,2,positive
"In comparison, algorithms like Perfect-Match [22, 33] that ideally capture only the stable features, perform well on both membership inference and attribute inference.",1,neutral
"However, in
real datasets, augmentations can also provide perfect matches, leading to the Hybrid approach using both the MatchDG and augmented Perfect-Match.",1,neutral
"To evaluate on a more practical scenario, we use the dataset of Chest X-rays images from [33], that comprises of data from different hospital systems: NIH [54], ChexPert [24] and RSNA [1].",2,positive
"There are also works that are inspired by causality [4, 17, 32, 33, 40, 43] and take different interpretations towards a causal framework for domain generalization.",1,neutral
"We consider two types of ML models: standard empirical risk minimizers, and state-of-theart domain generalization learning algorithms that claim to learn stable features for OOD generalization [4,33,41].",1,neutral
"Perfect-Match [22, 33].",0,negative
"For simulated datasets like Rotated-MNIST and Fashion-MNIST, we know the groundtruth matches for each input and use that to evaluate the mean rank metric (the same matches are not provided to the DG methods, except to the ideal Perfect-Match method).",1,neutral
"Improving upon the class-based regularization above, matching methods based on causal inference also been proposed that aim to match representations of inputs that share the base object [33].",1,neutral
"• Among ML algorithms, a method based on selfaugmentations (Perfect-Match [22,33]) provides the best privacy robustness.",1,neutral
"For the matching based methods (Random-Match, MatchDG, Perfect-Match), we use the final classification layer of the network as φ and for the matching loss regularizer (Eq 2 ).",1,neutral
"[51] show that causal learning methods [4,33] that aim to learn stable features across distributions alleviate membership privacy risks compared to associational models such as neural networks.",1,neutral
"Instead, we use the property described by causal domain generalization work [19, 33] that input images that share the base object have the same stable (or causal) features.",1,neutral
ate whether a model has learned stable features or not [33].,1,neutral
"Here, the different images across domains share the same base causal object and provide access to true matches required for Perfect-Match approach.",1,neutral
"Other work includes causal inference, which is challenging or even impossible without strong assumptions that we do not use (Arjovsky et al., 2019; Mahajan et al., 2020), and adversarial robustness, where the goal is to build classifiers that are locally Lipschitz or smooth in a given radius around training data (Goodfellow et al.",1,neutral
"Other work includes causal inference, which is challenging or even impossible without strong assumptions that we do not use (Arjovsky et al., 2019; Mahajan et al., 2020), and adversarial robustness, where the goal is to build classifiers that are locally Lipschitz or smooth in a given radius around…",1,neutral
