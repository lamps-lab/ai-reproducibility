text,target_M6_predict,target_predict_M6_label
"Merge multiple teachers FedDF [141] addresses the quality loss issue[87] of BN, break the knowledge barriers among heterogeneous client models END(2) [157] distilling the distribution of the predictions from an ensemble instead of the average prediction AE-KD [47] regard the ensemble knowledge distillation as a multi-objective optimization problem FedFTG [258] a data-free knowledge distillation method, which relieves the issue of direct model aggregation Merge multiple students Batch Ensemble [235] mini-batch friendly, parallelizable within a device, minor memory overhead Hydra [220] improve distillation performance while capturing the uncertainty behavior of the original ensemble LatentBE [163] average a student with multiple subnetworks, giving a single student network with no additional inference cost",2,positive
"Ensemble distillation (ED) [141, 157] distills the average output of multiple teachers to a student model, which can make up for the shortcomings of a single teacher model and provide more diversified and comprehensive information to the student model.",2,positive
"Therefore, Uep = H( 1 M ∑M i=1 p ) − 1 M ∑M i=1 H(p ) [13].",1,neutral
"According to the variational Dirichlet, the uncertainty can be estimated by differential entropy, which is regarded as a more effective estimation method in various uncertainty scenarios (data and knowledge uncertainties) [24].",1,neutral
"The non-Bayesian uncertainty estimation method utilizes the model’s output as variational Dirichlet parameters [24, 25, 26] and depicts the predicted probability distribution based on its prior conjugate properties of the multinomial distribution.",1,neutral
"As shown in Figure 1, this work makes each sub-model estimate the uncertainty through Dirichlet prior [24, 25, 26] in the learning phase and combines the diversity constraints to obtain the sub-model space for selection.",1,neutral
"Various prior work which uses the Dirichlet distribution and function-space regularization (Malinin and Gales, 2018; Malinin et al., 2020; Joo et al., 2020; Sensoy et al., 2018, 2021) can be viewed as a special case of fVI.",1,neutral
"They can be used to distill a trained ensemble into a single model (Malinin et al., 2020).",1,neutral
"In addition, EnD2-H and EnD2-S are able to outperform EnD2 on both benchmarks, indicating the effectiveness of the unification operations.",2,positive
Type (a) covers EnD [24] and EnD(2) [31].,1,neutral
"For Type (b), the unification operators are applied to EnD and EnD2 to enhance their robustness against the certainty inconsistency issue.",1,neutral
"To satisfy such a need, the concept of ensemble-distillation [23], [24], [25], [26], [27], [28], [29], [30], [31] can be leveraged since it provides a flexible platform for com-",1,neutral
"These variants can be classified into three main types: type (a), type (b), and type (c), as shown in Table I. Type (a) covers EnD [24] and EnD2 [31].",1,neutral
"This category includes EnD-H, EnD-S, EnD2-H, and EnD2-S.",1,neutral
"(1), while EnD2 is a variant of EnD elaborated in Section A1 of the appendix, available online.",0,negative
(a) SensorNet [14] ✓ ✓ ✗ ✗ (b) EnD2 [15] ✓ ✓ ✓ ✗ (c) CLUE ✓ ✓ ✓ ✓,1,neutral
"TABLE I SOURCES OF TASK UNCERTAINTY THAT CAN BE DETECTED BY SENSORNET [14], END2 [15], AND CLUE",1,neutral
"Detection of Unreliable Task under Various Sources We compare the predictions of CLUE with another sampling-free uncertainty estimator, EnD2 [15].",1,neutral
"Additionally, Ensemble distribution distillation (EnD2) is proposed to explicitly model a distribution over output distribution of deep ensemble [15].",1,neutral
CLUE learns to explicitly parameterize this distribution by adopting the concept of distilling distribution from a DNN ensemble [15].,1,neutral
"single DNN to explicitly model a distribution over the outputs [31], [46].",1,neutral
"Predictive Uncertainty considers misclassifications that occur randomly and typically close to samples inside the generalization area [1], [6], [31], [46].",1,neutral
"Most existing ensemble methods [30, 24, 11] average the output of each model, which neglects the diversity.",1,neutral
"Furthermore, some works attempt to address specific challenges in KD, such as distilling from ensembles of teachers [56, 57] and investigating the impact of different teachers on student performance [58, 38].",1,neutral
"…feature selection (Heo et al., 2019a;b; Chen et al., 2021b), distribution learning (Ahn et al., 2019; Huang & Wang, 2017; Passalis & Tefas, 2018; Malinin et al., 2020; Yang et al., 2021), attention rephrasing (Kim et al., 2018; Ji et al., 2021), and reuse of teacher classifier (Chen et al.,…",2,positive
", 2021b), distribution learning (Ahn et al., 2019; Huang & Wang, 2017; Passalis & Tefas, 2018; Malinin et al., 2020; Yang et al., 2021), attention rephrasing (Kim et al.",2,positive
"Many one-stage KD variants have been presented recently, including but not limited to (Guo et al., 2020a; Chung et al., 2020; Malinin et al., 2020; Wu & Gong, 2021).",2,positive
"There are methods including Knowledge Distillation (KD) [43, 21] and Ensemble Distribution Distillation (EDD) [34, 13] that attempt to distil the knowledge",1,neutral
"We also investigate if Knowledge Distillation (KD) [21] and Ensemble Distribution Distillation (EDD) [34, 44] are able to imitate the uncertainties produced by a single or ensemble systems respectively.",2,positive
"There are methods including Knowledge Distillation (KD) [43, 21] and Ensemble Distribution Distillation (EDD) [34, 13] that attempt to distil the knowledge
ar X
iv :2
30 5.",1,neutral
"The performance of the proxies is compared to two baseline systems: KD when capturing confidence or entropy of a single model, and EDD in capturing mutual information from an ensemble.",1,neutral
"Finally, we perform downstream out-of-distribution detection using confidence, entropy, and MI scores from T5 Large ensemble, EDD (T5 Large), and Proxy Large.",2,positive
"One of the common practices in knowledge distillation is employing ensembles as a teacher model (Malinin et al., 2020; Ryabinin et al., 2021) based on the superior performance of the ensemble of deep neural networks (Lakshminarayanan et al., 2017; Ovadia et al., 2019), and several works already…",1,neutral
"One of the common practices in knowledge distillation is employing ensembles as a teacher model (Malinin et al., 2020; Ryabinin et al., 2021) based on the superior performance of the ensemble of deep neural networks (Lakshminarayanan et al.",1,neutral
Malinin et al.[20] argued that using the averaging operation for combination harmed the diversity of the models in an ensemble.,1,neutral
"1, 2, 3, 22 [31] Andrey Malinin, Bruno Mlodozeniec, and Mark Gales.",0,negative
[31] and normalize the area under the curve by that of an oracle and subtracts a baseline score with randomly sorted samples.,1,neutral
"(2015), condensing the knowledge from a possibly complex teacher model into a simpler student surrogate has been an active research topic (e.g. Vadera et al., 2020; Malinin et al., 2020; Ryabinin et al., 2021; Zhou et al., 2022; Hen et al., 2021).",2,positive
"Since the pivotal work of Hinton et al. (2015), condensing the knowledge from a possibly complex teacher model into a simpler student surrogate has been an active research topic (e.g. Vadera et al., 2020; Malinin et al., 2020; Ryabinin et al., 2021; Zhou et al., 2022; Hen et al., 2021).",1,neutral
"By incorporating the knowledge learned by a more complex model, the student’s performance can be enhanced [15, 26, 33].",1,neutral
"Ensemble models on the other hand, have been shown to be promising teachers from the early work of [9] until recent works in various domains [10, 67, 53, 55] and with techniques to boost the their performance [51, 17, 41].",1,neutral
"A common criticism of Deep Ensembles is that they are expensive to deploy [14, 25, 50], as the cost scales linearly with the ensemble size.",1,neutral
"There are various methods (Pearce et al., 2018; Malinin et al., 2019; Abdar et al., 2021) (both Bayesian and non-Bayesian) to combine with model ensembling to estimate confidence.",1,neutral
[15] proposed an ensemble approach to improve the calibration of the model by using the prior networks [14] in the distillation framework.,1,neutral
"…(first-order) categorical distributions P1(Θ), where
Θ = { θ = (θ1, . . . , θK) ∈ [0, 1]K | ∥θ∥1 = 1 } (see (Sensoy et al., 2018; Malinin & Gales, 2018; 2019; Malinin et al., 2020b; Charpentier et al., 2020; Huseljic et al., 2020; Kopetzki et al., 2021; Tsiligkaridis, 2021; Bao et al., 2021;…",1,neutral
"…2019; Malinin et al., 2020b; Charpentier et al., 2020; Huseljic et al., 2020; Kopetzki et al., 2021; Tsiligkaridis, 2021; Bao et al., 2021; Hammam et al., 2022) and regression (Amini et al., 2020; Ma et al., 2021; Malinin et al., 2020a; Charpentier et al., 2022; Oh & Shin, 2022; Pandey & Yu, 2022).",0,negative
"Different loss functions of this kind have been proposed for classification (Sensoy et al., 2018; Malinin & Gales, 2018; 2019; Malinin et al., 2020b; Charpentier et al., 2020; Huseljic et al., 2020; Kopetzki et al., 2021; Tsiligkaridis, 2021; Bao et al., 2021; Hammam et al., 2022) and regression (Amini et al., 2020; Ma et al., 2021; Malinin et al., 2020a; Charpentier et al., 2022; Oh & Shin, 2022; Pandey & Yu, 2022).",2,positive
"Here, the most commonly used
parameterised class of second-order distributions P2(M) is the set of Dirichlet distributions with parameter space
M = { m = (m1, . . . ,mK) |mi > 0, i = 1, . . . ,K } having support on the (first-order) categorical distributions P1(Θ), where
Θ = { θ = (θ1, . . . , θK) ∈ [0, 1]K | ∥θ∥1 = 1 } (see (Sensoy et al., 2018; Malinin & Gales, 2018; 2019; Malinin et al., 2020b; Charpentier et al., 2020; Huseljic et al., 2020; Kopetzki et al., 2021; Tsiligkaridis, 2021; Bao et al., 2021; Hammam et al., 2022)).",1,neutral
"Different loss functions of this kind have been proposed for classification (Sensoy et al., 2018; Malinin & Gales, 2018; 2019; Malinin et al., 2020b; Charpentier et al., 2020; Huseljic et al., 2020; Kopetzki et al., 2021; Tsiligkaridis, 2021; Bao et al., 2021; Hammam et al., 2022) and regression…",1,neutral
"Another line of research modifies (8) to the regression setting (Malinin et al., 2020a; Charpentier et al., 2022).",1,neutral
"In general, more models yield better ensemble gains (Malinin, Mlodozeniec, and Gales 2019).",2,positive
"Some methods estimate uncertainty using Bayesian methods [15, 18, 47] and deep ensemble [21,26].",1,neutral
Malinin & Gales (2018); Malinin et al. (2020); Sensoy et al. (2018); Charpentier et al. (2020) assume the classifier outputs are sampled from a latent Dirichlet distribution and treat low-likelihood samples as misclassified samples.,2,positive
"Some of the works also explore training a single Student model from an ensemble of networks to achieve higher performance with fewer computations [26], [27], [28], [29].",1,neutral
"As a result, over the past several years the area of knowledge distillation has gained increasing attention (Hinton et al., 2015; Freitag et al., 2017; Malinin et al., 2019; Lin et al., 2020; Park et al., 2021; Zhao et al., 2022).",2,positive
From the literature on Bayesian networks we can borrow a more informative metric: the mutual information (MI) between the model distribution and the output distribution [44].,1,neutral
"This problem can be well handled by using an additional assistant model [39], distilling intermediate features [33], multiple teacher models [49], and the ensemble of distributions [36].",1,neutral
"The idea of distillation from the ensemble of model for uncertainty estimation is also investigated [8, 26, 29, 30].",1,neutral
"metrics based on ROC [18] or Rejection-Accuracy curves [10, 14]), however, it has already been observed that these metrics favour models that have higher test accuracy [4, 21].",1,neutral
"A recently proposed metric that allows comparison of different models in this aspect, agnostic to their individual accuracy, is Prediction Rejection Ratio (PRR) [21].",1,neutral
The PRR ranges from -1 to 1.,0,negative
"On ImageNet-R, the only model with positive PRR is Swin-L, and the models with highest negative PRR are ConvNeXt-XL and L, followed by BiT-R50x1 and 101x1.",2,positive
"◦ The fact that several models are severely overconfident and wrong on
ImageNet-R (PRR) while showing low calibration errors indicate that the calibration analysis should be complemented with experiments such as misclassification detection to understand their reliability.",0,negative
This is captured by the sign of PRR (reported in %).,0,negative
"Disentangling these two sources of uncertainty is generally non-trivial [56] and ensemble methods are usually superior [23,52].",1,neutral
"Yet, the best performing approaches are computationally expensive [45], while faster variants struggle to disentangle different types of uncertainty [23,52,56].",1,neutral
"The knowledge of stochastic DNNs such as ensemble networks or MC dropout is distilled to a deterministic DNN [22], [23].",1,neutral
"In this paper, we present ModelNet, a deterministic sampling-free DNN that distills the knowledge of a stochastic DNN and estimates model uncertainty of object detection, both spatial and semantic uncertainty, with small computational cost.",2,positive
"This paper makes following key contributions:
• We propose ModelNet, a deterministic sampling-free DNN that distills the predictive distribution of a stochastic DNN and allows to quantify spatial/semantic model uncertainty of an object detector.",2,positive
"We are considering ModelNet in SSD Mobilenet-v1 which provides both detection and uncertainty predictions (IV-D1) in this section, but task DNN with ModelNet as uncertainty assistant (IV-D2) also can be employed.",2,positive
"Bayesian inference using dropout in neural networks (MC dropout) [16], [17] has been suggested to model epistemic uncertainty through a standard DNN training procedures and expanded to object detection in [18], [19].",1,neutral
"The distribution over predictive distribution of stochastic DNNs is approximated to maintain the information of diversity in stochastic DNNs and estimate model uncertainty [24], [27].",1,neutral
Monte Carlo based dropout [16] is suggested to approximate the Bayesian inference through a standard DNN training procedures.,1,neutral
This paper proposes ModelNet that estimates model uncertainty of a complex DNN based object detection without storing N models or running N forward passes.,1,neutral
"1, we first implement a stochastic DNN, MC dropout model specifically, from a target task model to quantify the model uncertainty of the task.",1,neutral
"Some prior works adopt knowledge distillation [21] and achieve model uncertainty estimation with low complexity [22]–[24], but these works are limited on classification tasks or semantic uncertainty estimation.",1,neutral
"Therefore, many other efforts seek to reduce the computational cost and utilize existing DNN framework [10], [11], [13].",2,positive
"ModelNet adopts the concept of distilling predictive distribution of stochastic DNN [24], [27], and extends it for object detection to estimate the spatial and semantic uncertainties of multiple objects.",2,positive
"However, it requires a significant modification of training procedures, excessive computational cost, and also not scalable to existing DNN architecture with largescale dataset.",2,positive
"gence metric for distillation is forward KL (FKL) divergence in Equation (2), while reverse KL divergence (RKL), which exchanges the two distributions in Equation (2), is recommended in prior work in dealing with out-of-distribution data [3], [27].",1,neutral
"This intuition tends to be demonstrated on toy, low-dimensional data [24, 26].",1,neutral
"Deep Ensembles [19] are ensembles of DNNs trained on the same dataset with different random parameter initialisations and data shuffling, Θ={θ(m)}Mm=1.",1,neutral
"com/Guoxoug/ens-div-ood-detect diversity of a Deep Ensemble is an intrinsic indicator of input distributional shift, as the members will disagree more on data that are further away from training data [24, 26].",1,neutral
"Deep Ensembles [19] have been shown to reliably be superior for OOD detection compared to single models [17, 24, 31].",1,neutral
"They have been shown to be a reliable and scalable approach for improving both the predictive performance and quality of uncertainty estimates of DNNs [19, 24, 28, 31].",1,neutral
"Thus, uncertainty scores U directly derived from diversity should be useful for OOD detection [24, 26, 31].",1,neutral
"Note that we will avoid the discussion of epistemic and aleatoric uncertainty [14,24,26,29] in this work and instead directly focus the discussion on the task of OOD detection.",2,positive
", 2020), or treating predictions from teachers as a set of samples from an implicit distribution (Malinin et al., 2020; Ryabinin et al., 2021), or amplifying the diversity via input perturbations (Nam et al.",1,neutral
"…inside the ensemble teacher, e.g., dynamically assign weights to teachers (Du et al., 2020), or treating predictions from teachers as a set of samples from an implicit distribution (Malinin et al., 2020; Ryabinin et al., 2021), or amplifying the diversity via input perturbations (Nam et al., 2021).",1,neutral
"[10] Andrey Malinin, Bruno Mlodozeniec, and Mark JF Gales, “Ensemble distribution distillation,” in International Conference on Learning Representations, 2020.",2,positive
"The main downside of ensembles is their computational and memory cost, but there has been work on overcoming this limitation [10, 34, 35].",1,neutral
"Until recently most work on uncertainty estimation[10, 6, 11, 12, 13, 14, 15] and robust generalisation has focused on small- and medium-scale image and text classification tasks, such as MNIST [16], SVHN [17], and CIFAR10/100 [18].",1,neutral
"Table 1: Predictive performance, RMSE
Dataset Single EnsembleSGB SGLB KGB SGB SGLB KGB Boston 3.06 3.12 2.81 3.04 3.10 2.82 Concrete 5.21 5.11 4.36 5.21 5.10 4.30 Energy 0.57 0.54 0.33 0.57 0.54 0.33 Kin8nm 0.14 0.14 0.11 0.14 0.14 0.10 Naval 0.00 0.00 0.00 0.00 0.00 0.00 Power 3.55 3.56 3.48 3.52 3.54 3.43 Protein 3.99 3.99 3.79 3.99 3.99 3.76 Wine 0.63 0.63 0.61 0.63 0.63 0.60 Yacht 0.82 0.84 0.52 0.83 0.84 0.50 Year 8.99 8.96 8.97 8.97 8.93 8.94
Table 2: Error and OOD detection
Dataset PRR AUCSGB SGLB KGB SGB SGLB KGB Boston 36 37 43 80 80 88 Concrete 29 29 37 92 92 93 Energy 36 31 60 100 100 99 Kin8nm 18 19 20 45 45 41 Naval 55 56 35 100 100 100 Power 8 9 31 72 73 76 Protein 30 29 35 99 99 100 Wine 25 19 37 74 72 87 Yacht 74 78 86 62 60 69 Year 30 30 32 67 57 71
Experiment on real datasets Uncertainty estimates for GBDTs have been previously analyzed by Malinin et al. (2021).",0,negative
PRR measures how well uncertainty estimates correlate with errors and rank-order them.,1,neutral
"Detecting errors can be evaluated via the Prediction-Rejection Ratio (PRR) (Malinin, 2019; Malinin et al., 2020).",1,neutral
"In [25] the authors address this by using an auxiliary unsupervised dataset, as we can get diverse ensemble predictions on this dataset and use it to train our distilled model.",2,positive
"While some previous works tried to address this issue [25, 28], they either depended on additional unsupervised data that might not be readily available, e.",0,negative
"We compare our method against the following baselines: (1) EnDD [25], Ensemble Distribution Distillation framework, trained with the same train data used to capture the ensemble.",2,positive
"While this was done in [9, 25], there are several limitations with these approaches.",1,neutral
"We show that for our method the mixup adds a significant boost to performance, and also for EnDD on two out of three datasets.",2,positive
"(2) EnDDAUX [25], Ensemble Distribution Distillation framework, trained with the train data and an auxiliary dataset.",2,positive
"It is interesting to note that while EnDDAUX uses additional unlabeled examples from the training distribution (between 50k and 100k), FED is on par and often outperforms this baseline across all metrics.",2,positive
EnDD builds on the method in [24] which tries to emulate the behaviour of an ensemble with a Dirichlet distribution parameterized by a NN.,1,neutral
Ensemble Distribution Distillation (EnDD) [25] was the first to propose such an approach.,1,neutral
"We propose an ensemble distillation [15, 25] method that mimics an ensemble of models using a lightweight model.",1,neutral
"We take a similar approach to [9, 25] where we treat the ensemble distillation as a conditional generative model.",1,neutral
"We also show results for out-of-distribution (OOD) detection in Figure 3 using ROC curves (EnDD was excluded due to poor performance), To score the examples we used the knowledge uncertainty which is obtained by subtracting the aleatoric uncertainty from the total uncertainty [10, 25].",2,positive
"We also see that even when EnDD is trained with mixup, our approach returns better accuracy, ECE and also has greater diversity.",2,positive
"Thus, we show results for both FED and EnDD trained on the training set and on the mixup dataset as the auxiliary dataset.",2,positive
"Both [25, 9] are suitable for classification tasks only and assume a Dirichlet distribution behaviour which may be too restrictive.",1,neutral
"In distillation of uncertainty, Malinin et al. (2019) propose EnDD by using a prior network (Malinin & Gales, 2018) as the student, however, their approach requires further finetuning on auxiliary data to fully capture the ensemble’s uncertainty and it works only for classification problems.",2,positive
"Ensemble distillation uses multiple teachers to improve accuracy in vision and other applications [3, 17, 34, 36, 40].",1,neutral
", distillation of an ensemble into a single model [28]).",1,neutral
"Typical examples of Dirichlet distribution [69], [82] and subjective opinion.",1,neutral
"alleviate overconfidence problem [33], [69], [82].",1,neutral
Ensemble distribution distillation [69] obtains the Dirichlet distribution by distillation from the predictions of multiple models.,1,neutral
"To capture the predictive uncertainty for single view, the Dirichlet distribution is considered to provide more trusted predictions [33], [69],",1,neutral
"Recently, uncertainty estimation algorithms based on Dirichlet distribution [22], [33], [68], [69], [70] have been proposed.",1,neutral
"It is observed that in case of ensemble models PRR due to total uncertainty is higher than knowledge uncertainty which is consistent with the observations in other studies.(12,21)",1,neutral
Epistemic (knowledge) uncertainty is estimated as dispersion or “disagreement” level of the model within the ensemble.(21),1,neutral
A higher value of PRR suggests that the model can recognize and reject inaccurate predictions based upon uncertainty measures.(21),1,neutral
The error can be evaluated by Prediction-Rejection-Ratio (PRR) which ranks the uncertainty estimation values with error margins.(21) A higher value of PRR suggests that the model can recognize and reject inaccurate predictions based upon uncertainty measures.,1,neutral
"Ensemble distillation Recently, there has been a growing interest in approximating the probabilistic output of an ensemble by a single model [2, 15, 18, 21].",1,neutral
"The computational challenges have prompted work on producing a single model to approximate the output of an ensemble; so called “ensemble distillation” [2, 15, 18, 21].",1,neutral
"Most of the focus has been on classification [15, 18], where the goal is to predict the class of the image.",1,neutral
"Lack of structure in distillation methods Previous methods focus on: classification problems [15, 18], approximating only the mean of the ensemble [2], or modelling independent per-pixel variance [21].",1,neutral
"To tackle this issue ensemble distribution distillation (En2D) was developed [Malinin et al., 2020].",2,positive
"This integrated training allows the user to bypass training a separate expensive teacher ensemble while distribution distillation [Malinin et al., 2020] allows the student to capture the diversity and model a distribution over ensemble member predictions.",2,positive
"Furthermore, since the Dirichlet distribution has bounded ability to represent diverse ensemble predictions [Malinin et al., 2020], simply generating multiple teacher prediction by propagating through the last layer will not be the limiting factor in this model.",1,neutral
"L2 : ∆ (2) K × Y −→ R+ (5) comparing level-2 predictions Q(x) with level-0 observations y, so that minimising L2 on the training data D yields a “good” level-2 predictor? This is the basic idea of direct epistemic uncertainty prediction [22, 18, 19, 20, 3, 11, 15].",1,neutral
"Yet another quite popular idea is to estimate uncertainty in a more direct way, and to let the learner itself predict, not only the target variable, but also its own uncertainty about the prediction [22, 18, 19, 20, 3, 11, 15].",1,neutral
"These key and challenging requirements have stimulated numerous solutions and research directions leading to significant progress in the estimation of DNNs uncertainty [4, 15, 17, 25, 27, 30, 31, 32, 50].",1,neutral
"…prediction over multiple base predictors through Bayesian neural network (Mackay, 1992; Gal and Ghahramani, 2016; Kendall and Gal, 2017; Malinin and Gales, 2018; Maddox et al., 2019) or ensemble methods (Lakshminarayanan et al., 2016; Ovadia et al., 2019; Huang et al., 2017; Malinin et al., 2019).",1,neutral
"In recent years, many methods [15]–[19] have been studied to quantify aleatoric uncertainty.",1,neutral
"However, ensemble models come with large memory and computational cost (Malinin et al., 2019).",1,neutral
"Ensemble models have been shown to give robust measures of uncertainty as they provide a probabilistic framework allowing knowledge uncertainty to be linked to Bayesian or non-Bayesian model uncertainty (Malinin et al., 2019).",1,neutral
"This includes popular methods such as Bayesian dropout [21, 33], deep ensembles [35, 37, 44] and variational inference [55], Laplace approximation [18, 34] and tempered posteriors [2, 3, 32] to cite a few.",1,neutral
"A series of papers developed ways of approximating the distribution obtained using an ensemble of models by a single probabilistic model [49, 50, 66].",1,neutral
"[50] Andrey Malinin, Bruno Mlodozeniec, et al.",1,neutral
Ensemble distribution distillation (EnD(2)) is a closely related approach that aims to distill the collective predictive distribution outputs of the models in an ensemble into a neural network that predicts the parameters of a Dirichlet distribution [18].,1,neutral
"Furthermore, modifications to the standard KD objective allow one to distill the knowledge in an ensemble of teachers into one student network while preserving the benefits in uncertainty estimation of ensembles [Malinin et al., 2020, Tran et al., 2020].",2,positive
"To this end, we use the Area Under Receiving Operator Characteristics Curve (AUC-ROC) with aleatoric scores u alea (Alea) and epistemic scores u (v) epist (Epist) similarly to [14, 102, 60, 63, 61, 57].",2,positive
"Important examples explicitly parameterize prior distributions [86, 63, 60, 61, 6] or posterior distributions [14, 15].",1,neutral
[17] distilled distribution of the prediction from multiple teachers to a student rather than distilled the averaged prediction directly.,1,neutral
"Many researchers have studied reducing the cost to estimate ensemble PU [1, 6, 33, 47, 50].",1,neutral
"For training purposes, they apply two different training objectives using the equivalent of the reverse KL objective of Malinin & Gales (2019) as well as of the knowledge distillation objective of Malinin et al. (2020b), which does not require OOD data for regularization purposes.",0,negative
"Multivariate evidential regression There are also some works offering solutions for multivariate regression problems: Malinin et al. (2020a) can be seen as a multivariate generalization of the work of Amini et al. (2020), where a combined Normal-Wishart prior is formed to fit the now Multivariate…",1,neutral
"Alternatively, Malinin et al. (2020b); Stadler et al. (2021); Amini et al. (2020) show or measure the area under the prediction / rejection curve, graphing how task performance varies as predictions on increasingly uncertain inputs is suspended.",1,neutral
"Malinin et al. (2020b) exploit this idea and show that prior networks can also be distilled using an ensemble of classifiers and their predicted Categorical distributions (akin to learning Figure 4e from Figure 4a), which does not require regularization at all, but comes at the cost of having to…",1,neutral
"…(Malinin & Gales, 2018; 2019; Nandy et al., 2020; Zhao et al., 2019; Hu et al., 2020; Charpentier et al., 2020; 2022), spiral data (Malinin et al., 2020b) or polynomials for regression (Amini et al., 2020; Oh & Shin, 2022; Meinert & Lavin, 2021; Malinin et al., 2020a; Charpentier et al., 2022).",1,neutral
"Instead of training an ensemble, diverse predictions are obtained from the teacher network through the use of Gaussian dropout, which are distilled into a Dirichlet distribution as in Malinin et al. (2020b).",1,neutral
"Furthermore, some approaches (Malinin & Gales, 2018; 2019; Nandy et al., 2020; Malinin et al., 2020a) require out-of-distribution data points during training.",2,positive
"Additional cost is mostly only produced when using knowledge distillation (Malinin et al., 2020b; Fathullah & Gales, 2022), adding normalizing flow components like for posterior networks (Charpentier et al., 2020; 2022; Stadler et al., 2021) or using generative models to produce synthetic OOD data…",2,positive
"…using Gaussians (Malinin & Gales, 2018; 2019; Nandy et al., 2020; Zhao et al., 2019; Hu et al., 2020; Charpentier et al., 2020; 2022), spiral data (Malinin et al., 2020b) or polynomials for regression (Amini et al., 2020; Oh & Shin, 2022; Meinert & Lavin, 2021; Malinin et al., 2020a; Charpentier…",1,neutral
"…2019; Nandy et al., 2020; Shen et al., 2020; Chen et al., 2018; Zhao et al., 2019; Hu et al., 2021; Sensoy et al., 2020), knowledge distillation (Malinin et al., 2020b;a) or the incorporation of density estimation (Charpentier et al., 2020; 2022; Stadler et al., 2021), which we discuss in more…",2,positive
"The methods listed in Table 3 either choose the Normal-Inverse Gamma distribution (Amini et al., 2020; Charpentier et al., 2022), inducing a scaled inverse-χ2 posterior (Gelman et al., 1995),13 or a Normal-Wishart prior (Malinin et al., 2020a).",1,neutral
"There are however some ideas to make them less expensive by distilling their uncertainties into simpler models (Malinin et al., 2019; Tran et al., 2020; Havasi et al., 2020; Antorán et al., 2020).",2,positive
"…to use a hierarchical output distribution, for instance a Dirichlet distribution (Milios et al., 2018; Sensoy et al., 2018; Malinin & Gales, 2018; Malinin et al., 2019; Malinin & Gales, 2019; Hobbhahn et al., 2020; Nandy et al., 2020), such that the model uncertainty can be encoded in the…",1,neutral
"There are however some ideas to make them less expensive by distilling their uncertainties into simpler models [39, 55, 26, 3].",1,neutral
"Another popular idea is to use a hierarchical output distribution, for instance a Dirichlet distribution [40, 50, 38, 39, 30], such that the model uncertainty can be encoded in the Dirichlet and the data uncertainty in its Categorical distribution samples.",1,neutral
"Some works study the knowledge transfer process on multi-teacher KD approaches, such as multiple heterogeneous teachers [28] or homogeneous teachers [7, 23].",1,neutral
Distillation of ensemble models [41] could be an interesting avenue to explore in this regards.,1,neutral
Distillation of ensembles is a good direction to explore in this context [41].,1,neutral
Recently Malinin et al. (2019) and Ryabinin et al. (2021) proposed ensemble distribution distillation (EnD2) - an approach to distill an ensemble into a single model which preserves both the ensemble’s improved performance and full set of uncertainty measures at low inference cost.,2,positive
"(5)
Originally, Malinin et al. (2019) implemented EnD2 on the CIFAR10, CIFAR100 and TinyImageNet datasets.",2,positive
"Recently, Malinin et al. (2019) proposed ensemble distribution distillation (EnD2) as an approach to distill an ensemble into a single prior network model (Malinin and Gales, 2018), such that the model retains information about ensemble diversity.",1,neutral
"through distillation into a single model [65, 94]).",1,neutral
"[65] Andrey Malinin, Bruno Mlodozeniec, and Mark Gales.",0,negative
"Following [9] and [26], we compute the two types of uncertainty as follows:",1,neutral
"Prior Networks have also been used for Ensemble Distribution Distillation [34, 33, 35] — a distillation approach through which the predictive performance and uncertainty estimates of an ensemble are captured within a single Prior Network, reducing the inference cost to that of a single model.",1,neutral
"instead of directly predicting the output [90], [45].",1,neutral
"5) Making Ensemble Methods more Efficient: Compared to single model methods, ensemble methods come along with a significantly increased computational effort and memory consumption [217], [45].",1,neutral
"1 [71], [72] 2 [46] 3 [36], [35] 4 [32] 5 [44] 7 [35] 8 [73], [74], 9 [75], [30] 10 [76], [77] 11 [20] 12 [78], [79], [80] 13 [81] 14 [82] 15 [83] 16 [63] 17 [84] 18 [31] 19 [85] 20 [86] 21 [87], [88], [89] 22 [90], [45] 23 [39] 24 [40] 25 [91]",0,negative
[45] modelled ensemble members and the distilled network as prior networks,1,neutral
"This limits the deployment of ensemble methods in many practical applications where the computation power or memory is limited, the application is time-critical, or very large networks with high inference time are included [45].",1,neutral
"it has been shown that they are capable of delivering good and for some experiments even comparable results [90], [45], [247].",1,neutral
"Quantiles and prediction intervals can also be obtained by aggregating multiple predictors, such as using Bayesian neural networks [41, 24, 32, 44, 42] or ensembles [37, 49, 28, 45].",1,neutral
"[28] proposed a method to model the implicit distribution over predictive distributions from which the ensemble component predictive distributions are drawn, rather than just the ensemble model average.",1,neutral
"In order to avoid the computational cost of the multiple forwards, [17] and [18] proposes to use distillation.",2,positive
"Consequently, it is no longer possible to obtain estimates of knowledge uncertainty which is particularly useful for anomaly detection [5, 10].",1,neutral
"Ensemble Distribution Distillation offers a straightforward way to model the ensemble predictions [10, 13].",1,neutral
"[10] recently proposed a class of distillation techniques called Ensemble Distribution Distillation (EnD(2)), where the goal is to capture both the mean and the diversity of an ensemble within a single model.",1,neutral
"Recently, several works have proposed distillation procedures that capture information about both the mean as well as the distribution of ensemble predictions within a single model [10, 11, 12, 13].",1,neutral
"For classification, we compare NatPN to Reverse KL divergence Prior Networks (R-PriorNet) (Malinin & Gales, 2019), Ensemble Distribution Distillation (EnD2) (Malinin et al., 2020b) and Posterior Networks (PostNet) (Charpentier et al., 2020).",2,positive
"However, only two works (Amini et al., 2020; Malinin et al., 2020a) have focused on regression by learning parameters of a Normal Inverse-Gamma (NIG) distribution as conjugate prior.",1,neutral
"Rejection curves are summarised using the Prediction Rejection Ratio (PRR) (Malinin, 2019; Malinin et al., 2020), describe in appendix D.",2,positive
The ensemble-distillation schemes include EnD [43] and its recent revision EnD(2) [50].,2,positive
"It is observed that the student models trained under our proposed framework with fChannel (i.e., ‘Ours (Channel)’) is able to outperform the previous ensemble-distillation baselines, i.e., EnD [43] and EnD2 [50], by a margin of 6.64% mIoU and 6.02% mIoU on GTA5→Cityscapes, and 6.41% mIoU and 4.57% mIoU on SYNTHIA→Cityscapes, respectively.",2,positive
The authors in [50] aimed at resolving the diversity collapse issue in the ensemble-distillation problem.,1,neutral
", EnD [43] and EnD(2) [50], by a margin of 6.",1,neutral
"The evaluation results of EnD [43] and EnD(2) [50] are obtained from our self-implemented models, while those of the remaining baselines are directly obtained from their original papers.",0,negative
The ensemble-distillation schemes include EnD [43] and its recent revision EnD2 [50].,2,positive
"It is observed that the performance of the students trained with ‘Ours (Pixel)’, EnD, and EnD2 all degrade when the number of under-performing members increases.",0,negative
89 Ours (Channel) Ours (Pixel) EnD[43] EnD [50] 2,1,neutral
"[27] distilled an ensemble into the prior network [28], which models a conditional distribution over categorical distributions by parameterizing a Dirichlet distribution to disentangle the total uncertainty into data uncertainty and knowledge uncertainty.",1,neutral
"Knowledge distillation aims to transfer the knowledge from one deep learning model (the teacher) to another (the student), such as distilling a large network into a smaller one [19, 49, 2, 48, 12] or ensembling a collection of models into a single model [29, 37, 27, 45].",1,neutral
"Due to the nature of using multiple networks, it is also effective in various problem settings such as adversarial attack and out-of-distribution detection [13, 6, 5, 17].",1,neutral
"Due to the nature of ensembles that use multiple networks, they are also effective in various problem settings such as adversarial attack and out-of-distribution detection [13, 5, 17].",1,neutral
EnD2 [41] distilled the distribution of the predictions of the ensemble into a single model.,1,neutral
"Similarly, EnD2 [41] also distills the prediction distribution from an ensemble into a single model.",1,neutral
"Following EnD2 [41], we compare FFSD with EnD2 on VGG-16 [59].",2,positive
"Therefore, the distillation of both dropout sampling and ensemble techniques into a single network has been proposed in [14], [15].",1,neutral
Malinin et al. (2020a)’s approach focuses on estimating both model and data noise uncertainty.,2,positive
This enables optimizing the upper UB as acquisition function without the need for further approximation via ensemble distillations[25].,1,neutral
"In a recent working paper (and concurrent to our work), Malinin et al. (2020a) report on progress extending their idea to regression.",2,positive
"In this Section, we highlight several differences of NOMU compared to prior regression networks that were recently introduced in a working paper by Malinin et al. (2020a).",1,neutral
"[25] Andrey Malinin, Bruno Mlodozeniec, and Mark Gales.",0,negative
"See also Kuleshov et al. (2018) for a non-linear calibration method.
function without the need for further approximation via ensemble distillations(Malinin et al., 2020b).",1,neutral
"By OOD they refer to input training points only far away from the training data, e.g., in (Malinin et al., 2020a, Section 3) µOOD only has support far away from the convex hull of the input training points.",1,neutral
"Some work using Bayesian epistemic uncertainty for OOD detection explicitly rejects the use of aleatoric uncertainty (Malinin & Gales, 2018; Malinin et al., 2020; Wen et al., 2019; Choi et al., 2018; Postels et al., 2020), while other work implicitly combines aleatoric and epistemic uncertainty by looking at the overall predictive entropy (Lakshminarayanan et al.",1,neutral
"The most typical approach to Bayesian OOD distribution detection uses epistemic uncertainty (Lakshminarayanan et al., 2017; Malinin & Gales, 2018; Choi et al., 2018; Wen et al., 2019; Malinin et al., 2020; Postels et al., 2020).",1,neutral
"Some work using Bayesian epistemic uncertainty for OOD detection explicitly rejects the use of aleatoric uncertainty (Malinin & Gales, 2018; Malinin et al., 2020; Wen et al., 2019; Choi et al., 2018; Postels et al., 2020), while other work implicitly combines aleatoric and epistemic uncertainty by…",1,neutral
"Another line of work improves calibration by aggregating the probabilisitic predictions over multiple models, using either an ensemble of models (Lakshminarayanan et al., 2016; Malinin et al., 2019; Wen et al., 2020; Tran et al., 2020), or randomized predictions such as Bayesian neural networks (Gal & Ghahramani, 2016; Gal et al.",1,neutral
"…calibration by aggregating the probabilisitic predictions over multiple models, using either an ensemble of models (Lakshminarayanan et al., 2016; Malinin et al., 2019; Wen et al., 2020; Tran et al., 2020), or randomized predictions such as Bayesian neural networks (Gal & Ghahramani, 2016; Gal…",1,neutral
"Recent deep CNNs have also been shown to benefit from this strategy (Geiger et al. 2020; Ilg et al. 2018; Lan, Zhu, and Gong 2018; Lee and Chung 2020; Lee et al. 2015; Malinin, Mlodozeniec, and Gales 2020).",2,positive
"In many resource-constrained settings it is important to be able to use a representative neural-network to make predictions, rather than needing to run the inputs through many samples from a posterior (e.g. Malinin et al., 2019).",1,neutral
Ensembles involve memory and computational cost which is not acceptable in many application [197].,1,neutral
"[197] Image Neural Networks Ensemble Distribution Distillation (EnD2) L(φ,Dens) = − 1 N ∑N",1,neutral
"Let us consider from the posterior sampled ensemble of models {P (y|x, θ)}m=1 as follows [197]:",1,neutral
"An important, quickly growing family of models is the Dirichlet-based uncertainty (DBU) family (Malinin & Gales, 2018a; 2019; Sensoy et al., 2018; Malinin et al., 2019; Charpentier et al., 2020; Zhao et al., 2020; Nandy et al., 2020; Shi et al., 2020; Sensoy et al., 2020).",2,positive
"…a loss that computes the sum of squares between the on-hot encoded true label y∗(i) and the predicted categorical p(i) under the Dirichlet distribution:
LEvNet = 1
N ∑ i Ep(i)∼Dir(α(i))||y ∗(i) −p(i)||2 (3)
Ensemble Distribution Distillation (DDNet) (Malinin et al., 2019) is trained in two steps.",1,neutral
"Ensemble Distribution Distillation (DDNet) (Malinin et al., 2019) is trained in two steps.",2,positive
"One possible solution is to distill the ensemble into a single model [14, 15].",1,neutral
"Recently Malinin et al. (2020) showed that by instead distilling the distribution over the ensemble into a prior network (Malinin and Gales, 2018), the student can learn to model both the epistemic and aleatory uncertainty of the ensemble.",1,neutral
Ensemble Distribution Distillation [15] proposes a way to both preserve the distributional information of an ensemble and improve the performance of a Prior Network.,1,neutral
"This distribution could be seen as the ensemble estimate of the posterior, p(θ|D), (Malinin et al., 2019; Malinin and Gales, 2020).",1,neutral
"Recently, there have been ensemblebased attempts [4, 41, 38, 22] to train a student network based on many peers or students without considering the single teacher network, which is a slight lack of considerar X iv :2 00 9.",0,negative
"To examine the effectiveness of using deep ensembles as teachers [32], we train an ensemble of deterministic neural networks with aleatoric uncertainty [25].",1,neutral
"Most similar to our method is [32], which uses the Dirichlet distribution to approximate predictive distribution of an ensemble of networks.",1,neutral
Dirichlet distribution is not used to approximate teacher’s predictive distribution for classification as in [32] because we empirically found it very numerically unstable and led to failure of convergence.,1,neutral
[32] proposed a similar distillation approach from an ensemble of networks.,1,neutral
"(2018), Malinin et al. (2020) and Vadera et al.",1,neutral
"In recent work, Wang et al. (2018), Malinin et al. (2020) and Vadera et al. (2020a) have leveraged this decomposition to explore a range of down-stream tasks that rely on uncertainty quantification and decomposition.",1,neutral
"For example, Depeweg et al. (2017) and Malinin et al. (2020) describe the decomposition of the entropy of the posterior predictive distribution (the total uncertainty) into expected data uncertainty and knowledge uncertainty.",1,neutral
"KD has been used to provide a fast approximation to Bayesian Neural Networks [4, 38, 16].",1,neutral
Bayesian Neural Networks implicitly estimate the uncertainty via MonteCarlo sampling of the network parameters.,1,neutral
", 2020) and ensemble distribution distillation (Malinin et al., 2020).",2,positive
"…et al., 2018), scalable Gaussian processes (Milios et al., 2018), sampling-free uncertainty estimation (Postels et al., 2019), data augmentation (Patel et al., 2019; Thulasidasan et al., 2019; Yun et al., 2019; Hendrycks et al., 2020) and ensemble distribution distillation (Malinin et al., 2020).",1,neutral
"This suggests that the curriculum learning introduced by temperature annealing is useful, but not as critical as it is for classification EnD(2) [21].",1,neutral
"Curiously, standard ensemble distillation (EnD) does much worse than even a single model, which was an effect that was also observed for classification models in [21].",1,neutral
"In Table 2 we compare uncertainty measures derived from all models on the tasks of error detection and OOD detection, which are evaluated using Prediction Rejection Ratio (PRR) [21, 15] and AUCROC [28], respectively.",2,positive
Table 7 shows the error detection peformane of all models in terms of prediction-rejection ratio (% PRR) [15].,1,neutral
"An interesting task which Prior Networks can solve is Ensemble Distribution Distillation (EnD(2)) [21], where the distribution of an ensemble’s predictions is distilled into a single model.",1,neutral
Similarly to [21] we propose a temperature-annealing trick to make the optimization process easier.,1,neutral
"This can be overcome via Ensemble Distribution Distillation (EnD(2)) [21], which is an approach that allows distilling ensembles into Prior Networks such that measures of ensemble diversity are",1,neutral
"To reduce inference time of ensembles, one could use a single network to mimic behavior of ensembles as pioneered by born-again tree [6] and knowledge distillation [5, 7, 19, 32].",1,neutral
"Table 2 shows that measures of knowledge uncertainty yield superior OOD detection performance compared to total uncertainty in terms of AUC-ROC, which is consistent with results for non-GBDT models [10, 26, 24].",2,positive
"Test errors can occur due to both data and knowledge uncertainty, so we expect that ranking elements by total uncertainty would give better values of PRR. Table 2 shows that measures of total uncertainty consistently yield better PPR results across all classification and regression datasets.",1,neutral
"This is consistent with results obtained for ensembles of neural networks models [14, 10, 26, 24].",1,neutral
For PRR and AUC-ROC results (for classification and Years) we highlight the best value.,0,negative
"Error detection and rejection based on measures of uncertainty is assessed via the Prediction-Rejection Ratio (PRR) [10, 24], which measures how well uncertainty estimates correlate with errors and rank-order them.",1,neutral
"More recently, there has also been work on the importance of distillation from an ensemble of model [24], which provides a complementary view on the role of predictive diversity.",1,neutral
") [26], which learns Dirichlet distributions with maximum likelihood by using soft-labels from an ensemble of networks.",1,neutral
"For example, Depeweg et al. (2017) and Malinin et al. (2020) describe the decomposition of the entropy of the posterior predictive distribution (the total uncertainty) into expected data uncertainty and knowledge uncertainty.",1,neutral
(2018) and Malinin et al. (2020) has investigated leveraging multiple statistics of ensembles (both general ensembles and Monte Carlo representations of Bayesian posteriors) for performing tasks that leverage uncertainty quantification and uncertainty decomposition including out-ofdistribution detection and uncertainty-based ranking.,1,neutral
"In recent work, Wang et al. (2018) and Malinin et al. (2020) have leveraged this decomposition to explore a range of down-stream tasks that rely on uncertainty quantification and decomposition.",1,neutral
"Ensemble distribution distillation (EnD2) is a closely related approach that aims to distill the collective outputs of the models in an ensemble into a neural network that predicts the parameters of a Dirichlet distribution (Malinin et al., 2020).",1,neutral
"We compare the GPED framework to the full Monte Carlo ensemble as well as to an adaptation of Ensemble Distribution Distillation (EnD2) (Malinin et al., 2020).",2,positive
We further show that our direct generalized posterior distillation framework outperforms an adaptation of the approach of Malinin et al. (2020) both on terms of distillation performance and in terms of several downstream tasks that leverage uncertainty quantification.,2,positive
"Our goal in this paper is broadly similar, although we focus specifically on distilling much larger Monte Carlo posterior ensembles and we avoid the parametric distribution assumptions of (Malinin et al., 2020) by directly distilling posterior expectations of interest.",2,positive
"Indeed, recent work including Wang et al. (2018) and Malinin et al. (2020) has investigated leveraging multiple statistics of ensembles (both general ensembles and Monte Carlo representations of Bayesian posteriors) for performing tasks that leverage uncertainty quantification and uncertainty…",1,neutral
We instead use Algorithm 1 with the Dirichlet log likelihood distillation loss used by Malinin et al. (2020) (see Appendix A.3 for EnD2 implementation details).,2,positive
"We compare the GPED framework to the full Monte Carlo ensemble as well as to an adaptation of Ensemble Distribution Distillation (EnD2) (Malinin et al., 2020). In particular, Malinin et al. (2020) materialize a complete ensemble, which is not feasible in our case due to the large number of samples in the Bayesian ensemble (⇠ 105 samples).",2,positive
"We compare the GPED framework to the full Monte Carlo ensemble as well as to an adaptation of Ensemble Distribution Distillation (EnD2) (Malinin et al., 2020). In particular, Malinin et al. (2020) materialize a complete ensemble, which is not feasible in our case due to the large number of samples in the Bayesian ensemble (⇠ 105 samples). We instead use Algorithm 1 with the Dirichlet log likelihood distillation loss used by Malinin et al. (2020) (see Appendix A.",2,positive
"In particular, Malinin et al. (2020) materialize a complete ensemble, which is not feasible in our case due to the large number of samples in the Bayesian ensemble (⇠ 105 samples).",1,neutral
(2018) and Malinin et al. (2020) have leveraged this decomposition to explore a range of down-stream tasks that rely on uncertainty quantification and decomposition.,1,neutral
[151] point out that estimating the model’s uncertainty is crucial since it ensures more reliable knowledge to be transferred.,1,neutral
"Although various methods have been proposed to extract knowledge from logits, some works [37], [151], [161], [267] show that KD is not always practical due to knowledge uncertainty.",1,neutral
[59] combined KD with ensemble learning and proposed ensemble distribution distillation to improved classification performance.,1,neutral
", 2015), increase their robustness to label noise (Lee & Chung, 2020), and estimate outputs uncertainty (Malinin et al., 2020; Ilg et al., 2018).",2,positive
"Additionally, different work on Distillation [Malinin et al., 2019, Vadera et al., 2020] takes larger models and distills them into a smaller one.",2,positive
We discuss the differences between our proposed framework and the method by Malinin et al. (2019) in more detail in section 3.2 and section 5.,2,positive
This particular choice of distributions recovers the method proposed by Malinin et al. (2019).,1,neutral
Malinin et al. (2019) notes that increasing ensemble size have quickly diminishing returns for a Dirichlet distribution but it is not clear if this is a general property.,1,neutral
"Similarly to what was noted in Malinin et al. (2019), the behaviour of the ensemble might differ outside of the in-distribution data, making need of a more flexible representation of the ensemble than the one chosen.",1,neutral
"This decomposition of uncertainty with ensembles has been extensiveley explored, e.g., by Kendall and Gal (2017); Lakshminarayanan et al. (2017); Malinin and Gales (2018); Malinin et al. (2019).",1,neutral
", 2015) and for distribution distillation (Malinin et al., 2019).",2,positive
"A special case of this has recently
been proposed by Malinin et al. (2019) for classification problems, using a Dirichlet distribution to model the ensemble predictions.",1,neutral
"Recently, there has been a surge of effort in modelling and estimating the uncertainty in deep neural networks (e.g. Malinin et al., 2019; Kendall and Gal, 2017; Guo et al., 2017; Widmann et al., 2019).",1,neutral
"This tweak of the original distribution is expressly used in order to more easily distinguish differences in logit space, both for ordinary distillation (Hinton et al., 2015) and for distribution distillation (Malinin et al., 2019).",1,neutral
"This is in contrast with the recent work by Malinin et al. (2019), who also consider distribution distillation.",1,neutral
"This problem has been studied for many years [12] and has been discussed in several views such as rejection [8, 5], anomaly detection [1], open set recognition [2], and uncertainty estimation [22, 23, 24].",1,neutral
Results in table 12 show that score-based measures of uncertainty derived using a product-of-expectation consistently yield a higher PRR for sequence-level error detection.,1,neutral
"This yields the prediction rejection area ratio PRR:
PRR = ARuns ARorc
(24)
A rejection area ratio of 1.0 indicates optimal rejection, a ratio of 0.0 indicates ‘random’ rejection.",1,neutral
This will negatively impact the PRR.,0,negative
"Rejection curves are summarised using the Prediction Rejection Ratio (PRR) (Malinin, 2019; Malinin et al., 2020), which is 100% if the measures of uncertainty perfectly correlate with sentence level measure of ’error’ (BLEU/WER), and 0% if they are completely uninformative.",1,neutral
"If PRR is negative, then the measures of uncertainty are inversely related to sentence-level ’error’.",1,neutral
"Additionally, the results show that uncertainty-based rejection works better for ASR models than for NMT models, as they achieve a higher PRR.",2,positive
"One way to quantify uncertainty is through model uncertainty (Depeweg et al., 2017; Malinin et al., 2019) which measures the spread or disagreement of an ensemble.",1,neutral
"As discussed in (Hinton et al., 2015; Malinin et al., 2019) the distribution of the teacher network is often “sharp"", which can limit the common support between the output distribution of the model and the target empirical distribution.",1,neutral
"However, the uncertainty scales of Prior Networks is larger than the ones of both the ensemble and Hydra, leading to an overall better model uncertainty.",1,neutral
"This retains the diversity of ensemble member predictions which is otherwise lost in knowledge distillation.
distillation makes it impossible to estimate measures of uncertainty such as model uncertainty (Depeweg et al., 2017; Malinin et al., 2019).",1,neutral
"Here Prior Networks are not applicable because for the case of probabilistic regression we cannot take averages of distributions.3 For regression Hydra outperforms knowledge distillation in terms of predictive performance (NLL) because Hydra produces a more flexible output in the form of a Gaussian mixture model with one Gaussian component per head, whereas Knowledge Distillation can produce only a single Gaussian component.",1,neutral
"To overcome this limitation, Malinin et al. (2019) proposed to model the entire distribution of an ensemble using a Dirichlet distribution parametrized by a neural network, referred to as a prior network (Malinin & Gales, 2018).",1,neutral
"In-distribution model uncertainty (MU) is comparable for both Prior Networks (0.0280) and Hydra (0.0074) but quite a bit smaller compared to target ensemble MU of 0.1055, meaning it is possible to improve uncertainty quantification in all distillation methods tested.",1,neutral
"Model uncertainty, introduced by Depeweg et al. (2017); Malinin et al. (2019), is a measure of the spread or disagreement of an ensemble based on mutual information.",1,neutral
"We compare our work with two core distillation approaches, Knowledge Distillation (Hinton et al., 2015) and Prior Networks (Malinin et al., 2019; Malinin & Gales, 2018).",2,positive
"After having finished the experiment for this chapter, we found several similar published works on arXiv [60, 11], published at around the same time we were working on this project.",2,positive
"We found several similar published works on arXiv [60, 11] at around the same time we were working on this idea.",2,positive
"One of the common practices in knowledge distillation is employing ensembles as a teacher model (Malinin et al., 2020; Ryabinin et al., 2021) based on the superior performance of the ensemble of deep neural networks (Lakshminarayanan et al., 2017; Ovadia et al., 2019), and several works already…",1,neutral
"One of the common practices in knowledge distillation is employing ensembles as a teacher model (Malinin et al., 2020; Ryabinin et al., 2021) based on the superior performance of the ensemble of deep neural networks (Lakshminarayanan et al.",1,neutral
"However, existing ensemble combination methods apply deterministic point estimation techniques that are ineffective in capturing the underlying ensemble member diversity and robustly represent the combined model predictive uncertainty [21], [26]–[28].",1,neutral
"Accordingly, the ensemble models are well suited for developing robust FD applications due to the ability to obtain improved predictive performance [12]–[18], protect against adversarial attacks [19], [20], and decompose the total predictive uncertainty into epistemic and aleatoric uncertainty [21]–[25].",1,neutral
"Proposed methods rely on adapted loss functions (Kendall & Gal, 2017), quantile regression (Tagasovska & Lopez-Paz, 2019), or ensemble distillation (Malinin et al., 2019) and try to capture the uncertainty induced by noisy trainable data points at inference time.",1,neutral
"[Malinin et al., 2020], modified EWM can be regarded as the reflection of data uncertainty to each augmented data, focusing more on less uncertain augmentation and vice versa.",1,neutral
"Considering the definition of the data uncertainty by Malinin et al. [Malinin et al., 2020], modified EWM can be regarded as the reflection of data uncertainty to each augmented data, focusing more on less uncertain augmentation and vice versa.",1,neutral
"[Malinin et al., 2020], considering the entropy from softmax output could represent the data uncertainty (with a difference in that we only use a single network prediction to calculate the data uncertainty).",1,neutral
"For uncertainty estimation, we refer to the well-stated definition of the uncertainty by Malinin et al. [Malinin et al., 2020], considering the entropy from
softmax output could represent the data uncertainty (with a difference in that we only use a single network prediction to calculate the data…",1,neutral
"From another point of view, according to the previous study [Malinin et al., 2020], overall uncertainty measurement from neural network prediction can be divided into knowledge uncertainty and data uncertainty.",1,neutral
"Considering the data uncertainty should be extracted from multiple number of the target network predictions [Malinin et al., 2020], it is possible that the calculated entropy could not have reflected the data uncertainty to an accurate level.",2,positive
"For classification, we compare NatPN to Reverse KL divergence Prior Networks (R-PriorNet) [45], Ensemble Distribution Distillation (EnD(2)) [46] and Posterior Networks (PostNet) [9].",1,neutral
"…in a more direct way, and to let the learner itself predict, not only the target variable, but also its own uncertainty about the prediction [Sensoy et al., 2018, Malinin and Gales, 2018, 2019, Malinin et al., 2020, Charpentier et al., 2020, Huseljic et al., 2020, Kopetzki et al., 2021].",2,positive
"This is the basic idea of direct epistemic uncertainty prediction [Sensoy et al., 2018, Malinin and Gales, 2018, 2019, Malinin et al., 2020, Charpentier et al., 2020, Huseljic et al., 2020, Kopetzki et al., 2021].",1,neutral
"Next to predictive variance, Mutual Information (MI) [76] has been proposed as a measure of epistemic uncertainty, as intuitively it captures the amount of information that would be gained about model parameters through ‘‘knowledge’’ of the true outcome [77].",1,neutral
"…not explored in this work is how to incorporate uncertainty decomposition within the proposed method in order to improve the OOD detection, as was done in Bayesian neural networks and/or ensemble approaches [Vadera et al., 2020b, Malinin et al., 2019, Vadera et al., 2020a, Depeweg et al., 2018].",2,positive
"It is also effective against problems such as adversarial attack and out-of-distribution detection due to the nature of using multiple networks [15,6,5,21,26].",1,neutral
"Ensemble is also effective against problems such as adversarial attack and out-of-distribution detection due to it using multiple networks [6,15,5,21,26].",1,neutral
"Ensembling is proposed to solve medical image segmentation uncertainty prediction Mehrtash et al. (2020). It carries out calibration improvement with simple average of ensemble model, a simplified version of Bayesian inference Wilson & Izmailov (2020). Also, deep ensemble is a strong baseline for image classification calibration Lakshminarayanan et al.",2,positive
"Ensembling is proposed to solve medical image segmentation uncertainty prediction Mehrtash et al. (2020). It carries out calibration improvement with simple average of ensemble model, a simplified version of Bayesian inference Wilson & Izmailov (2020).",2,positive
"We refer to the well-stated definition of the uncertainty (Malinin et al., 2020), considering the entropy from softmax output could represent the data uncertainty with a difference in that we only use a single network to calculate the data uncertainty.",2,positive
"Considering the data uncertainty should be extracted from multiple number of the target network predictions (Malinin et al., 2020), it is possible that the calculated entropy could not have reflected the data uncertainty to an accurate level.",2,positive
"Considering the definition of the data uncertainty by Malinin et al. (2020), modified EWM can be regarded as a reflection of data uncertainty to each augmented data, focusing more on less uncertain augmentation and vice versa.",2,positive
"From another point of view, according to the previous work by Malinin et al. (2020), overall uncertainty measurement from neural network prediction can be divided into knowledge uncertainty and data uncertainty.",1,neutral
"Lastly, Malinin et al. (2020b) show that prior networks can also be distilled using an ensemble of classifiers and their predicted categorical distributions (akin to learning Figure 2e from Figure 2a), which does not require regularization at all (but training the ensemble).",1,neutral
"The methods listed in Table 3 either choose the NormalInverse Gamma distribution (Amini et al., 2020; Charpentier et al., 2021), inducing a scaled inverseχ2 posterior (Gelman et al., 1995),9 as well as a Normal-Wishart prior (Malinin et al., 2020a).",1,neutral
"For training purposes, they apply the reverse KL objective of Malinin & Gales (2019) as well as the knowledge distillation objective of Malinin et al. (2020b).",2,positive
"Malinin et al. (2020a) can be seen as the multivariate generalization of the work of Amini et al. (2020), where a combined Normal-Wishart prior is formed to fit the now multivariate normal likelihood.",1,neutral
"…weaknesses of Dirichlet networks as well: In order to achieve the right behavior of the distribution and thus guarantee sensible uncertainty estimates, some approaches Malinin & Gales (2018; 2019); Nandy et al. (2020); Malinin et al. (2020a) require out-of-distribution data points during training.",2,positive
"The authors of Ensemble Distribution 40 Distillation (EnD(2)) [2] address this issue by using the output of an ensemble to train a so-called Prior Network (PN) [3], 41 distilling the ensemble down to a single model while also preserving its uncertainty decomposition abilities.",1,neutral
PRR is the prediction rejection 97 area ratio introduced in Appendix B of [2].,1,neutral
73 • EnD(2): A single model distribution-distilling ENSM trained according to [2].,2,positive
"Following [8] and [25], we compute the two types of uncertainties as follows:",1,neutral
"Many researchers have studied reducing the cost to estimate ensemble based prediction variation [1, 6, 34, 49, 52].",1,neutral
"The authors ofEnsembleDistributionDistillation (EnD(2)) [2] address this issue by using the output of an ensemble to train a so-called Prior Network (PN) [3], distilling the ensemble down to a single model while also preserving its uncertainty decomposition abilities.",2,positive
PRR is the prediction rejection area ratio introduced in Appendix B of [2].,1,neutral
• EnD(2): A single model distribution-distilling ENSM trained according to [2].,2,positive
"Most existing ensemble methods [34, 28, 13] average the output of each model, which neglects the diversity.",1,neutral
"While classical game theory assumes that all players make rational decisions EGT replaces it by biologically inspired notions, such as natural selection [10].",1,neutral
"To name a few, BorderlineSMOTE [10] was introduced to strengthen the decision boundaries of classifiers, by replacing SMOTE’s random selection of minority samples with a directed selection of examples that are close to the class border.",1,neutral
"In [10] (the most similar contribution) authors provide an evaluation of synthetic datasets under utility perspectives related to machine learning, leaving room for improvements.",1,neutral
"In [10], exploratory SR are also utilized along with trophallaxis (i.",1,neutral
"LM-GAN is a modification of Conditional Generative Adversarial Nets [10], which is a modification of the traditional Generative Adversarial Nets (GAN) [6].",1,neutral
[10] applied a convolutional neural networks to extract features from text data by sequentially filtering features from training and test sets (AUC = 0.,1,neutral
"Arguably, purposes for which SR can unleash their full potential relate mostly to exploration: localization [7], disaster rescue missions [8], or scenery mapping [9,10].",1,neutral
"The most successful application of GAN is computer vision, including image translation [8], image super-resolution [10], image synthesis [29], video generation [26], face aging [2], 3D object generation [23] or detection of small objects [30].",1,neutral
"We compare our proposal, SMOTE-BFT, with popular oversampling techniques: the original SMOTE [1], Borderline-SMOTE [10] and ADASYN [12].",2,positive
", based on industrial machinery – to the concept of digital manufacturing [10].",1,neutral
"Malinin et al. (2020) distill ensembles by leveraging prior networks (Malinin and Gales, 2018), which model a conditional distribution over outputs.",1,neutral
"We investigate the particular setting in which both teacher and student models are ensembles (Lan et al., 2018; Malinin et al., 2020; Tran et al., 2020); our hope is that the more precise modeling the teacher deep ensemble can be inherited by the student batch ensemble, despite the student’s…",2,positive
"We investigate the particular setting in which both teacher and student models are ensembles (Lan et al., 2018; Malinin et al., 2020; Tran et al., 2020); our hope is that the more precise modeling the teacher deep ensemble can be inherited by the student batch ensemble, despite the student’s significantly reduced parameter space.",2,positive
