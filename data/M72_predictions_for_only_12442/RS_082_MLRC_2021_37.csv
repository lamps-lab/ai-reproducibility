text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"We chose representative trajectory prediction methods with different base network architectures (LSTM, CNN, GCN), including Social-LSTM[2], STGAT[13], Y-net[22], and Social-STGCNN[23].",1,related,1,positive
"In addition, comparing with Y-net [32] with the lowest error in trajectory-image methods, the performance of our method is still increased by 3.83% on ADE, and which obtains the second-lowest error on FDE.",1,related,1,positive
The output settings are in line with the data format of YNet [2].,1,related,1,positive
"We have made some improvements on the basis of Y-Net, adding FairMOT [3] for pedestrian tracking, which combines object detection, tracking, and trajectory prediction.",1,related,1,positive
"The data alignment results are processed by YNet, which are shown as video in Fig.",1,related,0,negative
"To prevent YNet from processing too much data and slowing down the prediction process, we introduce a frame rule, which limit the distance between the frames and the tracked objects.",1,related,1,positive
"Therefore, we convert the world coordinates into pixel coordinates using
the homography matrices from Y-net (Mangalam et al., 2021).",1,related,1,positive
The performance of Y-net drops severely from 7.85/11.85 (when it’s trained also on SDD) to 30.59/51.43.,1,related,0,negative
"For comparison, we choose Y-net and NSP-SFM as baselines and show the results in Tab.",1,related,1,positive
"We use NSP-SFM, Y-net, and S-CSR as baselines.",1,related,1,positive
"Following (Mangalam et al., 2021), we extract trajectories with a time step 0.4 seconds and obtain 20-frame samples for an 8/12 setting, i.e., given the first 8 frames (3.2 seconds, th = 7), we aim to predict the future 12 frame trajectories (4.8 seconds, tf = 12).",1,related,1,positive
"Following the standard leave-one-out evaluation protocol (Gupta et al., 2018; Mangalam et al., 2021, 2020), we train our model on four subdatasets and test it on the remaining one in turn.",1,related,1,positive
"We compare our BNSP-SFM in both standard-sampling and ultra-sampling with a wide range of baselines: Social GAN (S-GAN) (Gupta
et al., 2018), Sophie (Sadeghian et al., 2019), NEXT (Liang, Jiang, Niebles, Hauptmann, & Fei-Fei, 2019), P2TIRL (Deo & Trivedi, 2020), SimAug (Liang, Jiang, & Hauptmann, 2020), PECNet (Mangalam et al., 2020), Y-Net (Mangalam et al., 2021), S-CSR (Zhou, Ren, Yang, Fan, & Huang, 2021), SocialVAE (Xu et al., 2022), V2Net (Wong et al., 2022), and NSP-SFM (Yue et al., 2022).",1,related,1,positive
"In SCAN, we only use a neural network, namely Y-net [25], for global pedestrian trajectory prediction.",1,related,1,positive
"We have implemented a crowd navigation environment as shown in Figure 2, where each pedestrian tries to follow its global trajectory generated by Y-Net [25].",1,related,1,positive
"4, we studied the prediction capability of the Di-Long model on longer time horizons than t f = 30 sec, and compared to Goal-SAR and Y-Net.",1,related,1,positive
"We sample only a single waypoint, conditioned on the final predicted goal, in the same manner as proposed in Y-Net [16].",1,related,1,positive
"In this scenario, our Di-Long model outperforms Goal-SAR and Y-Net by a considerable margin in terms of ADE ( − 0 .",1,related,1,positive
We compare our increasingly improving Di-Long model with Goal-SAR and Y-Net.,1,related,1,positive
"3 w.r.t. Y-Net), FDE ( − 3 .",1,related,1,positive
"View Vertically used a different sample rate on the eth scene and a slightly different split of the hotel scene in the ETH dataset; thus, we retrain and reevaluate it on the standard split used by most other methods ([45, 35, 34, 53, 16, 60]).",1,related,1,positive
"Unlike the split of the SDD in TrajNet [88] (which includes only pedestrians), the SDD we use includes all categories of annotations.
lines, including Linear Least Square4, SoPhie [20], S-BiGAT [19], E-SR-LSTM [85], Multiverse [87], MANTRA [89], TF [28], PECNet [52], STAR [29], Trajectron++ [67], SimAug [86], TPNMS [90], Introvert [91], LB-EBM [92], Agentformer [44], Y-net [53], STC-Net [93], SpecTGNN [77], CSCNet [83], MSN [33], MID [94], SHENet [95], SEEM [96], Social-SSL [66], P-LSTM [40], Zero-Vel [97] , and PV-LSTM [40].",1,related,1,positive
"Compared with the current state-ofthe-art method Y-net, E-V2-Net-DFT achieves about 16.3% and 11.5% improvement in ADE and FDE, respectively.",1,related,0,negative
* means the results are reproduced using the official released code of [30].,1,related,0,negative
"To train Y-Net, we follow [22] to make the encoded feature with shape (C,H,W ) average pooled in the spatial dimension to get a C dimensional vector, and perform PCL on it.",1,related,1,positive
"We reproduced the results of Y-Net using the official released code of [30] with 42 as the random seed, since the original method does not have a fix seed.",1,related,1,positive
We use ETH-UCY and Nuscenes in the way same as our backbone Traj++ EWTA [28] and SDD in the way same as our backbone Y-Net [30].,1,related,1,positive
"Another strong baseline we experiment on is Y-Net [30], which uses a U-Net backbone and achieves state-of-the-art results on SDD.",1,related,1,positive
"We also plug our module into Y-Net, the results are shown in Table 3.",1,related,1,positive
Quantitative comparisons on Y-Net on SDD.,1,related,1,positive
"To circumvent this issue, we utilize a goal-estimation [14] module for estimating the goals in the inference time.",1,related,1,positive
"We adopt the goal module proposed in [14], [13] for this purpose.",1,related,1,positive
We empirically demonstrate the efficiency of MoSA on the state-of-the-art model Y-Net [2] on the heterogenous SDD [14] and inD [15] datasets in various style transfer setups.,1,related,1,positive
"Following [19], we use past 8 frames to predict future 12 frames.",1,related,1,positive
"On ETH/UCY datasets, we compare our model with the state-of-the-art methods: SS-LSTM [35], Social-STGCN [22], MANTRA [20], AgentFormer [37], YNet [19].",1,related,1,positive
"Following the existing works [1, 19, 20, 35], we calculate the mean squared error (MSE) between the predicted trajectory and the ground-truth trajectory on ETH/UCY datasets: Ltra = 1 Tfut ∑Tfut t=1 ‖y p − ŷ p‖(2)2.",1,related,1,positive
"performance compared with previous HTP methods: SS-LSTM [35], Social-STGCN [22], Next [16], MANTRA [20], YNet [19].",1,related,1,positive
"Since our goal sampling network and Fenv need to work in the pixel space, we project the world coordinates in ETH/UCY into the pixel space using the homography matrices provided in Y-net [37].",1,related,1,positive
"We compare our NSP-SFM with an extensive list of baselines, including published papers and unpublished technical reports: Social GAN (S-GAN) [18], Sophie [49], Conditional Flow VAE (CF-VAE) [9], Conditional Generative Neural System (CGNS) [29], NEXT [33], P2TIRL [14], SimAug [31], PECNet [38], Traj++ [50], Multiverse [32], Y-Net [37], SIT [56], S-CSR [77], Social-DualCVAE [16] and CSCNet [71].",1,related,1,positive
"Finally, for SDD and ETH/UCY, we follow previous work [37,48] to segment trajectories into 20-frame samples and split the dataset for training/testing.",1,related,1,positive
"We demonstrate that our NSP model outperforms the state-of-the-art methods [18,49,9,29,33,14,31,38,50,32,37,56,77,16,71] in standard trajectory prediction tasks across various benchmark datasets [46,43,28] and metrics.",1,related,1,positive
"Following previous research [37,38], we adopt the standard leave-one-out evaluation protocol, where the model is trained on four sub-datasets and evaluated one.",1,related,1,positive
"In the future, we intend to examine the integration of environmental features [21] into our Multiclass-SGCN model to further improve prediction accuracy.",1,related,0,negative
"Furthermore, for the goal module we use L = 5 down- and up-sampling blocks with number of channels (32, 32, 64, 64, 64) and (64, 64, 64, 32, 32) for the encoder and decoder, respectively, as in the standard implementation [25].",1,related,1,positive
"To evaluate our method’s longterm forecasting performance, we use data in [27], including 1222 training and 174 test trajectories with 5-second history and 30-second future with the 1Hz sampling rate.",1,related,0,negative
Our results are again achieved without the manually annotated semantic maps in Y-net [27].,1,related,1,positive
Note that our method did not use the image data and apply any post-processing such as the TestTime Sampling Trick (TTST) [28].,1,related,0,negative
"Input Sampling ETH HOTEL UNIV ZARA1 ZARA2 AVG ADE FDE ADE FDE ADE FDE ADE FDE ADE FDE ADE FDE
SoPhie [37] T + I 20 0.70 1.43 0.76 1.67 0.54 1.24 0.30 0.63 0.38 0.78 0.54 1.15 CGNS [22] T + I 20 0.62 1.40 0.70 0.93 0.48 1.22 0.32 0.59 0.35 0.71 0.49 0.97 Social-BiGAT [19] T + I 20 0.69 1.29 0.49 1.01 0.55 1.32 0.30 0.62 0.36 0.75 0.48 1.00 MG-GAN [6] T + I 20 0.47 0.91 0.14 0.24 0.54 1.07 0.36 0.73 0.29 0.60 0.36 0.71 Y-Net [28] + TTST T + I 10000 0.28 0.33 0.10 0.14 0.24 0.41 0.17 0.27 0.13 0.22 0.18 0.27 Social-GAN [12] T 20 0.81 1.52 0.72 1.61 0.60 1.26 0.34 0.69 0.42 0.84 0.58 1.18 Causal-STGCNN [2] T 20 0.64 1.00 0.38 0.45 0.49 0.81 0.34 0.53 0.32 0.49 0.43 0.66 PECNet [29] T 20 0.54 0.87 0.18 0.24 0.35 0.60 0.22 0.39 0.17 0.30 0.29 0.48 STAR [49] T 20 0.36 0.65 0.17 0.36 0.31 0.62 0.26 0.55 0.22 0.46 0.26 0.53 Trajectron++ [38] T 20 0.39 0.83 0.12 0.21 0.20 0.44 0.15 0.33 0.11 0.25 0.19 0.41 LB-EBM [32] T 20 0.30 0.52 0.13 0.20 0.27 0.52 0.20 0.37 0.15 0.29 0.21 0.38 PCCSNET [45] T 20 0.28 0.54 0.11 0.19 0.29 0.60 0.21 0.44 0.15 0.34 0.21 0.42 †Expert [53] T 20 0.37 0.65 0.11 0.15 0.20 0.44 0.15 0.31 0.12 0.26 0.19 0.36 †Expert [53]+GMM T 20×20 0.29 0.65 0.08 0.15 0.15 0.44 0.11 0.31 0.09 0.26 0.14 0.36 MID T 20 0.39 0.66 0.13 0.22 0.22 0.45 0.17 0.30 0.13 0.27 0.21 0.38
the comparison between our method and existing methods on the Stanford Drone dataset.",1,related,1,positive
"Specifically, our MID outperforms the current state-of-the-art T+I method Y-Net+TTST on the ADE metric.",1,related,1,positive
Appendix F shows the limitation of the SDD segmentation provided by Y-net [29] to explain the low ECFL discussed in Sec.,1,related,0,negative
"1 and compare the performance of MUSE-VAE with Trajectron++ (T++) [38], Y-net [29], and AgentFormer (AF) [50] baselines, using their public code.",1,related,1,positive
"We apply our method on top of the Y-Net [50], and compare our modular adaptation strategy against the standard fine-tuning of the entire model for lowshot transfer.",1,related,1,positive
"In particular, we have selected (1) Social GAN (SGAN) [Gupta et al. 2018], one of the earliest models; (2) Trajectron++ (T++) [Salzmann et al. 2020], a SOTA model for short-term trajectory prediction; and (3) PECNet (PECN) [Mangalam et al. 2020b], a SOTA model for long-term trajectory prediction.",1,related,1,positive
"In this paper, we focus on three SOTAmethodologies to showcase our benchmark dataset: SocialGAN [Gupta et al. 2018], PECNet [Mangalam et al. 2020b], and Trajectron++ [Salzmann et al. 2020].",1,related,1,positive
We choose them because PECNet [Mangalam et al. 2020b] shows an outstanding performance on the long-term trajectory while the short-term trajectory is most well predicted in Trajectron++ [Salzmann et al. 2020].,1,related,1,positive
"We choose several methods as our baselines, including S-GAN [14], SoPhie [42], Social-BiGAT [21], E-SR-LSTM [60], MANTRA [33], Multiverse [26], SimAug [25], PECNet [29], STAR [57], TPNMS [27], TF [13], Trajectron++ [44], Introvert [45], LB-EBM [37], Agentformer [58], Y-net [28], and SpecTGNN [3].",1,related,1,positive
"We propose an architecture that considers long-term goals similar to [9, 5, 3, 4] but adds a key component of frame-wise intention estimation which is used to condition the trajectory prediction module.",1,related,1,positive
"Second, we see a similar trend in multi-shot prediction setting as well with our model outperforming PECNet by 33% in ADE and 9% in FDE for pedestrians and a delta of 26% in ADE and 13% in FDE for moving vehicles.",1,related,0,negative
"We benchmark against PECNet [3], a strong scene agnostic trajectory prediction method with state-of-the-art performance on standard intention agnostic prediction datasets.",1,related,1,positive
"Second, we see a similar trend in multi-shot prediction setting as well with our model outperforming PECNet by 33% in ADE and 9% in FDE for pedestrians and a delta of 26% in ADE and 13% in FDE for moving vehicles.",1,related,0,negative
"We benchmark against PECNet [77], a strong scene agnostic trajectory prediction method with state-of-the-art performance on standard intention agnostic prediction datasets.",1,related,1,positive
"During testing, instead of picking the position with highest probability, we adopt the test-time sampling trick (TTST) introduced by [1] to sample goals for better performance.",1,related,1,positive
"For each interval (8 seconds long), we subsample at FPS = 2.5 to get 20 frames, where the first 8 frames are used as input for Y-net [1] and S-CSR [4].",1,related,1,positive
Figure 2 demonstrates that our method (NSP) has better performance in avoiding collisions than Y-net and S-CSR.,1,related,1,positive
"When comparing the median ADE from PTPC to the second-best median ADE from Y-net (CWS), we observe ≈ 5% improvement.",1,related,1,positive
"We adapt GoalGAN, PECNet, andY-net to our task setting, which we then use as baselines for benchmarking performance on the task of trajectory prediction on the Talk2CarTrajectory.",1,related,1,positive
"Additionally, when we compare with the second-best median FDE from Y-net (TTST + CWS), we observe an improvement of ≈ 14%.",1,related,1,positive
"In addition to our own model, we adapt the following baselines for our task setting: A, CNN, LSTM, GoalGAN [53], PECNet [55], Y-net [57], and evaluate their performance on the metrics defined in V-B.",1,related,1,positive
"We thus used the
123820 VOLUME 10, 2022
Text2Conv method in the base version of the PTPC and the Y-net model featured in Table 1.",1,related,1,positive
"the command representation into the path prediction pipeline and interpolating the full trajectory from the samples of key locations along the path, inspired by [57], resulting in amodel that achieves state-of-the-art performance on our task setting.",1,related,1,positive
"D. TRAJECTORY PREDICTION BASELINES In addition to our own model, we adapt the following baselines for our task setting: A∗, CNN, LSTM, GoalGAN [53], PECNet [55], Y-net [57], and evaluate their performance on the metrics defined in V-B.",1,related,1,positive
"In the case of PTPC and Y-net, we predict three goals per command and three trajectories per goal, resulting in nine trajectories.",1,related,1,positive
"In the case of Y-net, we also evaluate the use of TTST, CWS, and the combination of the two during inference.",1,related,1,positive
This section has been referenced from the original paper [9] (Section 3).,1,related,0,negative
"The following paper is a reproducibility report for From Goals, Waypoints & Paths To Long Term Human Trajectory 2 Forecasting [9].",1,related,0,negative
"Reproducibility report of From goals, waypoints & paths to longterm human trajectory forecasting for ML Reproducibility
Challenge 2021
Anonymous Author(s) Affiliation Address email
Scope of Reproducibility1
The following paper is a reproducibility report for From Goals, Waypoints & Paths To Long Term Human Trajectory2 Forecasting [9].",1,related,1,positive
