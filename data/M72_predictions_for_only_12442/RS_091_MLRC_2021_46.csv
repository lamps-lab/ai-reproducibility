text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
", [Pan et al. 2022, 2021; Shi et al. 2021]) which explicitly model the lighting efects through lighting models, our method can be categorized as the implicit representation learned from the dataset.",1,related,1,positive
"We compare our method with ShadeGAN [Pan et al. 2021], LiftedStyleGAN [Shi et al. 2021], EG3D+Deep Portrait Relighting [Zhou et al. 2019], EG3D+StyleFlow [Abdal et al. 2021], and 3DFaceShop [Tang et al. 2022] as alternative 3D-aware methods.",1,related,1,positive
"We also quantitatively compare texture and rendered faces by rendering it on a mesh with other methods, including LiftedGAN [49], DECA [15], and OSTeC [18].",1,related,1,positive
"• Identity consistency (ID): As in [6, 7, 31, 36], we compute ID with the mean Arcface [8] cosine similarity, after rendering a face with two random camera views.",1,related,1,positive
"For depth consistency, following previous work [6, 31, 36], we estimate pseudo ground truth depth from Deng et al.",1,related,1,positive
"To measure the geometry properties, we followed [5, 41], utilizing a pre-trained 3D face reconstruction model to extract a “pseudo” ground truth depth map from the source image.",1,related,1,positive
"To this end, we use the pre-trained model of Unsup3D [45] since it has been widely applied in unsupervised face reconstruction [50, 30, 35, 49] in recent years.",1,related,1,positive
"We address this concern by analyzing both the FID score and the depth accuracy metric used in [5, 32].",1,related,0,negative
"We evaluate PV3D and baseline models by Frechet Video Distance (FVD) (Unterthiner et al., 2018), Multi-view Identity Consistency (ID) (Shi et al., 2021), Chamfer Distance (CD), and Multi-view Image Warping Errors (WE) (Zhang et al., 2022a;b).",1,related,1,positive
"Besides, we provide the FID, aMP, and FRS scores of LiftedGAN by using their randomly generated samples instead of the attribute-edited samples.",1,related,1,positive
"We notice that LiftedGAN and DiscoFaceGAN cannot directly perform editing tasks, such as “Gender” and “Age”.",1,related,0,negative
"Finally, we also adopt a 3D-aware LiftedGAN [Shi et al. 2021] to compare multiple-view generation.",1,related,1,positive
"Specifically, for the “Smile” attribute, our TT-GNeRF (S) achieve 1899.6 aMP and 0.812 FRS scores, which are better than 1484.0 aMP and 0.464 FRS scores of LiftedGAN, 1347.4 aMP, and 0.587 FRS scores of DiscoFaceGAN.",1,related,1,positive
"We also follow the similar strategy in [42] to measure identity preservation, where we use all frontal images from the held-out FFHQ set and perform pose editing at different angles to compute the identity cosine similarity between the edited faces and the original ones.",1,related,1,positive
"We compare with prior 3D-controllable GANs [10, 45, 46, 42, 26, 28, 6], and show more results in Supplementary.",1,related,1,positive
"results of Unsup3d [34], LiftedGAN [33] and our proposed method.",1,related,1,positive
"We adopt the pretrained face embedding model used in [33] for Lid and Llow, which is an ResNet-18 [47].",1,related,1,positive
We observe our method performs better than Unsup3d [34] and LiftedGAN [33].,1,related,1,positive
"We then adopt a pre-trained face recognition network [33] f(·) to regularize the face identities, which can be denoted as",1,related,1,positive
"In Table I, we show the quantitative results of Unsup3d [34], LiftedGAN [33] and our proposed method.",1,related,1,positive
Both LiftedGAN [33] and our method set the resolution as 256.,1,related,1,positive
"In Figure 8, we show the rotated results of related works [33], [34] using the unsupervised method to reconstruct the 3D shapes.",1,related,1,positive
"Row FID↓ KID×100 ↓ ID↑ Depth↓ Pose↓
2 5 6 2
1 LiftedGAN [58] 29.8 – 0.58 0.40 0.023 2-1 D2A (ϵ = 1/64) 13.4 0.920 0.69 0.60 0.004 2-2 D2A (ϵ = 1/128) 13.5 0.867 0.70 0.60 0.004 2-3 D2A (ϵ = 1/256) 11.7 0.644 0.70 0.63 0.005 2-4 D2A (ϵ = 1/512) 12.6 0.684 0.69 0.62 0.005 3 GMPI 11.4 0.738 0.70 0.53 0.004
Similarly, we approximate depth Dvtgt via
Dvtgt = L∑ i=1 bi · α′i · i−1∏ j=1 (1− α′j)  , (S4) where bi is the distance mentioned in Eq.",1,related,1,positive
"At a resolution of 2562, 1) GMPI outperforms GIRAFFE, pi-GAN, LiftedGAN, and GRAM on FID/KID while outperforming StyleSDF on FID; 2) GMPI demonstrates better identity similarity (ID) than GIRAFFE, pi-GAN, and LiftedGAN; 3) GMPI outperforms GIRAFFE regarding depth; 4) GMPI performs best among all baselines on pose accuracy.",1,related,1,positive
"Different from our proposed approach, because of the transformation map, LiftedGAN is not strictly view-consistent.",1,related,1,positive
"3 Portrait image generation with 3D control To evaluate the performance of the proposed 3D-controllable StyleGAN, we report the qualitative and quantitative comparison with state-of-the-art models [34,9,72,57] whose generator allows explicit control over pose.",1,related,1,positive
"In contrast to classical NeRF [38], we do not utilize view direction conditioning since it worsens multi-view consistency [7] in GANs which are trained on RGB datasets with a single view per instance.",1,related,1,positive
"Compared to upsampler-based 3D GANs [15, 43, 72, 79, 6, 78], we use a pure NeRF [38] as our generator G and utilize the tri-plane representation [6, 8] as the backbone.",1,related,1,positive
"Apart from that, we also compare to pi-GAN [7] and GRAM [12], which are non-upsampler-based GANs.",1,related,1,positive
"In this paper, we adopt the state-of-the-art pre-trained 3D generator of G3D [27] for 3D face modeling, which can disentangle the generation process of a 2D generator G2D instantiated by StyleGAN [17] into different 3D modules for a 3D shape representation.",1,related,1,positive
"(2)
By optimizing the objective function (1), we can obtain the optimal w∗ and get the 3D face as {s, t} = G3D(w∗).",1,related,1,positive
"Given a face recognition model f(x) : X → Rd, we optimize the parameter of w for the generator by minimizing the distance between the original face image and the rendered image of x′ as
min w
Df (x′,x) + λ∥x′ − x∥1, (1)
where x′ := R(G3D(w);V0, L0) with R being a differentiable renderer, and V0 and L0 are corresponding parameters of neutralized viewpoint and lighting; and λ is a balancing hyperparameter.",1,related,1,positive
"Specifically, we adopt a 3D generator [27] to synthesize 3D face information, including texture, shape, viewpoint, and lighting, using only a single-view face image.",1,related,1,positive
"(1); 4: w ← w − η∇wJ ; 5: end for 6: Forward pass the optimal w∗ into G3D to the 3D face {sa, ta}; 7: Initializing t∗0 = x b; ▷ Stage II: Optimize t∗
8: for k in MaxIterations N2 do 9: t∗k = t
a ⊙ (1−M) + t∗k ⊙M; 10: Construct 3D adversarial face {sa, t∗k}; 11: Get importance probability P̂i,j from Eq.",1,related,1,positive
"(5) as
Pi,j = 1
Z eJf (R(s a,t∗;Vi,Lj),x b), (7)
Algorithm 1 Face3DAdv
Require: A pre-trained 3D generative model G3D, a FR model f , a real face image xa, a target face image xb, 2D transformation function T .",1,related,1,positive
We evaluate shape quality by calculating MSE against pseudo-ground-truth depth-maps (Depth) and poses (Pose) estimated from synthesized images by [10]; a similar evaluation was introduced by [59].,1,related,1,positive
"We compare our methods against three stateof-the-art methods for 3D-aware image synthesis: πGAN [4], GIRAFFE [49], and Lifting StyleGAN [59].",1,related,1,positive
"We do not rely on hand-crafted pixel space augmentations [12, 36], human-labeled data [28, 73, 80, 90, 91] or post-processing of GAN-generated datasets using domain knowledge [10, 57, 83, 90].",1,related,1,positive
"We do not rely on hand-crafted pixel space augmentations [144, 155], human-labeled data [151,152,158,160,161] or post-processing of GAN-generated datasets using domain knowledge [146,149,150,160].",1,related,1,positive
"2 Qualitative results Figure 4 compares our method with other controllable and 3D-aware face synthesis methods based on StyleGAN2, including LiftedGAN [23], InterfaceGAN [22], StyleFlow [1], and EG3D [4].",1,related,1,positive
"The FID scores of our method, LiftedGAN [23], and StyleGAN2 [13] are 11.",1,related,1,positive
"Figure 4: Visual comparison of our generated faces with InterfaceGAN [21, 22], LiftedGAN [23], StyleFlow [1], EG3D [4] in different yaw angles.",1,related,1,positive
