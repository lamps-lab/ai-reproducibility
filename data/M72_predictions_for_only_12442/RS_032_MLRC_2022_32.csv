text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"To do this, we leverage resuls from [2], [3], where the authors show that cost sharing mechanisms can be efficiently learned for large-scale instances.",1,related,1,positive
"We also consider the popular data valuation approaches: (6) Permutation Sampling-based Shapely value (Perm-SV) (Jia et al., 2019b), (7) Least Cores (LC) (Yan & Procaccia, 2021), (8) TMC-Shapley (TMC-SV) and (9) G-Shapley (G-SV) (Ghorbani & Zou, 2019).",1,related,1,positive
"For larger datasets, since it is impractical to compute the exact data value, we compare the performance of data value estimates on data removal task, following existing data valuation literature (Ghorbani & Zou, 2019; Jia et al., 2019a;c; Wang et al., 2020; Yan & Procaccia, 2020).",1,related,1,positive
"For LC estimation, we use Monte Carlo (MC) approach (Yan & Procaccia, 2020) as the baseline.",1,related,1,positive
"In the experiment, when we talk about the LC, we always refer to the vector ψ that has the smallest `2 norm, following the tie-breaking rule in Yan & Procaccia (2020).",1,related,1,positive
"As a side note, the Shapley value estimated by Permutation sampling is superior to the Least core estimated by Monte-Carlo algorithm, which does not agree with the experiment results in Yan & Procaccia (2020).",1,related,1,positive
"We follow Yan & Procaccia (2020) and define the (ε, δ)-probably approximate least core to be the vector ψ ∈ Rn s.t. PrS∼D [∑ i∈S ψi + e ?",1,related,1,positive
"When σ = 0, Theorem 2 recovers Theorem 2 in Yan & Procaccia (2020) for the case of v̂ = v.",1,related,1,positive
"In this paper, when we talk about the least core, we always refer to the least core vector that has the smallest `2 norm, following the tie-breaking rule in the original literature [18].",1,related,1,positive
