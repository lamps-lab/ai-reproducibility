text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"We are left with the question: why does the network’s test performance improve dramatically upon continued training, having already achieved nearly perfect training performance? Recent answers to this question vary widely, including the difficulty of representation learning (Liu et al., 2022), the scale of parameters at initialisation (Liu et al.",1,related,0,negative
"1 Review of the Clock Algorithm As in past work, we find that after training both Model A and Model B, embeddings (Ea,Eb in Figure 1) usually describe a circle [8] in the plane spanned by the first two principal components of the embedding matrix.",1,related,1,positive
"Liu et al. (2022) demonstrate that grokking generalization speed can be accelerated in a simple embeddingdecoder model by using a larger learning rate for the embedding than for the decoder. This is in line with findings by Heckel & Yilmaz (2020) on epoch-wise double descent, where decreasing learning rates in later layers (which learn faster) aligns pattern learning speeds. We consider this further evidence that both grokking and epoch-wise double descent occur as a result of similar learning dynamics resulting from different speeds of pattern development. Nanda & Lieberum (2022) investigate grokking through mechanistic interpretability, with findings in line with our results (specifically observing the development of a Type 3 pattern).",1,related,1,positive
"1, we provide additional evidence that weight decay is necessary for grokking: smaller amounts of weight decay causes the network to take significantly longer to grok (echoing the results on toy models from Liu et al. (2022)), and our networks do not grok on the modular arithmetic task without weight decay or some other form of regularization.",1,related,1,positive
Our findings differ from those of [8] in that we discuss ensemble/average phenomena.,1,related,1,positive
"Setup We take the toy addition setup in (Liu et al., 2022), where each input digit 0 ≤ i ≤ p − 1 (output label 0 ≤ k ≤ 2(q − 1)) is embedded as a vector Ei (Yk).",1,related,1,positive
"As reported in (Power et al., 2022; Liu et al., 2022), we see that there exists a critical training set size below which generalization is impossible.",1,related,1,positive
"1 ALGORITHMIC DATASETS Setup We take the toy addition setup in (Liu et al., 2022), where each input digit 0 ≤ i ≤ p − 1 (output label 0 ≤ k ≤ 2(q − 1)) is embedded as a vector Ei (Yk).",1,related,1,positive
