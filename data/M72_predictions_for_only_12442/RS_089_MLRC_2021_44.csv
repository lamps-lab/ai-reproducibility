text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"We use the following backbones: ResNet20, ResNet50, ResNet152 [8], DenseNet [11] and 8-layer CoveNet [6].",1,related,1,positive
"In (A), according to PSNR, MSE, SSIM and LPIPS, the first reconstructed image is evaluated to have more privacy leakage [7, 6] than the second one (i.",1,related,0,negative
"For each dataset and architecture, we consider the model training with standard transformations, randomly selected policies, hybrid policies chosen by our earlier work [15], the top-2 of our searched policies and their hybrid version.",1,related,1,positive
TABLE VI COMPARISONS BETWEEN POLICIES CHOSEN FROM [15] AND THE VARIANCE INTEGRATED ALGORITHM ON CIFAR100 AND F-MNIST WITH RESNET20,1,related,1,positive
"Besides, we can also follow the works [48] to further mitigate the indirect leakage from the gradients.",1,related,0,negative
"In this paper, we propose to leverage image transformation technologies [15] to preprocess the video frames in the edge, prior to the edge-cloud video analytics process.",1,related,1,positive
"As such, our proposed approach improves the privacy of sensitive data in FL. 2) Maintaining the FL performance.",1,related,0,negative
"In this work, we proposed a practical and effective defence against model inversion attacks in FL.",1,related,1,positive
"Our algorithm ensures that the gradient after introducing the concealing samples is still aligned with that of training samples (including sensitive data), and thus maintains the learning capabilities of the FL.",1,related,1,positive
We hope our defence could provide a new perspective for defending against model inversion attacks in FL.,1,related,0,negative
"We can conclude that our defence method also provide an effective protection for the sensitive data on the ImageNet dataset against the model inversion attacks in FL.
A.4 Model Architectures
Details of the models used in this study are shown in Table 6.",1,related,1,positive
"But in the appendix, we show the comparison with such defences like PRECODE (PRivacy EnhanCing mODulE) (Scheliga et al., 2022) and ATS (Automatic Transformation Search) (Gao et al., 2021).",1,related,1,positive
"For ATS, we built upon the repository released alongside (Gao et al., 2021).",1,related,1,positive
"We then turn to practical evaluation and experiment with several recently proposed defenses (Sun et al., 2021; Gao et al., 2021; Scheliga et al., 2021) based on different heuristics and demonstrate that they do not protect from gradient leakage against stronger attacks that we design specifically…",1,related,1,positive
"We use the ConvNet architecture with a width of 64 also proposed in (Gao et al., 2021) and train with the augmentations ”7-4-15”, ”21-13-3”, ”21-13-3+7-4-15” which perform the best on ConvNet with CIFAR100.",1,related,1,positive
Our experiments in Section 6 on the network architecture and augmentations introduced in Gao et al. (2021) indicate that an attacker can successfully extract large parts of the input despite heavy image augmentation.,1,related,0,negative
"We then turn to practical evaluation and experiment with several recently proposed defenses (Sun et al., 2021; Gao et al., 2021; Scheliga et al., 2021) based on different heuristics and demonstrate that they do not protect from gradient leakage against stronger attacks that we design specifically for each defense.",1,related,0,negative
"To verify the claims made by the authors of [5], we reproduce their experiments.",1,related,0,negative
"44 In this reproducibility report, we evaluate the main claims made by the authors of [5] by reproducing their experiments.",1,related,0,negative
"Overall the results in [5] are reproducible, except Figure 4, with a large discrepancy between our result and the original 213 one - we are still in contact with the authors on this issue.",1,related,0,negative
"In [5], 86 k = 3 is chosen and the policies are denoted by the indices of the transformations within the AutoAugment library.",1,related,1,positive
"83 In [5], transformations from AutoAugment1 [3] are repurposed to protect sensitive training data from reconstruction 84 attacks.",1,related,0,negative
