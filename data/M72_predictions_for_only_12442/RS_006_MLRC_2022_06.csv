text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"We find the optimal θ to be around [1, 20], depending on the specific attributes of interests.",1,related,1,positive
"Our work differ in the way that we include a contrastive loss that measures the similarity between the input embeddings and extractor embeddings, which allows the style extractor to capture more precise text style representations[1, 22].",1,related,1,positive
"As the primary loss function, we use the normalized temperature-scaled cross-entropy loss (NT-Xent loss) function [27] because minimizing it also guarantees maximizing a lower bound on the mutual information between the input and the representation [41].",1,related,1,positive
"Effect of embedding size on feature suppression in MNIST RandBit(Chen et al., 2021).",1,related,1,positive
"To provide empirical evidence for this, we conduct two sets of experiments:
First, we train 5-layer convolutional networks on the RandomBit dataset with the same setup as in (Chen et al., 2021), but we vary the embedding size (see details in Appendix H).",1,related,1,positive
"First, we train 5-layer convolutional networks on the RandomBit dataset with the same setup as in (Chen et al., 2021), but we vary the embedding size (see details in Appendix H).",1,related,1,positive
"Second, we train ResNet18 (He et al., 2016) on the CIFAR-10/100 RandBit Dataset, constructed similarly to the MNIST RandBit dataset but with images from CIFAR10/100 (Krizhevsky et al., 2009) (see Appendix H.1).",1,related,1,positive
"We empirically examine the impact of the joint loss on MNIST RandBit, CIFAR-100, and CIFAR-100 RandBit.",1,related,1,positive
"In this proof, we follow the proof idea of Tu et al. (2019) (see Lemma 5 in Tu et al. (2019)). Since our setup is different from Tu et al. (2019), we need to modify the proof and add several new techniques.",1,related,1,positive
"In this proof, we follow the proof idea of Tu et al. (2019) (see Lemma 5 in Tu et al.",1,related,1,positive
"In this proof, we follow the proof idea of Tu et al. (2019) (see Lemma 5 in Tu et al. (2019)).",1,related,1,positive
"Contrastive: We treat contrastive SSL as a baseline and employ the SoTA SimCLR [7] algorithm, which uses the InfoNCE [8, 15, 26] contrastive loss to push embeddings of different views of the same image closer and pull apart those of different images.",1,related,1,positive
"We attribute this to the fact that NCL learning algorithms can work effectively with fewer number of data samples, since there is no explicit need for a higher number of samples, as in the case of CL methods [1, 10].",1,related,1,positive
We note that this loss is similar to the generalized InfoNCE loss proposed by Chen et al. (2021).,1,related,1,positive
"CAN on ViT-L outperforms MoCLR with R200×2 backbone (similar parameter counts), where we note that MoCLR performs as well or better than BYOL and MoCo-v3 on IN-1K (Tian et al., 2021).",1,related,1,positive
"Therefore, we identify two main drawbacks of Masked Siamese ConvNets: (I) Regular erase-based masking operation disrupts the global features that are important for the contrastive objective [7, 11].",1,related,1,positive
"From the results, we can reach to three observations: (1) The GCL-based methods generally yield higher performances than classical unsupervised learning methods, indicating the effectiveness of utilizing instance-level supervision; (2) RGCL, AD-GCL, and GASSL achieve better performances than GraphCL, which empirically proves the conclusion that InfoMax object could bring overwhelmed redundant information and thus suffer from feature suppression issue; (3) Our proposed GraphCV and DGCL consistently outperform other baselines, proving the advantage of disentangled representation.",1,related,1,positive
"The results are shown in left two subplots of Figure 4, we compare our method with GASSL under different perturbation bounds and attack steps to demonstrate its robustness against adversarial attacks.",1,related,1,positive
"Under the unsupervised representation learning setting, we compare GraphCV with the eight SOTA self-supervised learning methods GraphCL [51], InfoGraph[33], MVGRL [14], AD-GCL[34], GASSL[48], InfoGCL[45], RGCL [22] and DGCL[21], as well as three classical unsupervised representation learning methods, including node2vec [11], graph2vec [28], and GVAE[19].",1,related,1,positive
"Since both our model and GASSL use GIN as the backbone network, we hereby add the performance of GIN as the compared baseline.",1,related,1,positive
"Therefore, feature suppression is not just a prevalent issue in
supervised learning, but also in SSL. Due to the page limitation, we provide more discussion about the relation between feature suppression and GCL in Appendix B",1,related,1,positive
"Contrastive Learning: Following the definition in Oord et al. (2018); Wang & Isola (2020); Chen et al. (2021a); Radford et al. (2021), we formulate the contrastive loss as
Lc(fθ, gφ; τ,S) := E U ,V ∼S U−i 6=U V −j 6=V
[ − log e
−τd(fθ(U),gφ(V ))∑ j∈[M ] e −τd(fθ(U),gφ(V −j )) + ∑ i∈[M ] e…",1,related,1,positive
"To the best of our knowledge, the highest top-1 accuracies reported on ImageNet with SimCLR in 100 epochs are around 66.8% (Chen et al., 2021a).",1,related,0,negative
"We propose here a first solution to this issue by studying RandBits-CIFAR10 (Chen et al., 2021), a CIFAR10 based dataset where k noisy bits are added and shared between views of the same image (see Appendix D.3).",1,related,1,positive
"We propose here a first solution to this issue by studying RandBits-CIFAR10 (Chen et al., 2021), a CIFAR10 based dataset where k noisy bits are added and shared between views of the same image (see Appendix D.",1,related,1,positive
"Once trained, we use its representation to define the kernel KV AE .
noted previously (Chen et al., 2021), that β-VAE is the only method insensitive to the number of added bits, but its representation quality remains low compared to other selfsupervised approaches.",1,related,1,positive
"We provide a solution to the feature suppression issue in CL (Chen et al., 2021) and also demonstrate SOTA results with weaker augmentations on visual benchmarks (both on natural and medical images).",1,related,1,positive
"…)
= − 1
N
∑
i,j∈MB
sim(zi, zj)/τ
︸ ︷︷ ︸
Lalignment
+ 1
N
N∑
i
log ( 2N∑
k=1
1[k 6=i]exp(sim(zi, zk)/τ)
︸ ︷︷ ︸
Ldistribution
) (3)
and as such there are two parts of the loss, namely, alignment and distribution, as proposed by [Wang and Isola, 2020] and further studied by [Chen et al., 2020b].",1,related,1,positive
"Equation (2) can be rewritten accordingly by applying the logarithmic rules
LNT−Xent = − 1
N
∑
i,j∈MB
(
sim(zi, zj)/τ − log
2N∑
k=1
1[k 6=i]exp(sim(zi, zk)/τ) )
= − 1
N
∑
i,j∈MB
sim(zi, zj)/τ
︸ ︷︷ ︸
Lalignment
+ 1
N
N∑
i
log ( 2N∑
k=1
1[k 6=i]exp(sim(zi, zk)/τ)
︸ ︷︷ ︸
Ldistribution
) (3)
and as such there are two parts of the loss, namely, alignment and distribution, as proposed by [Wang and Isola, 2020] and further studied by [Chen et al., 2020b].",1,related,1,positive
"Finally, we broadcast the class assignments back to the original dimensions of the image I via nearest neighbor interpolation, akin to [9].",1,related,1,positive
We follow Chen & Li (2020) to see how local features are agglomerated across layers.,1,related,1,positive
"To study why the large generalization gap exists, in Figure 4, we follow Chen & Li (2020) to see how features aggregate in space.",1,related,1,positive
"To further understand why unsupervised finetuning is nontrivial, we follow the analysis in the work [8] about the contrastive loss, which represents the generalized contrastive loss in the below form:",1,related,1,positive
"We follow [5, 42] and rewrite L̂ in terms of explicit ‘pull‘ and ‘push‘ terms as :",1,related,1,positive
"7Unlike the reported results by Chen et al. (2021), smaller dimensionality, i.e., 32 gives better downstream accuracy on CIFAR-100 than 64 or 128.",1,related,1,positive
"To further understand why unsupervised pretraining has worse clustering quality than supervised pretraining, we follow [34, 9] and decouple the widely used unsupervised learning loss, i.",1,related,1,positive
", discard) certain input features [4, 11].",1,related,1,positive
"This policy requires the model to match each instance’s embedding into the predefined prior distribution with high entropy (Chen and Li, 2020; Wang and Isola, 2020).",1,related,1,positive
We provide a solution to the feature suppression issue in CL [10] and also demonstrate SOTA results with weaker augmentations on visual benchmarks.,1,related,0,negative
"Given MI(x,y) = H(x)−H(x|y), the two right-hand side terms can be linked to the following two properties [7, 31]: ∗ Uniformity H(x): Maximizing entropy leads to uniformly distributed latent vectors.",1,related,1,positive
"1) In this work, we investigate the dense feature representation in terms of alignment and uniformity inspired by the pioneering analyses of [7, 31].",1,related,1,positive
"…)
= − 1
N
∑
i,j∈MB
sim(zi, zj)/τ
︸ ︷︷ ︸
Lalignment
+ 1
N
N∑
i
log ( 2N∑
k=1
1[k 6=i]exp(sim(zi, zk)/τ)
︸ ︷︷ ︸
Ldistribution
) (3)
and as such there are two parts of the loss, namely, alignment and distribution, as proposed by [Wang and Isola, 2020] and further studied by [Chen et al., 2020b].",1,related,1,positive
"Equation (2) can be rewritten accordingly by applying the logarithmic rules
LNT−Xent = − 1
N
∑
i,j∈MB
(
sim(zi, zj)/τ − log
2N∑
k=1
1[k 6=i]exp(sim(zi, zk)/τ) )
= − 1
N
∑
i,j∈MB
sim(zi, zj)/τ
︸ ︷︷ ︸
Lalignment
+ 1
N
N∑
i
log ( 2N∑
k=1
1[k 6=i]exp(sim(zi, zk)/τ)
︸ ︷︷ ︸
Ldistribution
) (3)
and as such there are two parts of the loss, namely, alignment and distribution, as proposed by [Wang and Isola, 2020] and further studied by [Chen et al., 2020b].",1,related,1,positive
"For transformer, we leverage pre-trained models on ImageNet (Deng et al., 2009) from ViT (Dosovitskiy et al., 2021), DeiT (Touvron et al., 2021), DINO (Caron et al., 2021), MoCo-v3 (Chen et al., 2021b), and MAE (He et al., 2022).",1,related,1,positive
"Specifically, we leverage MoCo-v3 (Chen et al., 2021b), the ViT version of MoCo, and Supervised ViT.",1,related,1,positive
"We find that the MoCo-v3 degradation is larger with patch shuffling, but smaller with gamma distortion.",1,related,1,positive
"We make several observations: 1) Even without using any attribute information, our method performs significantly better as compared to other structure-only based methods like Spectral Clustering and Node2Vec, which demonstrates the effectiveness of our loss formulation and training methodology that promotes clusterability, which is also in line with recent observations [10, 55].",1,related,1,positive
"For all three datasets we omit the Gaussian blur and solarization as described in [Chen et al.,
2020a].",1,related,1,positive
"As to correspond with the BYOL procedure, we employ the same image augmentations as described in [Chen et al., 2020a; Grill et al., 2020].",1,related,1,positive
"M = 2B − 1 in [Chen et al., 2020a] whereB is the batch size.",1,related,1,positive
"To evaluate the quality of representations learned during selfsupervised training we employ the standard linear evaluation protocol described in [Chen et al., 2020a; Grill et al., 2020].",1,related,1,positive
"Augmentation procedure is key to the success of selfsupervised learning, therefore to compare our performance against BYOL, we employ the same image augmentations reported in [Grill et al., 2020; Chen et al., 2020a].",1,related,1,positive
