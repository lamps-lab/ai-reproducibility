text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"We noticed an inconsistency in LIC [17], when varying the language encoders, where two language encoders are utilized as classifiers; BERT [11], and LSTM [18].",1,related,1,positive
We chose T5-generation and Merged as it well balances LIC and Error.,1,related,0,negative
"Given y ∈ Dg, FPG instantiated by first masking gender words and replacing corresponding tokens with the mask token to avoid revealing the gender, following [18].",1,related,1,positive
"First, based on the observations in previous work [5,8,18,44,51], we hypothesize that there exist two different types of biases affecting captioning models:",1,related,1,positive
"Bias metrics We mainly rely on three metrics to evaluate our framework: 1) LIC [18], which compares two gender classifiers’ accuracies trained on either generated captions by a captioning model or human-written captions.",1,related,1,positive
"Data: Gender Analysis Sets We choose to analyze gender as our protected attribute since this is generally recognized as a universal attribute that can be applied to all humans and its biases have been studied and recognized as significant in the context of vision models [16, 17, 26, 31, 32, 36, 37, 41, 42].",1,related,1,positive
We observed only one other work [83] that makes use of this metric outside the scope of the VQA(-CP) datasets.,1,related,1,positive
"Gender bias amplification analysis Using our proposed metric, we compare the performance of multi-label classifiers trained on COCO and imSitu, two standard benchmarks for bias amplification metrics (Zhao et al. 2017; Wang et al. 2019; Wang and Russakovsky 2021; Hirota, Nakashima, and Garcia 2022).",1,related,1,positive
