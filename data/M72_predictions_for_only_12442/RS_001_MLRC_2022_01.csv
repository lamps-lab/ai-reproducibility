text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"For G-Mixup, we use the same hyper-parameters reported in [7].",1,related,1,positive
"Only in the case of the Reddit-5K dataset with label rates of larger than 25 labeled graphs per class, G-Mixup outperforms our proposed method.",1,related,1,positive
"We apply our proposed Graph Dual Mixup on the Graph Convolution Network (GCN) baseline [11] and compare our proposed method against 5 other graph augmentation methods from the literature: DropNode [25], DropEdge [14], M-Mixup [21], SoftEdge [5] and G-Mixup [7].",1,related,1,positive
Our result generalizes that of [16] by allowing arbitrary convex combinations and any complexon dimension.,1,related,1,positive
"Note that for complexons of dimension 1, when γi = λ, γj = 1 − λ, and γk = 0 for every k ̸= i, j, Theorem 1 reduces to the result for pairwise graphon mixup in [16].",1,related,1,positive
"Step (1) of SC-MAD is common for mixup methods, where samples are interpolated in an embedding space [16, 17, 28].",1,related,1,positive
"In particular, we assume that for each class y, there is a finite set of discriminative simplicial complexes Fy such that for every labeled simplicial complex (K, y), there exists at least one F ∈ Fy that is a subcomplex of K [16], that is, there is a homomorphism from F to K.",1,related,1,positive
"We present the following result on the structural similarities between a complexon mixture and one of the complexons, inspired by a similar result for graphon mixup [16].",1,related,1,positive
"The core idea of Mixup is to linearly combine two samples as follows:
𝑥syn = 𝛼 ∗ 𝑥0 + (1 − 𝛼) ∗ 𝑥1, (1)
where 𝑥0 and 𝑥1 are the two selected source samples and 𝛼 ∈ [0.0, 1.0] controls the composition of𝑥syn.",1,related,1,positive
"In addition, the graphs we considered in the experiment all have node features, while G-Mixup [49] only applies to undirected graphs without node features, and therefore is not within the scope of our baselines.",1,related,1,positive
"Note that our proposed mixup approach is different from traditional mixup approaches [15, 49, 54] in data augmentation, where they usually follow a form similar to M (mix) = λMa + (1 − λ)Mb .",1,related,1,positive
"Note that G-mixup relies on a strong assumption that graphs from the same class can be generated by the same graph generator (i.e., graphon).",1,related,1,positive
"We compare our methods with the following baseline methods, including (1) DropEdge (Rong et al., 2020), which uniformly removes a certain ratio of edges from the input graphs; (2) DropNode (Feng et al., 2020; You et al., 2020), which uniformly drops a certain portion of nodes from the input graphs; (3) Subgraph (You et al., 2020), which extract subgraphs from the input graphs via a random walk sampler; (4) M-Mixup (Verma et al., 2019; Wang et al., 2021b)4, which linearly interpolates the graph-level representations; (5) SubMix (Yoo et al., 2022), which mixes random subgraphs of input graph pairs; (6) G-Mixup (Han et al., 2022), which is a class-level graph mixup method by interpolating graphons of different classes.",1,related,1,positive
"(19)
For the setting of classification, 𝑃 (𝑆𝑝,𝑞 | 𝑦 = 𝑐) ∼ N ( 𝜇 𝑝,𝑞 𝑐 , (𝜎 𝑝,𝑞 𝑐 )2 ) , (20)
To extend G-Mixup for regression, we slightly modify the augmentation process to adapt it for regression tasks as
𝑃 (𝑆𝑝,𝑞 | 𝑦) ∼ N ( 𝜇𝑝,𝑞 + 𝜎𝑝,𝑞
𝜎𝑦 𝜌𝑝,𝑞
( 𝑦 − 𝜇𝑦 ) , ( 1 − (𝜌𝑝,𝑞)2 ) (𝜎𝑝,𝑞)2 ) ,
(21)
where 𝜇 and 𝜎 are the mean and standard deviation of the weight for each edge, 𝜌 is the correlation coefficient between 𝑆𝑝,𝑞 and 𝑦.
C-Mixup [93] shares the same process with the V-Mixup.",1,related,1,positive
"Baselines For baseline augmentation models, we employ two classical graph augmentation methods: DropEdge [4] and DropNode [5], and three Mixup-based augmentations: SubMix [10], ManifoldMixup (M-Mixup) [8], and G-Mixup [11].",1,related,1,positive
"The detailed statistics of each dataset are shown in Appendix A.
Baselines For baseline augmentation models, we employ two classical graph augmentation methods: DropEdge [4] and DropNode [5], and three Mixup-based augmentations: SubMix [10], ManifoldMixup (M-Mixup) [8], and G-Mixup [11].",1,related,1,positive
"However, we cannot use Mixup directly because it is suitable for regular, Euclidean data [54], while the user’s rating is discrete and non-interpolative, and there is no label for supervised learning.",1,related,1,positive
"They learn to create new examples that preserve the properties of original graphs (Kong et al., 2022; Han et al., 2022; Luo et al., 2022).",1,related,1,positive
"Baselines and implementation: Besides GIN, there are three lines of baseline methods: (1) selfsupervised learing methods including EDGEPRED, ATTRMASK, CONTEXTPRED in (Hu et al., 2019), INFOMAX (Velickovic et al., 2019), JOAO (You et al., 2021), GRAPHLOG (Xu et al., 2021), and D-SLA (Kim et al., 2022), (2) semi-supervised learning methods including self-training with selected unlabeled graphs (ST-REAL) and generated graphs (ST-GEN) and INFOGRAPH (Sun et al., 2020), and (3) graph data augmentation (GDA) methods including FLAG (Kong et al., 2022), GREA (Liu et al., 2022), and G-MIXUP (Han et al., 2022).",1,related,1,positive
"↑ ogbg-HIV ogbg-ToxCast ogbg-Tox21 ogbg-BBBP ogbg-BACE ogbg-ClinTox ogbg-SIDER
# Training Graphs 32,901 6,860 6,264 1,631 1,210 1,181 1,141
GIN 77.4(1.2) 66.9(0.2) 76.0(0.6) 67.5(2.7) 77.5(2.8) 88.8(3.8) 58.1(0.9)
Se lf-
Su pe
rv is
ed EDGEPRED 78.1(1.3) 63.9(0.4) 75.5(0.4) 69.9(0.5) 79.5(1.0) 62.9(2.3) 59.7(0.8) ATTRMASK 77.1(1.7) 64.2(0.5) 76.6(0.4) 63.9(1.2) 79.3(0.7) 70.4(1.1) 60.7(0.4) CONTEXTPRED 78.4(0.1) 63.7(0.3) 75.0(0.1) 68.8(1.6) 75.7(1.0) 63.2(6.5) 60.7(0.8) INFOMAX 75.4(1.8) 61.7(1.0) 75.5(0.4) 69.2(0.5) 76.8(0.2) 73.0(0.2) 58.6(0.5) JOAO 76.2(0.2) 64.8(0.3) 74.8(0.5) 69.3(2.5) 75.9(3.9) 69.4(4.5) 60.8(0.6) GRAPHLOG 74.8(1.1) 63.2(0.8) 75.4(0.8) 67.5(2.3) 80.4(3.6) 69.0(6.6) 57.0(0.9) D-SLA 76.9(0.9) 60.8(1.2) 76.1(0.1) 62.6(1.0) 80.3(0.6) 78.3(2.4) 55.1(1.0)
Se m
iSL INFOGRAPH 73.3(0.7) 61.5(1.1) 67.6(0.9) 61.6(4.4) 75.9(1.8) 62.2(5.5) 56.3(2.3)
ST-REAL 78.3(0.6) 64.5(1.0) 76.2(0.5) 66.7(1.9) 77.4(1.8) 82.2(2.4) 60.8(1.2) ST-GEN 77.9(1.6) 65.1(1.0) 75.8(0.9) 66.3(1.5) 78.4(3.0) 87.3(1.3) 59.3(1.3)
G D A FLAG 74.6(1.7) 59.9(1.6) 76.9(0.7) 66.6(1.0) 79.1(1.2) 85.1(3.4) 57.6(2.3) GREA 79.3(0.9) 67.5(0.7) 77.2(1.2) 69.7(1.3) 82.4(2.4) 87.9(3.7) 60.1(2.0) G-MIXUP 77.1(1.1) 55.6(1.1) 64.6(0.4) 70.2(1.0) 77.8(3.3) 60.2(7.5) 56.8(3.5)
DCT (Ours) 79.5(1.0) 68.1(0.2) 78.2(0.2) 70.8(0.5) 85.6(0.6) 92.1(0.8) 63.9(0.3) Molecule Regression: MAE ↓ Polymer Regression: MAE ↓ Bio: AUC (%)↑
ogbg-Lipo ogbg-ESOL ogbg-FreeSolv GlassTemp MeltingTemp ThermCond O2Perm PPI # Training Graphs 3,360 902 513 4,303 2,189 455 356 60,715
GIN 0.545(0.019) 0.766(0.016) 1.639(0.146) 26.4(0.2) 40.9(2.2) 3.25(0.19) 201.3(45.0) 69.1(0.0)
Se lf-
Su pe
rv is
ed EDGEPRED 0.585(0.008) 1.062(0.066) 2.249(0.150) 27.6(1.4) 47.4(2.8) 3.69(0.50) 207.3(41.7) 63.7(1.1) ATTRMASK 0.573(0.009) 1.041(0.041) 1.952(0.088) 27.7(0.8) 45.8(2.6) 3.17(0.32) 179.9(30.8) 64.1(1.8) CONTEXTPRED 0.592(0.007) 0.971(0.027) 2.193(0.151) 27.6(0.3) 46.7(1.9) 3.15(0.24) 191.2(35.2) 62.0(1.2) INFOMAX 0.581(0.009) 0.935(0.018) 2.197(0.129) 27.5(0.8) 46.5(2.8) 3.31(0.25) 231.0(52.6) 63.3(1.2) JOAO 0.596(0.016) 1.098(0.037) 2.465(0.095) 27.5(0.2) 46.0(0.2) 3.55(0.26) 207.7(43.7) 61.5(1.2) GRAPHLOG 0.577(0.010) 1.109(0.059) 2.373(0.283) 29.5(1.3) 50.3(3.3) 3.01(0.17) 229.7(48.3) 62.1(0.6) D-SLA 0.563(0.004) 1.064(0.030) 2.190(0.149) 27.5(1.0) 51.7(2.5) 2.71(0.08) 257.8(30.2) 65.0(1.2)
Se m
iSL INFOGRAPH 0.793(0.094) 1.285(0.093) 3.710(0.418) 30.8(1.2) 51.2(5.1) 2.75(0.15) 207.2(21.8) 67.7(0.4)
ST-REAL 0.526(0.009) 0.788(0.070) 1.770(0.251) 26.6(0.3) 42.3(1.2) 2.64(0.07) 256.0(17.5) 68.9(0.1) ST-GEN 0.531(0.031) 0.724(0.082) 1.547(0.082) 26.8(0.3) 42.0(0.9) 2.70(0.03) 262.2(10.1) 68.6(0.6)
G D A FLAG 0.528(0.012) 0.755(0.039) 1.565(0.098) 26.6(1.3) 44.2(2.0) 3.05(0.10) 177.7(60.7) 69.2(0.2) GREA 0.586(0.036) 0.805(0.135) 1.829(0.368) 26.7(1.0) 41.1(0.8) 3.23(0.18) 194.0(45.5) 68.8(0.2)
DCT (Ours) 0.516(0.071) 0.717(0.020) 1.339(0.075) 23.7(0.2) 38.0(0.8) 2.59(0.11) 165.6(24.3) 69.5(0.2)
(ogbg-HIV, ogbg-ToxCast, ogbg-Tox21, ogbg-BBBP, ogbg-BACE, ogbg-ClinTox, ogbg-SIDER), three molecule regression tasks (ogbg-Lipo, ogbg-ESOL, ogbg-FreeSolv) from open graph benchmarks (Hu et al., 2020), four polymer regression tasks (GlassTemp, MeltingTemp, O2Perm, and thermal conductivity prediction ThermCond), and also protein function prediction (PPI) (Hu et al., 2019).",1,related,1,positive
"More specifically, inspired by theMixup technology employed in the field of computer vision [35] [5], we first generate new negative samples’ representation by mixing positive sample embeddings into the negative sample embedding.",1,related,1,positive
"It is worth noting that the drop edge technique we use here is different to the standard data augmentation techniques such as DropEdge (Rong et al., 2019), and G-Mixup (Han et al., 2022b), which either add slightly modified copies of existing data or generate synthetic based on existing data.",1,related,1,positive
"It is worth noting that the drop edge technique we use here is different to the standard data augmentation techniques such as DropEdge [77], and G-Mixup [78], which either add slightly modified copies of existing data or generate synthetic based on existing data.",1,related,1,positive
com/gasteigerjo/gdc [13] G-mixup ICML 2022 GI https://github.,1,related,0,negative
"• Graph Data Augmentation: DropEdge (Rong et al., 2020), GREA (Liu et al., 2022), FLAG (Kong et al., 2022), M-Mixup (Wang et al., 2021), G-Mixup (Han et al., 2022).",1,related,1,positive
"We choose three data augmentation baselines, DropEdge, FLAG and G-Mixup, which augment graphs from different views.",1,related,1,positive
"Note that G-Mixup [15] has gfeat as (10) and glabel as (1b)
cp (2− α) for α ∈ [1, 2].",1,related,1,positive
"For mixup of graph data gfeat, we compare GraphMAD’s clusterpath data mixup (7) with linear graphon mixup [15].",1,related,1,positive
"Similarly to [15], our graph descriptors are SBM graphon approximations, where Wi ∈ [0, 1]D×D is obtained for each graph Gi by sorting and smoothing (SAS) [22] with D denoting the fineness of the graphon estimate.",1,related,1,positive
Note that G-Mixup [15] has gfeat as (10) and glabel as (1b) Fig.,1,related,1,positive
"In this work, similar to [15], we adopt the graphon, a bounded symmetric function W : [0, 1](2) → [0, 1] that can be interpreted as a random graph model associated with a family of graphs with similar structural characteristics [18–20].",1,related,1,positive
"In addition to the baselines in Section 4.1, we also compare with previous graph augmentation methods, including DropEdge (Rong et al., 2020), M-Mixup (Wang et al., 2021), G-Mixup (Han et al., 2022), and FLAG (Kong et al., 2022).",1,related,1,positive
com/gasteigerjo/gdc [13] G-mixup ICML 2022 GI https://github.,1,related,0,negative
