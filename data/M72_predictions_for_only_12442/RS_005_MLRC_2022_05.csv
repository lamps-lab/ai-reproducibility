text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"Additionally, we release the checkpoints of the latter two monolingual BERT models (BERT-small and BERT-base) mentioned above (Garcia, 2021).",1,related,1,positive
"…with 12 hidden layers) provided by Devlin et al. (2019), we evaluate the following monolingual models: Bertinho-base, with 12 layers (Vilares, Garcia, and GómezRodŕıguez, 2021), and two models of BertGalician (‘base’ and ‘small’) released by Garcia (2021), with 12 and 6 layers, respectively.",1,related,1,positive
"In addition, we have created a new dataset in GalicianPortuguese, English and Spanish that includes examples of homonymy and synonymy in context, also used to compare various contextualization models and strategies [21].",1,related,1,positive
