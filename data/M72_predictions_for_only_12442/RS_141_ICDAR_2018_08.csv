text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"The source codes of 6 papers were not executable [30, 33, 34, 36, 38, 39].",1,related,0,negative
"We build the TFE as an attention bi-directional GRU network [38], [39] to recurrently process word embeddings in",1,related,1,positive
"We unfortunately could not directly evaluate the approaches presented in References [5, 25, 30, 41, 47, 89] using our cybersecurity corpus documents, because their respective implementations were not available online.",1,related,0,negative
"Experimentation achieved the highest accuracy of 90.17 for Bi-GRU, applying learned word class features along with embedding with GloVe.",1,related,0,negative
"(ii) LSTM [42], Bi-LSTM [43], GRU [44], and Bi-GRU [45] are investigated as classifiers (with one-dimensional convolution layer).",1,related,1,positive
"Maximum average accuracy is achieved by the GloVe-WCFBi-GRU model, which is 90.41% for the Saraiki-Hindi dataset, while for the English-Bengali-Saraiki-Hindi-Roman Urdu mix dataset, minimum accuracy is observed (i.e., 85% in Figure 7).",1,related,1,positive
"*e highest average accuracy is achieved for the GloVe-WCFBi-GRU model for which the optimized approach is presented
in Figure 7.",1,related,1,positive
"On the other hand, for the Parzen estimator, highest accuracy is achieved by Bi-GRU implemented on top of GloVe for Eng-Bengali scripts.",1,related,1,positive
"*e architecture gets data and performs on dataset word-by-word analysis and feeds the representation into LSTM, Bi-LSTM, GRU, and Bi-GRU.",1,related,1,positive
"In our work, the input is captured by tweet as the token as an underlying layer of RNN variants as LSTM, BI-LSTM, GRU, and Bi-GRU as word embedding.",1,related,1,positive
"*erefore, dataset consisting of five different languages (3 cursive and 2 noncursive) is selected for effective validation of the proposed method
*e significance of this study is to explore (1) two kinds of word embeddings; (2) four classifiers (LSTM, Bi-LSTM, GRU, and Bi-GRU); (3) various deep neural network architectures; (4) optimal value of different hyperparameters to find the optimal language detection for the mixed-script dataset consisting of Roman Urdu, English, Saraiki, Hindi, and Bengali languages.",1,related,1,positive
"We compare the performance of our tabstruct-net against seven benchmark methods â€” deepdesrt [7], tablenet [12], graphtsr [14], splerge [10], dgcnn [9], Bi-directional gru [15] and Image-to-Text [11].",1,related,1,positive
" 57.40 52.20 pdf2table [40] N 59.51 57.52 58.50 TABFIND [36] N 70.52 68.74 69.62 Ours GTE N 94.70 94.49 94.57 Academic Systems Tensmeyer [37] Y 94.64 95.89 95.26 Nurminen [7] Y 94.09 95.12 94.60 Khan [17] Y 90.12 96.92 93.39 TABFIND [36] Y 64.01 61.44 62.70 Ours GTE Y 95.74 95.39 95.55 Cell Structure Ablation Study To analyze our GTE-Cell network further, we compare the several variations in Table 4 u",1,related,1,positive
