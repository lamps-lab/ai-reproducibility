text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"We implement a U-Net-based CA (UNetCA) baseline consisting of a modified version of our U-Net with 48 initial output feature maps as opposed to 24 and with all convolutions except the first changed to 1⇥1 to respect typical NCA restrictions [7, 30].",1,related,1,positive
"To train the ViTCA update rule, we follow a “pool sampling”-based training process [7, 30] along with a curriculum-based masking/noise schedule when corrupting inputs.",1,related,0,negative
It was therefore unclear how to get the dataset labels for the t‐SNE latent space visualization in Figure 4 as it is not mentioned in the paper [1].,1,related,0,negative
"Presented in figure 1 is a corrected version of figure 2 from the original paper [1], that is slightly misleading as it appears as if the NCA decoder ends with a doubling operation, severely limiting the decoders expressive power.",1,related,1,positive
"2 Datasets The dataset used by the original authors [1] is the publicly available statically binarized version of theMNIST dataset [2], which contains binary images of size 28×28.",1,related,1,positive
"5 Computational requirements The original paper’s code implementation was run on some type of GPU, for which the exact specifications are not presented in [1].",1,related,0,negative
"3 Hyperparameters The hyperparameters are described in section 3 of [1] where it was stated that unless oth‐ erwise specified, all models are trained using a batch size of 32, the Adam optimizer [7] was used, a learning rate of 10−4 was applied, gradient clipping was used with a norm of 10 [8], a latent vector of size |Z| = 256 was used and the models were trained for a total of 100, 000 gradient updates.",1,related,1,positive
