text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
com/wanyu-lin/ICML2021-Gem/ SubgraphX [62] https://github.,1,related,0,negative
"Following routinely adopted settings (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021), we can safely assume that both models can correctly use the informative components(motifs) in the input graphs to make predictions.",1,related,1,positive
"In our experiments, we employ the Fidelity+ and Fidelity− [28] to evaluate the fidelity of the explanations.",1,related,1,positive
"We compare non-generative methods, including the heuristic Occlusion (Zeiler & Fergus, 2014), gradient-based meth-ods Saliency (Baldassarre & Azizpour, 2019), Integrated Gradient (Sundararajan et al., 2017), and Grad-CAM (Pope et al., 2019), and perturbation-based methods GNNExplainer (Ying et al., 2019), PGMExplainer (Vu & Thai, 2020) and SubgraphX (Yuan et al., 2021).",1,related,1,positive
"To evaluate the performance of explanation quantitatively, we adopt the metrics of fidelity score and the sparsity score following previous works [39, 45, 46].",1,related,1,positive
"Finally, the third strategy is called SubgraphX (Yuan et al. 2021).",1,related,1,positive
"Given the input data, instancelevel techniques [29, 31, 52, 54, 65, 68] have been present main stream of GNN explanation, which aim to acquire explanations for a target instance.",1,related,1,positive
"As shown in Case 1. of Figure 1(a) is a 6-carbon ring found by SubgraphX [68] as an explanation for the mutagenic class in the MUTAG dataset; however its prediction score from the GNN for the underlying label is only 0.0028, which means the model does not recognize this structure asmutagenic.",1,related,1,positive
of Figure 1(a) is a 6-carbon ring found by SubgraphX [68] as an explanation for the mutagenic class in the MUTAG dataset; however its prediction score from the GNN for the underlying label is only 0.,1,related,1,positive
"This is because the influence score is defined to be less sensitive to the change in probabilities and xPath pays more attention to avoiding label change, i.e., better accuracy fidelity than SubgraphX.",1,related,0,negative
"In this paper, we compare our methods with state-of-the-art GNN explanation methods, including GNNExplainer (Ying et al. 2019), PGEXplainer (Luo et al. 2020) SubgraphX (Yuan et al. 2021) and ReFine (Wang et al. 2021), and inherently interpretable GNN methods, including GAT and GSAT (Miao, Liu, and Li 2022).",1,related,1,positive
"In future work, we plan on combining the strengths of PROVEXPLAINER with those of generic GNNbased explainers e.g., GNNExplainer and SubgraphX, we hope to contextualize the relative degrees of global trends and edge-case logic involved in a particular prediction.",1,related,1,positive
"the unique contributions of each feature to the final prediction, we calculated the SHapley Additive exPlanations (SHAP) values for each model [28].",1,related,1,positive
"To examine the unique contributions of each feature to the final prediction, we calculated the SHapley Additive exPlanations (SHAP) values for each model [28].",1,related,1,positive
"We also used transductive explainers (GNNExplainer [1], SubgraphX [30], CF2 [31]) for a Planted Clique case study (Supp.",1,related,1,positive
"We also used transductive explainers (GNNExplainer [1], SubgraphX [30], CF(2) [31]) for a Planted Clique case study (Supp.",1,related,1,positive
We randomly select a number of similar molecules and visualize the explanations generated by DEGREE and SubgraphX.,1,related,1,positive
We can find that DEGREE has competitive performance compared to GNN-LRP and SubgraphX.,1,related,1,positive
"F.2 QUALITATIVE COMPARISON
In this section we make a qualitative comparison between DEGREE and SubgraphX.",1,related,1,positive
"Figure 8 shows the ACC of DEGREE, GNN-LRP and SubgraphX under various sparsity.",1,related,1,positive
We can find that none of the subgraphs generated by SubgraphX include the ’N-,1,related,1,positive
"F.1 QUANTITATIVE EVALUATION
In this section, we perform additional experiments comparing DEGREE with GNN-LRP Schnake et al. (2020) and SubgraphX Yuan et al. (2021).",1,related,1,positive
"For this explainability problem, we could use GNNExplainer [51], which is designed to derive insights from the hidden layers of GNNs, and SubgraphX [53].",1,related,1,positive
"…the outputs of the subgraph are connected, which lacks explanations for the message passing scheme in GNNs. Shapley-value based approaches SubgraphX Yuan et al. (2021) and GraphSVX Duval & Malliaros (2021) are computationally expensive especially for exploring different subgraphs with the MCTS…",1,related,1,positive
"In this section, we compare GFlowExplainer with a shapley-value based approache SubgraphX Yuan et al. (2021) and DEGREE on accuracy.",1,related,1,positive
"As for the accuracy calculation, we follow similar setting in SubgraphX and DEGREE for fair comparison.",1,related,1,positive
SubgraphX [48] uses the Shapley value [34] and performs Monte Carlo Tree Search (MCTS) on subgraphs.,1,related,1,positive
"To get the importance of a path, we first use a mean-field approximation for the joint probability by multiplying 𝑃 (𝑒) together, and we normalize each
Algorithm 1 PaGE-Link
GNNExp [45] PGExp [25] SubgraphX [48] PaGE-Link (ours) 𝑂 ( |E𝑐 |𝑇 ) 𝑂 ( |E |𝑇 ) / 𝑂 ( |E𝑐",1,related,1,positive
"We do not compare to other search-based explainers like SubgraphX [48]
because of their high computational complexity (see Section 5.4).",1,related,1,positive
"We do not compare to other search-based explainers like SubgraphX [46] because they have high computational complexity, as discussed in Section 5.",1,related,1,positive
GNNExp [43] PGExp [25] SubgraphX [46] PaGE-Link (ours),1,related,1,positive
"The proposed method outperformed GNNExplainer and PGExplainer, whereas SubgraphX displayed the best performance.",1,related,0,negative
"Adopting a simple numeric operation for all node embeddings is a common graph pooling method [81, 107], since it is easy to use and obeys the permutation invariant.",1,related,1,positive
"Similarly, GIN [107] shows us that the injective relabeling function in the WL algorithm can be replaced with a simple numeric operation.",1,related,1,positive
"Despite the existence of other works proposing explainers, which occasionally fall outside the aforementioned categorization [96, 54, 75, 52, 38, 101, 70], we limited our analysis on a subset.",1,related,1,positive
"More specifically, we fall short of understanding the influence of the input graph elements on both the changes in model parameters and the generalizability of a trained model [Ying et al., 2019, Huang et al., 2022, Yuan et al., 2021, Xu et al., 2019b, Zheng et al., 2021].
ar X
iv :2
21 0.",1,related,1,positive
"In the process, we use one of the available Local Explainers [35,21,38,30,26,24] to obtain a local explanation for each sample in the dataset.",1,related,1,positive
"We leave to [37,18] a detailed overview about Local Explainers, who recently proposed a taxonomy to categorize the heterogeneity of those.",1,related,1,positive
"In this work, we will broadly refer to all of those whose output can be mapped to a subgraph of the input graph (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Vu & Thai, 2020; Shan et al., 2021; Pope et al., 2019).",1,related,1,positive
"In principle, every Local Explainer whose output can be mapped to a subgraph of the input sample is compatible with our pipeline (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Vu & Thai, 2020; Shan et al., 2021; Pope et al., 2019).",1,related,1,positive
"We incorporate eight GNN explainability methods, including gradient-based: Grad29, GradCAM11, GuidedBP6, Integrated Gradients30; perturbation-based: GNNExplainer14, PGExplainer10, SubgraphX31; and surrogate-based methods: PGMExplainer13.",1,related,1,positive
"Still, this faithfulness is relatively weak, only 0.001 better than
Method GEA (↑) GEF (↓) GES (↓) GECF (↓) GEGF (↓)
Random 0.148 ± 0.002 0.579 ± 0.007 0.920 ± 0.002 0.763 ± 0.003 0.023 ± 0.002
Grad 0.193 ± 0.002 0.392 ± 0.006 0.806 ± 0.004 0.159 ± 0.004 0.039 ± 0.003
GradCAM 0.222 ± 0.002 0.452 ± 0.006 0.263 ± 0.004 0.010 ± 0.001 0.020 ± 0.002
GuidedBP 0.194 ± 0.001 0.557 ± 0.007 0.432 ± 0.004 0.067 ± 0.002 0.021 ± 0.002
IG 0.142 ± 0.002 0.545 ± 0.007 0.727 ± 0.005 0.110 ± 0.003 0.021 ± 0.002
GNNExplainer 0.102 ± 0.003 0.534 ± 0.007 0.431 ± 0.008 0.233 ± 0.006 0.027 ± 0.002
PGMExplainer 0.133 ± 0.002 0.541 ± 0.007 0.984 ± 0.001 0.791 ± 0.003 0.096 ± 0.004
PGExplainer 0.194 ± 0.002 0.557 ± 0.007 0.217 ± 0.004 0.009 ± 0.000 0.029 ± 0.002
SubgraphX 0.324 ± 0.004 0.254 ± 0.006 0.745 ± 0.005 0.241 ± 0.006 0.035 ± 0.003
Dataset Method GEA (↑) GEF (↓)
Mutag
Random 0.044 ± 0.007 0.590 ± 0.031
Grad 0.022 ± 0.006 0.598 ± 0.030
GradCAM 0.085 ± 0.012 0.672 ± 0.029
GuidedBP 0.036 ± 0.007 0.649 ± 0.030
Integrated Grad (IG) 0.049 ± 0.010 0.443 ± 0.031
GNNExplainer 0.031 ± 0.005 0.618 ± 0.030
PGMExplainer 0.042 ± 0.007 0.503 ± 0.031
PGExplainer 0.046 ± 0.007 0.504 ± 0.031
SubgraphX 0.039 ± 0.007 0.611 ± 0.030
Benzene
Random 0.108 ± 0.003 0.513 ± 0.012
Grad 0.122 ± 0.007 0.262 ± 0.011
GradCAM 0.291 ± 0.007 0.551 ± 0.012
GuidedBP 0.205 ± 0.007 0.438 ± 0.012
Integrated Grad (IG) 0.044 ± 0.003 0.182 ± 0.010
GNNExplainer 0.129 ± 0.005 0.444 ± 0.012
PGMExplainer 0.154 ± 0.006 0.433 ± 0.012
PGExplainer 0.169 ± 0.007 0.375 ± 0.012
SubgraphX 0.371 ± 0.009 0.513 ± 0.012
Fl-Carbonyl
Random 0.087 ± 0.007 0.440 ± 0.26
Grad 0.132 ± 0.010 0.210 ± 0.021
GradCAM 0.005 ± 0.007 0.500 ± 0.026
GuidedBP 0.089 ± 0.010 0.315 ± 0.024
Integrated Grad (IG) 0.091 ± 0.007 0.174 ± 0.019
GNNExplainer 0.094 ± 0.009 0.423 ± 0.026
PGMExplainer 0.078 ± 0.008 0.426 ± 0.026
PGExplainer 0.079 ± 0.009 0.372 ± 0.025
SubgraphX 0.008 ± 0.002 0.466 ± 0.026
9Scientific Data | (2023) 10:144 | https://doi.org/10.1038/s41597-023-01974-x
random explanation.",1,related,1,positive
"We incorporate eight GNN explainability methods, including gradient-based: Grad [29], GradCAM [11], GuidedBP [6], Integrated Gradients [30]; perturbation-based: GNNExplainer [14], PGExplainer [10], SubgraphX [31]; and surrogate-based methods: PGMExplainer [13].",1,related,1,positive
"In this part, we firstly introduce several explanation methods like SubgraphX [13] and XGNN [11].",1,related,1,positive
"Fidelity+ = 1
N N∑ i=1 (f(Gi)yi − f(G 1−mi i )yi) (4)
Fidelity− = 1
N N∑ i=1 (f(Gi)yi − f(G mi i )yi) (5)
Where N is the total number of samples, and yi is the class label. f(Gi)yi and f(G1−mii )yi are the prediction probabilities of yi when using the original graph Gi and the occluded graph G 1−mi i , which is gained by occluding important features found by explainers from the original graph.",1,related,1,positive
"The higher Fidelity+, the more necessary the explanation.",1,related,1,positive
"With the trained graph models, we quantitatively and qualitatively compare our FlowX with eight baselines, including GradCAM [34], DeepLIFT [43], GNNExplainer [27], PGExplainer [28], PGMExplainer [31], SubgraphX [29], GNNGI [37], GNN-LRP [37].",1,related,1,positive
"Specially, for subgraph-based method SubgraphX [29], we pick the explainable subgraph out, then assign the edges in this subgraph instead of nodes as the explanation.",1,related,1,positive
"The metrics we apply are Fidelity [29], [42], Accuracy [27], and Sparsity [42].",1,related,1,positive
"In our experiments, we compare the following methods: Random gives every edge and node feature a random value between 0 and 1; Distance assigns higher importance to edges that have lower distance to the target node; PageRank measures the importance of edges following the personalized PageRank strategy with automatic restart on the target node [9, 40]; Saliency (SA) measures node importance as the weight on every node after computing the gradient of the output with respect to node features [6]; Integrated Gradient (IG) avoids the saturation problem of the gradient-based method Saliency by accumulating gradients over the path from a baseline input (zero-vector) and the input at hand [36]; Grad-CAM is a generalization of class activation maps (CAM) [33]; Occlusion attributes the importance of an edge as the difference of the model initial prediction prediction on the graph after removing this edge [10]; GNNExplainer computes the importance of graph entities (node/edge/node feature) using the mutual information [46]; PGExplainer is very similar to GNNExplainer, but generates explanations only for the graph structure (nodes/edges) using the reparameterization trick to overcome computation intractability [22]; PGM-Explainer perturbs the input and uses probabilistic graphical models to find the dependencies between the nodes and the output [38]; and SubgraphX explores possible explanatory subgraphs with Monte Carlo Tree Search and assigns them a score using the Shapley value [49].",1,related,1,positive
"To explain the decisions made by the GNNs, we adopt different classes of explainers including structure-based methods such as Distance and personalized PageRank [8, 36], gradient/featurebased methods such as SA [5], IG [5] and Grad-CAM [30], and perturbation-based methods such as GNNExplainer [41], PGM-Explainer [34], SubgraphX [44], PGExplainer [20] and Occlusion to generate the explanatory subgraphs.",1,related,1,positive
We can also observe a limitation of USIB that it considers edges independently but ignores the substructures of graphs whose importance is emphasized in previous work [41].,1,related,1,positive
"Our method is related to graph explainability in that the predicted transformation probabilities from our augmentation model g is similar to explainability scores of some graph explainability methods (Maruhashi et al., 2018; Yuan et al., 2020; 2021).",1,related,1,positive
"In order to correlate this explanation with our motif-based expected explanation scored, for each graph we computed the six Jaccard similarities between the single SubgraphX-produced motif and the six injected motifs.",1,related,1,positive
"sion of the differences with two relatively similar explainers, GNNExplainer [27] and SubgraphX [29]; we use the same experimental settings of the previous section, results are shown in Figure 3.",1,related,0,negative
"However, for the sake of comparison, we include a brief discussion of the differences with two relatively similar explainers, GNNExplainer [27] and SubgraphX [29]; we use the same experimental settings of the previous section, results are shown in Figure 3.",1,related,0,negative
"We compare our MotifExplainer model with several state-of-the-art baselines: GNNExplainer, Sub-
graphX, PGExplainer, and ReFine.",1,related,1,positive
"Table 10 shows the comparison results with four state-of-the-art GNN explanation models: MotifExplainer, SubgraphX, PGExplainer, GNNExplainer, and ReFine.",1,related,1,positive
"While SubgraphX and GraphSVX were shown to perform better than prior alternatives, as we show in Section 3, the Shapley value they try to approximate is non-ideal as it is non-structure-aware.",1,related,1,positive
"3, we further evaluate on GIN [40] and GAT [36] on certain datasets following [44].",1,related,1,positive
"In fact, GNNExplainer, PGExplainer, and SubgraphX can never generate explanations including only disconnected -",1,related,1,positive
"We compare with 5 strong baselines representing the SOTA methods for GNN explanation: GNNExplainer [41], PGExplainer [25], SubgraphX [44], GraphSVX [9], and OrphicX [21].",1,related,1,positive
"We follow [44, 43] to employ Fidelity, Inverse Fidelity (Inv-Fidelity), and Sparsity as our evaluation metrics.",1,related,1,positive
"SubgraphX [44] uses the Shapley value as its scoring function on subgraphs
4As some baselines take over 24 hours on full GraphSST2, we randomly select 30 graphs for this analysis.
selected by Monte Carlo Tree Search (MCTS), and GraphSVX [9] uses a least-square approximation to the Shapley value to score nodes and their features.",1,related,1,positive
"SubgraphX [44] uses the Shapley value as its scoring function on subgraphs (4)As some baselines take over 24 hours on full GraphSST2, we randomly select 30 graphs for this analysis.",1,related,1,positive
"Following [44], we study the empirical efficiency of GStarX by explaining 50 randomly selected graphs from BBBP.",1,related,1,positive
Our results for the baselines are similar to [44].,1,related,0,negative
We also follow [44] to show the Fidelity vs.,1,related,1,positive
"We follow [44] to train GIN on MUTAG and GAT on GraphSST24, and show results in Table 2.",1,related,1,positive
We employ the fidelity score to evaluate how the explanation is faithful to the GCN model [55].,1,related,1,positive
"Different from the practice in GNNEXPLAINER [35], we do not mask node features since we only highlight the critical topology of the biological graph to GNN’s prediction, which is more intuitive and human-intelligible [39].",1,related,1,positive
"Concerning the explainers we use GNNExplainer [25], PGExplainer [17] and SubgraphX [28].",1,related,1,positive
For SubgraphX we used the hyperparameters of the original implementation [28].,1,related,1,positive
"SubgraphX retrieves explanations similar to Grad, but has the drawback of very high runtime, see Appendix B, available in the online supplemental material.",1,related,0,negative
"We could only run SubgraphX for the small synthetic dataset, due to its long runtime (see Appendix B), available in the online supplementalmaterial.",1,related,1,positive
"For a comprehensive quantitative evaluation we chose our baselines from the three different categories of post-hoc explanations models consisting of (i) soft-masking approaches like GNNExplainer, which returns a continuous feature and edge mask and PGE [24] learns soft masks over edges in the graph (ii) surrogate model based hard-masking approach, PGM [40], which returns a binary node mask, iii) Shapely based hard masking approach SubgraphX [48], which returns a subgraph as an explanation, and (iv) gradient-basedmethodsGrad&GradInput [34] which utilize gradients to compute feature attribu-",1,related,1,positive
"For a comprehensive quantitative evaluation we chose our baselines from the three different categories of post-hoc explanations models consisting of (i) soft-masking approaches like GNNExplainer, which returns a continuous feature and edge mask and PGE [24] learns soft masks over edges in the graph (ii) surrogate model based hard-masking approach, PGM [40], which returns a binary node mask, iii) Shapely based hard masking approach SubgraphX [48], which returns a subgraph as an explanation, and (iv) gradient-basedmethodsGrad&GradInput [34] which utilize gradients to compute feature attributions.",1,related,1,positive
"PGExplainer [47], and SubgraphX [48], etc.",1,related,1,positive
"SA [54], [55] Instance-level ✗ GC/NC N/E/NF ✗ Backward ✗ Guided BP [54] Instance-level ✗ GC/NC N/E/NF ✗ Backward ✗ CAM [55] Instance-level ✗ GC N ✗ Backward ✗ Grad-CAM [55] Instance-level ✗ GC N ✗ Backward ✗ GNNExplainer [46] Instance-level ✓ GC/NC E/NF ✓ Forward ✓ PGExplainer [47] Instance-level ✓ GC/NC E ✗ Forward ✓ GraphMask [57] Instance-level ✓ GC/NC E ✗ Forward ✓ ZORRO [56] Instance-level ✗ GC/NC N/NF ✓ Forward ✓ Causal Screening [58] Instance-level ✗ GC/NC E ✓ Forward ✓ SubgraphX [48] Instance-level ✓ GC/NC Subgraph ✓ Forward ✓ LRP [54], [59] Instance-level ✗ GC/NC N ✗ Backward ✗ Excitation BP [55] Instance-level ✗ GC/NC N ✗ Backward ✗ GNN-LRP [60] Instance-level ✗ GC/NC Walk ✗ Backward ✓ GraphLime [61] Instance-level ✓ NC NF ✓ Forward ✗ RelEx [62] Instance-level ✓ NC N/E ✓ Forward ✓ PGM-Explainer [63] Instance-level ✓ GC/NC N ✓ Forward ✓ XGNN [45] Model-level ✓ GC Subgraph ✓ Forward ✓",1,related,1,positive
"SubgraphX uses a Monte-Carlo optimization algorithm to find the most relevant subgraph, whereas we use either a local best guess or a random sampling approach (cf. Appendix D of the Supplement, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/ 10.1109/TPAMI.2021.3115452).",1,related,1,positive
"SubgraphX uses Shapley values for the subgraph scoring, whereas we use a backward propagation pass.",1,related,1,positive
SubgraphX [14] uses the Shapley value and onbtain the most important subgraphs with Monte Carlo Tree Search (MCTS).,1,related,1,positive
"Explainability methods We compare non-generative methods: Saliency [6], Integrated Gradient [36], Occlusion [53], Grad-CAM [32], GNNExplainer [48], PGMExplainer [39], and SubgraphX [52], with generative ones: PGExplainer [27], GSAT [29], GraphCFE (CLEAR) [28], D4Explainer and RCExplainer [42].",1,related,1,positive
"Despite the existence of other works proposing explainers, which occasionally fall outside the aforementioned categorization [92, 53, 72, 51, 36, 97, 67], we limited our analysis on a subset.",1,related,1,positive
"While SubgraphX and GraphSVX were shown to perform better than prior alternatives, as we show in Section 4, the Shapley value they try to approximate is unideal due to not having structure-awareness.",1,related,1,positive
"Although SubgraphX and GraphSVX use L-hop subgraphs and thus technically they use the graph structure, we discuss why this approach has limitations in achieving structure-awareness in Appendix F.",1,related,1,positive
"We compare with 4 strong baselines representing the state-of-the-art methods for GNN explanation: GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), SubgraphX (Yuan et al., 2021), and GraphSVX (Duval & Malliaros, 2021).",1,related,1,positive
"A Summary of Open-source Implementations
Model ear Framework Link Sourc
e
AGILE 2022 PyTorch https://github.com/clvrai/agile [30] GTA-RL 2022 PyTorch https://github.com/udeshmg/GTA-RL [34] SubgraphX 2021 PyTorch https://github.com/divelab/DIG [35] SUGAR 2021 Tensorflow https://github.com/RingBDStack/SUGAR [15]
CORL 2021 PyTorch https://github.com/huawei-
noah/trustworthyAI/tree/master/gcastle
[124]
RioGNN 2021 PyTorch https://github.com/safe-graph/RioGNN [73] IG-RL 2021 PyTorch https://github.com/FXDevailly/IG-RL [142] TITer 2021 Python https://github.com/JHL-HUST/TITer/ [128] SparRL 2021 PyTorch https://github.com/rwickman/SparRL-PyTorch [37] PAAR 2021 PyTorch https://github.com/seukgcode/PAAR [87] Policy-GNN 2020 PyTorch https://github.com/lhenry15/Policy-GNN [41] CARE-GNN 2020 PyTorch https://github.com/YingtongDou/CARE-GNN [36]
RL-BIC 2020 Tensorflow https://github.com/huawei-
noah/trustworthyAI/tree/master/Causal_Structure_Learning
/Causal_Discovery_RL
[123]
RL-based
Graph2Seq
2020 PyTorch https://github.com/hugochan/RL-based-Graph2Seq-for-
NQG
[126]
DGN 2020 Tensorflow https://github.com/PKU-AI-Edge/DGN/ [27]
DeepGraphM
olGen
2020 PyTorch https://github.com/dbkgroup/prop_gen [154]
KG-A2C 2020 PyTorch https://github.com/rajammanabrolu/KG-A2C [171]
GPA 2020 PyTorch https://github.com/ShengdingHu/GraphPolicyNetworkActi
veLearning
[14]
GAEA 2020 Tensorflow https://github.com/salesforce/GAEA [172] CompNet 2019 PyTorch https://github.com/WOW5678/CompNet [7]
GRPI 2019 Python https://github.com/LASP-UCL/Graph-RL [111]
DRL+GNN 2019 PyTorch https://github.com/knowledgedefinednetworking/DRL-
GNN
[145]
GPN 2019 PyTorch https://github.com/qiang-ma/graph-pointer-network [161] PGPR 2019 PyTorch https://github.com/orcax/PGPR [173] DGN 2018 PyTorch https://github.com/PKU-AI-Edge/DGN [27] GCPN 2018 Python https://github.com/bowenliu16/rl_graph_generation [10] KG-DQN 2018 PyTorch https://github.com/rajammanabrolu/KG-DQN [174] ASNets 2018 Tensorflow https://github.com/qxcv/asnets [150]
S2V-DQN 2017 C+Python https://github.com/Hanjun-Dai/graph_comb_opt [148] DeepPath 2017 Tensorflow https://github.com/xwhan/DeepPath [3] MINERVA 2017 Tensorflow https://github.com/shehzaadzd/MINERVA [92] KBGAN 2017 PyTorch https://github.com/cai-lw/KBGAN [95]",1,related,1,positive
"Source Citation Task
MUTAG 188 17.93 19.79 [97] SUGAR[15], SubgraphX[35], XGNN[18],
GraphAug[98]
Graph Classification
PTC 344 14.29 14.69 [99] SUGAR[15] Graph Classification
PROTEINS 1113 39.06 72.82 [100] SUGAR[15], GraphAug[98] Graph Classification
D&D 1178 284.32 715.66 [101] SUGAR[15] Graph Classification
NCI1 4110 29.87 32.30 [102] SUGAR[15], GraphAug[98] Graph Classification
NCI109 4127 29.68 32.13 [102] SUGAR[15], GraphAug[98] Graph Classification
BBBP 2039 24.06 25.95 [103] SubgraphX[35] Graph Classification
GRAPH-SST2 70042 10.19 9.20 [104] SubgraphX[35] Graph Classification
Table .",1,related,1,positive
"…be developed in Section 7 belongs to the explainablility category, where the objective is to discover qualitative justifications on the learning outcome of a GCN; see (Luo et al., 2020; Lin et al., 2021; Schlichtkrull et al., 2021; Ying et al., 2019; Yuan et al., 2021) and the references therein.",1,related,1,positive
"The statistic model to be developed in Section 7 belongs to the explainablility category, where the objective is to discover qualitative justifications on the learning outcome of a GCN; see (Luo et al., 2020; Lin et al., 2021; Schlichtkrull et al., 2021; Ying et al., 2019; Yuan et al., 2021) and the references therein.",1,related,1,positive
"We compare the proposed method with popular post-hoc explanation techniques including the GNN-Explainer (Ying et al. 2019), PGEExplainer (Luo et al. 2020), GradCAM (Pope et al. 2019) and SubgraphX (Yuan et al. 2021)2.",1,related,1,positive
"Specially, for subgraph-based method SubgraphX (Yuan et al., 2021), we pick the explainable subgraph out, then assign the edges in this subgraph instead of nodes as the explanation.",1,related,1,positive
"We shall discuss and compare 19 recent GNN interpretability methods from the aforementioned categories: GNNExplainer [36], PGExplainer [37], GraphMask [38], SubgraphX [39], PGMExplainer [40], RelEx [41], GraphLime [42], RCExplainer [43], DnX [44], GCFExplainer [45], CF2 [46], SA [26], GuidedBP [26], CAM [21], Grad-CAM [47], LRP [48], GNNLRP [49], ExcitationBP [21], and XGNN [50].",1,related,1,positive
"We shall discuss and compare 19 recent GNN interpretability methods from the aforementioned categories: GNNExplainer [36], PGExplainer [37], GraphMask [38], SubgraphX [39], PGMExplainer [40], RelEx [41], GraphLime [42], RCExplainer [43], DnX [44], GCFExplainer [45], CF(2) [46], SA [26], GuidedBP [26], CAM [21], Grad-CAM [47], LRP [48], GNNLRP [49], ExcitationBP [21], and XGNN [50].",1,related,1,positive
