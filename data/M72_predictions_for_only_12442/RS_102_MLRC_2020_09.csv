text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"Similarly, given a mini-batch of BU unlabeled data, taking popular consistency regularization frameworks (Sohn et al., 2020) as an example, the unsupervised loss is",1,related,1,positive
", FixMatch (Sohn et al., 2020)) might confuse fθ due to the label space mismatch.",1,related,1,positive
"To regularize gψ, we design SCR based on consistency regularization (Bachman et al., 2014; Xie et al., 2020; Sohn et al., 2020), which minimizes the gap between the outputs of two variants of samples that are transformed by different data augmentations.",1,related,1,positive
"As the oracle SSL methods, We used three representative SSL methods: UDA (Xie et al., 2020), FixMatch (Sohn et al., 2020), and FreeMatch (Wang et al., 2023).",1,related,1,positive
", Therefore training with the synthetic samples via unsupervised losses using pseudo training labels in Y (e.g., FixMatch (Sohn et al., 2020)) might confuse f θ due to the label space mismatch.",1,related,1,positive
"For the hyperparameters of oracle SSL methods, we followed the default settings of the original papers (Xie et al., 2020; Sohn et al., 2020; Wang et al., 2023).",1,related,1,positive
We apply weak and strong augmentations to every incoming data sample inspired by FixMatch [50].,1,related,1,positive
"Following the methodology proposed in [34], a fixed threshold of 0.",1,related,0,negative
"Drawing inspiration from previous works [3,42,50], our strong data augmentation encompasses a combination of random augmentation techniques [9], cutout [11], and the utilization of common data augmentation.",1,related,1,positive
"Note that, FixMatchRA [29] uses a constant threshold (0.",1,related,1,positive
"On these two datasets, we compare our method SPF-RA with several state-of-the-art methods closely related to ours, including FixMatch-RA [29], Dash-RA [33], RYS2 [27].",1,related,1,positive
"Unless stated otherwise, we implement our method SPF-RA by incorporating SPF to FixMatch-RA [29].",1,related,1,positive
"We perform our experiments by varying the amount of labeled data, following standard SSL evaluation protocols [2, 3, 22, 29].",1,related,0,negative
"We compare our SPF-RA method with several latest methods, including MeanTeacher [30], LP [10], DAG [17], and FixMatch-RA [29], under 4k and 10k labels.",1,related,1,positive
We adopt the implementation of FixMatch [35] and integrate NetAug-,1,related,1,positive
"For the labeled training data in all of our experiments, we follow the sampling strategy mentioned in [1] to choose an equal number of images from each class in order to avoid model bias.",1,related,1,positive
"Instead, we follow [1] to ignore the labels in the original training data and deem it as unlabeled data.",1,related,0,negative
"Following [1], we set τ in (3) to be 0.",1,related,1,positive
"For inference, we follow [1] to use the maintained exponential moving average of the trained parameters.",1,related,1,positive
"Inspired by FixMatch [26], we also expect the above property holds for strongly-augmented data.",1,related,1,positive
"95 as in FixMatch [26] or a dynamic generated scalar as in FlexMatch [44], then ỹ will be used as a pseudo-label for the corresponding unlabeled sample.",1,related,1,positive
The test model is updated by EMA with a decay rate of 0.999.,1,related,0,negative
"We apply SAA on the top of FixMatch [39] and FlexMatch [52], respectively.",1,related,1,positive
"Then we updateHti with exponential moving average (EMA), which can be expressed as:
Hti = (1− a)Ht−1i + αl t i .",1,related,1,positive
"Note that the parameter α introduced is not an additional model parameter, as the model parameters are also updated with EMA [39, 52].",1,related,1,positive
"Specifically, in the sample selection module, we first update the historical loss of the samples with exponential moving average (EMA) in each epoch, then these samples will be divided into two parts.",1,related,1,positive
"(4)
Note that the parameter α introduced is not an additional model parameter, as the model parameters are also updated with EMA [39, 52].",1,related,1,positive
"Being aware of the existence of OOD samples, we further extend FixMatch to also assign less-confident (controlled by threshold thr) pseudo-labeled samples as OOD instead of ignoring them in regular SSLs.",1,related,1,positive
"To verify the effectiveness of our method, we conducted two experiments that combined Prototype Fission with two popular SSL frameworks: ORCA for open-world SSL that
formulates novel classes from unlabeled data, and FixMatch for evaluating our contribution to standard SSL. CIFAR-10 and CIFAR-100 (Krizhevsky, Hinton et al. 2009) are used to evaluate the performance on both a limited number and a larger number of classes.",1,related,1,positive
Note that the original form of FixMatch and PF+FixMatch are not compared to in this experiment due to their incapability of recognizing novel classes.,1,related,1,positive
"Specifically, we form FixMatch and PF+FixMatch as follows:
(1) FixMatch-Sigmoid: This manipulation of FixMatch ditches the Softmax activation function and uses Sigmoid as σ instead as a more proper form of FixMatch in open-set settings.",1,related,1,positive
The probability threshold thr is set fixed as 0.95 as in the original FixMatch across four aforementioned variants.,1,related,1,positive
Prototype Fission + FixMatch: We combine Prototype Fission with FixMatch (Sohn et al. 2020) to validate its effectiveness on typical SSL methods without non-trivial open-set-specific efforts.,1,related,1,positive
"(2) PF+FixMatch-Sigmoid: By injecting PF to FixMatch, we firstly replaced its N -class fully connected layer with with our V N prototypes.",1,related,1,positive
", consistency regularization), data augmentation, and the assignment of pseudo-labels to unlabeled data points, in order to utilize unlabeled data (e.g., Chen et al., 2020; Sohn et al., 2020; Zhang et al., 2021; Wang et al., 2023).",1,related,1,positive
"Taking the classical method, FixMatch [28], as an example, we can observe that adding extra outliers does hurt the classification accuracy compared to the standard SSL setting with no outlier, because it is impossible to obtain correct seen-class pseudo-labels for these outliers.",1,related,1,positive
"For standard SSL methods, we focus on the latest state-of-the-arts, including MixMatch [3], ReMixMatch [2], FixMatch [28], CoMatch [20], FlexMatch [41], SimMatch [43] and FreeMatch [34].",1,related,1,positive
"For Semi-Supervised baselines, we choose MeanTeacher [73], MixMatch [11], ReMixMatch [10], FixMatch [68], and DASO [53] due to their previous powerful performances.",1,related,1,positive
"To ensure consistency with previous work, we apply data augmentation techniques to the samples in the semi-supervised learning models, including MeanTeacher [73], MixMatch [11], ReMixMatch [10], FixMatch [68], and DASO [53].",1,related,1,positive
The 1st row reports the result of a bare baseline model - DeepLabV3+ with plain consistency regularization [22].,1,related,1,positive
"Following the FixMatch [16], we simply adopt a pre-defined threshold,
denoted as τ , to filter out the unlabeled data with less confident pseudo-labels.",1,related,1,positive
"Following the FixMatch [16], we simply adopt a pre-defined threshold,",1,related,1,positive
"Furthermore, FixMatch [33] uses the weakly augmented unlabeled instances to create a pseudo label and enforce consistent prediction against its strong augmented version.",1,related,1,positive
"In this study, we evaluate the effectiveness of our proposed verification model in the LAVe framework by comparing it with the confidence score-based threshold method [11, 12, 13].",1,related,1,positive
"We follow the common practice in semi-supervised learning [47, 50] and update the teacher model based on the student model through the exponential mean average (EMA) strategy.",1,related,1,positive
FixMatch [44] computes an artificial label for each unlabeled sample by computing the model’s predicted class distribution given a weakly-augmented version.,1,related,1,positive
FixMatchDistill uses a trained Segformer-b2 to distill knowledge to ResNet50 model as described in Appendix F. FixMatchEnsemble is an ensemble of two ResNet50 model is uses 20M parameters more than ours.,1,related,1,positive
"Our Diverse Co-training, compared with cotraining baseline, can better segments the small objects that FixMatch and co-training baseline tends to ignore (e.g. the forth and fifth row).",1,related,1,positive
"The semisupervised nature of co-training brings noise into pseudo labels for unlabeled data [67, 59, 90], thus we also provide confidence thresholding following FixMatch to filter out noisy pseudo labels in which the model has low confidence.",1,related,1,positive
"First, we can observe the better results obtained by co-training methods (i.e. (d) and (e)) as shown in the third and last row, where FixMatch is prone to under-segmentation (classifies many foreground pixels as background).",1,related,1,positive
"As shown in the first section of Table 11, our model outperforms both FixMatchDistill and FixMatch-Ensemble consistently by a large margin.",1,related,1,positive
"Instead of labeling the unlabeled data before training, consistency regularization typically enforce invariance to perturbations on the unlabeled data in an online manner [65, 44, 85, 4, 55, 3, 67, 81].",1,related,1,positive
"We demonstrate the superiority of co-training over FixMatch in Figure 2, from which co-training outperforms FixMatch consistently on all partitions and thresholds.",1,related,1,positive
"The FixMatch and co-training baseline tends to ignore some foreground while our Diverse Cotraining does not, such as the visualization of the second row.",1,related,1,positive
"From Table 8, we show knowledge transfer do take effect improving the original FixMatch baseline by 3% 1%, which can be attributed to the diverse inductive bias and the high-quality pseudo label introduced by the transformer model.",1,related,1,positive
Then we compare FixMatch-Distill and FixMatchEnsemble which uses exactly the same or more parameters than ours but a different learning paradigm.,1,related,1,positive
We also display the paradigms used in FixMatch in (a) of Figure 1.,1,related,1,positive
Figure 4: Visualization of C obtained in training process of FixMatch [27] on CIFAR-10 with the same setting as in Figs.,1,related,0,negative
"For example, pseudo-labeling based methods [19, 27, 20, 34, 38] set ηi = 1/p i , i ∈ { i | i = argmax(p) ∧ p i ≥ τ } and ηj = 0, j ∈ {j | j ∈ (1, · · · , k) ∧ j ̸= i} and, i.",1,related,1,positive
Take the current most popular SSL method FixMatch [27] as an example.,1,related,1,positive
"Current prevailing SSL methods [2, 27, 38, 7, 11] utilize the model trained on the labeled data to impute pseudo-labels for the unlabeled data, thereby boosting the model performance.",1,related,1,positive
"Following [27], our models are trained for 2(20) iterations, respectively using the backbone of WideResNet-28-2 (WRN) [37] for CIFAR-10, WRN-28-8 for CIFAR-100 and ResNet-18 [12] for mini-Imagenet.",1,related,1,positive
"In this section, PRG is mainly implemented as a plugin to FixMatch [27] and SimMatch [41].",1,related,1,positive
Figure 2: Results of FixMatch [27] in MNAR and the conventional SSL setting (i.,1,related,1,positive
Figure 5c illustrates the fraction of wrong pseudo-labels surpassing the threshold when training Fixmatch on MNIST (orange) and BCS-MNIST (blue).,1,related,1,positive
Figure 4c visualizes the entropy over the number of pseudo-labeled instances per class that Fixmatch would choose for training for BCI-MNIST (blue) and MNIST (orange).,1,related,1,positive
"Figures 5d to 5f denote the learning curves of Flexmatch, Fixmatch, and PL when increasing the labeled pool actively.",1,related,1,positive
We further include Fixmatch [58] as it is a well-established consistency regularization technique and Flexmatch [74] as a strong method tackling confirmation bias [67].,1,related,1,positive
"According to the hypothesis that the output distribution of the sample remains unchanged after the consistency regularization [12] adds a certain disturbance or enhancement to the input sample, we apply data weakaugment ( ) w weak x Augment x  = and strong-augment ( ) s strong x Augment x  = on the samples respectively, realize regularization training to make the model output stably even when the input is disturbed, greatly enrich the pixel content of the image without changing the sample label, deeply explore more sample information learning, improve the accuracy of decision boundary and the generalization ability of the model.",1,related,1,positive
"Following the suggestion from [45], we also report the median error rates of the last 20 checkpoints in Table 9.",1,related,0,negative
"These configurations follow the original papers [45, 49, 54].",1,related,0,negative
This list of transformations is similar to the original list used in FixMatch [45] and FlexMatch [54].,1,related,1,positive
"We mainly compare our proposed method with recent state-of-the-art methods such as UDA [49], FixMatch [45], FlexMatch [54], CoMatch [27], SimMatch [56], and AdaMatch [7].",1,related,1,positive
"For 10% experiments, we follow the settings in [27, 45, 56].",1,related,1,positive
FixMatch generate the pseudo labels and only keep the pseudo labels with high confidence [11].,1,related,1,positive
"When the MoCo pre-trained model is adopted, our SimMatchV2 achieves a 53.13% Top1 accuracy, which is +10.08% better than the from-scratch performance, and +11.85% better than FixMatch + CCSSL.",1,related,0,negative
"Basically, if only node-node consistency is adopted, the setting is equivalent to the FixMatch [43] with distribution alignment [2], which is our baseline in the first row of the table; Next, we can see that the node-edge, edge-node, and edge-node consistencies give +7.1%, +0.7%, +1.5% improvements, respectively.",1,related,1,positive
"Basically, if only node-node consistency is adopted, the setting is equivalent to the FixMatch [43] with distribution alignment [2], which is our baseline in the first row of the table; Next, we can see that the node-edge, edge-node, and edge-node consistencies give +7.",1,related,1,positive
"Our SimMatchV2 achieves 43.05% Top-1 accuracy when training from scratch, which is +11.84% better than the state-of-the-art method (FixMatch + CCSSL).",1,related,0,negative
We primarily provide some notations and review a common practice in semi-supervised learning (SSL) (Sec.,1,related,1,positive
"In this work, we propose a new semi-supervised surgical instrument segmentation framework, termed as SegMatch, building upon the state-of-the-art semi-supervised image classification pipeline, FixMatch [43].",1,related,1,positive
"Our proposed SegMatch algorithm adapts the stateof-the-art semi-supervised image classification framework, FixMatch [43], to semi-supervised semantic segmentation.",1,related,1,positive
"The teacher models
B.3.1 Hyperparameter setting
We train the model using FixMatch.",1,related,1,positive
by adding feature masking to FixMatch [14].,1,related,1,positive
We trained semi-supervised baselines using FixMatch [14].,1,related,1,positive
"2) Unsupervised Branch: As proposed in [34], consistency is kept by the combination of teacher-student model and strong-weak data augmentation on unlabeled data, and confidence is forced by introducing a hard-threshold on model predictions.",1,related,1,positive
"Table 4: Accuracy of cross-domain SSL in comparison with the peer method Fixmatch [Sohn et al., 2020].",1,related,1,positive
"In addition, we use CIFAR10, STL10 [Coates et al., 2011], MNIST [Lecun and Bottou, 1998] and SVHN [Netzer et al., 2011] to construct cross-domain SSL settings, which has few source labeled data and much unlabeled target data and show DRSSL’s advantages in cross-domain SSL over Fixmatch [Sohn et al., 2020].",1,related,1,positive
"Practically, we plug DRL into Fixmatch.",1,related,1,positive
"Under the single domain setting, FixMatch trains the model using source labeled data and source unlabeled data.",1,related,1,positive
Incorporating Fixmatch with DRL improves the original Fixmatch by a relative 17% increase in accuracy under the cross-domain setting.,1,related,1,positive
"2 is the ‘consistency loss’ in FixMatch: Lu = 1 M ∑M m=1 I(max(P̂ (ywt |xwt )) > η)H(ŷwt , P̂ (yst |xst )), where xwt and y w t represent the weakly-augmented target data, x s t and yst represent the strongly-augmented version of the same image data, and η is a threshold for generating pseudo-labels ŷwt .",1,related,1,positive
"To test the performance of LION, we compare it with several state-of-the-art methods, including Pseudo-Labeling [Lee et al., 2013], MixMatch [Berthelot et al., 2019], UDA [Xie et al., 2020], Margin-Mix [Florea et al., 2020], ReMixMatch [Berthelot et al.,2020], FixMatch [Sohn et al., 2020], and Ada-CM [Li et al., 2022], on all the three datasets with different ratios of labeled data.",1,related,1,positive
"Second, all the reliable samples will be used to train the model like FixMatch [Sohn et al., 2020].",1,related,1,positive
"In this experiment, we use FixMatch [33] as the baseline for semi supervised learning and follow the same configuration for training.",1,related,1,positive
"For the additional FixMatch and DASO hyperparameters we followed prior works on imbalanced SSL [8, 9].",1,related,1,positive
DASO extends FixMatch and provides state-of-the-art results for balanced and imbalanced SSL [8].,1,related,1,positive
We use FixMatch [7] and DASO [8] for our investigations.,1,related,1,positive
"We use the RandAugment algorithm [22] which offers automated random data augmentation and is used in other related work [7, 8].",1,related,1,positive
"8In Appendix E.2, we further bridge the conceptual notion of DAC regularization in Equation (9) with common DAC regularization algorithms in practice like FixMatch [Sohn et al., 2020].",1,related,1,positive
"To bridge Equation (9) with common DAC regularization algorithms in practice, in Example E.1, we instantiate FixMatch [Sohn et al., 2020] – a state-of-the-art semi-supervised learning algorithm that leverages DAC regularization by encouraging similar predictions of proper weak and strong data…",1,related,1,positive
"2, we further bridge the conceptual notion of DAC regularization in Equation (9) with common DAC regularization algorithms in practice like FixMatch [Sohn et al., 2020].",1,related,1,positive
"Example E.1 (FixMatch [Sohn et al., 2020]).",1,related,1,positive
"Furthermore, we unify the existing analysis [Yang et al., 2023] tailored for data augmentation consistency (DAC) regularization [Sohn et al., 2020] into the cluster-aware SSL framework (Section 6).",1,related,1,positive
"We remark that with DAC regularizations like FixMatch [Sohn et al., 2020] and MixMatch [Berthelot et al., 2019], model predictions are matched against pseudo-labels that are gauged based on data augmentations, usually via the cross-entropy loss.",1,related,1,positive
"We take FixMatch [Sohn et al., 2020]–a state-of-the-art algorithm in the lowlabel-rate regime–as the semi-supervised learning baseline.",1,related,1,positive
"We remark that with DAC regularizations like FixMatch [Sohn et al., 2020] and MixMatch [Berthelot et al.",1,related,1,positive
"1, we instantiate FixMatch [Sohn et al., 2020] – a state-of-the-art semi-supervised learning algorithm that leverages DAC regularization by encouraging similar predictions of proper weak and strong data augmentations xw,xs ∈ A(x) of the same sample x.",1,related,1,positive
"In this section, the proposed method is applied to another prevalent semisupervised self-training framework, FixMatch [33], to validate its scalability.",1,related,1,positive
"Furthermore, the compatibility with FixMatch [33] confirms that our method is a general plug-and-play approach.",1,related,1,positive
"Our method builds upon the training frameworks of Mean Teacher [31] and FixMatch [33], illustrating their continued effectiveness.",1,related,0,negative
"Confidence will help efficiently improve the quality of the model, and help annotate the vast majority of unlabeled data in a scalable manner (Wang et al., 2022; Sohn et al., 2020; Xu et al., 2021).",1,related,1,positive
"FlexMatch seed0 seed1 seed2 seed3 seed4
Learning rate 0.00036 0.00016 0.00016 0.00068 0.00006 Weight decay 0.00259 0.00001 0.00371 0.00023 0.002103 Unlabeled loss coefficient 2.22 0.82 5.00 1.94 6.09
FixMatch seed0 seed1 seed2 seed3 seed4
Learning rate 0.00074 0.00034 0.00392 0.00102 0.00037 Weight decay 0.00045 0.00315 0.00001 0.00005 0.00058 Unlabeled loss coefficient 3.08 6.70 1.85 1.46 0.47
CoMatch seed0 seed1 seed2 seed3 seed4
Learning rate 0.00124 0.00145 0.00061 0.00026 0.00113 Weight decay 0.00042 0.00009 0.00005 0.00009 0.00017 Unlabeled loss coefficient 0.30 1.71 1.26 2.74 0.46 Contrastive loss coefficient 1.26 2.21 3.71 0.56 1.37
MixMatch seed0 seed1 seed2 seed3 seed4
Learning rate 0.00028 0.00003 0.00018 0.00009 0.00005 Weight decay 0.000005 0.00195 0.00005 0.00085 0.00082 Beta shape α 0.2 0.9 0.9 0.8 0.7 Unlabeled loss coefficient 9.13 37.96 8.06 25.16 11.17
Mean Teacher seed0 seed1 seed2 seed3 seed4
Learning rate 0.00062 0.00022 0.00005 0.00128 0.00125 Weight decay 0.00189 0.00001 0.00008 0.00001 0.00001 Unlabeled loss coefficient 67.67 0.87 1.25 7.60 13.56
Pseudo-label seed0 seed1 seed2 seed3 seed4
Learning rate 0.00007 0.00021 0.00005 0.00063 0.00060 Weight decay 0.00033 0.00093 0.00383 0.00005 0.00087 Unlabeled loss coefficient 0.19 0.16 8.73 0.82 0.25
SwAV seed0 seed1 seed2 seed3 seed4
Learning rate 0.00065 0.00325 0.00012 0.00086 0.00196 Weight decay 0.0001497 0.0000056 0.0000006 0.0000021 0.0000003 Number of prototypes 845 131 36 201 59
MoCo seed0 seed1 seed2 seed3 seed4
Learning rate 0.00288 0.00023 0.00043 0.00005 0.02629 Weight decay 0.000002 0.0000008 0.0000003 0.0000005 0.0000004 temperature 0.09331 0.07097 0.10987 0.07414 0.07080 Momentum 0.99242 0.99672 0.99267 0.99950 0.99538
SimCLR seed0 seed1 seed2 seed3 seed4
Learning rate 0.00217 0.00131 0.000640 0.00380 0.00136 Weight decay 0.00002 0.00001 0.00001 0.00001 0.00001 temperature 0.11719 0.10426 0.08652 0.07784 0.11478
SimSiam seed0 seed1 seed2 seed3 seed4
Learning rate 0.0002 0.00056 0.00013 0.00338 0.00098 Weight decay 0.000066 0.000046 0.000023 0.000001 0.000001
BYOL seed0 seed1 seed2 seed3 seed4
Learning rate 0.000245 0.001308 0.000371 0.001653 0.001959 Weight decay 0.0000007 0.0000057 0.0000004 0.000003 0.000001 Momentum 0.9928618 0.996167 0.9988484 0.9940063 0.9934791",1,related,1,positive
"FixMatch generates two augmentation of an unlabeled sample, one with weak augmentation and the other using strong augmentations (e.g., RandAug (Cubuk et al., 2020)).",1,related,1,positive
"Inspired by [19], we adopt two augmentations - strong augmentation and weak augmentation.",1,related,1,positive
"We compare FedLabel with 3 classes of baselines: i) Supervised FL baselines with Fully Labeled Data denoted as 100% (FedAvg (100%) [1], FedProx (100%) [17]), ii) Supervised FL baselines with Partially Labeled Data (FedAvg, FedProx), and iii) SSFL baselines [13, 14] with Partially Labeled Data ((FedAvg+UDA, FedAvg+FixMatch, FedProx+UDA, FedProx+FixMatch, FedTriNet [8], FedMatch [23]).",1,related,1,positive
"We observe that methods that consider localization uncertainty (3DIoUMatch, MonoLiG) perform better than methods that only use classification uncertainty (FixMatch).",1,related,1,positive
"We compare with FixMatch [41], which uses the confidence score to filter out uncertain pseudolabel, and 3DIoUMatch [46], which, in addition to the confidence score, uses an estimated IoU for filtering.",1,related,1,positive
"Similar to previous works [40, 65, 66], we incorporate consistency regularization during training to further boost the performance of the classifier on non-abstained samples at various abstaining rates.",1,related,0,negative
"FedAvg+UDA, FedProx+UDA, FedAvg+Fixmatch, and FedProx+Fixmatch: a naive combination between semi-supervised methods
(UDA [41] and Fixmatch [36]) and FL algorithms.",1,related,1,positive
"FedAvg+UDA, FedProx+UDA, FedAvg+Fixmatch, and FedProx+Fixmatch: a naive combination between semi-supervised methods (UDA [41] and Fixmatch [36]) and FL algorithms.",1,related,1,positive
"Due to the computation resource limitations, we reduce the unlabeled data amounts and the number of layers of the model structure compared to the original setting in [41].",1,related,1,positive
"We follow the settings given in [41] to train each target model, with details shown in Table 1.",1,related,0,negative
"We adopt the ResNet-16 and WRN-16 as the model structure to train the target model, by following the settings as in [41].",1,related,1,positive
"In addition, we compare our method with some pseudo-labeling based SSL methods, namely – FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al., 2021), and FullMatch (Peng et al., 2023).",1,related,1,positive
"On the blood cell classification dataset, we compare our method with MT (Tarvainen and Valpola, 2017), SRC-MT (Liu et al., 2020), FixMatch (Sohn et al., 2020), DARP (Kim et al., 2020), FlexMatch (Zhang et al., 2021) and FullMatch (Peng et al., 2023).",1,related,1,positive
"In addition, we compare our method with some pseudo-labeling based SSL methods, namely – FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al.",1,related,1,positive
"As Table VI shows, our method (CCT) outperforms FixMatch [15] for all choices of τ .",1,related,1,positive
"We compared our method with i) baseline methods: only train the labeled target samples with cross entropy loss (CE), and CE with entropy minimization for unlabeled target samples (ENT) [58], ii) semi-supervised domain adaptation (SSDA) methods: minimax entropy (MME) [6], CDAC [7], iii) semi-supervised learning (SSL) methods: MixMatch [59], FixMatch [15], iv) semi-supervised model adaptation (SSMA) methods: SHOT++ [32], SSHT [12], and v) a universal model adaptation method: UMA [29].",1,related,1,positive
"4c shows, our method (CCT) consistently outperforms MixMatch [59] and FixMatch [15] in all cases.",1,related,1,positive
"We follow the vanilla consistency regularization in SSL [14], [15] and",1,related,1,positive
"To answer the above question, we propose a collaborative consistency training (CCT) framework which extends the vanilla consistency regularization applicable to a single model [14], [15] to our double-model case.",1,related,1,positive
"Furthermore, compared with MixMatch [59] and FixMatch [15], the performance gap increases as the size of |C̄t| increases, which implies that our method (CCT) is more suitable for the scenarios with large target-private categories.",1,related,1,positive
"For FixMatch [3], the best result can reach 94.54% acc. but the worst is only up to 86.68% on a single 40-label split of CIFAR-10 with different random seeds.",1,related,0,negative
"For FixMatch [3], the best result can reach 94.",1,related,1,positive
"Specifically, the error rate of FixMatch is decreased by 7.67% on CIFAR-10 and 7.4% on ImageNet at most after imposing our method.",1,related,0,negative
"For a fair SSL comparison, we use the same experimental setup and common hyperparameters following [3, 27, 7].",1,related,1,positive
"As shown in Table 5, training a
model will cost 113.7 GPU-hours with FixMatch (more time with other SSL methods) on CIFAR-10.",1,related,0,negative
"In an effort to verify the efficacy of our proposed method, we select four competing state-of-the-art semi-supervised image classification algorithms, including two for dermoscopic image classification (GLM [15] and NM [51]), one for generic image classification (Self-train [13]), and a popular and recent consistency-based algorithm (FixMatch [40]).",1,related,1,positive
"In addition, consistency regularization methods [47, 27, 40] enforce the model",1,related,1,positive
"Note that the results of SupOne, FixMatch, FeatMatch, DSDGN and BPL are reported by [8].",1,related,0,negative
"We compare our method with the following state-of-the-art methods, including semi-supervised learning (SSL) methods (like FixMatch [24] and FeatMatch [25]) and recent SSDG works (like DSDGN [7] and BPL [8]).",1,related,1,positive
"This pretrained model was used to retrieve logits of the 4.5 million unlabelled images from ExoNet, which were then thresholded in a FixMatch manner.",1,related,0,negative
"Our best model consisted of an ImageNet-pretrained model of MobileViT XS scaled to match our input image resolution of 224×224; stochastic gradient descent (SGD) with 0.9 momentum and Nesterov acceleration; randomly initialized weights; pseudo-label cutoff τ of 0.98, a batch difference ratio of 4; unsupervised loss weight λ of 1; initial learning rate = 0.05; labelled batch size of 64; and a FixMatch variation of cosine weight decay learning policy.",1,related,1,positive
We experimented with cosine weight decay as in FixMatch [19] and cosine decay with restarts [28].,1,related,1,positive
"Learning Method Algorithms FLOPs(G) Params(M) Inference Time (ms)
Supervised WRN 194.63 1.4673 5.98
WRN+ CP 194.63 1.4673 7.98
Semi-supervised
Pseudo-label 778.51 1.4673 5.98
FixMatch 778.51 1.4673 5.99
FlexMatch 778.51 1.4673 5.98
ours 778.56 1.4695 7.00
To solve the problem that the labeled tongue samples are limited in tongue image database, SSL method is used in the training to allow the unlabeled samples participate in training, which maximize the use of the information of the dataset.",1,related,1,positive
"Learning Method Algorithms Macro Precision Macro Recall Macro F1 Accuracy
Supervised WRN 0.579 0.485 0.528 0.703
WRN+ CP 0.585 0.476 0.525 0.698
Semi-supervised
Pseudo-label 0.564 0.522 0.542 0.712
FixMatch 0.595 0.517 0.553 0.711
FlexMatch 0.572 0.535 0.553 0.723
ours 0.606 0.541 0.572 0.729
Table 5 shows the ablation results on the basis of FlexMatch.",1,related,1,positive
"Inspired by advanced semi-supervised methods such as FixMatch  (Sohn & Berthelot, 2020), we use the cross-entropy loss between the pseudo-label yt
i of target instance xi with
topology-based selection and its output h (
xt i
)
on the classifier to optimize target classification loss, iff. the…",1,related,1,positive
"Inspired by advanced semi-supervised methods such as FixMatch  (Sohn & Berthelot, 2020), we use the cross-entropy loss between the pseudo-label yt
i of target instance xi with
topology-based selection and its output h (
xt i
)
on the classifier to optimize target classification loss, iff. the output confidence exceeds a threshold ( =0.95), formalized as
where h (
xt i
)
yt i
denotes the yt i -th component of the vector-valued function h
(
xt i
)
to obtain the confidence of pseudo-label yt
i for target instance xi.",1,related,1,positive
2020b) follows Fixmatch (Sohn et al. 2020a) to generate pseudo labels at the image level using weakly augmented images and then train the model on strongly augmented ones.,1,related,1,positive
"Motivated by recent works on self-training with consistency (Berthelot et al. 2020; Yang et al. 2020) that utilize augmentation and consistency regularization to enhance the stability of the self-training process, we propose a robust single-view self-training approach.",1,related,1,positive
"Following FixMatch (Yang et al. 2020), we use filp-and-shift as weak augmentation and the four approaches as strong augmentation.",1,related,1,positive
"Thus, we leverage a new effective training approach named FixMatch (Berthelot et al. 2019; Yang et al. 2020), which improves the robustness of self-training via entropy minimization.",1,related,1,positive
An overview of STAC method is shown in Fig.,1,related,1,positive
"Different from the STACwhich adopts hard labels, soft labels are applied to Humble teacher [37] which gets the soft label targets from the predicted distribution of the class probabilities and offsets of all possible classes when the head is performing class-dependent bounding box regression.",1,related,1,positive
"Therefore, we recommend using DC3 proposals [46], which have been shown to perform better on real-world ambiguous data than common network predictions and work with many semi-supervised methods [51,30,53].",1,related,1,positive
we build upon the intuition of consistency regularization via weak and strong data augmentation strategies a() and A() [33] as follows.,1,related,1,positive
"For strong augmentations, A(), we used the RandAugment strategy with the data augmentation procedures used in FixMatch and described in Appendix D of [33].",1,related,1,positive
"16T ) with η being the initial learning rate, t the current training step and T = 20000 the total amount of training steps following [33].",1,related,0,negative
", sample selection, selects only a part of unlabeled instances in terms of (1) model confidence to avoid over-noisy pseudo labels [Sohn et al., 2020; Bhat et al., 2021], (2) prediction uncertainty to obtain informative instances and enhance performance on the hard ones [Mukherjee and Hassan Awadallah, 2020; Jiao et al.",1,related,1,positive
"This paradigm iteratively produces pseudo labels for
2Code available at https://github.com/peterfengyx/KEST.
massive unlabeled data and reduces labeling bottleneck, facilitating varied downstream tasks where massive unlabeled in-domain text exists, including NLU [Vu et al., 2021; Du et al., 2021; Bhat et al., 2021; Chen et al., 2021], Image Classification [Han et al., 2019; Xie et al., 2020; Sohn et al., 2020], Speech Recognition [Park et al., 2020; Kahn et al., 2020], and Neural Machine Translation (NMT) [Zhang and Zong, 2016; He et al., 2020; Jiao et al., 2021].",1,related,1,positive
"…first line, i.e., sample selection, selects only a part of unlabeled instances in terms of (1) model confidence to avoid over-noisy pseudo labels [Sohn et al., 2020; Bhat et al., 2021], (2) prediction uncertainty to obtain informative instances and enhance performance on the hard ones [Mukherjee…",1,related,1,positive
"In our preliminary experiments, we observed that the model performance was robust to the choice of ∆ ∈ [0.01, 0.1] and use ∆ = 0.1 in all our experiments; we set τ = 0.01 and set confidence threshold η = 0.95 following FixMatch.",1,related,1,positive
"We use SSL-VAE and FlowGMM as the baselines for generative semi-supervised methods and Π Model [Rasmus et al., 2015], Pseudo-Labelling [Lee et al., 2013], Mean Teacher [Lee et al., 2013], MixMatch [Berthelot et al., 2019] and FixMatch [Sohn et al., 2020] as baselines for discriminative semi-supervised methods.",1,related,1,positive
"For the labeled samples, we compute the cross-entropy loss using ground truth labels on both samples diffused to time τ and a uniformly sampled time t.
LLCE = E t∼U(0,T ) (x0,y)∼xL x∼pt(x|x0) [− log pφ(y|x, t)] + E (x0,y)∼xL x∼pτ (x|x0) [− log pφ(y|x, τ)] (7)
For the unlabeled examples, we use samples diffused to time τ for obtaining pseudo-labels; following FixMatch, predictions having confidence greater than some confidence threshold η are used in computing classification loss on the unlabeled examples diffused to uniformly sampled time t:
LUCE = E t∼U(0,T ) x0∼xU
x∼pt(x|x0)
[−1 {pφ(yτ |xτ , τ) ≥ η} log pφ(yτ |x, t)] (8)
where, yτ = maxy pφ(y|xτ , τ) and 1 is a binary indicator function that is 1 if the network predicts yτ with a confidence greater than η.
Finally, we introduce a temporal consistency loss (Figure 6) to allow the classifier to also learn from unlabeled examples without confident predictions.",1,related,1,positive
Firstly we show that using model augmentation instead of image augmentation [21] as a SSL framework yields higher performance for both tasks.,1,related,1,positive
"To conduct an empirical comparison between our proposed method and the SOTAmethods on the wearable-based stress detection task, we reproduced the following methods that have been proven in computer vision tasks: two consistency regularization-based SOTA methods such asΠ-model [26] and virtual adversarial training (VAT) [32] and two hybrid semi-supervised learning methods including interpolation consistency training (ICT) [50], MixMatch [5], and FixMatch [41].",1,related,1,positive
"Motivated by the success of self-training in domain adaptation [2, 28] and semi-supervised [7, 40], we use the segmentation model to pseudo-label the images in the rest of the dataset, i.",1,related,1,positive
"We implement FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al., 2021a), ABC (Lee et al., 2021), and DARP (Kim et al., 2020) on long-tailed CIFAR100, the first two are general semi-supervised methods, and the last two are elaborately designed for long-tail learning.",1,related,1,positive
"We implement FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al.",1,related,1,positive
"For unlabeled data, we are interested in threshold-based consistency regularization approaches [19, 48, 7, 49].",1,related,1,positive
"where τ is a threshold value, μ is the ratio of unlabeled-to-labeled data, and p̂m(y|ω(x)) represents the pseudo-label, which can either be a “hard” one-hot label [7] or a sharpened “soft” label [19].",1,related,1,positive
"Following the protocol from [41], we use a Wide-ResNet-28-2 [53] as the backbone and FixMatch [7] as the base semi-supervised learning algorithm.",1,related,1,positive
"Inspired by self-labeling approaches from the SSL literature [27, 48, 59], we follow a teacher-student approach where a teacher version of our models generates pseudo-labeled temporal segments for training the student.",1,related,1,positive
"Combined with FixMatch [1] for semi-supervised learning, FedRPO outperformed FedAvg on precision by 50.",1,related,1,positive
We implement naive extensions of FixMatch [1] with both FedAvg and FedRPO.,1,related,1,positive
"These improvements are observed both in the FL framework, as well as the SSFL framework, when our method is combined with an extended version of FixMatch [1], a novel algorithm for semi-supervised learning.",1,related,1,positive
"Next, we examine another aspect of our regularization approach by replacing two strong augmentations with one weak and one strong augmentation (similar to FixMatch).",1,related,1,positive
"Thus, unlike FixMatch [23], MixMatch [12], and ReMixMatch [13], which use weak augmentations for their supervised modules, we apply hard augmentations on the labelled samples in our supervised module.",1,related,1,positive
"For our main experiments, we follow the standard semi-supervised evaluation protocol from prior works [23], and present the results for four datasets: CIFAR-10 [29], CIFAR-100 [29], SVHN [30], and STL-10 [31].",1,related,1,positive
"In this work, we consider an alternative formulation, inspired by semi-supervised learning (SSL) methods [4,5,28, 34, 35] where a classifier trained on labelled data produces pseudo-labels for unlabeled examples.",1,related,1,positive
"Center: SSL [21, 34] methods produce a pseudo-label that is accepted or rejected by thresholding a confidence score.",1,related,0,negative
"Regardless of pseudo label generation, using only sintra and replacing ps with features and p∗,p− with classifier weights, the above Equation (4) is actually the FixMatch loss [67].",1,related,1,positive
"Along with source-only training with ERM, we experiment with Unsupervised Domain Adaptation (UDA) methods that aim to improve target performance with unlabeled target data (FixMatch (Sohn et al., 2020), DANN (Ganin et al.",1,related,1,positive
"Along with source-only training with ERM, we experiment with Unsupervised Domain Adaptation (UDA) methods that aim to improve target performance with unlabeled target data (FixMatch (Sohn et al., 2020), DANN (Ganin et al., 2016), CDAN (Long et al., 2018), and BN-adapt (Li et al., 2016)).",1,related,1,positive
"Furthermore, we follow the data augmentation guidelines given in STAC [23] and FixMatch [22] for training and pseudo-label generation.",1,related,1,positive
"To learn more compact representations, we also use consistency regularization [1,39,45] which is a powerful solution in semi-supervised learning.",1,related,1,positive
"Inspired by the success of consistency regularization in previous works [1, 39, 45], we integrate it into our framework to learn a more robust model.",1,related,1,positive
"To effectively hallucinate the fine supervision signal with the weak localization guidance, we come up with a self-training framework [36, 37, 47].",1,related,0,negative
"To prevent this, we adopt confidence modeling [23, 45] inspired by self-training approaches [45, 1], to verify the accuracy and reliability of each ray before the distillation process.",1,related,0,negative
"The pseudo-labeling pipeline of our method is inspired by FixMatch [10] that combined consistency regularization with confidence-based filtering, surpassing SOTA semi-supervised techniques at the time.",1,related,1,positive
"Furthermore, we take advantage of this text-only pre-trained classifier by employing it in a pseudo-labeling pipeline (inspired by FixMatch [10]), to further finetune the CLIP vision encoder on an unlabeled image collection.",1,related,1,positive
"Contrary to Fixmatch, in our unsupervised finetuning we set αw as an identity transformation.",1,related,1,positive
"Inspired by Fixmatch [10], for each training image x, we generate two views: the weakly-augmented view αw(x) and the strongly-augmented view αh(x), where α denotes a stochastic augmentation function.",1,related,1,positive
"The idea of consistent label predictions under input augmentations stems from FixMatch [33], which considers a classification setting; we instead promote consistency at the level of intermediate features in a dense fashion, lifting the need for explicit pseudolabels.",1,related,1,positive
methods 92 183 366 732 1464 FixMatch [61] 63.,1,related,1,positive
"Results on SensatUrban test.
methods 92 183 366 732 1464 FixMatch [61] 63.9 73.0 75.5 77.8 79.2
+ ERDA 73.5 74.9 78.0 78.5 80.1
Table 6.",1,related,0,negative
"Specifically, we study an important task of semi-supervised segmentation on image [68, 85, 8] and implement our method to the popular baseline, FixMatch [61], which combines the pseudo-labels with weak-to-strong consistency and is shown to benefit from stronger augmentation [85].",1,related,1,positive
"For FixMatch [61], we use the publicly available implementation here.",1,related,1,positive
"To further increase diversity across heterogeneous views for the contrastive loss [5, 32, 46, 50], we add channel-wise and sequence-wise dropout during training.",1,related,0,negative
"Here, we are interested in how about the accuracy of LEMWEC if using a fixed threshold for pseudo-labels generation (i.e., follow the idea of FixMatch) as well as if changing the parameter r in the percentile scoring.",1,related,1,positive
"Second, LEMWEC achieves significant improvements over its supervised version LEMWEC-, and it outperforms two weakly-supervised algorithms FixMatch and FlexMatch in 0.1%, 0.5% and 1% cases.",1,related,1,positive
"Then, we consider the weakly-supervised learning algorithms denoted by (c) FixMatch [20]: TextCNN is used as the base classifier and the fixed thresholding strategy is used to generate pseudolabels where the confidence threshold is set to 0.",1,related,1,positive
"Restrictions apply.
data, the Macro-F1 of LEMWEC is slightly smaller but is competitive to FixMatch.",1,related,0,negative
"Then, we consider the weakly-supervised learning algorithms denoted by (c) FixMatch [20]: TextCNN is used as the base classifier and the fixed thresholding strategy is used to generate pseudolabels where the confidence threshold is set to 0.95; (d) FlexMatch [7]: TextCNN is also used as the base classifier and the dynamic thresholding strategy is used.",1,related,1,positive
"In this section, we conduct additional experiments on ST approaches, including VAT, DASH, and FIXMATCH to demonstrate that our findings are applicable to other ST approaches as well.",1,related,0,negative
"We implement five state-of-the-art ST approaches, including VAT (Miyato et al., 2018), FixMatch (Sohn et al., 2020), Dash (Xu et al., 2021b), FlexMatch (Zhang et al., 2021), and AdaMatch (Berthelot et al., 2022) (see descriptions of these approaches in Appendix §B).",1,related,1,positive
"DASH (Xu et al., 2021b) extends FIXMATCH by introducing a mechanism with a dynamically adjusted threshold of loss to select a subset of training examples from the unlabelled data for performing SSL.
FLEXMATCH.",1,related,1,positive
FIXMATCH.,1,related,1,positive
"To back up our claim that we offer a framework that can be used for a wide-range of problems, we apply Tied-Augment to a semi-supervised learning algorithm: FixMatch (Sohn et al., 2020).",1,related,1,positive
"In this section, we apply the Tied-Augment framework to FixMatch (Sohn et al., 2020) as a case study to demonstrate the easy adaptability of our framework.",1,related,1,positive
"Other SSL algorithms integrated with BERT are implemented in a unified semi-supervised learning benchmark (USB) (Wang et al., 2022a) for classi-
6https://github.com/google-research/bert 7https://github.com/andersjo/pyrouge
fication, including Mean Teacher (Tarvainen and Valpola, 2017), VAT (Miyato et al., 2019), FixMatch (Sohn et al., 2020), CRMatch (Fan et al., 2022), AdaMatch (Berthelot et al., 2022), and SimMatch (Zheng et al., 2022), all utilizing unlabeled data for invariant predictions.",1,related,1,positive
"…7https://github.com/andersjo/pyrouge
fication, including Mean Teacher (Tarvainen and Valpola, 2017), VAT (Miyato et al., 2019), FixMatch (Sohn et al., 2020), CRMatch (Fan et al., 2022), AdaMatch (Berthelot et al., 2022), and SimMatch (Zheng et al., 2022), all utilizing unlabeled data…",1,related,1,positive
"We consider a general scheme (Lee et al., 2013; Gong et al., 2021) that unifies many prior semisupervised algorithms (including FixMatch and FlexMatch):
θn+1 ← argmin θ {Ls (θ) + µuLu (θ; θn)} , (3)
where θn denotes the model parameters at the n-th iteration, Ls(θ) is a supervised loss.",1,related,1,positive
"Consistency regularization generally involves generating pseudo-labels and applying suitable data augmentation (Tschannen et al., 2019; Berthelot et al., 2019b; Xie et al., 2020; Sohn et al., 2020; Gong et al., 2021).",1,related,1,positive
"Hyperparameters For a fair comparison, we use the same hyperparameters as FixMatch (Sohn et al., 2020).",1,related,1,positive
"This setting is related to semi-supervised learning but does not assume we know all of the classes in the data [5, 41].",1,related,1,positive
"Specifically, in our implementation, we adopt data augmentation consistency [51], [52] to integrate such invariance knowledge into the meta-regularization term in Eq.",1,related,1,positive
"In practice, we often instantiate A with strong data augmentations as in [52], which can ensure that ddac = d − c holds.",1,related,1,positive
"The DAC stems from recent advances in semi-supervised learning [51], [52], and to the best of our knowledge, we exploit it to meta-regularize the complexity of meta-model function class for the first time, which enforces the model facilitated by meta-model to output similar predictions under input data augmentations.",1,related,1,positive
"Here,Aw denotes a function of weak data augmentation [6], fθ is a parameterized Keyword Spotting (KWS) model, and ` is the cross-entropy function.",1,related,1,positive
"Here, As denotes a function of strong data augmentation [6], and τ is a threshold value.",1,related,1,positive
"By simplifying the above methods, FixMatch (Sohn et al., 2020) simply combined the consistency regularization and pseudo labeling.",1,related,1,positive
"The most commonly used strategy adopted by the SSL method called FixMatch (Sohn et al., 2020) is to select the label with the largest probability as the ground-truth one:",1,related,1,positive
"To exploit the information of unlabeled data, inspired by recent SSL works (Berthelot et al., 2019b; Sohn et al., 2020), an intuitive strategy is to provide the unlabeled examples with pseudo labels based on model outputs.",1,related,1,positive
"(1)), which is similar to FixMatch (Sohn et al., 2020) that selects the most probable label as the groundtruth one; Top-k (Eq.",1,related,1,positive
"The most commonly used strategy adopted by the SSL method called FixMatch (Sohn et al., 2020) is to select the label with the largest probability as the ground-truth one:
ŷk = 1 if k = arg maxc∈[q] fc(x),0 otherwise.",1,related,1,positive
"To adapt FixMatch for a segmentation task, we added Gaussian noise as weak augmentation and “RandomAug” [46] for strong augmentation; 4) “self-loop [47]”, which solves a self-supervised jigsaw problem as pre-training and combines with pseudo-labelling.",1,related,1,positive
"Data Supervised Semi-Supervised Labelled 3D U-net FixMatch CCT CPS SegPL SegPL+VI Volumes [45](2015) [5](2020) [12](2020) [13](2021) (Ours, 2022) (Ours, 2022) 2 56.",1,related,1,positive
"Then the output of the weakly augmented input will be used to generate a pseudo label as the ground truth for training the output of the strongly augmented input, the workflow of FixMatch is illustrated in (d) in Fig.1.",1,related,1,positive
"Data Supervised Semi-Supervised Labelled 2D U-net Self-Loop FixMatch CPS SegPL SegPL+VI Slices [45](2015) [47](2020) [5](2020) [13](2021) (Ours, 2022) (Ours, 2022) 50 54.",1,related,1,positive
"We compared SegPL with state-of-the-art consistency based methods: 1) “cross pseudo supervision” or CPS [13], which is considered the current state-of-the-art for semisupervised segmentation; 2) another recent state-of-the-art model “cross consistency training” [12], denoted as “CCT”, due to hardware restriction, our implementation shares most of the decoders apart from the last convolutional block; 3) a classic model called “FixMatch” (FM) [5].",1,related,1,positive
"Furthermore, our evaluations show that PCP outperforms state-of-the-art semi-supervised approaches [80, 94, 96, 99, 12] with greater simplicity, eliminating the need for an iterative process and extra data augmentation (§4.",1,related,1,positive
"Additionally, we compare the proposed PCP with four state-of-the-art semi-supervised approaches, including FixMatch [80], Dash [96], FlexMatch [99], and AdaMatch [12] (see descriptions of these approaches in Appendix §C), where back-translation [64] is used for data augmentation as previous works [94, 77] and prompt-based FT (hard) is used as the backbone.",1,related,1,positive
"For our proposed DLP, we take FixMatch (Sohn et al.",1,related,1,positive
"In particular, we use FixMatch (Sohn et  al.",1,related,1,positive
"For our proposed DLP, we take FixMatch (Sohn et al. 2020) as the semi-supervised method.",1,related,1,positive
"Since FedAvg and FedProx are not semi-supervised learningmethods, we use the stateof-the-art semi-supervised strategy FixMatch [38] to train labeled and unlabeled data jointly.",1,related,1,positive
"Since FedAvg and FedProx are not semi-supervised learning methods, we use the state-of-the-art semi-supervised strategy FixMatch [38] to train labeled and unlabeled data jointly.",1,related,1,positive
"In terms of unsupervised loss Lui , there are two typical forms: pseudo-labels (Sohn et al., 2020) based on labeled data and consistency regularization (Xie et al., 2020) based on data augmentation.",1,related,1,positive
We use consistency learning [27] and view gvk as a slightly perturbed version of gv.,1,related,1,positive
"Inspired by FixMatch [37], a newly recent SOTA algorithm for semi-supervised image classification, we incorporate pseudo-labeling into consistencybased regularization and then can measure this similarity directly with cross-entropy.",1,related,1,positive
"Since ours bears many resemblances to FixMatch [37], we also compare with it but replace those image data augmentations originally used by FixMatch with our signal data augmentations.",1,related,1,positive
It should be pointed out that our proposed method shares many similarities with FixMatch [37].,1,related,1,positive
"More formally, a classifier F is trained so that the consistent regularizer R(F ) is small while a supervised loss or a loss between pseudo labeler are minimized [40, 35].",1,related,1,positive
"In the case of the balanced error, the validity of this assumption is justified by the fact that the existing work [35] using consistency regularizer on data augmentation obtains classifier F , that achieves high accuracy (i.",1,related,1,positive
"Alongside self-training, we adopt a consistency training [55, 59, 57], in which the student network Fθ is trained on augmented target images, whereas, the mean teacher network Fφ predicts pseudo-labels for actual target images.",1,related,1,positive
"The baselines we consider in our experiments are those prior works similar to FixMatch, such as Π-Model [26], Pseudo Label [28], Mean Teacher [39], MixMatch [6], ReMixMatch [5], VAT [32], UDA [45], FlexMatch [47]and DebiasPL [42].",1,related,1,positive
"Our model also did not show a significant difference compared to the previous result not using the threshold, consistent with the study [25], where the authors proposed to introduce the confidence threshold in the SSL approach, however, the approach did not show performance improvement.",1,related,0,negative
"In principle, we could add additional semi-supervised losses, such as the FixMatch loss [36] to propagate label information from the labeled set to the unlabeled set for better performance, but this would add additional hyperparameters and entangle the effect of our methods.",1,related,1,positive
"We consider a set of different baselines: based on ImageNet initializations we consider IN+TRANSFER (fine-tunes ImageNet representations using only the labeled data), and IN+FIXMATCH [36] (fine-tunes the ImageNet representation using labeled and unlabeled data), and based on source model initializations we fine-tune the highest-ranked source model of each source architecture.",1,related,1,positive
"Inspired by FixMatch [10], the distribution alignment module modifies the consistency of the features passed through the classifier.",1,related,1,positive
“Single Stage2 SSL” applies only FixMatch[22].,1,related,1,positive
"1https://dfu-2021.grand-challenge.org
Our prior work [9] combined the unlabeled data with FixMatch [10] to provide some useful results.",1,related,0,negative
Our prior work [9] combined the unlabeled data with FixMatch [10] to provide some useful results.,1,related,1,positive
"We compared SPFSeg’s segmentation performance with other state-of-the-art semi-supervised frameworks (including Mean Teacher [12] (MT), FixMatch [15] (FM), and Cross Pseudo Supervision [26] (CPS)) in terms of Dice score and Jaccard Index.",1,related,1,positive
"Performance evaluation: We evaluated the segmentation performance of the following five methods: 1) Supervised, which use only a small amount of training data by a standard cross-entropy; 2) Pseudo [9], which gives pseudo labels based on the confidence of each patch; 3) Fix match [10], which uses consistency with augmented data; 4) Uncertainty [20], which selects effective pseudo labels on the basis of the model uncertainty; and 5) the proposed method (Ours).",1,related,1,positive
"…semi-supervised learning (Embedding-SSL) approaches Embedding-FixMatch and Embedding-CoMatch against the following baselines: First, we use the algorithms FixMatch (Sohn et al. 2020) and CoMatch (Li, Xiong, and Hoi 2021) without an embedding model as semi-supervised learning baselines (SSL).",1,related,1,positive
"We benchmark our proposed embedding semi-supervised learning (Embedding-SSL) approaches Embedding-FixMatch and Embedding-CoMatch against the following baselines: First, we use the algorithms FixMatch (Sohn et al. 2020) and CoMatch (Li, Xiong, and Hoi 2021) without an embedding model as semi-supervised learning baselines (SSL).",1,related,1,positive
We denote these two instantiations of the proposed approach as Embedding-FixMatch and Embedding-CoMatch respectively.,1,related,1,positive
FixMatch calculates q̂j = argmax(qj) to obtain a hard pseudo-label.,1,related,1,positive
"For further technical details, we refer to Sohn et al. (2020) for FixMatch and Li, Xiong, and Hoi (2021) for CoMatch.",1,related,1,positive
"In detail, we use FixMatch and CoMatch as both have demonstrated that they can outperform other semi-supervised approaches (Sohn et al. 2020;
Li, Xiong, and Hoi 2021).",1,related,1,positive
"Whereas FixMatch’s loss function consists of a supervised and an unsupervised loss term, CoMatch includes an additional contrastive loss term:
L = 1 l l∑ i=1
H(hbini ,Φex(Φemb(Augw(xi))))︸ ︷︷ ︸ Ls +
λu ∗ 1
u u∑ j=1
1(max(qj) ≥ τ)H(q̂j ,Φex(Φemb(Augs(xj))))︸ ︷︷ ︸ Lu +
λc ∗ 1
u u∑ j=1
H(Ŵ qj , Ŵ z j )︸ ︷︷ ︸
Lc (CoMatch only)
.",1,related,1,positive
"Additionally, we compared with FedSemi methods that can be easily
translated into the label set mismatch scenario including FSSL [22] and FedAvg with FixMatch [18].",1,related,1,positive
(a) FedAvg with 100% labeled data (b) FedAvg [15] (c) FedAvg* [15] (d) FedProx* [8] (e) MOON* [7] (f) FedRS [10] (g) FPSL [2] (h) FedAvg-FixMatch* [18] (i) FSSL* [22].,1,related,1,positive
translated into the label set mismatch scenario including FSSL [22] and FedAvg with FixMatch [18].,1,related,1,positive
"Inspired by [47], [33], we assume that the prediction label owns high confidence if the model assigns a high probability to one of the possible classes.",1,related,1,positive
"…leveraging unlabeled data with DNNs. Self-training methods train a model to fit pseudo-labels (i.e., predictions on unlabeled data made by a previously-learned model) to boost the model’s performance (Yarowsky, 1995; Grandvalet & Bengio, 2004; Lee et al., 2013; Wei et al., 2020; Sohn et al., 2020).",1,related,1,positive
"The closest method to our work is FixMatch (Sohn et al., 2020) which adopted Consistency Regularization (Bachman et al.",1,related,1,positive
"The closest method to our work is FixMatch (Sohn et al., 2020) which adopted Consistency Regularization (Bachman et al., 2014) to semisupervised learning and achieved state-of-the-art results on image classification tasks.",1,related,1,positive
"Our SSL framework is inspired by semisupervised approaches [9], [10], which is composed of supervised loss and consistency loss between the contrastive features of unlabeled samples.",1,related,1,positive
"First of all, as self-training (ST) has demonstrated great success in semisupervised learning [38, 49] and domain adaptation [29] by exploiting unlabeled data, we propose to employ selftraining for TTAOD by learning from the unlabeled testing samples.",1,related,1,positive
"the source-free domain adaptation methods [74], our framework is based on a reliable pre-trained model on the source domain, where the domain gap is not too large.",1,related,1,positive
"Our work is closely related to the recent line of SSL methods based on pseudo-labeling [44, 45, 46], where they produce artificial label for unlabeled data samples and train the model to predict the artificial label when fed unlabeled samples as input, and consistency training [30, 25, 47] wherein they enforce the model predictions to be consistent across a sample and its perturbed version.",1,related,1,positive
"The reason is that UDA, FixMatch and FlexMatch only retain the samples with reliable pseudo labels based on the confidence scores.",1,related,0,negative
"We compare with state-of-the-art semisupervised approaches, including UDA [30], FixMatch [25] and FlexMatch [34].",1,related,1,positive
"Formally, an exemplar of consistency regularization [104] is expressed as:",1,related,1,positive
"We compare our proposed framework with four baselines and some recent SOTA semi-supervised segmentation methods, including Entropy Minimization (Ent-Mini) (Vu et al., 2019), Cross Consistency Training (CCT) (Ouali et al., 2020), FixMatch (Sohn et al., 2020), Regularized Dropout (R-Drop) (Wu et al., 2021), Cross Pseudo Supervision (CPS) (Chen et al., 2021b), Uncertainty Rectified Pyramid Consistency (URPC) (Luo et al., 2022b) and Cross Teaching between CNN
and Transformer (CTCT) (Luo et al., 2022a).",1,related,1,positive
We also utilize counterfactual reasoning and adaptive margins proposed in DebiasPL [28] to remove the bias of pseudo label in FixMatch.,1,related,1,positive
"For the Softmax classifier, we compute the standard cross-entropy lossLCE for labeled data and the FixMatch loss LFM for the high-certainty inliers I𝑢 identified by the EDL detector.",1,related,1,positive
"To illustrate the effectiveness of our method on Open-set SSL, we compare it against several baseline methods, including FixMatch [24], MTC [32] and OpenMatch [20].",1,related,1,positive
"For hyperparameters of FixMatch, we set them the same as in OpenMatch [20].",1,related,1,positive
"During training, with EDL excluding outliers, we adopt an SSL method (i.e., FixMatch following OpenMatch [20]) to our Softmax head to enhance representation quality and classification accuracy.",1,related,1,positive
"We obtain FixMatch [17] by choosing τs to be strong augmentations, τt to be weak augmentations, t to share weights with s, f to filter out predictions with a probability below a given threshold, and p to set the predicted values to 1 if they are above that threshold.",1,related,1,positive
"We choose three of them to combine with split-learning architecture in this work: the least demanding (MeanTeacher [19]), the most demanding (FixMatch [22]), and one between (Virtual Adversarial Training VAT [25]).",1,related,1,positive
"When combined with confidencebased pseudo-labeling (Sohn et al., 2020; Zhang et al., 2021), at each iteration, the process can be summarized as follows:
1.",1,related,1,positive
"For results in Table 1, we follow the default training settings of FixMatch (Sohn et al., 2020), yet reduce the number of iterations to 6 × 216 following CREST (Wei et al., 2021).",1,related,1,positive
"FixMatch (Sohn et al., 2020) and evaluate it on the balanced CIFAR10 (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), and STL-10 (Coates et al., 2011) benchmarks using the default FixMatch training settings.",1,related,1,positive
"To help explain our approach, Inlier Pseudo-Labeling (InPL), we first overview the consistency regularization with confidence-based pseudo-labeling framework (Sohn et al., 2020; Zhang et al., 2021; Xie et al., 2020a) that state-of-the-art SSL methods build upon, as our method simply replaces one step — the pseudo-labeling criterion.",1,related,1,positive
"To evaluate the proposed InPL, we integrate it into the classic FixMatch (Sohn et al., 2020) framework and the recent state-of-the-art imbalanced SSL framework ABC (Lee et al.",1,related,1,positive
"For this, we integrate InPL into the FixMatch framework (Sohn et al., 2020) (denoted as FixMatch-InPL), and compare it with the following FixMatch variants, which all use confidence-based pseudo-labeling: UDA (Xie et al., 2020a), FixMatch-Debiased (Wang et al., 2022), and UPS (Rizve et al., 2021).",1,related,1,positive
"…our approach, Inlier Pseudo-Labeling (InPL), we first overview the consistency regularization with confidence-based pseudo-labeling framework (Sohn et al., 2020; Zhang et al., 2021; Xie et al., 2020a) that state-of-the-art SSL methods build upon, as our method simply replaces one step — the…",1,related,1,positive
"We compare InPL with the confidence-based methods UDA (Xie et al., 2020a) and FixMatch (Sohn et al., 2020).",1,related,1,positive
"When combined with confidencebased pseudo-labeling (Sohn et al., 2020; Zhang et al., 2021), at each iteration, the process can be summarized as follows:",1,related,1,positive
"Due to limited computation, we are unable to use large batch sizes as in FixMatch (Sohn et al., 2020) (1024 for labeled data and 5120 for unlabeled data).",1,related,1,positive
"In this section, we first compare our energy-based pseudo-labeling approach to confidence-based pseudo-labeling approaches that build upon FixMatch (Sohn et al., 2020) and ABC (Lee et al., 2021) framework and compare to state-of-the-art imbalanced SSL approaches.",1,related,1,positive
"We conduct ablation studies to better understand our approach InPL, where we integrate it into the framework of FixMatch (Sohn et al., 2020) and ABC (Lee et al., 2021).",1,related,1,positive
"To evaluate the proposed InPL, we integrate it into the classic FixMatch (Sohn et al., 2020) framework and the recent state-of-the-art imbalanced SSL framework ABC (Lee et al., 2021) by replacing their vanilla confidence-based pseudo-labeling with our energy-based pseudo-labeling.",1,related,1,positive
"FixMatch (Sohn et al., 2020) InPL (ours)
Confidence threshold τ = 0.95 τ = 0.8 τ = 0.7 τ = 0.6 -
Accuracy 73.73 74.12 71.53 73.55 77.03
Table A: Comparison to FixMatch with various confidence thresholds on CIFAR10-LT.",1,related,1,positive
"…SSL methods (Kim et al., 2020; Wei et al., 2021; Lee et al., 2021) are build upon the pseudo-labeling and consistency regularization frameworks (Sohn et al., 2020; Xie et al., 2020a) by augmenting them with additional modules that tackle specific imbalanced issues (e.g., using per-class…",1,related,1,positive
"High-Quality Baselines Following existing literature (Liu et al., 2021; Sohn et al., 2020b; Tang et al., 2021; Xu et al., 2021), we evaluate our approach for semi-supervised detection on VOC and COCO 2017 datasets.",1,related,1,positive
"As is common practice [44, 48], the teacher’s parameters θ̄ are updated from the student’s via θ̄ = EMA(θ) at each training step.",1,related,0,negative
"All hyper-parameters related to Soft Teacher remain the same, including the EMA momentum, which defaults to 0.999 following common practice in the semi-supervised classification literature (Sohn et al., 2020a; Tarvainen & Valpola, 2017).",1,related,1,positive
"Soft Teacher vastly improves upon STAC (Sohn et al., 2020b) and Unbiased Teacher (Liu et al., 2021) by enabling end-toend pseudo-labeling on unlabeled images.",1,related,1,positive
"At test time, we resize all instances to the shorter side of 800 resolution, but otherwise do not perform any test-time augmentation, following standard supervised (Ren et al., 2015) and semi-supervised (Liu et al., 2021; Sohn et al., 2020b; Tang et al., 2021; Xu et al., 2021) protocols.",1,related,1,positive
"As is common practice (Sohn et al., 2020a; Tarvainen & Valpola, 2017), the teacher’s parameters θ̄ are updated from
the student’s via θ̄ = EMA(θ) at each training step.",1,related,0,negative
"Train model with ensembled FixMatch for epoch←0 to max_epoch do:
for j←0 to batches do: compute loss on 𝑋𝑙 : 𝑙𝑠
𝑙𝑜𝑔𝑖𝑡𝑠1, 𝑙𝑜𝑔𝑖𝑡𝑠𝐸𝑀𝐴1 = model(Augment1(𝑋𝑢 )), EMA (Augment1(𝑋𝑢 )) 𝑙𝑜𝑔𝑖𝑡𝑠2, 𝑙𝑜𝑔𝑖𝑡𝑠𝐸𝑀𝐴2= model(Augment2(𝑋𝑢 )), EMA (Augment2(𝑋𝑢 )) ensembeled logit 𝑙𝑒 = (𝑙𝑜𝑔𝑖𝑡𝑠1 + 𝑙𝑜𝑔𝑖𝑡𝑠𝐸𝑀𝐴1 + 𝑙𝑜𝑔𝑖𝑡𝑠2 + 𝑙𝑜𝑔𝑖𝑡𝑠𝐸𝑀𝐴2) / 4
compute loss on (Augment3(𝑋𝑢 ), 𝑙𝑒 ) : 𝑙𝑢 𝜆 ← 𝐶𝑜𝑠𝑖𝑛𝑒_𝑑𝑒𝑐𝑎𝑦 (𝜆𝑠 , 𝜆𝑓 , 𝑒𝑝𝑜𝑐ℎ) 𝑙 = 𝑙𝑠 + 𝜆𝑙𝑢 update model weights
end end
𝑠ℎ𝑎𝑟𝑝𝑒𝑛 (𝑙,𝑇 ) = 𝑙
1 𝑇
𝑖∑𝐿 𝑗=0 𝑙 1 𝑇 𝑗
(3)
After calculating the loss between the generated pseudo label and the predicted value of the strongly-augmented image, sharpening is applied to the prediction to reduce the entropy of the label distribution, as Equation 3.",1,related,1,positive
Equation 1 is a loss term of unlabeled data in FixMatch.,1,related,1,positive
"𝑙𝑠 = 𝐻 (𝑦, 𝑝 (𝑦 |𝑥𝑙 ))
𝑙𝑢 = 𝐻 (𝑞, 𝑝 (𝑦 |𝐴 (𝑥𝑢 ))) (1)
𝑙 = 𝑙𝑠 + 𝜆𝑙𝑢
Sohn et al.’s FixMatch [15] is a semi-supervised learning method using consistency regularization and pseudo-labeling.",1,related,1,positive
"The proposed method is based on FixMatch, and it is described in Algorithm 1.",1,related,1,positive
"For the unlabeled target data, we adopt pseudo-labels for cross-entropy calculation, following a semi-supervised learning method, FixMatch [41].",1,related,1,positive
"For Camelyon17 WILDS, the extended SSL methods are compared to reimplemented UDG methods (DARLING, DiMAE) but also to the Semi-Supervised Learning method FixMatch (Sohn et al., 2020) and SSL method SWaV trained with additional data from the target domain.",1,related,1,positive
Extended SSL methods with BSS even surpass FixMatch and SWaV trained with additional unlabeled data from the target domain.,1,related,1,positive
"The straightforward way is to directly extend the FixMatch [28] from images to point clouds, i.",1,related,1,positive
"To avoid this phenomenon, different from the fixed threshold methods [38], [32], we propose a dynamic threshold strategy.",1,related,1,positive
"Pseudo-labeling methods [10, 21] employ weak data augmentation to annotate pseudo-labels for unlabeled data.",1,related,1,positive
"In our setting, threshold like FixMatch [30] does not play a major role in selecting high-confidence unlabaled samples.",1,related,0,negative
"3) FedAvg+MixMatch [3] and 4) FedAvg+FixMatch [30] apply supervised FL on fully-labeled clients and semi-supervised learning by MixMatch/FixMatch on partially-labeled clients, then local models are aggregated on the server by FedAvg.",1,related,1,positive
FixMatch is a multimixed SSL method proposed by Google Brain [24].,1,related,1,positive
"We compare RoPAWS (and PAWS) with various baselines: Pseudo-label (Lee et al., 2013), FixMatch (Sohn et al., 2020), Self-training (Hinton et al.,
8RoPAWS does not suffer from representation collapse similar to PAWS, as discussed in Appendix B.
2014), and CCSSL (Yang et al., 2022), trained from…",1,related,1,positive
"We compare RoPAWS with PAWS and FixMatch (Sohn et al., 2020), together with state-of-the-art robust Semi-SL (OOD filtering) methods: UASD (Chen et al., 2020d), DS3L (Guo et al., 2020), OpenMatch (Saito et al., 2021), and OpenCos (Park et al., 2021).",1,related,1,positive
"We compare RoPAWS with PAWS and FixMatch (Sohn et al., 2020), together with state-of-the-art robust Semi-SL (OOD filtering) methods: UASD (Chen et al.",1,related,1,positive
"Thus, one can adapt the softmax classifier of previous algorithms, such as FixMatch (Sohn et al., 2020), as a GMM classifier to estimate the uncertainty of samples.",1,related,1,positive
"Semi-supervised Learning We select FixMatch (Sohn et al. 2020), a simple yet effective SSL example, as the base of the proposed SSL algorithm.",1,related,1,positive
"Unlike the multi-class classification setting in (Sohn et al. 2020) that accepts or rejects a whole example, the feedbackenhanced thresholds filter a set of confidently accepted (rejected) atomic dialog actions (i.",1,related,1,positive
"Unlike prior SSL algorithms (Sohn et al. 2020; Zhang et al. 2021a), we do not pre-split the data examples but determine them in each batch adaptively during training.",1,related,0,negative
"Unlike the multi-class classification setting in (Sohn et al. 2020) that accepts or rejects a whole example, the feedbackenhanced thresholds filter a set of confidently accepted (rejected) atomic dialog actions (i.e., confident classes) for each example.",1,related,1,positive
"Table 7 presents the results on the Office-Home dataset when combining our method DUC with the semi-supervised learning method FixMatch Sohn et al. (2020), where the hyper-parameter τ is set to 0.8.",1,related,1,positive
"Here, we consider one representative semi-supervised learning method: FixMatch (Sohn et al., 2020).",1,related,1,positive
"95, which is consistent with the parameters used in the Fixmatch [9].",1,related,1,positive
FixMatch [9] selects pseudo-labels as consistency,1,related,1,positive
"In our study, we have chosen k = 7 and threshold τ equal to 0.95, which is consistent with the parameters used in the Fixmatch [9].",1,related,1,positive
"DPT outperforms competitive baselines [14, 13, 12] substantially with four labels per class, achieving an impressive state-of-the-art error rate of 4.",1,related,0,negative
"53% on CIFAR-10, which even outperforms the performance of SOTA methods [12, 14, 13] on CIFAR-10 with 25 labels per class.",1,related,0,negative
"Note that SSL is applied to data with high shares (typically over 80% [Sohn et al., 2020, Arazo et al., 2020]) of unlabeled data, where initial overfitting is more likely than final overfitting.",1,related,0,negative
"Furthermore, we compare our debiased version of Fixmatch (Sohn et al., 2020), designed to handle informative labels, with its original counterpart (Fixmatch) and its debiased version for MCAR labels (DeFixmatch) (Schmutz et al., 2023) on the CIFAR-10 dataset (Krizhevsky, 2009).",1,related,1,positive
"In the CIFAR-10 dataset and considering Setting S2., we compare the original version of Fixmatch (Sohn et al., 2020) with its debiased versions for MCAR data (Schmutz et al., 2023) and for MNAR data3.",1,related,1,positive
"FixMatch generates various data views through image augmentations followed by Cutout (DeVries and Taylor, 2017).",1,related,1,positive
"5We adapted the FixMatch implementation https://github.com/kekmodel/FixMatch-pytorch
We denote weak augmentation of input x by α(x) and the strong augmentation by A(x).",1,related,1,positive
"It shows that, on natural images CIFAR10 for which FixMatch is developped, while the original FixMatch with random labeled data is still outperformed by SOEL, FixMatch with our proposed querying strategy k-means++ has a comparable performance with SOEL.",1,related,1,positive
"We noticed that, although FixMatch focuses on making use of the unlabeled data, its performance is highly affected by the quality of the labeled data subset.",1,related,1,positive
"On F-MNIST, SOEL outperforms all ablations clearly under all query budgets, while on CIFAR-10, SOEL outperforms all ablations except for FixMatch when query budget is low.",1,related,1,positive
Train the detector with the FixMatch loss Eq.,1,related,1,positive
"The training objective function we used is
LFixMatch(θ) = 1 |Q| ∑ j∈Q ( yjL θ 1(α(xj)) + (1− yj)Lθ0(α(xj)) ) + 1
|U| ∑ i∈U 1(S(α(xi)) < q0.",1,related,1,positive
"To this end, we adapted an existing semi-supervised learning framework FixMatch (Sohn et al., 2020a) to our setup and compared with our method in Fig.",1,related,1,positive
"As follows, we will first describe the experiment results and then state the adaptation of FixMatch to anomaly detection we made.",1,related,1,positive
"To mitigate this problem, many semisupervised techniques [1, 26] are proposed to exploit large amounts of unlabeled data by automatically generating pseudo labels without introducing manual annotation.",1,related,1,positive
"Based on the consistency regularization (Bachman, Alsharif, and Precup 2014; Sohn et al. 2020), which holds that the model should output similar predictions when fed augmented versions of the same image, we regard the memory feature mi as an augmentation of v A i , proposing to achieve in-domain self-matching via the loss function:",1,related,1,positive
"Based on the consistency regularization (Bachman, Alsharif, and Precup 2014; Sohn et al. 2020), which holds that the model should output similar predictions when fed augmented versions of the same image, we regard the memory feature mAi as an augmentation of v A i , proposing to achieve in-domain…",1,related,1,positive
"Our self-supervised approach is based on the success of the semi-supervised learning algorithm FixMatch (Sohn et al., 2020).",1,related,1,positive
"For the exact implementation of RandAugment, we directly use the implementation of Sohn et al. (2020).",1,related,1,positive
"We include the following three algorithms: FixMatch (Sohn et al., 2020), Noisy Student (Xie et al.",1,related,1,positive
"We include the following three algorithms: FixMatch (Sohn et al., 2020), Noisy Student (Xie et al., 2020a), Selective Entropy Optimization via Committee Consistency (SENTRY (Prabhu et al., 2021)).",1,related,1,positive
"We fix the step at which λt reaches its maximum value λ be 40% of the total number of training steps, matching the implementation to (Sohn et al., 2020; Sagawa et al., 2021).",1,related,1,positive
We adapted our implementation from Sagawa et al. (2021) which matches the implementation of Sohn et al. (2020) except for one detail.,1,related,0,negative
"The consistency regularization can be enforced via pseudo-labels [2, 37] or prediction distribution alignment [20, 42, 44, 46].",1,related,1,positive
Deterministic Methods FixMatch [7] 43.,1,related,1,positive
"We evaluated NP-match on these two datasets following the evaluation settings used in previous works [7], [8], [15].",1,related,1,positive
"5, ADDA [32], CDAN [21], and FixMatch [28] along with ""Supervised Learning"" which train only on source label data without domain adaptation and lastly, our proposed method, GaitSADA.",1,related,1,positive
"We repropose FixMatch [28], a semi-supervised learning method, for our unsupervised domain adaptation problem.",1,related,1,positive
"5 Evaluation We evaluate themodel without any domain adaptation (supervised learning) as a baseline comparing with the five representative domain adaptation methods including GAN [16], GRL [10], ADDA [32], CDAN [21], and FixMatch [28] along with our method, GaitSADA, on the four different locations (laboratory, conference room, server room, and office) to explore the spatial domain adaptation using 1-3 days of data for training and the rest of data for testing.",1,related,1,positive
"hybrid methods on benchmarks [21], [25], we include the three SOTA methods in the comparison.",1,related,1,positive
"Considering
that Mean Teacher outperforms other consistency regularization methods [19], and that FixMatch and SimPLE outperform other hybrid methods on benchmarks [21], [25], we include the three SOTA methods in the comparison.",1,related,1,positive
"Second, to test the requirement of labeled sample for efficient training of the proposed SS-TBN, we just use 100 manually annotated image slices for training, and the results show that our method achieves better performance compared to Mean Teacher [19], FixMatch [21], and ResNet-50 with single semi-supervised learning strategies, maintaining high classification accuracies (>92",1,related,1,positive
"Second, to test the requirement of labeled sample for efficient training of the proposed SS-TBN, we just use 100 manually annotated image slices for training, and the results show that our method achieves better performance compared to Mean Teacher [19], FixMatch [21], and ResNet-50 with single semi-supervised learning strategies, maintaining high classification accuracies (>92%) with an extremely small training sample.",1,related,1,positive
"4 QUALITATIVE ANALYSIS In this section, we provide a qualitative comparison on CIFAR-10 with 250 labels of FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al.",1,related,1,positive
"In confidence thresholding (Arazo et al., 2020; Sohn et al., 2020; Xie et al., 2020), we can view the sample weights as being computed from a step function with confidence max(p) as the input and a pre-defined threshold τ as the breakpoint.",1,related,1,positive
"We compare SoftMatch with two strong baselines: FixMatch (Sohn et al., 2020) and FlexMatch (Zhang et al.",1,related,1,positive
"On the one hand, a high confidence threshold as exploited in FixMatch (Sohn et al., 2020) ensures the quality of the pseudo-labels.",1,related,1,positive
"…amount of unlabeled data, has shown great potential in practical applications for significantly reduced requirements on laborious annotations (Fan et al., 2021; Xie et al., 2020; Sohn et al., 2020; Pham et al., 2021; Zhang et al., 2021; Xu et al., 2021b;a; Chen et al., 2021; Oliver et al., 2018).",1,related,0,negative
"We compare SoftMatch with two strong baselines: FixMatch (Sohn et al., 2020) and FlexMatch (Zhang et al., 2021).",1,related,1,positive
"We fine-tune the pre-trained BERT-Base (Devlin et al., 2018) model for all datasets using UDA (Xie et al., 2020), FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al., 2021), and SoftMatch.",1,related,1,positive
"Although we can set the parameters to fixed values as in FixMatch (Sohn et al., 2020) or linearly interpolate them within some pre-defined range as in Ramp-up (Tarvainen & Valpola, 2017), this might again oversimplify the PMF assumption as discussed before.",1,related,1,positive
"In this section, we provide a qualitative comparison on CIFAR-10 with 250 labels of FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al., 2021), and SoftMatch from different aspects, as shown in Fig.",1,related,1,positive
"FixMatch [46] first generated a pseudo label using the model’s predictions on weakly-augmented unlabeled images, and then putted a strongly-augmented version of the same image into the model for self-training, aiming to achieve consistency regularization.",1,related,1,positive
"001); data transformation: (RandAugmentMC weak, Standard) Semi-SL For our Semi-SL models we follow [47] with regard to HP selection as closely as possible.",1,related,1,positive
For Semi-SL methods we fix all HPs with the exception of learning rate and weight decay noted as crucial in [47].,1,related,1,positive
"For a more thorough overview over Semi-SL we refer interested readers to [4, 40] In our experiments we use FixMatch [47] as Semi-SL method which combines both aforementioned principles of consistency and uncertainty reduction in a simple manner.",1,related,1,positive
"Semi-SL Semi-SL training is identical to the one proposed with the FixMatch method [47], except that we do not use exponentially moving average models and restrict the training step from 1e6 to 2e5.",1,related,1,positive
On imbalanced datasets we change the supervised term to the weighted CE-Loss and use distribution alignment on every dataset except for CIFAR-10 (where it does not improve performance [47]).,1,related,1,positive
"And as Semi-SL method, we use FixMatch [47] which combines the principles of consistency and uncertainty reduction in a simple manner.",1,related,1,positive
"On the contrary, the seminal SSL method FixMatch [30] outperforms all evaluated OSSL methods in terms of closed-set accuracy.",1,related,1,positive
"For comparison, we include the widely used closedset SSL method FixMatch [30] and a fully supervised baseline trained using only the labeled subset.",1,related,1,positive
"For samples confidently predicted as ID, we apply a pseudo-labeling loss similar to those of FixMatch [30] and UDA [35].",1,related,1,positive
"The student model at each client is updated by optimizing labeled and unlabeled losses through FixMatch (Sohn et al., 2020) and an additional FedProx (Li et al.",1,related,0,negative
"We demonstrate that FedSwitch achieves state-ofthe-art generalization performance on the CIFAR-10 dataset and improves on existing approaches such as FedMatch (Jeong et al., 2021), FedRGD (Zhang et al., 2021) and FedProx-FixMatch (Sohn et al., 2020).",1,related,1,positive
"The student model at each client is updated by optimizing labeled and unlabeled losses through FixMatch (Sohn et al., 2020) and an additional FedProx (Li et al., 2020) loss term.",1,related,0,negative
"Unlike the FixMatch method (Sohn et al., 2020), where a single model both generates pseudo-labels and trains on them, in the teacher-student framework with EMA, we maintain two separate models, where θt and θs are the learnable parameters of the teacher and the student model respectively.",1,related,1,positive
"Unlike the FixMatch method (Sohn et al., 2020), where a single model both generates pseudo-labels and trains on them, in the teacher-student framework with EMA, we maintain two separate models, where θt and θs are the learnable parameters of the teacher
and the student model respectively.",1,related,1,positive
"FixMatch [40] generates the pseudo-label for a weakly-augmented unlabeled image first, then the model is trained to predict the pseudolabel of a strongly-augmented version of the same image.",1,related,1,positive
"We display the results of FixMatch (Sohn et al., 2020) and MixMatch (Berthelot et al., 2019).",1,related,1,positive
"It is noticeable that DS3L underperforms supervised baseline when all the unlabeled data are drawn
3We note that there are also other methods like FixMatch (Sohn et al., 2020) and MixMatch (Berthelot et al., 2019) focus on augmentation or other tricks.",1,related,1,positive
"It is noticeable that DS3L underperforms supervised baseline when all the unlabeled data are drawn 3We note that there are also other methods like FixMatch (Sohn et al., 2020) and MixMatch (Berthelot et al.",1,related,1,positive
", 2021) mainly changed GD setting through defining some policies over unlabeled data by using superclasses of the CIFAR100 and using the FixMatch method (Sohn et al., 2020).",1,related,1,positive
"DM (Smith et al., 2021) mainly changed GD setting through defining some policies over unlabeled data by using superclasses of the CIFAR100 and using the FixMatch method (Sohn et al., 2020).",1,related,1,positive
"Given an unlabeled set U , a common practice in semisupervised learning literature [110, 104, 77] is to adopt the Pseudo Labeling technique [45], which assumes that the decision boundary usually lies in low-density areas and data samples in a high-density area have the same label.",1,related,1,positive
"Normal Benign in situ carcinoma Invasive carcinoma
Model Accuracy P R P R P R P R
Supervised VGG (9) 56.3 ( ± 10.9) 60.0 60.0 61.1 55.0 60.0 60.0 62.5 45.5
Inception (26) 71.3 ( ± 9.9) 68.4 65.0 71.4 75.0 68.1 75.0 66.7 77.8
ResNet101 (44) 86.3 ( ± 7.5) 89.4 85.0 81.8 90.0 80.9 85.0 94.4 85.0
Semi-supervised Pseudo-Labeling (30) 61.3 ( ± 10.7) 57.9 55.0 61.9 65.0 52.4 55.0 73.7 70.0
Mean Teacher (48) 70.0 ( ± 10.0) 66.7 70.0 68.4 65.0 70.0 70.0 75.0 75.0
MixMatch (49) 87.5 ( ± 7.2) 90.0 90.0 80.9 85.0 85.0 85.0 94.7 90.0
FixMatch (14) 87.5 ( ± 7.2) 94.4 85.0 85.0 85.0 81.8 90.0 90.0 90.0
Semi-His-Net 90.0 (± 6.6) 90.0 90.0 89.4 85.0 85.7 90.0 95.0 95.0
We show the average Accuracy with a 95% confidence interval in parentheses, Precision (P) and recall (R) (",1,related,1,positive
Our model draws on the strategy of generating artificial labels via consistency regularization and pseudo-labeling in FixMatch (14).,1,related,1,positive
"Taking the fully supervised ResNet101 as the baseline, we could find that the performance of the model with the semi-supervised strategy, MixMatch, FixMatch and the proposed Semi-His-Net has been significantly improved, with the accuracy increased by 1.2%, 1.2% and 3.7% respectively.",1,related,1,positive
"Inspired by this novel SSL mechanism, we propose extending FixMatch (14) to WSIs analysis.",1,related,1,positive
"This image shows the procedure of FixMatch, image is taken from [130].",1,related,1,positive
"Specifically, we build on FixMatch[14] framework except replace all CNNs with ViTs.",1,related,1,positive
"Following [14], we sample 10% labeled images from the ImageNet training set and leave the rest as unlabeled data.",1,related,0,negative
"In this section, we will first review FixMatch[14] algorithm and then elaborate proposed MAE branch.",1,related,1,positive
"We demonstrate that pure ViTs can outperform CNN-based[14, 16, 17] and joint[13] SSL frameworks.",1,related,1,positive
"We can divide the SSL in two approaches: consistency-based [4, 7, 33, 34] and contrastive energy-based [8, 10, 16, 19, 25, 41].",1,related,1,positive
", BadNets and DeNeB, pseudo-labeling based SSL algorithms [1, 2, 15, 19, 29, 35] strive to assign correct labels to unlabeled examples, while the poisoned unlabeled examples coming from different classes expect themselves to be misclassified into the target class.",1,related,1,positive
"5.1.2 SSL algorithms: We select some representative SSL algorithms from three types introduced in Section 2.3: consistency regularization is PI-Model [30], MeanTeacher [38], VAT [25], and ICT [41], pseudo-labeling is PseudoLabel [19], pseudo-labeling with
consistency regularization is FixMatch [35].",1,related,1,positive
"On FixMatch, our poisoning performs the best, while on VAT, the ASR is the lowest.",1,related,0,negative
"FixMatch [35] performs weak augmentation and strong augmentation for each unlabeled example, and the predicted label of weak augmentation is used as the pseudo label of strong augmentation.",1,related,1,positive
"Therefore, by applying the PGCL to previous SSL approaches, we aim to demonstrate that it can effectively improve the segmentation performance.",1,related,1,positive
"Future works could push the data scarcity in OD even further to consider very few labeled examples for each class, and better understand how to match the performance of SSL methods for image classification in this setting [29].",1,related,1,positive
"We use strong augmentations based on RandAugment [20], but adjust some parameters and delete color augmentation to fit the ultrasound data.",1,related,0,negative
"Inspired by previous works in self-training (Xie et al., 2020; Sohn et al., 2020; Xu et al., 2021), we use strong data augmentation during Phase-II to counteract the noise in pseudo boxes and further boost the performance of GOOD.",1,related,0,negative
"To avoid collapsing, we balance the ratio of labeled and unlabeled data in one batch to 1:7 similarly to [55].",1,related,1,positive
"To evaluate the effect of different SSL methods in the proposed pipeline, we implement two other SSL methods, Mean Teacher (Tarvainen and Valpola 2017) and FixMatch (Sohn et al. 2020), as a substitute for MixMatch algorithm and compare their performance in the proposed pipeline.",1,related,1,positive
"For weak and strong augmentation in the FixMatch and Mean Teacher algorithms, we used the same augmentation sets but with a higher rate of change for strong ones.",1,related,1,positive
We show that our suggested technique outperforms FixMatch [4] under the custom augmentation,1,related,1,positive
"We compare our results with two semi-supervised learning techniques, MSMatch [23] and FixMatch [4] (with tweaked augmentation) on three datasets, EuroSAT [9] [10], UC Merced Land Use (UCM) dataset [12] and WHU-RS19 [13] [14].",1,related,1,positive
"In this paper, we introduce a semi-supervised learning approach based on [4] and a distribution alignment strategy [22], to address the problem of labeling land use and land cover",1,related,1,positive
We show that our suggested technique outperforms FixMatch [4] under the custom augmentation by 2.,1,related,1,positive
"Proposed Method: (a) Model trained using baseline SSL algorithm, Fixmatch [4] (b) Pseudo labeling is done on unlabeled data and classrebalancing is applied with multiple generations",1,related,1,positive
"Our first guidance method is inspired by FixMatch [51], which leverages both pseudolabeling [29] and consistency regularization methods [2, 45].",1,related,1,positive
"We apply the stopgradient operation to the strong features, preventing the strong prediction from learning the weak prediction [6, 51].",1,related,1,positive
We adopt horizontal flip and random crop as weak augmentation and augmentation used in FixMatch [29] as strong augmentation.,1,related,1,positive
"We use MLP as the trunk network and use the same
strategy from FixMatch [8] to train the network.",1,related,1,positive
We use MLP as the trunk network and use the same strategy from FixMatch [8] to train the network.,1,related,1,positive
FixMatch simplifies some existing semisupervision and enhances the prediction results [34].,1,related,1,positive
"We compare our method with some advanced SSL methods, including π - Model [47], Pseudo-Label [35], Mean Teacher [51], VAT [40], MixMatch [4], ReMixMatch [3], UDA [52], MutexMatch [14], CCSSL [55] and FixMatch [49].",1,related,1,positive
"Following [49], we use the weak augmentation α(·) to transform all the training data and additionally use a strong augmentation A(·) to produce an extra view of the unlabeled data.",1,related,1,positive
"Besides Lc, we follow [49, 59] and consider two additional loss terms to build the overall training",1,related,0,negative
"In Table 3, the results demonstrate that CCL can significantly improve the performance of FixMatch [49] and FlexMatch [59] under the label-scarce setting.",1,related,1,positive
"Semi-Supervised Learning Methods We perform our analysis on FixMatch (Sohn et al., 2020) which achieves a classification accuracy of 94.93% on CIFAR-10 with only 250 labeled samples.",1,related,1,positive
"Like Sohn et al. (2020), we used a cosine learning rate decay and quoting from them, we set the “learning rate to ηcos ( 7πk 16K ) , where η is the initial learning rate, k is the current training step, and K is the total number of training steps.”",1,related,1,positive
"We largely follow the experimental details from (Sohn et al., 2020), using a WideResNet-28-2 (Zagoruyko and Komodakis, 2016) architecture, RandAugment (Cubuk et al., 2020) for strong augmentation, and horizontal flipping and cropping for weak augmentation.",1,related,1,positive
"Semi-Supervised Learning Methods We perform our analysis on FixMatch (Sohn et al., 2020) which achieves a classification accuracy of 94.",1,related,1,positive
"For the FixMatch implementation, we closely followed the training set up from Sohn et al. (2020).",1,related,0,negative
"We largely follow the experimental details from (Sohn et al., 2020), using a WideResNet-28-2 (Zagoruyko and Komodakis, 2016) architecture, RandAugment (Cubuk et al.",1,related,1,positive
"We compare our method with FairGen, and use the same attribute classifier for both methods, which is trained by a semi-supervised learning technique called FixMatch [41].",1,related,1,positive
"Regarding the specific techniques used, we reproduce the main ingredients of existing methods [12, 16], by following FixMatch [32] and applying MixUp [39] augmentation.",1,related,1,positive
2: Perform SSL training [32] using (soft)-labeled D1 as labeled data and D2 as unlabeled data (see Sec.,1,related,1,positive
"Their proposed algorithm is a federated version of the FixMatch technique [4] and, therefore, is dependent on data augmentations to perturb the images.",1,related,1,positive
"While it is common to use a trained binary classifier for pseudo-labeling (Lee et al., 2013; Sohn et al., 2020), we argue that it may be sub-optimal for anomaly detection with distribution shift as the decision boundaries of binary classifiers could be highly biased by the small labeled data.",1,related,1,positive
"The core idea of our framework, Semi-supervised Pseudo-labeler Anomaly Detection with Ensembling (SPADE), is based on self-training, following recent advances in semi-supervised learning (Sohn et al., 2020; Chen et al., 2020a).",1,related,1,positive
FixMatch [37] and PseudoSeg [38] encourage the consistency of the predictions under stronglyaugmented and the weakly-augmented.,1,related,1,positive
"In order to ensure that the training process is regularized, pseudolabel prototypes [43], or consistency regularization [26,28], or domain mix-up are implemented [29, 47].",1,related,1,positive
"Here, we evaluate how our approaches to TTA that take advantage of this information perform compared to the baseline VAS
without TTA, as well as state-of-the-art TTA baselines discussed in Section 3.2 (where FixMatch is adapted to also take advantage of observed labels).",1,related,1,positive
"• We propose two new variants of test-time adaptation (TTA) variants of VAS: (a) Online TTA and (b) Stepwise TTA, as well as an improvement of the FixMatch stateof-the-art TTA method [24].",1,related,1,positive
"Even as we adapted them, TTT and FixMatch do not fully take advantage of the rich information obtained at decision time in the VAS context as we proceed through each input task: we not only observe the input image x, but also observe query results over time during the search.",1,related,1,positive
"The TTT approach performs the worst, followed by (out adaptation of) FixMatch, which is only slightly better than TTT. Stepwise TTA (our approach) outperforms both TTT and FixMatch, albeit slightly, and Online TTA is, somewhat surprisingly much better than all others (this is surprising since it has a lower frequency of model update compared to Stepwise TTA).",1,related,1,positive
"Note that we can readily compose both of these TTA approaches with conventional TTA methods, such as TTT and FixMatch.",1,related,1,positive
"In this case, we would expect the conventional TTT and FixMatch methods to be more competitive, as they have been specifically designed to account for distribution shift.",1,related,1,positive
": Given the unlabelled dataset XU = (xi)Ni=1, we aim to sample a subset Xl = (xi) n i=1, 1 ≤ n   N , to form the labelled set XL = (xi, yi) n i=1 for downstream use by the SSL method S along with the unlabelled set XU .",1,related,1,positive
"We experiment with the labelled set selection and supervision policy modules on 8 well-known semi-supervised methods as backbones: PiModel [8], PseudoLabel [12], MeanTeacher [10], VAT [9], MixMatch [4], ReMixMatch [15], UDA [14], and FixMatch [3].",1,related,1,positive
": Next, we utilize the best policy from Table VIII on different SSL methods, and summarize the results in Table IX.",1,related,1,positive
The batch size of the unlabelled data is the same as the original SSL implementations.,1,related,0,negative
": To train an SSL backbone S, the labelled subset X∗L can be used according to a supervision policy P , where P dictates: (a) how many labelled samples XPL = (xi, yi) p i=1 ∈ X∗L are used at epoch e, and (b) the order in which XPL are used in the training process.",1,related,1,positive
"in Simultaneously, we also compute the pseudo-labels of all the unlabeled data using the current model, and perform strong augmentations of those target examples, for which we are confident about their pseudo-labels [12], [13], [13].",1,related,1,positive
"By K-fold cross-filtering, we can find noisy data, discard their labels, and warm up the main network using the SSL method presented in [46].",1,related,1,positive
"(4)
For the unsupervised loss function, we exploit consistency regularization loss, a function used by FixMatch [46], one of the most prevalent modern SSL frameworks.",1,related,1,positive
"Additionally, by utilizing consistency regulation as previously presented in [46], we train the model on all data included in the training dataset X .",1,related,0,negative
"For the unsupervised loss function, we exploit consistency regularization loss, a function used by FixMatch [46], one of the most prevalent modern SSL frameworks.",1,related,1,positive
"When SplitNet is removed, it becomes a consistency regularization [46] in general.",1,related,1,positive
"The current SSL techniques [23, 34, 41] use predicted classes from a discriminative classifier as pseudo-labels, for the unlabeled data, with a threshold to filter out low-confidence predictions.",1,related,1,positive
"We use the exponential moving average of model parameters to report the final performance, as most SSL methods [3, 23, 34] do.",1,related,1,positive
"We remark that we downplay low-confidence pseudo-labels rather than simply ignore them as current methods do [23, 34].",1,related,1,positive
We follow [34] to adopt Wide ResNet-28-2 for CIFAR-10 and Wide ResNet-28-8 [40] for CIFAR-100.,1,related,1,positive
"Following [34], we evaluate our methods in the settings of training with 4, 25, and 400 labels per class, respectively.",1,related,0,negative
"We train our model on 1000 labels, with 100 for each class, following [34].",1,related,0,negative
"We also follow [34] to report results of our models trained on 4, 25, and 100 labels per class, respectively.",1,related,0,negative
"For simplicity, we use Fixmatch [28] for pseudo-label-based consistency learning in our method.",1,related,1,positive
"Fedprox is a generalization and re-parametrization of FedAvg, which adopts a proximal term to tackle the heterogeneity inherent in FL. FedproxFixMatch considers the systems and statistical heterogeneity while solving the problems of labeled clients and unlabeled clients.",1,related,1,positive
"Weak-strong augmentation schema To generate highquality pseudo-labels and improve generalization, we employ the recently weak-strong data augmentation schema [33].",1,related,1,positive
"We compare Okapi against two baselines, ERM and FixMatch [67], both according to our reimplementation and according to the original implementation given in [63].",1,related,1,positive
"To highlight the efficiency of Okapi, we provide estimates in 6 of the carbon footprint associated with the running of it and of the ERM and FixMatch baselines on the iWildCam dataset, using the same hyperparameter configuration used to generate the results in the main text.",1,related,0,negative
"Namely, on three datasets from the WILDS 2.0 benchmark, representing two different tasks (classification and regression) and modalities (image and text), we show that Okapi outperforms both the ERM and FixMatch baselines according to the relevant OOD metrics.",1,related,1,positive
"Our method belongs to the broad family of consistency-based methods, characterised by methods such as FixMatch [67], where the idea is to enforce similarity between a model’s outputs for two views of an unlabelled image.",1,related,1,positive
The authors reported accuracy gains ranging from 1 to 3 percent with respect to FixMatch.,1,related,0,negative
"To this end, we experiment with an extremely simple semi-supervised learning technique (similar to [41]) to aid active learning.",1,related,1,positive
"We apply LP-A3 in FixMatch [36] and compare it with the original FixMatch and InfoMin [40], a learnable augmentation method for semi-supervised learning.",1,related,1,positive
"We detail our intuition for the FixMatch [48] algorithm, but it applies to any semi-supervised algorithms [7], [6], [60], [54] that use pseudo-labeling and consistency regularization (Section II-A).",1,related,1,positive
"As proposed in original works [48], [7], we use the same number of the labeled samples for each of the 10 classes, i.",1,related,1,positive
We briefly describe these two techniques as in [48] followed by the semi-supervised algorithms we consider in this work.,1,related,1,positive
"We compare our method with seven semisupervised learning methods, including PL [22], mean teacher (MT) [12], VAT [41], MixMatch [24], UDA [31], ReMixMatch [42], FixMatch [26], and AdaMatch [43].",1,related,1,positive
"The distribution alignment used the distribution of a model's aggregated class predictions to match the marginal distribution of ground‐truth class labels, while the augmentation anchoring was an upgraded version of the consistency regularization method.20 Sohn et al.21 proposed FixMatch, a combination of ReMixMatch and pseudolabeling, to capture the relationships in labeled data.",1,related,1,positive
"As Berthelot et al.,19 Sohn et al.,21 and Lee59 suggest, τ > 0.50 (i.e., high‐confidence) is helpful to reduce the entropy of a model with the pseudolabeling technique on unlabeled data.",1,related,1,positive
"We compare the representations of semi-supervised (Fixmatch (Sohn et al., 2020)), contrastive (SimCLR (Chen et al.",1,related,1,positive
"We compare the representations of semi-supervised (Fixmatch (Sohn et al., 2020)), contrastive (SimCLR (Chen et al., 2020a), Barlow-twins (Zbontar et al., 2021)) and supervised learning in Fig.",1,related,1,positive
"Given a specified sampling budget, we compare the test accuracy of semi-supervised learning (FixMatch) on sampled data in Table 2.",1,related,1,positive
"We adopt FixMatch for semi-supervised learning with the coefficient of 0.1 on the pseudo-labeled loss, the moving average factor of 0.9, and the batch size of 64 for DomainNet and 128 for Digits.",1,related,1,positive
"Then, we adopt the popular semi-supervised learning method, FixMatch [46], to train the classifier.",1,related,1,positive
"To solve the learning problems including FixMatch and distillation-based compression, we use stochastic gradient descent with the momentum of 0.9 and the weight decay of 5× 10−4.",1,related,1,positive
"To test the effectiveness of our approach, we compared it with several popular self-training methods: UDA (Xie et al., 2020), MixText (Chen et al., 2020a), FixMatch (Sohn et al., 2020).",1,related,1,positive
"FixMatch(Kurakin et al., 2020) is another SSL algorithm that we considered post-hoc.",1,related,1,positive
[15] formulates the consistency loss as Equation (1):,1,related,1,positive
"Equation (3) is simply the combination of Equations (1) and (2) provided in [15], where α refers to the weak data augmentation and A refers to the strong data augmentation.",1,related,1,positive
"During the experiment, we compare FocalMatch with several sate-of-the-art semisupervised learning models on the aforementioned three datasets: Π model [38], Mean Teacher [39], MixMatch [40], ReMixMatch [24], UDA [25], and FixMatch [15].",1,related,1,positive
"This subsection compares the performance of G2NetPL with the state-of-the-art semi-supervised models that exploit the pseudo labels to train the network, such as FixMatch [35] and",1,related,1,positive
"This subsection compares the performance of G2NetPL with the state-of-the-art semi-supervised models that exploit the pseudo labels to train the network, such as FixMatch [35] and
Figure 4: Comparison between our proposed model and semi-supervised models on SSPL.
UPS [33], on two datasets VoC and COCO under different SSPL settings.",1,related,1,positive
"4, G2NetPL outperforms FixMatch and UPS in both datasets.",1,related,1,positive
"Witnessing the impressive results achieved by a series of pseudolabeling based semi-supervised methods [2, 25, 36], we divide the",1,related,1,positive
"FixMatch [46] uses consistency regularization with confidence-based pseudolabelling, SemCo [32] builds on FixMatch but leverages label semantics (via a knowledge graph) to account for preknown similarities among classes, and MeanTeacher [49] uses momentum teachers for SSL.",1,related,1,positive
"Another approach to SSL is using pseudolabels which are generated by either training the model on the labelled samples and pruning the confident predictions on unlabelled data [5, 6, 25, 32, 41, 46], or by us-",1,related,1,positive
"To this end, we perform simple but effective modifications to FixMatch [26] to adapt it for a larger class of dense or structured task, staying as close as possible to the original formulation.",1,related,1,positive
We adapt FixMatch [26] for its use in structured and dense prediction tasks in the semi-supervised setting.,1,related,1,positive
Our SSL algorithm is based on FixMatch (see Fig.,1,related,1,positive
"For fair comparison with existing noisy-label learning methods, we shorten the training schedules typically used by FixMatch implementations (we use 100,000 training iterations with µ = 4 rather than 1,000,000 iterations with µ = 8).",1,related,1,positive
"Following existing implementations of FixMatch (such as by TorchSSL [59]), we use Exponential Moving Average (EMA) models to perform temporal ensembling [24].",1,related,1,positive
"In this paper, we utilize the SSL method FixMatch [45], which uses pseudo-labelling thresholds and consistency between strong and weak augmentations to regularize training through consistency regularization [3, 24, 48] and entropy minimization [15, 26].",1,related,1,positive
"In our implementation of FixMatch, we drop the noisy label of supervised samples and strong augmentations of unsupervised samples 50% of the time.",1,related,1,positive
"In SSL, the class distribution of the labeled data is usually adopted as a prior to calibrating the pseudolabel distribution of the unlabeled data [2, 17].",1,related,1,positive
"Inspired by FixMatch [8], we use two types of augmentations, strong and weak, denoted asA(·) and α (·) respectively, as the perturbed versions for unlabeled instances.",1,related,1,positive
"Inspired by FixMatch [8], we use two types of augmentations, strong and weak, denoted asA(·) and 𝛼 (·) respectively, as the perturbed versions for unlabeled instances.",1,related,1,positive
"For better representation learning of unlabeled target data, we take the self-training technique widely applied in semi-supervised learning [26, 32].",1,related,1,positive
"To further eliminate the local-optimization effect that the overfitting brings, we add the entropy of the UA outputs to avoid the outcome overconfidence [38], [39], which is Lentropy = −∑j∈Ks ∑ i∈N eij ln eij, where enk = exp(znk)/ ∑ i∈N exp(zjk).",1,related,1,positive
"At last, we will further improve the robustness of our framework with some famous semisupervised deep learning methods such as FixMatch [57] and FMixCutMatch [58].",1,related,1,positive
"Inspired by recent success in semi-supervised learning, we make use of unlabeled data and very few labeled data for model training which reduces heavy expert labeling labor [16].",1,related,1,positive
Corruption rout rin CE M DB JoSRC ELR EDM DSOS RRL SNCF PLS INet32 0.2 0.2 63.68 66.,1,related,0,negative
"Following standard evaluation protocols [4, 29], we discard the labels of samples as the unlabeled set but leave a small portion with class-balanced labels as the labeled set to construct SSL scenarios.",1,related,1,positive
"Methods STL-10
1000 labels
Supervised (w. RandAugment [9]) 20.66 ± 0.83 Π-Model [17] 26.23 ± 0.82 (-5.57) Mean Teacher [31] 21.43 ± 2.39 (-0.77) MixMatch [4] 10.18 ± 1.46 (+10.48) UDA [34] 7.66 ± 0.56 (+13.00) FixMatch [29] 7.98 ± 1.50 (+12.68) Ours 6.46 ± 0.62 (+14.20)
advantages of semantic modal knowledge enriching supervision signals and the more efficient utilization of unlabeled samples.",1,related,1,positive
"Our method is built on top of the general SSL framework, especially FixMatch [29], but we introduce semantic modal knowledge in order to alleviate the training dilemma which is frequently caused by lack of supervision.",1,related,1,positive
"Then, we apply a sharpening function to reduce the entropy of the label distribution instead of a argmax function used in [29] since some potential associations between different class embeddings are covered.",1,related,1,positive
"Following others [4, 29], we use the WRN-28-2 backbone (1.",1,related,1,positive
"Note that the results with ‡ are reported by our re-implementation and others are from [4, 29] or reported by the original works.",1,related,0,negative
"Overview: the overall structure of the proposed method is shown in Figure1, our approach is built on top of the popular student-teacher framework for semi-supervised learning [35, 34, 47, 28, 43].",1,related,1,positive
"Following the common practice [34], we also adopt the weak-strong augmentation paradigm by feeding the teacher model weakly-augmented images and the student strongly-augmented images.",1,related,1,positive
"Inspired by Tarvainen & Valpola (2017); Sohn et al. (2020); Chen et al. (2022), we use a student-teacher model and an online memory bank refinement to generate pseudo labels.",1,related,1,positive
"To verify the effectiveness of our approach, we compare the proposed HiCo with supervised ResNet18 backbones (i.e., “ImageNet” pretrained on ImageNet dataset, “Supervised” pretrained on US-4 dataset), and other backbones pretrained on US-4 dataset with semi-supervised methods (i.e., Temporal Ensembling (TE) [59], Π Model [59], FixMatch [60], USCL [3]) and self-supervised methods (i.e., MoCo v2 [13], SimCLR [10]).",1,related,1,positive
"For training the final model in BORT2, the pseudo-labels are generated in the same way as [33].",1,related,1,positive
"(2) Inspired by FixMatch [33], we also put a threshold τ to select the most confident “hard” labels.",1,related,1,positive
54% on two FixMatch [33] variants (or see #3 vs.,1,related,0,negative
"In order to strongly and fairly quantify the contribution of ActorCLR, we first extend the SOTA SSL methods in the image domain [9]–[11] into the video domain as a baseline.",1,related,1,positive
"Though our formulation is based on FixMatch [9] for simplicity, we have also integrated our method with recent strong SSL methods [10], [11].",1,related,1,positive
"Subsequently, we integrate this procedure with the powerful SSL teacher-student models [9]–[11] to optimize the contrastive loss and consistency regularization objectives jointly.",1,related,1,positive
"While we adopt image-based SSL methods [9]–[11] for video, we define two types of data augmentations T : weak and strong augmentations.",1,related,1,positive
"Moreover, in the eth (e ∈ E) round of the local iterations, the unlabeled data set Dm is first expanded by using two different types of data augmentations (DA) in [33], i.",1,related,1,positive
"We compare our framework with state-of-the-art semisupervised methods, including mean teacher (MT inshort) [4], uncertainty-based mean teacher (UA-MT in short) [18], crossconsistency training (CCT in short) [22], FixMatch [23], cross pseudo supervision (CPS in short) [24], dual-task consistency (DTC in short) [26], conservative-radical network (CoraNet in short) [21], cross teaching between CNN and transformer (CTBCT in short) [25].",1,related,1,positive
"We compare our framework with state-of-the-art semisupervised methods, including mean teacher (MT inshort) [4], uncertainty-based mean teacher (UA-MT in short) [18], crossconsistency training (CCT in short) [22], FixMatch [23], cross pseudo supervision (CPS in short) [24], dual-task consistency",1,related,1,positive
"The use of unlabeled data in the surrogate model allows us to use any task-specific semi-supervised learning algorithm such as FixMatch (Sohn et al., 2020), MixMatch (Berthelot et al.",1,related,1,positive
"The use of unlabeled data in the surrogate model allows us to use any task-specific semi-supervised learning algorithm such as FixMatch (Sohn et al., 2020), MixMatch (Berthelot et al., 2019), or Noisy Student (Xie et al., 2020).",1,related,1,positive
"In our active sampling and robustness experiments, we use FixMatch (Sohn et al., 2020) as our semi-supervised algorithm (see Appendix A for more details).",1,related,1,positive
"We describe FixMatch (Sohn et al., 2020), the semi-superivsed learning algorithm used in the experiments in Section 5.",1,related,1,positive
"In our work we use FixMatch (Sohn et al., 2020), a general-purpose semi-supervised learning method, and pseudo-labeling and show they are key to the success of our framework.",1,related,1,positive
"Student Learning To utilize the readily available unlabeled images Du, we use the pseudo-labeling method to generate labels for Du to train the student model, which has been shown effective for semi-supervised image classification [29,27] and object detection [19,18].",1,related,1,positive
"Notably, our model outperforms the latest SOTA method FixMatch [27] by 4.",1,related,1,positive
"Inspired by the recent advances in SSL for image classification [29,27], we use the teacher-student pseudo labeling method as the training paradigm of our framework.",1,related,1,positive
"For effective and efficient distillation, we initialize the student model with the weights of the teacher model and apply strong data augmentations on input images to the student following the common SSL paradigm [27].",1,related,1,positive
"Considering that the 3D voxel resolution of Pix3D is 128(3), which increases the model complexity, we compare it with the supervised method and the two state-of-the-art methods of MeanTeacher [29] and FixMatch [27]",1,related,1,positive
"Secondly, we extend stateof-the-art SSL methods for image classification such as MeanTeacher [29], MixMatch [1] and FixMatch [27], to the task of 3D reconstruction, which serve as strong semi-supervised baselines.",1,related,1,positive
"Further, as BottleGAN should be considered a federated semi-supervised learning (FSSL) algorithm due to its capability to include unlabeled clients, we also compare against the naive combination of FedAvgM and the state-of-theart semi-supervised learning algorithm FixMatch [28].",1,related,1,positive
"Method IOU ECE NLL
FedAvgM 0.470 0.061 0.339 + FixMatch 0.613 0.016 0.193
BottleGAN 0.671 0.011 0.177 + FixMatch 0.671 0.013 0.180
Table 1: The IOU (↑), ECE (↓), and NLL (↓) results on the test set for all evaluated methods.",1,related,1,positive
"More importantly, the learnable parameters of our proposed OpenPrompt are far less than the existing methods, e.g., 23.44M → 0.16 M compared with OpenMatch and 23.8M → 0.16 M compared with FixMatch Sohn et al. (2020), which are no more than 1%.",1,related,1,positive
"Therefore, we set the prediction results from student network as pseudo-labels Y̌ = {y̌i}ni=1 for supervision signals and use a threshold Sohn et al. (2020) η = 0.7 to make sure all used pseudo-labels are reliable.",1,related,1,positive
"Therefore, we set the prediction results from student network as pseudo-labels Y̌ = {y̌i}i=1 for supervision signals and use a threshold Sohn et al. (2020) η = 0.",1,related,1,positive
"Following Saito et al. (2021), we train two models, one using only labeled samples (Labeled Only) and the other one employing the FixMatch Sohn et al. (2020) method.",1,related,1,positive
FixMatch [2] retains the pseudo labels only when the model produces a high-confidence prediction.,1,related,1,positive
"Similar to [2], we set a confidence threshold to filter out the detected bounding boxes that have a confidence level below the threshold.",1,related,1,positive
"Similarly, we can see the benefit of learning invariant representations in the teacher network using FixMatch and the consistency regularization as opposed to simply doing self-distillation (No FixMatch).",1,related,1,positive
"Unlike [51, 49], we minimize the l2 distance between the strong and the weak embedding on few samples from the target distribution",1,related,1,positive
We implement this by consistency regularization and FixMatch [51] as shown in Figure 2a.,1,related,1,positive
"Following the standard SSL evaluation protocols [3], [10], [11], we evaluate our method on four popular benchmark image classification datasets: CIFAR-10, CIFAR-100, SVHN and STL-10.",1,related,1,positive
"Our evaluation is carried out with 4/25/400 labeled samples per class on CIFAR-10, 4/25/100 labeled samples per class on CIFAR-100 and SVHN, 100 labeled samples per class on STL-10 following the setting of [3].",1,related,0,negative
Here we choose the state-ofthe-art method FixMatch [3] as a base model to instantiate our framework.,1,related,1,positive
"To improve the representation ability of SoLar, we further involve consistency regularization [19] and Mixup [25] techniques on reliable examples; see Appendix B.",1,related,1,positive
"Similar to [25], we infer pseudo labels from weak augmentation of data and simultaneously enforce consistency with the corresponding strong augmentation.",1,related,1,positive
"In this way, we set up three groups of experiments with 50, 200, and 500 labeled examples, which is a commonly used settings (Huang et al. 2020; Wei and Zou 2019; Sohn et al. 2020).",1,related,1,positive
"The teacher model is updated from the student model using exponential moving average, and box proposals are generated using FixMatch [11].",1,related,1,positive
"3.3 presents our modification of the multi-view similarity loss [1], used in SSL, to the SSDA setting.",1,related,1,positive
"Table 7 Evaluation of different learning techniques on CIFAR-10
Learning type Method 1% labels 5% labels
Top-1 Top-5 Top-1 Top-5
Supervised SimCLR [16] 48.3 75.5 65.6 87.8 Semi-Supervised FixMatch [39] – – 71.5 89.1 Self-supervised PIRL [15] 30.7 57.5 60.4 83.7 PCL [21] – 75.6 – 86.2 Proposed SWIN-TCSSL 73.5 89.9 88.1 96.9
Note: The values are as reported in the respective works and (−) indicate values not reported in the manuscript",1,related,1,positive
"For the final annotations, we chose to keep the soft-labels and not round them, as utilizing soft-labels for model training was shown to be a more robust approach when dealing with noisy data [70].",1,related,1,positive
"We notice that PercentMatch gains bigger improvement when the labeled ratio is big comparing to FixMatch, partially due to the fact that higher labeled ratio leads to earlier involvement of unlabeled loss by exceeding the uniform Υ0 we use for all experiments.",1,related,1,positive
"As we cannot find any reported MS-COCO benchmark on multi-label semi-supervised classification, we re-implement FixMatch using the same hyperparameters, except percentile thresholds κ± are replaced with fixed confidence thresholds τ+ = 0.95 and τ− = 0 following the original paper.",1,related,1,positive
"It is worth mentioning that UPS is much slower than FixMatch and our method, as it performs a big number of inference on each sample for uncertainty calculation and the training process need to be repeated for multiple generations of teacher-student cycles.",1,related,0,negative
"To address these challenges, we propose PercentMatch, an percentile-based
dynamic thresholding framework that maintains the simplicity of FixMatch and naturally introduces dynamical score thresholds for positive and negative pseudo-labels.",1,related,1,positive
"For CIFAR-10 and CIFAR-100, we perform a strong augmentation (containing Cutout [46] and random horizontal flip) for each training image as done in [47] 3.",1,related,1,positive
"Using a large unlabeled set, the FixMatch SSL method [Sohn et al., 2020] delivers error below 2.5%, while even more recent work has pushed below 2% [Xu et al., 2021, Han et al., 2020].",1,related,1,positive
"We also integrate a semi-supervised method, FixMatch [26], into our framework to explore the useful information in the abandoned noisy samples, which has significantly advanced the model performance for noise-robust learning.",1,related,1,positive
We set the trade-off coefficient λ in loss function as 1 and the threshold c in FixMatch as 0.95 following [26].,1,related,1,positive
"Our framework is flexible that can easily integrate the semi-supervised technology (SSL) to further boost the generalization performance, denoting SFT+ (with FixMatch) or SFT+∗ (with MixMatch).",1,related,1,positive
"To further exploit the useful knowledge in the discarded examples, we adopt the idea of semi-supervised learning and incorporate FixMatch into our SFT.",1,related,1,positive
"Hence, to further explore the knowledge in the discarded noise set, we introduce FixMatch [26] to the main learning stage.",1,related,1,positive
"Since FixMatch is play-and-plug for SFT, we denote Self-Filtering with FixMatch as SFT+ in the following section.",1,related,1,positive
We compare our PLMCL with the existing semi-supervised models such as FixMatch [32] and UPS [30] on COCO dataset.,1,related,1,positive
This is because FixMatch and UPS are trained based on the supervised loss over the set of observed labels while ignoring the unobserved labels of the labeled set as explained in subsection 2.2.,1,related,1,positive
"We also adapt some semi-supervised models to be applicable for multi-label classification problems, by replacing Softmax layer with Sigmoid, that are designed for multi-class classification, such as [32].",1,related,1,positive
"Since our method is based on FixMatch [55], we first briefly review its core idea (§3.",1,related,1,positive
"Moreover, our framework, along with its precedents, such as the FixMatch serials [55, 6, 5, 72, 61] and UDA [64], heavily relies on the pseudo labeling quality on unlabeled images.",1,related,1,positive
"To pursue elegance and effectiveness meantime, we adopt the weak-to-strong consistency regularization framework from FixMatch [55], which can be end-to-end trained in a single stage with a single model.",1,related,1,positive
"Here, we further examine the superiority of our UniMatch compared with its FixMatch baseline [55].",1,related,1,positive
"In this work, we focus on the weak-to-strong consistency regularization framework, that is popularized by FixMatch [55] from the field of semisupervised classification, and then impacts many other relevant tasks [41, 57, 66, 45, 63, 67].",1,related,1,positive
"We tackle the pseudo-label noise problem which may cause severe performance degradation (Sohn et al. 2020) by
confidence thresholding (τ).",1,related,1,positive
"After sample selection is conducted for each class, we adopt semi-supervised learning [1, 42] to train with all clean samples D as labeled data and all noisy samples D as unlabeled data.",1,related,1,positive
"The semisupervised methods include our baseline based on WideResNet28-2 model with limited data, MixMatch [33], MarginMix [34] and Ada-CM [35], all of these methods are for scenarios where only part of the label data is available.",1,related,1,positive
"In this experiment, we mainly compare our recognition performance with MixMatch and MarginMix methods, which selected the WideResNet-28-2 as the backbone.",1,related,1,positive
"In Table III, compared with MixMatch and MarginMix in Table III, our method achieved an accuracy improvement of about 10%.",1,related,0,negative
"In this supplementary material, we additionally provide the benchmark results of FixMatch [5]-based ConMatch (ConMatch with FixMatch) on SVHN and STL-10 datasets.",1,related,1,positive
"Class-wise Results with Other SSL Techniques In Table 2 and Table 3 of the main paper, we demonstrated that semi-supervised learners [5, 6] combined with ConMatch outperform their baselines by a significant margin in most SSL benchmark settings.",1,related,1,positive
"Per-class quantitative evaluation on ConMatch and base semisupervised learners [5, 6] on CIFAR-10 with 40 labels.",1,related,1,positive
ConMatch w/ [5] achieves performance improvement over FixMatch in all the classes except for the dog class.,1,related,1,positive
"SVHN STL-10 Method 40 250 1,000 FixMatch [5] 3.",1,related,0,negative
This verifies that ConMatch w/ [5] effectively alleviate the confirmation bias by making use of confidence estimator.,1,related,1,positive
"ConMatch w/ [5] on the other hand, records high accuracy score for all classes including cat class.",1,related,0,negative
"Specifically, we analyze the time taken to achieve the best accuracy of FixMatch [5], 86.",1,related,1,positive
"TC-BiLSTM (Fan et al., 2019) follows the design of the work for target-oriented sentiment classification (Tang et al., 2016) and concatenate an opinion target embedding for each word position to perform sequence labeling.",1,related,1,positive
We refer the reader to [44] for details.,1,related,0,negative
"MeanTeacher [11] 0.9275 0.9243 0.9260 0.9246 0.9138
MixMatch [16] 0.9317 0.9271 0.9306 0.9271 0.9167
FixMatch [13] 0.9239 0.9181 0.9240 0.9185 0.9069
VAT [12] 0.9214 0.9090 0.9114 0.9096 0.8967
VATNM [14] 0.9315 0.9268 0.9300 0.9177 0.9053
GLM [45] 0.9450 0.9420 0.9490 0.9465 0.9410
TSC [46] 0.9500 0.9499 0.9501 0.9483 0.9427
Ours 0.9528 0.9506 0.9518 0.9507 0.9436
The bold text indicate its performance outperforms other methods
1 3
regularization is integrated into VAT.",1,related,1,positive
"In addition, we compared the performance of our semi-supervised method with other consistent-learning methods (MeanTeacher, MixMatch, and FixMatch) in different confidence (τ ) to investigate whether the high confidence improves performance.",1,related,1,positive
FixMatch [13] proposed the weak and strong data augmentation on unlabeled data sharing the same pseudo-label.,1,related,1,positive
"…2017] to obtain pseudo labels for unlabelled data where a model is expected to make consistent predictions on an unlabeled instance and its augmented versions, cf. [Miyato et al., 2019, Xie et al., 2020a, Verma et al., 2019, Berthelot et al., 2019, Sohn et al., 2020, Zhu et al., 2021, inter alia].",1,related,1,positive
"For fair comparison of different semi-supervised frameworks, we reimplement Original Mean Teacher and FixMatch with the proposed FlowFormer model.",1,related,1,positive
"Specifically, as shown in Table 1a, it takes about 335 GPU days (279 GPU days without ImageNet) to evaluate FixMatch [20] with TorchSSL [21].",1,related,1,positive
"We implement 14 SSL algorithms in the codebase for USB, including Π model [35], Pseudo Labeling [59], Mean Teacher [36], VAT [37], MixMatch [28], ReMixMatch [23], UDA [29], FixMatch [20], Dash [24], CoMatch [60], CRMatch [61], FlexMatch [21], AdaMatch [62], and SimMatch [47], all of which exploit unlabeled data by encouraging invariant predictions to input perturbations [13, 14, 63, 64, 65, 66, 67].",1,related,1,positive
"We highly recommend reporting ImageNet [8] results since it is a reasonable dataset for hill-climbing [20, 47, 21].",1,related,0,negative
"To tackle this task, StyleMatch [14] is proposed via extending the FixMatch [15] with a couple of new ingredients, which resorts to enhancing the diversity from the image level and the classifier level.",1,related,1,positive
"In our method, we merely use the cross-entropy loss to train our model as in FixMatch [15].",1,related,1,positive
"Inspired by the theory of the multi-domain learning, we extend the FixMatch [15] 1 to a multi-task learning method, named MultiMatch, for semi-supervised domain generalization.",1,related,1,positive
"Under the pseudo-labeling based SSL framework [38, 51, 49, 10], given a unlabeled sample and its pseudo label (x, ŷ), only when its confidence o is not smaller than the confidence threshold τ , it will contribute to loss Lu, as seen in (2).",1,related,1,positive
", FixMatch [51]; 2) The model is un/selfsupervised pretrained first and finetuned on the labeled data later [25, 14, 22]; 3) The model is self-supervised pretrained first and then semi-supervised finetuned on both labeled and unlabeled data [10].",1,related,1,positive
"At the final stage of semi-supervised fine-tuning, we adopt the EMA-Teacher framework [58, 10], an improved version over the popular FixMatch [51].",1,related,1,positive
", at the end of every training epoch, but for online pseudo-labeling [51, 10] the teacher model is updated continuously along with the student.",1,related,0,negative
"In this work, we attempt to complement the existing regularization techniques with an idea from the fields of self-supervised learning and consistency regularization [6, 15, 41, 48].",1,related,1,positive
"Under this assumption, we can use Cx to guide the prediction for u by distribution alignment [2,21], which can improve the performance of consistentency-based or pseudo-labeling based methods [2,23,21,10].",1,related,1,positive
"Following [23], we integrate consistency regularization into RDA.",1,related,1,positive
"We compare RDA mainly with three recent state-of-the-art SSL methods: (1) FixMatch [23], combining consistency regularization and entropy minimization; (2) FixMatch with distribution alignment [2]; (3) CoMatch [21], combining graphbased contrastive learning and consistency regularization.",1,related,1,positive
"Labelled 2D U-net Self-Loop FixMatch CPS SegPL SegPL+VI Slices [22](2015) [15](2020) [23](2020) [6](2021) (Ours, 2022) (Ours, 2022) 50 54.",1,related,1,positive
"We compared SegPL with state-of-the-art consistency based methods: 1) “cross pseudo supervision” or CPS [6], which is considered the current state-of-the-art for semi-supervised segmentation; 2) another recent state-of-the-art model “cross consistency training” [18], denoted as “CCT”, due to hardware restriction, our implementation shares most of the decoders apart from the last convolutional block; 3) a classic model called “FixMatch” (FM) [23].",1,related,1,positive
"Labelled 3D U-net FixMatch CCT CPS SegPL SegPL+VI Volumes [22](2015) [23](2020) [18](2020) [6](2021) (Ours, 2022) (Ours, 2022) 2 56.",1,related,1,positive
"It is probably because the AL and SSL methods rely on the i.i.d. assumption, and the SSDG method does not label and exploit the important source data.",1,related,1,positive
"But most of the SSL methods rely on the i.i.d. assumption, which can impair their generalization performance under domain shift.",1,related,1,positive
"CEG vs AL, SSL, SSDG methods.",1,related,1,positive
"Since the DG methods may not be good at tackling the label-limited task as they can only use the labeled data, we further compare our CEGmethod with AL, SSL, and SSDGmethods.",1,related,1,positive
"We further utilize the unknown and known regions by adopting augmentation consistency constraint [50] for the unlabeled data and prediction supervision for the labeled data, respectively.",1,related,1,positive
"Semi-supervised domain generalization (SSDG) [36, 47, 59, 65, 70] tackles domain shift under the SSL setting.",1,related,1,positive
95 as in [50]) selects high dependable data.,1,related,1,positive
"However, the existing AL and SSL methods mostly depend on the i.i.d. assumption, hence may not be favorably extended to the generalization scenarios under distinct domain shifts.",1,related,1,positive
We set the weights of Lce and Lac to 1 as in [50].,1,related,1,positive
"Once we have identified clean samples and relabeled noisy ones, we leverage off-the-shelf and well-established techniques, such as mixup regularization [50] and consistency regularization [36], to perform further SSL-based model training.",1,related,1,positive
"To better understand the performance of the NCLC step in label correction, we replace NCLC with an existing label correction scheme, called Confidence Thresholding (CT) [36], which relabels such samples whose pseudo-labels have a confidence value exceeding a predefined threshold, e.",1,related,1,positive
"We combine FixMatch [19], a recently published method for Semi-Supervised Learning , with the augmentation learning approach of DADA [13].",1,related,1,positive
"Inspired by [20, 26], we assume that a prediction label owns high confidence if the model assigns a high probability to one of the possible classes.",1,related,1,positive
"Similar to the existing approaches, such as FixMatch [35] and DS3L [14] , we consider three widely applied datasets, CIFAR-10, CIFAR100 [23], and compare our CAPT with such existing state-of-the-art SSL approaches.",1,related,1,positive
"Similar to the existing approaches, such as FixMatch [35] and DS(3)L [14] , we consider three widely applied datasets, CIFAR-10, CIFAR100 [23], and compare our CAPT with such existing state-of-the-art SSL approaches.",1,related,1,positive
"Specifically, for the SSL baseline, we choose the Pseudo-Labeling [25] and recently FixMatch [35], DS3L [14]; for the discovery of novel classes, we compare the CAPT with two current SOTA approaches, DTC [16], Auto-Novel [15].",1,related,1,positive
"Specifically, for the SSL baseline, we choose the Pseudo-Labeling [25] and recently FixMatch [35], DS(3)L [14]; for the discovery of novel classes, we compare the CAPT with two current SOTA approaches, DTC [16], Auto-Novel [15].",1,related,1,positive
"Using the notations of FixMatch [1] for consistency, let X = {(xb, pb) : b ∈ (1, .",1,related,1,positive
"Motivated by semi-supervised methods [33], we utilize the confident predictions of each detector on easy views, to enforce consistency of the other detector on hard views.",1,related,1,positive
"Compared to the baseline FixMatch (Sohn et al., 2020), our proposed L2AC results in about 4% performance gain over all evaluation metrics, and outperforms all the SOTA methods.",1,related,0,negative
"Baselines: We evaluate our L2AC based on two recent popular SSL methods, i.e., MixMatch (Berthelot et al., 2019b) and FixMatch (Sohn et al., 2020).",1,related,1,positive
"For unlabeled sample xm, its pseudo-label ŷm can be a ‘hard’ one-hot label (Lee et al., 2013; Sohn et al., 2020; Zhang et al., 2021a) or a sharpened ‘soft’ label (Xie et al.",1,related,1,positive
"The comparison methods includes: Vanilla, classifier retraining (cRT) (Kang et al., 2020), FixMatch (Sohn et al., 2020), DARP (Kim et al., 2020a) and ABC (Lee et al., 2021).",1,related,1,positive
"With pseudo-labeling techniques, current state-of-the-art SSL methods (Xie et al., 2020b; Sohn et al., 2020; Zhang et al., 2021a) generate pseudo-labels for unlabeled data to augment the training dataset.",1,related,1,positive
", 2019b) and FixMatch (Sohn et al., 2020), and compare it with the following methods: 1) The Vanilla model merely trained with labeled data; 2) Recent re-balancing methods that are trained with labeled data by considering class imbalance, including: Re-sampling (Japkowicz, 2000), LDAM-DRW (Cao et al.",1,related,1,positive
"For fair comparison, all the experiments are based on FixMatch (Sohn et al., 2020).",1,related,0,negative
"We evaluate the proposed L2AC upon FixMatch (Sohn et al., 2020) and compare it with the following methods: DARP (Kim et al., 2020a), CReST+ (Wei et al., 2021), ABC (Lee et al., 2021).",1,related,1,positive
"8 visualizes the confusion matrices of pseudo-labels of Fixmatch (Sohn et al., 2020) and our L2AC under the imbalance ratio γl = γu = 100.",1,related,1,positive
"3, our L2AC provides a relatively balanced per-class recall compared with the baseline FixMatch (Sohn et al., 2020) and other imbalanced SSL methods, e.",1,related,1,positive
"9 visualizes the confusion matrices of pseudo-labels for Fixmatch (Sohn et al., 2020) and our L2AC under the imbalance ratio γl = 100 while γu = 100 (Reversed).",1,related,1,positive
"We evaluate the proposed L2AC based on two widely-used SSL methods: MixMatch (Berthelot et al., 2019b) and FixMatch (Sohn et al., 2020), and compare it with the following methods: 1) The Vanilla model merely trained with labeled data; 2) Recent re-balancing methods that are trained with labeled…",1,related,1,positive
"We compare our proposed algorithm with the baseline model FixMatch (Sohn et al., 2020) and two stateof-the-art methods (Kim et al., 2020a) and (Lee et al., 2021), as L2AC uses the same code base with
these two methods.",1,related,1,positive
"2) pseudo-labeling based SSL methods where both labeled and unlabeled data is used (without considering class-imbalance), including: Pseudo-labels (Lee et al., 2013), MixMatch (Berthelot et al., 2019b) and FixMatch (Sohn et al., 2020).",1,related,1,positive
"3, our L2AC provides a relatively balanced per-class recall compared with the baseline FixMatch (Sohn et al., 2020) and other imbalanced SSL methods, e.g., DARP (Kim et al., 2020a) and ABC (Lee et al., 2021).",1,related,1,positive
"13, compared with FixMatch (Sohn et al., 2020), our L2AC certainly improves the separability of the tail classes from the head classes.",1,related,1,positive
"For unlabeled sample xm, its pseudo-label ŷm can be a ‘hard’ one-hot label (Lee et al., 2013; Sohn et al., 2020; Zhang et al., 2021a) or a sharpened ‘soft’ label (Xie et al., 2020a; Wang et al., 2021).",1,related,1,positive
"The comparison methods includes: Vanilla, cRT (Kang et al., 2020), FixMatch (Sohn et al., 2020), DARP (Kim et al., 2020a) and ABC (Lee et al., 2021).",1,related,1,positive
"In particular, Fixmatch surpasses the baseline by 1.90 to 2.95 mAPH.",1,related,1,positive
We compare our STE and CBV modules with the threshold-based method FixMatch [24].,1,related,1,positive
"Visualization of pseudo labels produced by our STE and CBV modules, or by threshold-based FixMatch [24].",1,related,1,positive
13 100% (∼ 80k Labels) Fixmatch [24] +2.,1,related,1,positive
74 50% (∼ 40k Labels) Fixmatch [24] +2.,1,related,1,positive
62 20% (∼ 16k Labels) Fixmatch [24] +2.,1,related,1,positive
"Then, we compare our ProficientTeachers model with a strong competitor, Fixmatch [24].",1,related,1,positive
"Since this filtering strategy derives from Fixmatch [24], we name it Fixmatch for simplicity.",1,related,1,positive
"By contrast, our method obtains much better results than Fixmatch, i.e., improving the baseline by 4.34 to 5.35 mAPH.",1,related,1,positive
"Fixmatch [7] obtains a pseudo label for unlabeled data using a weak augmentation, and then uses the pseudo label to monitor the strongly augmented output values.",1,related,1,positive
"Recall that state-of-theart SSL methods [35,29,39] leverage both weak and strong data augmentations to the unlabeled samples during the training.",1,related,0,negative
"We follow previous work [29,36,39,28,10] and adopt testing accuracy as the evaluation metric for target model performance.",1,related,0,negative
"Concretely, the state-of-the-art SSL methods [29,36,39] leverage weak augmentation to the labeled samples and trains them in a supervised manner.",1,related,1,positive
FixMatch improves performance by up to 0.13% compared to using SWR alone (up to 2.11% increase when NSP is applied).,1,related,0,negative
"To further validate the superiority of NSP, we conduct experiments while keeping SWR but replacing NSP with FixMatch [14] using this Pytorch implementation(2) on settings (a) and (b) in Table 1.",1,related,1,positive
"B.1 Impact of Number of Source Samples in SWR
As described in Section 3.1, the penalty vector w is calculated by employing the average cosine similarity between two gradient vectors g and g′ from N source
2 https://github.com/kekmodel/FixMatch-pytorch/blob/master/train.py
samples.",1,related,1,positive
"We can find that learning only input consistency, such as applying FixMatch, is not sufficient to handle the distribution shift between source and target.",1,related,1,positive
"To further validate the superiority of NSP, we conduct experiments while keeping SWR but replacing NSP with FixMatch [51] using this Pytorch implementation2 on settings (a) and (b) in Table 1.",1,related,1,positive
"We compare one of the SOTA methods, FixMatch [55], under the anomaly detection setting and report the results on CIFAR-10, CIFAR-100, and LSUN datasets in Table 3.",1,related,1,positive
To compute the pseudo labels we follow Sohn et al. [2020] and we first compute the prediction output using weakly augmented versions of the input sample and then we use the output as a pseudo label for the strongly augmented version of the same sample.,1,related,1,positive
FixMatch [34] further explicitly generated the pseudo labels from the data with weak augmentations and used them to guide the prediction from the strongly augmented samples.,1,related,1,positive
"Specifically, first of all, state-of-the-art SSL algorithms [1, 2] guess pseudo labels for those unlabeled examples, and then fit unlabeled examples on these pseudo labels.",1,related,1,positive
"In the SSL algorithm FixMatch [1], all training parameters are the same as the original paper, except that the initial learning rate on CIFAR10 is 0.003, and the initial learning rate on CIFAR100 is 0.01.",1,related,1,positive
"As with these attacks, our experiments also employ SSL algorithm FixMatch [1] with RandAugment.",1,related,1,positive
"In the SSL algorithm FixMatch [1], all training parameters are the same as the original paper, except that the initial learning rate on CIFAR10 is 0.",1,related,1,positive
"On SSL algorithm FixMatch [1], our attack reaches a favorable attack success rate, and can bypass backdoor defenses Neural Cleanse [8] and Activation Clustering[9] for supervised learning, as well as the recently proposed unlabeled backdoor detection DePuD [3] for SSL.",1,related,1,positive
"On SSL algorithm FixMatch [1], our attack reaches a favorable attack success rate, and can bypass backdoor defenses Neural Cleanse [8] and Activation Clustering[9] for supervised learning, as well as the recently proposed unlabeled backdoor detection DePuD [3] for SSL.
2.",1,related,1,positive
"For instance, Fixmatch retains the unlabeled data with pseudo-labels only when the prediction is confident enough [33].",1,related,1,positive
"On the one hand, pseudo-labeling refers to works that assign a hard label to each unlabeled sample with the given model [1, 33].",1,related,1,positive
"Please note that, unlike previous methods where the threshold τ is set by manual tuning [33, 45], we derive the threshold automatically.",1,related,1,positive
"MS-COCO: Similar as (Sohn et al. 2020b; Zhou et al. 2021), we separately randomly select 1%, 5% and 10% images from the COCO training set train2017 as the labeled data while the remaining are used as unlabeled data to evaluate the SSOD performance under different amounts of labeled data.",1,related,0,negative
"Method Remark 1% 5% 10% Supervised 10.0±0.26 20.92±0.15 26.94±0.111
STAC (Sohn et al. 2020b) arxiv 2020 13.97±0.35 24.38±0.12 28.64±0.21 Unbiased Teacher (Liu et al. 2021) ICLR 2021 20.75±0.12 28.27±0.11 31.50±0.10 Instant-Teaching (Zhou et al. 2021) CVPR 2021 16.00±0.20 25.50±0.05 29.45±0.15…",1,related,0,negative
"In the following experiments, we compare the proposed method with a supervised baseline and five other state-ofthe-art SSOD methods, including STAC (Sohn et al. 2020b), Unbiased Teacher (Liu et al. 2021), Instant-Teaching (Zhou et al. 2021), Humble Teacher (Tang et al. 2021) and Soft Teacher (Xu et al. 2021).",1,related,1,positive
"In the following experiments, we compare the proposed method with a supervised baseline and five other state-ofthe-art SSOD methods, including STAC (Sohn et al. 2020b), Unbiased Teacher (Liu et al. 2021), Instant-Teaching (Zhou et al. 2021), Humble Teacher (Tang et al. 2021) and Soft Teacher (Xu et…",1,related,1,positive
"Method Remark 1% 5% 10% Supervised 10.0±0.26 20.92±0.15 26.94±0.111
STAC (Sohn et al. 2020b) arxiv 2020 13.97±0.35 24.38±0.12 28.64±0.21 Unbiased Teacher (Liu et al. 2021) ICLR 2021 20.75±0.12 28.27±0.11 31.50±0.10 Instant-Teaching (Zhou et al. 2021) CVPR 2021 16.00±0.20 25.50±0.05 29.45±0.15 Instant-Teaching∗ (Zhou et al. 2021) CVPR 2021 18.05±0.15 26.75±0.05 30.40±0.05 Humble Teacher (Tang et al. 2021) CVPR 2021 16.96±0.38 27.70±0.15 31.61±0.28
Soft Teacher (Xu et al. 2021) ICCV 2021 20.46±0.39 30.74±0.08 34.04±0.14 Ours 23.55±0.25 32.10±0.15 35.30±0.15
actly follow the same experimental settings on both datasets as these methods and directly report the results released in their provenance in the following tables.",1,related,0,negative
"Following previous works (Sohn et al. 2020b; Zhou et al. 2021), we evaluate the proposed method on two commonly utilized SSOD benchmark s including PASCAL VOC (Everingham et al. 2010) and MS-COCO (Lin et al. 2014).",1,related,1,positive
"In our implementation, the magnitude for each transformation is also randomly selected, which is similar to [33].",1,related,1,positive
"However, with only 10% samples remained, our approach still outperforms FixMatch which uses 100% auxiliary data.",1,related,0,negative
98% over the previous best method FixMatch [33] on DomainNet.,1,related,1,positive
"When comparing our approach with methods in the second group, we find that the stabPA outperforms them in all situations, improving 5-shot accuracy by 5.98% over the previous best method FixMatch [33] on DomainNet.",1,related,1,positive
"For FixMatch [47], we use temporal flip (i.",1,related,1,positive
Stage II: Semi-supervised fine-tuning We implement temporal mask semisupervised learning following the pseudo label paradigm [47].,1,related,1,positive
"We select two top SSL methods (Mean Teacher [51] and FixMatch [47]), and a state-of-the-art TAD model based on a popular proposal generation method BMN [27] (using TSN features) and GTAD [63] (using I3D features).",1,related,1,positive
", setting to one-hot encoding based on the probabilities of different class labels (Sohn et al., 2020).",1,related,1,positive
"FixMatch [41] first generates pseudo labels on weakly augmented unlabeled images, and then the student model is trained to predict the same classifications on strong augmented data.",1,related,1,positive
"Especially, we use the one-hot class distributions for pseudo label generation as in [28, 45].",1,related,1,positive
Note that here we only calculate the augmented consistency loss for those pseudo-labelled pixels for which the uncertainty estimate Sk is less than a threshold μ to avoid error accumulation due to incorrect pseudo labels [14].,1,related,1,positive
"As in [60,48], we incorporate multiple augmentations in our experiments, including gaussian blur, color jitter and random scaling.",1,related,1,positive
"unlabelled data by enforcing model outputs to be invariant to data augmentation [68,60,53].",1,related,1,positive
"To mitigate this issue, inspired of recent studies in semi-supervised learning [25, 26], we perform disambiguation for candidate labels by introducing the consistency regularization between original images and their augmented versions to prevent the model from over-fitting to the noisy labels in the candidate set.",1,related,1,positive
"OpenLDN also outperforms other baseline methods: FixMatch [53], DS(3)L [21], CGDL [54], and SimCLR [12].",1,related,1,positive
"OpenLDN also outperforms other baseline methods: FixMatch [53], DS3L [21], CGDL [54], and SimCLR [12].",1,related,1,positive
"At this point, we are able to apply any standard closed-world SSL method [4,63,53,57].",1,related,1,positive
"We further improve our ST-CoNAL method by adopting the principle of entropy minimization used for SSL [3,4,11,22,35,40].",1,related,1,positive
"Data augmentation can provide additional artificial data [10,12,23,25].",1,related,0,negative
"During training, we followed previous work (Sohn et al., 2020; Zhang et al., 2021; Rizve et al., 2021; Li et al., 2021) to utilize the exponential moving average (EMA) technique.",1,related,1,positive
"We evaluated NP-match on these two datasets following the evaluation settings used in previous works (Sohn et al., 2020; Zhang et al., 2021; Li et al., 2021).",1,related,1,positive
"In this work, to set the criterion, we follow [Sohn et al., 2020] to simply adopt prediction confidence as reliability indicator.",1,related,1,positive
"Since our methods are implemented as a plug-in module to FixMatch, common network hyper-parameters, e.g., learning rates, batch-sizes, are the same as their original settings (Sohn et al., 2020).",1,related,1,positive
"Following previous works (Berthelot et al., 2019b; Sohn et al., 2020; Hu et al., 2021), we used Wide ResNet (WRN)-28-2 for CIFAR-10, WRN-28-8 for CIFAR-100, WRN-37-2 for STL-10 and ResNet-18 for mini-Imagenet.",1,related,1,positive
"…methods first train a model using the labeled data, then uses the model to impute the missing labels with the predicted pseudolabels for the unlabeled data (Van Buuren, 2018), and finally combine the true- and pseudo-labels to further improve the model (Sohn et al., 2020; Berthelot et al., 2019a).",1,related,1,positive
"Prevailing SSL methods first train a model using the labeled data, then uses the model to impute the missing labels with the predicted pseudolabels for the unlabeled data (Van Buuren, 2018), and finally combine the true- and pseudo-labels to further improve the model (Sohn et al., 2020; Berthelot et al., 2019a).",1,related,1,positive
"Performances of Fixmatch are the reported results in their paper (Sohn et al., 2020).",1,related,0,negative
"We design an experiment on CIFAR-10 to further illustrate how existing state-of-the-art SSL methods, e.g., FixMatch (Sohn et al., 2020), fail in the MNAR scenario.",1,related,1,positive
"As for the ID samples in U , although T < 1 was used in previous works (Xie et al. 2019; Sohn et al. 2020) to encourage the high-confidence output, we still set T > 1 in Eq (2) since exploiting the OOD samples in U plays an important role in OOD detection.",1,related,0,negative
"data, consistency regularization (Xie et al. 2019; Sohn et al. 2020) over U = U in ∪ U can be formulated as:",1,related,1,positive
"We compare our method with various related methods, including: Baseline (Hendrycks and Gimpel 2017); OE
(Hendrycks, Mazeika, and Dietterich 2019); MCD (Yu and Aizawa 2019); SSD (Sehwag, Chiang, and Mittal 2021); FixMatch (Sohn et al. 2020); UASD (Chen et al. 2020b).",1,related,1,positive
"Our method performs slightly worse than the FixMatch method, perhaps because we use temperature T > 1 for the samples of U in in Eq (2).",1,related,1,positive
"The SSD method and the FixMatch method are developed for the pure unlabeled ID data, while the OE method is developed for the pure unlabeled OOD data.",1,related,1,positive
"We compare our method with various related methods, including: Baseline (Hendrycks and Gimpel 2017); OE (Hendrycks, Mazeika, and Dietterich 2019); MCD (Yu and Aizawa 2019); SSD (Sehwag, Chiang, and Mittal 2021); FixMatch (Sohn et al. 2020); UASD (Chen et al.",1,related,1,positive
"Following that in Sohn et al. (2020), the augmentation A(·) is implemented with the standard data augmentations (random flip and crop), and the augmentation A′(·) is implemented with RandAugment.",1,related,1,positive
"As for the ID samples in U in, although T < 1 was used in previous works (Xie et al. 2019; Sohn et al. 2020) to encourage the high-confidence output, we still set T > 1 in Eq (2) since exploiting the OOD samples in U plays an important role in OOD detection.",1,related,1,positive
"But our method is developed for detecting OOD samples and performs much better than the FixMatch method in OOD detection, which is shown in Table 1.",1,related,0,negative
"As for the unlabeled
data, consistency regularization (Xie et al. 2019; Sohn et al. 2020) over U = U in ∪ Uout can be formulated as:
LCR = 1 |U | ∑ x∈U CE ( qθ ( A(x), T ) ∥ qθ ( A′(x) )) , (2)
where A(·) and A′(·) are different data augmentations.",1,related,1,positive
"When |Uout| = 0, our method is slightly worse than the FixMatch method, which is developed for the pure unlabeled ID data (|Uout| = 0).",1,related,1,positive
"Similar to Sohn et al. (2020), we normalize these losses with |U | to take the capacity of the selected sets into consideration.",1,related,1,positive
"The first branch being the weak augmentation of the unlabeled sample and the second branch being the strong augmentation of the same unlabeled sample (Sohn et al., 2020).",1,related,1,positive
"Moreover, we incorporate the widely-used semi-supervised method (FixMatch [24], abbreviated as FM) into the baseline FedAvg [16] and existing FL algorithms for the class imbalance problem, including the FedProx [12] (MLSys’20) and the FedAdam (ICLR’21) [18].",1,related,1,positive
"We draw inspirations from recent progress on self-training in transferring accuracy under domain shifts [61, 5, 70, 3, 49, 55].",1,related,0,negative
"Following FixMatch [55], we use the pseudo-labels generated by the teacher model as supervision for consistency training where the model should have consistent predictions under transformations.",1,related,1,positive
"Guided by the theoretical algorithm, we propose a practical self-training algorithm which builds upon Laftr [42], an adversarial learning method for fairness, and FixMatch [55], a self-training framework.",1,related,1,positive
"However, our proposed method outperformed all other SSL methods we compared against including FixMatch for both datasets.",1,related,1,positive
"We compared our proposed co-training with H and E views to a baseline ResNet18 model that uses RGB H&E images as input, as well as other state-of-the-art SSL methods, such as MixMatch and FixMatch, considering they are already widely used in histopathology image analysis [13].",1,related,1,positive
"For comparison with other state-of-the-art SSL methods, we used consistency regularization [17,9], MixMatch [3] and FixMatch [19].",1,related,1,positive
"Concretely, to figure out how SSL works, we select two similar species in the Semi-iNat dataset and analyze the predictions produced by FixMatch (Sohn et al., 2020), a representative SSL algorithm with state-of-the-art performance on balanced SSL benchmarks.",1,related,1,positive
"We also show in experiments that BiSTF improves over FixMatch  (Sohn et  al., 2020) by a large margin on imbalanced semi-supervised benchmarks.",1,related,1,positive
"We also tried consistency training by letting strongly-augmented samples learn the PLs generated by their weakly-augmented counterparts as suggested in [16, 20], however, it did not bring benefits in our experiments compared to directly learning PLs generated without augmentations.",1,related,1,positive
"In terms of the entropy loss for the target domain, we adopt a variant of the loss, FixMatch [43], in order to utilize the confident predictions of the target instances.",1,related,1,positive
"We can make use of CSA as the main label assignment method for SSL and integrate it into training deep learning model to build CSAMatch, analogous to FixMatch [42], MixMatch [3] and FlexMatch [58].",1,related,1,positive
"Another extension is to use CSA as the main label assignment method for SSL and integrate it into training deep learning model to build CSAMatch, analogous to FixMatch (Sohn et al., 2020) and MixMatch (Berthelot et al., 2019).",1,related,1,positive
"When integrated into FixMatch [32], our method performs significantly better than standard confidence-based pseudo-labeling methods when the training data is imbalanced across categories, which we believe better reflects real-world data distributions.",1,related,1,positive
"We compare to the latest methods developed for long-tailed SSL (DARP [15], CReST [36], and ABC [21]) and for balanced SSL (UDA [37], UPS [29], FixMatch [32], and FlexMatch [40]).",1,related,1,positive
"We first overview the framework for state-of-the-art SSL methods that combine consistency regularization with confidence-based pseudo-labeling [32, 40, 37], as our proposed approach simply replaces one step — the pseudo-labeling criterion.",1,related,1,positive
"Pseudo Labeling To generate confident pseudo labels, following [Sohn et al., 2020], only the class with an extremely high prediction probability is regarded as the pseudo label.",1,related,1,positive
"We compare the SDA-FL framework with SemiFL [Diao et al., 2021], Local Fixmatch [Sohn et al., 2020], and Local Mixup [Zhang et al., 2018] to show its effectiveness.",1,related,1,positive
We employ the FixMatch technique [49] of using data augmentation for training two multiple branches.,1,related,1,positive
"…loss against the model’s prediction pb for a strongly-augmented version of the same image:
Lcls = 1
B B∑ b=1 1(max qb ≥ τ)H(q̂b, pb) (2)
Following FixMatch (Sohn et al., 2020), we only use pseudo-labels with maximum scores above a threshold τ , and convert the soft labels qb into…",1,related,1,positive
"We construct our self-training objective by following three principles: (1) consistency regularization (Sohn et al., 2020; Laine & Aila, 2017) which enforces the model to output the same prediction when the input is perturbed; (2) entropy minimization (Grandvalet & Bengio, 2004) which encourages the model to give “sharp” predictions with low entropy; (3) prediction fairness (Berthelot et al.",1,related,1,positive
"We construct our self-training objective by following three principles: (1) consistency regularization (Sohn et al., 2020; Laine & Aila, 2017) which enforces the model to output the same prediction when the input is perturbed; (2) entropy minimization (Grandvalet & Bengio, 2004) which encourages…",1,related,1,positive
"Following FixMatch (Sohn et al., 2020), we only use pseudo-labels with maximum scores above a threshold τ , and convert the soft labels qb into “one-hot” hard labels by q̂b = arg max(qb).",1,related,1,positive
"During each update of θ, ∆ are updated with
∆ = µ∆ + (1− µ)θ (1)
Let qb denote the EMA teacher’s softmax prediction for the weakly-augmented image, we enforce a cross-entropy loss against the model’s prediction pb for a strongly-augmented version of the same image:
Lcls = 1
B B∑ b=1 1(max qb ≥ τ)H(q̂b, pb) (2)
Following FixMatch (Sohn et al., 2020), we only use pseudo-labels with maximum scores above a threshold τ , and convert the soft labels qb into “one-hot” hard labels by q̂b = arg max(qb).",1,related,1,positive
"• For FixMatch [68], we adopt it on ne-tuning from noisy labels.",1,related,1,positive
"Independent label noise Sketch, ResNet-18 Sketch, ResNet-101 MRPC, RoBERTa-Base
40% 60% 40% 60% 20% 40%
Early Stopping 72.41±3.53 53.84±3.09 77.14±3.09 61.39±1.28 81.39±0.73 66.05±0.63 Label Smooth [MKH19] 74.69±1.97 55.35±1.60 81.47±1.36 64.90±2.93 80.00±0.62 65.87±0.95 Mixup [ZCD+18] 70.65±1.85 58.49±3.25 76.04±2.29 60.12±2.37 80.62±0.14 68.37±1.33 FixMatch [SBL+20] 73.35±3.15 61.51±2.17 76.19±1.39 62.19±1.57 81.10±1.76 68.48±1.43 ELR [LNR+20] 74.29±2.52 63.14±2.05 80.00±1.45 64.35±3.98 82.78±0.86 67.86±1.44 APL [MHW+20] 75.63±1.81 64.69±2.72 78.69±2.45 64.82±2.41 80.49±0.24 66.49±0.93 SAT [HZZ20] 75.18±1.54 62.33±2.24 80.00±2.96 65.58±2.91 82.80±0.42 67.50±1.00 GJS [EA21] 73.22±2.34 59.63±5.15 77.14±2.46 63.27±2.37 81.42±1.03 67.57±1.53 DualT [YLH+20] 72.49±3.17 59.59±3.44 77.96±0.33 62.31±3.98 82.49±0.53 66.49±0.93 SupCon [GDC+20] 75.14±1.73 61.06±3.20 78.86±1.80 63.92±2.15 82.30±1.80 68.32±1.16 SAM [FKM+21] 77.63±2.16 64.53±2.84 80.57±2.70 67.27±2.39 82.61±0.91 69.06±1.41
Ours 81.96±0.98 70.00±1.71 85.44±1.26 71.84±2.72 83.55±0.63 72.64±1.77
Correlated label noise DomainNet, ResNet-18
Clipart Infograph Painting Quickdraw Real Sketch 41.47% 63.29% 44.50% 60.54% 34.64% 47.68%
Early Stopping 73.88±2.04 38.82±2.59 69.69±1.35 44.16±1.92 78.52±1.03 61.84±3.67 Label Smooth [MKH19] 74.56±2.30 38.40±2.67 70.76±1.74 46.50±3.03 81.39±0.93 62.29±2.48 Mixup [ZCD+18] 72.88±0.94 39.27±3.10 69.28±3.18 47.66±3.20 80.17±2.05 62.08±3.06 FixMatch [SBL+20] 77.04±2.52 41.95±1.52 73.31±2.10 48.74±2.08 86.33±2.54 64.61±3.28 ELR [LNR+20] 76.08±2.03 40.14±2.74 72.06±1.73 47.40±3.09 83.64±2.09 65.76±3.19 APL [MHW+20] 77.40±2.33 41.22±2.58 73.61±3.12 49.88±3.24 85.79±1.59 64.69±2.30 SAT [HZZ20] 75.24±2.79 39.58±1.47 70.69±2.69 48.18±2.95 81.90±1.07 65.39±2.77 GJS [EA21] 77.20±2.59 40.94±2.19 72.51±2.87 48.14±3.40 85.05±1.94 65.43±3.35 DualT [YLH+20] 75.24±2.02 38.75±2.12 70.27±2.24 46.62±3.16 83.33±3.01 65.47±1.91 SupCon [GDC+20] 76.56±3.53 40.38±1.94 72.51±2.45 49.20±2.63 81.87±0.84 65.67±2.90 SAM [FKM+21] 79.04±1.57 41.50±1.94 73.23±2.29 50.10±1.66 84.61±2.04 66.73±2.88
Ours 83.28±1.64 43.38±2.45 76.32±1.08 50.32±2.74 92.36±0.78 66.86±3.29
• First, we fine-tune ResNet-18 on the Sketch domain from the DomainNet dataset with 40% and 60% synthetic noise.",1,related,1,positive
"We consider the following three kinds of baselines: (i) Regularization: Finetuning with early stop (Early stopping), label smoothing [MKH19], Mixup [ZCD+18; WZV+20], and EarlyLearning Regularization (ELR) [LNR+20]; (iii) Selftraining: FixMatch [SBL+20] and Self-Adaptive Training (SAT) [HZZ20]; (ii) Robust loss function: Active Passive Loss (APL) [MHW+20], Generalized Jensen-Shannon Divergence (GJS) [EA21], dual transition estimator (DualT) [YLH+20], Supervised Contrastive learning (SupCon) [GDC+20], and Sharpness-Aware Minimization (SAM) [FKM+21].",1,related,1,positive
"To tackle this issue, we are inspired by the correlation between network’s stability and confidence and pseudo label accuracy [19, 26], and propose to filter out potentially incorrect pseudo labels.",1,related,1,positive
"In particular, we implement the strategy used in methods such [8, 37, 45], which consider a weak and a strong augmentation of an input sample denoted by α(.",1,related,1,positive
We compare our results for each of these datasets with the recent SOTA SSL methods: FixMatch [37] and MPL [33].,1,related,1,positive
"7% top-1 classification accuracy when using 4K CIFAR10 labeled and 46K CIFAR10 unlabeled examples [37], drops down to only 58.",1,related,0,negative
"For example, FixMatch- with 95.7% top-1 classification accuracy when using 4K CIFAR10 labeled and 46K CIFAR10 unlabeled examples [37], drops down to only 58.48% accuracy when using 4K CIFAR10 labels but 100K Tiny-Imagenet [22] unlabeled examples (more evidence and results in Section 5).",1,related,1,positive
Our work is also closely related to UDA [45] and FixMatch [37].,1,related,1,positive
We use RandAugment [13] with parameters as defined in [37] as the strong augmentation and random horizontal flips and random crops as the weak augmentation for AuxMix.,1,related,1,positive
"Recall that existing SSL methods [37, 44] compute the loss by using the labeled and highly confident unlabeled samples only, which are commonly believed to be the most reliable ones.",1,related,0,negative
"Following [37], we apply to unlabeled training samples the two types of strategies, i.",1,related,1,positive
"To challenge the current state of the art, we choose FixMatch [37] as the baseline.",1,related,1,positive
"We note that the gradients are closely related to the optimization dynamic [1, 14, 50] and the features characterize a certain level of semantics of the specific class [27,36,37], i.",1,related,1,positive
"We compare SAFE-STUDENT on test data that only contain seen-class instances with SSL baselines: Pseudo-Labeling [21], Pi-Model [31], Temporal Ensembling [18], Mean Teacher [33], Virtual Adversarial Training (VAT) [26], FixMatch [32], UASD [4], DS3L [9], Multi-Task Curriculum (MTC) [37], and Curriculum Labeling (CL) [3].",1,related,1,positive
"We compare SAFE-STUDENT on test data that only contain seen-class instances with SSL baselines: Pseudo-Labeling [21], Pi-Model [31], Temporal Ensembling [18], Mean Teacher [33], Virtual Adversarial Training (VAT) [26], FixMatch [32], UASD [4], DS(3)L [9], Multi-Task Curriculum (MTC) [37], and Curriculum Labeling (CL) [3].",1,related,1,positive
"We consider a new task setting that naturally addresses the limitations of both SSL and ZSL, namely Novel Class Discovery (NCD) [15, 16].",1,related,1,positive
"Requiring only a small amount of annotations, SSL addresses unannotated data using pseudo-labeling or consistency regularization, yet limited to known classes from existing annotations.",1,related,1,positive
"To verify the proposed propagation regularizer and model selection methods, we combine the proposed methods to each of UDA [32] and FixMatch [27], and we perform SSL image classification benchmarks.",1,related,1,positive
"We conduct experiments with FixMatch [27], a representative pseudo-labeling method in SSL, and three datasets: moon dataset, star dataset and CIFAR-10 [13].",1,related,1,positive
"For fair comparison [18,26], we use Wide ResNet-28-2 for CIFAR-10, Wide ResNet-288 for CIFAR-100, ResNet-18 for Mini-Imagenet and STL10, respectively.",1,related,1,positive
"To this end, we revisit the exponentially moving averaged (EMA) model in SSL and carefully study i) why the EMA model is employed merely for the testing instead of the training process in recent SOTA SSL methods [1, 13, 14, 18, 26], and ii) how the EMA model can benefit the distribution estimation on unlabeled samples.",1,related,1,positive
"As shown in Figure 1c, two stateof-the-art (SOTA) SSL methods, FixMatch [26] and CoMatch [18], can achieve promising results on CIFAR-10 with only 40 labeled samples when the labeled and unlabeled class distributions are matched, e.",1,related,1,positive
"While FixMatch obtains a higher error rate with EfficientNet-B2 than WRN28-8, Self-Tuning outperforms those methods on WRN-288.",1,related,1,positive
"Pseudo labeling [1,24,34,35,37,50] methods firstly train the classifiers with ground-truth annotations and generate pseudo-labels for unlabeled data, and finally retrain models with all data.",1,related,1,positive
"Inspired by FixMatch [34] and its applications in segmentation [2, 40], we introduce a simplified consistency regularization on the segmentation predictions to overcome this shortcoming.",1,related,1,positive
"Inspired by FixMatch [20], we further propose an effective IoUguided suppression strategy.",1,related,1,positive
Fixmatch [27] simplifies the learning process by training the model with high-confidence pseudo labels.,1,related,1,positive
"Inspired by recent 3D PSD [38] and 2D FixMatch [27], we combine the pseudo label and consistency regularization strategy in an end-to-end training scheme for large-scale point clouds.",1,related,1,positive
"Pseudo labeling [14] uses the model’s class prediction as supervision to train again, and benefits from the popular 2D Fixmatch [27].",1,related,1,positive
"Results on FERPlus show the same [11] 88.41% 86.15% FixMatch [49] 87.74% 86.45% PT(ours) 88.69% 86.60% ResNet-18 is Used as Backbone. y AffectNet is used as auxiliary dataset. z For RAF-DB, FERPlus is used as auxiliary dataset and vice versa.",1,related,1,positive
"Regarding the easy-to-adapt subdomain as a labeled set and the hard-to-adapt subdomain as an unlabeled set, we can leverage prevailing semi-supervised learning methods [7, 61] to solve the DABP problem.",1,related,1,positive
"In terms of random image augmentation methods, unlike [8], which applies weak augmentation to labeled and unlabeled data simultaneously, we configure a medium augmentation for labeled data, which may introduce some noise to the model judgment Ta bl e 1 D et ai ls of tr an sf or m at io n m et ho ds in w ea k au gm en ta tio n an d m ed iu m au gm en ta tio n",1,related,1,positive
"1 ] of im ag e he ig ht /w id th ,t ra ns la te by [10 ,1 0] re la tiv e to he ig ht /w id th ,r ot at e by [25 ,2 5] de gr ee s, sh ea r by [8, 8] de gr ee s",1,related,1,positive
"Note that on CIFAR-100, we compare our method to other methods that use similar architectures, and thus, we do not include FixMatch [22] since it reports results using WRN-28-8 which is multiple times larger than the above-mentioned architectures.",1,related,1,positive
"UDA [26] and FixMatch [22] employ RandAugment [7] for SSL, while FixMatch employs additionally Cutout [9] on top.",1,related,1,positive
"Besides the competitive performance, one further advantage of our method is that it requires only one-third of the number of training steps required by its closest competitor FixMatch [22].",1,related,0,negative
We trained one randomly-initialized Wide ResNet-28 for each of the five subsets of the training data using the FixMatch algorithm [33].,1,related,1,positive
"• FixMatch-FedAvg and FixMatch-FedProx: Naïve combination of the state-of-the-art SSL method, FixMatch [43], with the two FL frameworks.",1,related,1,positive
"Furthermore, inspired by the consistency regularization [84, 91] widely utilized in recent SSL algorithms, we also propose the multimodal consistency regularization (MCR) to improve the generalization capability of the student.",1,related,1,positive
The current SSL methods set the same fixed threshold for all the classes to select the unlabeled data for the model training [3; 11].,1,related,1,positive
"To this end, leading SSL methods first perform data augmentation, and then engage the consistency regularization to control the model to have consistent output for different augmented forms of the same data [1; 2; 3; 4; 5; 6; 7; 8; 9; 10].",1,related,1,positive
FixMatch [3] generates the one-hot labels for the unlabeled data whose predictions exceed the fixed threshold and applies distribution alignment to them.,1,related,1,positive
"overall loss is Lmix = Ls + {ymb ≥ }1Lcons , where is a balancing coefficient, and is a threshold to ensure low-entropy predictions [17].",1,related,1,positive
"In particular, we adopt FixMatch [32] that selects the high-confidence samples in Duk and relabel them by the model predictions (i.e., pseudo labels) estimated over multiple weak
augmentationsAw(·).",1,related,1,positive
"In particular, we adopt FixMatch [32] that selects the high-confidence samples in D k and relabel them by the model predictions (i.",1,related,1,positive
"…could be presumably regarded as a filter for the labels on heritage values Y HV, to only keep the samples with high inter-annotator (model) agreement [Nowak and Rüger, 2010] as the “ground-truth"" [pseudo-] labels, while treating the others as unlabeled [Lee et al., 2013, Sohn et al., 2020].",1,related,1,positive
"In addition to the commonly-chosen labeled amounts, following (Sohn et al., 2020), we further include the most challenging case of CIFAR-10: each class has only one labeled sample.",1,related,1,positive
"Following previous work (Sohn et al., 2020; Xu et al., 2021; Zhang et al., 2021), we conduct experiments with varying amounts of labeled data.",1,related,1,positive
"To handle barely supervised setting (Sohn et al., 2020) more effectively, we further propose a class fairness objective to encourage the model to produce diverse (i.",1,related,1,positive
"In this work, we adopt the speech chain reconstruction as a data augmentation method and focus on the FixMatch algorithm [15] which has recently been applied on S2S ASR [5].",1,related,1,positive
"Our design shares the general consistency regularization spirit whilst differentiating from typical data augmentation based consistency in formulation [39], [36].",1,related,1,positive
"Competitors We evaluate four representative closed-set (PseudoLabel [37], MeanTeacher [38], MixMatch [39] and FixMatch [36]) and three state-of-the-art open-set (MTCR [46], T2T [48] and OpenMatch [49]) SSL methods.",1,related,1,positive
"1) Self Labeling: The self labeling approach is adapted from FixMatch [21] for image classification, in which we generate the pseudo labels using the model itself.",1,related,1,positive
We follow [21] and only include pseudo labels whose confidence scores are above a threshold.,1,related,1,positive
"Here SSL can further distill information from unlabeled data and gradually propagate label information from labeled examples to unlabeled one during the training stage (Xie et al., 2020; Zhang et al., 2021c).",1,related,0,negative
"Curriculum Learning (CL) We further combine our acquisition function with advances in semi-supervised learning (SSL) (Berthelot et al., 2019a; Sohn et al., 2020), which also integrates abundant unlabeled data into learning.",1,related,1,positive
A more closely-related work to our proposed method is FixMatch [48].,1,related,1,positive
80% of the target datasets are used as annotated samples for semi-supervised methods (MixMatch and FixMatch).,1,related,1,positive
"Semi-Supervised Finetuning: In classification task we use FixMatch [3] as the semi-supervised learner, we stick to the original hyperparameters for CIFAR-10, CIFAR-100 and SVHN reported in [3].",1,related,1,positive
"We further evaluate two state-of-the-art SSL approaches, Self-Tuning [6] and FixMatch [3] with different initial weights.",1,related,1,positive
"For CIFAR-10, CIFAR-100 and SVHN, we follow the standard data splits for semi-supervised evaluation as in [3].",1,related,1,positive
"Besides the Non-IID methods, we utilize personalized federated learning methods combined with FixMatch for comparison: FedPer [9], LG-FedAvg [22] and pFedMe [11].",1,related,1,positive
"Further, we take the FedAvg-SL and FixAvg (FedAvg with FixMatch) as the baseline methods.",1,related,1,positive
"Compared methods: To validate our method, we compare the performance of UM-pFSSL with several key related methods for Non-IID federated learning: 1) FedProx-SL: FedProx [7] with fully labeled data, 2) FedBN-SL: FedBN [17] with fully labeled data, 3) FixProx: FedProx with semisupervised pseudo-labeling method FixMatch [31] and 4) FixBN: FedBN with FixMatch.",1,related,1,positive
"Compared Methods: To validate our method, we compare the performance of UM-pFSSL with several key related methods for Non-IID federated learning: 1) FedProx-SL: FedProx [7] with fully labeled data, 2) FedBN-SL: FedBN [17] with fully labeled data, 3) FixProx: FedProx with semi-supervised pseudo-labeling method FixMatch [31] and 4) FixBN: FedBN with FixMatch.",1,related,1,positive
"During backbone training, we adopt cross entropy loss as labeled loss and FixMatch [26] consistency loss as unlabeled loss.",1,related,1,positive
Our baseline is conducted with DGCNN backbone trained by FixMatch semi-supervised loss with all unlabeled weights fixed to 1.,1,related,1,positive
"The state-ofthe-art SSL method [26] generates pseudo-labels for weakly augmented samples with confident predictions, and then match them with the prediction of the strongly augmented ones.",1,related,1,positive
We migrate them to our point cloud setting and combine them with the same FixMatch SSL.,1,related,1,positive
"In our BAPS model, the two sub-networks are the modified version of FixMatch [22].",1,related,1,positive
"Specifically, we regard the probability of label ŷj in the prediction as its confidence [23]:",1,related,1,positive
"…& Bengio, 2005) 72.56±3.33 Pseudo Label (Lee et al., 2013) 74.49±2.26 Soft Pseudo Label 78.44±2.41 Consistency Regularization 79.17±1.79 FixMatch (Sohn et al., 2020) 74.31±3.27
UDA (Xie et al., 2020) 80.01±0.14 10 50 100 500 Pseudo Dataset Size (K images)
72
74
76
78
80
T op
-1 A
cc ur
ac…",1,related,1,positive
"Scratch 76.21±1.40 Pseudo Supervised 53.03±1.79 EntMin (Grandvalet & Bengio, 2005) 72.56±3.33 Pseudo Label (Lee et al., 2013) 74.49±2.26 Soft Pseudo Label 78.44±2.41 Consistency Regularization 79.17±1.79 FixMatch (Sohn et al., 2020) 74.31±3.27
UDA (Xie et al., 2020) 80.01±0.14 10 50 100 500 Pseudo Dataset Size (K images)
72
74
76
78
80
T op
-1 A
cc ur
ac y
(% )
BigGAN (ImageNet Pre-trained)
Figure 4: Top-1 accuracy of P-SSL when scaling pseudo dataset size
Label: attaching uniformly sampled source labels, Softmax: using softmax outputs of CAss (default), Temperature Softmax: applying temperature scaling to output logits of CAss and using the softmax output, Argmax: using one-hot labels generated by selecting the class with the maximum probability in the softmax output of CAss , Sparsemax (Martins & Astudillo, 2016): computing the Euclidean projections of the logit of Ct representing sparse distributions in the source label space, and Classwise Mean: computing the mean of softmax outputs of CAss for each target class and using it as representative pseudo source labels of the target class to generate pseudo samples.",1,related,1,positive
"We used six SSL algorithms: EntMin (Grandvalet & Bengio, 2005), Pseudo Label (Lee et al., 2013), Soft Pseudo Label, Consistency Regularization, FixMatch (Sohn et al., 2020), and UDA (Xie et al., 2020).",1,related,1,positive
"1) Batch Random Mask Block: In order to obtain more robust features and make the prediction of the network adapt to complex and changeable environments, we propose a batch random mask (BRM) block inspired by the Masked AutoEncoders (MAE) [50], in which the random patches of the image are masked to reconstruct these missing regions by the model.",1,related,1,positive
"Our results in Section 5.2 shows that while off-the-shelf
CLIP representations can be poor (especially for RGB-stacking see Figure 6 Right), adapting them through our proposed adapters results in similar performance as other adapted representations (such as MAE ones).",1,related,1,positive
", 2020) and masked auto-encoder (MAE) for ViT (He et al., 2022).",1,related,1,positive
", 2020) and for ViT we use masked auto-encoder (MAE) (He et al., 2022).",1,related,1,positive
"For NFNet we use CLIP pretraining (Radford et al., 2021), for ResNet we use BYOL (Grill et al., 2020) and for ViT we use masked auto-encoder (MAE) (He et al., 2022).",1,related,1,positive
"In addition to imagenet pretraining across all three architectures, we also evaluate using ALIGN (Jia et al., 2021) for NFNet, BYOL for ResNet (Grill et al., 2020) and masked auto-encoder (MAE) for ViT (He et al., 2022).",1,related,1,positive
"To demonstrate that the performance of the foundation model improves with the increase in the number of model parameters when pretrained using the same number of datasets in the remote sensing field, we pretrain models with different
KEUMGANG CHA et al.: A BILLION-SCALE FOUNDATION MODEL FOR REMOTE SENSING IMAGES 5
numbers of parameters using MAE [5] and the large-scale remote sensing imagery dataset, MillionAID [44].",1,related,1,positive
CV BYOL [1] ResNet200 2x 375 Million SimCLR v2 [2] ResNet152 3x w sk 795 Million DINO [3] ViT Base 84 Million iBOT [4] ViT Large 307 Million MAE [5] ViT Huge 632 Million ALIGN [6] EfficientNet-L2 800 Million CLIP [7] ViT Large 307 Million SEER [8] RegNety-256gf 1.,1,related,1,positive
"In the original MAE, pretraining is applied with 1600 epochs [5].",1,related,0,negative
"In this section, we discuss the details of the model architecture (vision transformer) [43], pretraining dataset (MillionAID) [44], and pretraining method (MAE) [5].",1,related,1,positive
"For our default model, we re-use weights from the publicly available MAE model.",1,related,1,positive
Stage 1: We follow settings from MAE [28].,1,related,0,negative
"Following [28], we adopt an encoder-decoder architecture during training and ask the model to reconstruct patches that are randomly masked out, at the pixel level.",1,related,1,positive
"We use masked input prediction[20, 28, 5, 75] objective for unimodal stages.",1,related,1,positive
"The difference between our approach and [20, 49] is that we follow the encoder-decoder structure in [28], where masked tokens are removed for the encoder and are reconstructed through a separate decoder.",1,related,1,positive
"• RGB mask Following MIM [41, 11, 36, 14, 3], we randomly mask blocks of 2D input images and take an additional UNet-like decoder after the feature extractor to reconstruct the masked regions.",1,related,1,positive
"Following (He et al., 2022), during linear probing we include BatchNorm without the affine transformation before the final classifier.",1,related,1,positive
"For audio, we use a fixed 2D sinusoidal positional encoding as described in (Huang et al., 2022) and (He et al., 2022).",1,related,1,positive
"In many cases, we have access to large-scale unlabeled data containing such corruptions and noise, and would like to exploit them to pre-train a VT that can be effectively adapted to a wide range of downstream tasks [2, 11,20, 61].",1,related,0,negative
"Compared to MAE, MB1 outperforms MAE by 2% in both UF1 and UAR, approximately.",1,related,0,negative
"We utilize the encoder and decoder parts of μ-BERT (without DMA and POI) to train previous SSL methods (MoCo V3 [4], BEIT [1], MAE [9]) and then continue learning the MER task on the large-scale database CASME3 [14].",1,related,1,positive
"We utilize the encoder and decoder parts of µ-BERT (without DMA and POI) to train previous SSL methods (MoCo V3 [4], BEIT [1], MAE [9]) and then continue learning the MER task on the large-scale database CASME3 [14].",1,related,1,positive
", [9] presented a Masked Auto Encoder (MAE) that utilizes the BERT framework.",1,related,1,positive
"6% top-1 [34], which aligns with our observations on fine-tuning the pre-trained ADM.",1,related,0,negative
"Inspired by the work of SimpleClick [35], we employ large models for feature encoding, such as the widely used MAE-pretrained Vision Transformer (ViT) [21].",1,related,1,positive
"As our ViT backbones are pre-trained on 224 × 224 pixel images by MAE [21], we resize the pre-trained absolute positional embeddings to match the size of our images.",1,related,1,positive
"In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 [19] (for MAE [16], we use ViT-base [11].) encoder Eg for 200 epochs.",1,related,1,positive
"In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 [19] (for MAE [16], we use ViT-base [11].",1,related,1,positive
"• MAE: Masked Autoencoders [15]: ViT-B16, ViTL16.",1,related,0,negative
"Motivated by scalability and powerful pretraining methods, we use an MAE [47] pre-trained Vision Transformer (ViT) [33] minimally adapted to process high resolution inputs [62].",1,related,1,positive
"Unless otherwise specified: (1) SAM uses an MAE [47] pre-trained ViT-H [33] image encoder and (2) SAM was trained on SA-1B, noting that this dataset includes only automatically generated masks from the final stage of our data engine.",1,related,0,negative
"Motivated by scalability and access to strong pre-training, we use an MAE [47] pre-trained Vision Transformer (ViT) [33] with minimal adaptations to process high resolution inputs, specifically a ViT-H/16 with 14×14 windowed attention and four equally-spaced global attention blocks, following [62].",1,related,1,positive
We initialize SAM from an MAE [47] pre-trained ViT-H.,1,related,0,negative
"While our model is initialized with a selfsupervised technique (MAE [47]), the vast majority of its capabilities come from large-scale supervised training.",1,related,0,negative
"For a comprehensive comparison, we also compare TDMR with MRKD which means a simple combination of model reprogramming (MR) and knowledge distillation (KD), and transfer learning method linear probing [7], [80].",1,related,1,positive
We compare finetuned ViT-B models trained with MoCo-v3 and MAE in Table 5.,1,related,0,negative
"We find that MoCo-v3 defended with PatchSearch and i-CutMix is better than MAE both in terms of Acc and FP for 1% labeled
finetuning data, but MAE quickly catches up in the 10% regime.",1,related,0,negative
"Restrictions apply.
from [44] indicate that MAE is robust against backdoor attacks.",1,related,0,negative
"Following previous pre-training approaches [14, 25], we use the default image input size of 224×224.",1,related,1,positive
Comparison with Hiera [52]: We show class-level performance (average precision and relative gain) of Hiera [52] (pre-trained on using MAE [24]) and ours.,1,related,1,positive
"Overall our method has a gain of +2.8 mAP compared to Video MAE [15, 49].",1,related,0,negative
"To further explicitly supervise motion feature generation, inspired by MAE [18], we design a decoder (Figure 3(b)) to reconstruct pixel motion dynamics.",1,related,1,positive
"The detailed pre-training hyper-parameters are in Table 7, which mainly follows the training settings used in the original MAE [37].",1,related,0,negative
The masked autoencoder (MAE) model [37] consists of an encoder and a decoder.,1,related,1,positive
"As proven in [10], a narrow decoder is enough for the MAE task, so we set L′′ to 1.",1,related,1,positive
"In this work, we use one such pre-training algorithm (MAE (He et al., 2021)) to explore scaling and adapting pre-trained visual representations (PVRs).",1,related,1,positive
"We now ask: how much does the relevance and diversity of the pre-training dataset and the model size matter? To study this, we fix the pre-training objective – MAE (He et al., 2021) – and then vary the composition of the pre-training dataset and the size of the visual backbone (ViTB with 86M parameters and ViT-L with 307M parameters).",1,related,1,positive
"Experiment Details of Training PVRs
To train the MAE models, we use the official codebase released by the authors on GitHub (He et al., 2021) and use the default hyperparameters provided by the repo to train the ViT-B and ViT-L models.",1,related,0,negative
"Specifically, in MAE adaptation we continue training the backbone network with the MAE (He et al., 2021) pre-training objective on task-specific data.",1,related,0,negative
"We train vision transformers (ViT-B and ViT-L) (Dosovitskiy et al., 2020) on these 4 datasets using Masked AutoEncoding (MAE) (He et al., 2021), and systematically analyze their performance on CORTEXBENCH.",1,related,1,positive
"To study this, we fix the pre-training objective – MAE (He et al., 2021) – and then vary the composition of the pre-training dataset and the size of the visual backbone (ViTB with 86M parameters and ViT-L with 307M parameters).",1,related,1,positive
"As the final step in this paper, but as a first step towards this open problem, we study adapting VC-1 with either task-specific training losses or datasets (via MAE (He et al., 2021)) to specialize VC-1 for each domain.",1,related,1,positive
"In this work, we refer to a certain attribute of an element as a field and formulate the various design tasks as a unified masked field prediction, which is inspired by the recent masked autoencoders [9,15] and multi-task models [19,36].",1,related,1,positive
"Results for the Masked Autoencoder (He et al., 2022) (Appendix B.",1,related,0,negative
"Specifically, we apply a linear layer to project the latent features Fl to patch pixels [86], and compute the mean squared error (MSE) between the reconstructed and original images on the masked pixels [29].",1,related,1,positive
"We follow the conventions in [29, 86] and mask random patches with 16× 16 pixels, and adopt a high masking ratio i.",1,related,1,positive
"Following MAE [27], ẑ is then “unmixed” to recover the input batch before mixing by inserting a special [MASK] token with M j .",1,related,1,positive
"In this section, we start by adopting mixing in MAE [27] with a simple baseline in Sec.",1,related,1,positive
"Implementation Details: We follow most of the practices of [1, 8].",1,related,0,negative
"To this end, we closely follow the approach of MAE [8], and employ the Masked Image Modeling strategy.",1,related,1,positive
We tailor the MAE approach for the endoscopic setting with three modifications: Layer Wise Learning Rate Decay: The MAE encoder and decoder consist of several layers.,1,related,1,positive
"This unsupervised learning style normally requires numerous data and computation resources [41], [42], so we put the training of it on the resourceful cloud which can collect a lot of data from multiple edges.",1,related,1,positive
"In this section, we will first revisit the original image-version Masked Autoencoders (MAE) (He et al., 2022) and the video extension of Masked Autoencoders (Feichtenhofer et al., 2022), then detail the tailored MAE for the specific tasks.",1,related,1,positive
"To implement our belief in a regressive manner where the learned loss function can be treated as image prior, the first choice is the masked autoencoders (MAE) proposed by He et al (He et al., 2022).",1,related,1,positive
"In this section, we will first revisit the original image-version Masked Autoencoders (MAE) (He et al., 2022) and the video extension of Masked Autoencoders (Feichtenhofer et al.",1,related,1,positive
"To solve this issue, we adopt an approach inspired by MAE [22], where we only feed the non-masked embeddings to the student .",1,related,1,positive
"For instance, if BERT4ETH follows the original masking ratio (15%) and uses 85% unmasked addresses to predict 15% masked addresses, the masked addresses have a high likelihood of being present in the unmasked addresses, leading to an overly easy prediction task [12].",1,related,1,positive
", 2022f) for example, supports a video masked encoder for MAE (He et al., 2022) losses in addition to a module similar to ALBEF.",1,related,1,positive
"We train the tokenizers of the columns C, the encoder, and the decoder by following the approach of the masked autoencoder (He et al., 2022); that is, we randomly select the columns with a masking ratio and replace the corresponding embeddings with a special embedding called a mask token.",1,related,1,positive
"While the original masked autoencoder in He et al. (2022) uses its decoder only in the pretext task and removes it in the downstream task, our model uses the decoder trained on the pretext task in the downstream tasks.",1,related,1,positive
"Furthermore, we note that our model does not use the [cls] token, unlike the approach by He et al. (2022).",1,related,1,positive
"Given the limited data scale, we leverage mask modeling [32] to make good use of the video data.",1,related,1,positive
"For plain architectures, we adopt the MAE [31] pre-training strategy to provide a good initialization for QFormerp, which has been proven effective in mitigating over-fitting in plain ViTs.",1,related,1,positive
"The official MAE pre-trained weights for the backbone are utilized, and the entire model is finetuned for 100 epochs on the MS COCO dataset.",1,related,0,negative
"For large models, we use plain ViT as the reference architecture and adopt the MAE pretrained [31] weights for initialization.",1,related,1,positive
"The hyperparameter for the proposed regularization term is set to 1, and all other parameters follow the same settings as in [31].",1,related,1,positive
"We use the official MAE pre-trained model to initialize the ViT-B backbone and the default training settings in MMPose, i.e., an input image size of 256×192 and a learning rate of 5e-4.",1,related,0,negative
"For the plain models, we adopt the fine-tuning practice proposed in [31], which involves pre-training the model on the ImageNet-1K dataset for 1,600 epochs using the ImageNet-1K training set, followed by supervised fine-tuning.",1,related,1,positive
We overcome the above-mentioned issues by leveraging generic Masked AutoEncoder (MAE) [19].,1,related,1,positive
We use the publicly available Masked Autoencoders [19] pretrained on ImageNet to assist blind defense.,1,related,0,negative
"Masked Autoencoders (He et al., 2022) are scalable self-supervised learners.",1,related,1,positive
"We use two pretrained Masked Autoencoders (He et al., 2022) that are available from their official repository.",1,related,0,negative
"In terms of supervision, in addition to the standard supervised pre-training on ImageNet-1k, we also consider self-supervised methods MoCo-V3 [17], SwAV [11], DINO [12], MAE [32] trained on ImageNet-1k, the weakly supervised SWAG [76] trained on 3.6B uncurated instagram images and CLIP [59] trained on 400M image-language pairs [71].",1,related,1,positive
"In terms of supervision, in addition to the standard supervised pre-training on ImageNet-1k, we also consider self-supervised methods MoCo-V3 [17], SwAV [11], DINO [12], MAE [32] trained on ImageNet-1k, the weakly supervised SWAG [76] trained on 3.",1,related,1,positive
"Masked auto-encoders assign mask tokens to an encoded latent space [16, 44], and adopt them to masked sequences at the decoder level.",1,related,1,positive
"the performance of our architecture against the BERTbased motion in-painting transformer [10], the encoderdecoder-based ∆-interpolator [31], the RNN-based approach TGcomplete [15], and the masked auto-encoder (MAE) architecture [16].",1,related,1,positive
"Moreover, inspired by MIM [2, 10], we propose a simple yet effective strategy named Masked Pose and Shape Estimation, which helps the global transformer further mine the inter-frame correlation on human motion.",1,related,1,positive
"During the global decoder phase, following [10], we pad the SMPL tokens onto the masked location and send the whole sequence into the global decoder to obtain a long-term representation.",1,related,1,positive
"Discriminative SSL methods [11, 22, 27, 79] learn the embeddings by enforcing closeness and/or distantness on the pairwise distance structure among the augmented training samples.",1,related,1,positive
We conjecture that an SSL pre-trained encoder is desirable to capture the demanding diverse semantics instead of a supervised one learned from pre-defined labels.,1,related,1,positive
"Given a frozen prediction model Pθ(y|x), and perturbed image x̃ with prompt corresponding to the label y, the training objective is formulated as:
argmin ϕ
− logPθ;ϕ(y|x̃)
While VP and AR optimize the input space visual prompt directly, we reparameterize the visual prompt to the prompt generation network hϕ(·) parameterized by ϕ = {ϕd, ϕt} ∈ Rd. Specifically, we build a novel autoencoderstyle network named Coordinator composed of a frozen encoder f(·) which is pre-trained on ImageNet [14] by selfsupervised learning (SSL) objective and followed by an extremely light-weight learnable decoder gϕd(·).",1,related,1,positive
"Therefore, for Coordinator, BlackVIP adopts an SSL encoder (i.e., Masked Auto-Encoder [26]).",1,related,1,positive
"Though the encoder can also be a supervised counterpart or lightweight learnable network, we adopt the SSL pre-trained encoder for the following three reasons: 1) It has been widely substantiated that self-supervised representation contains
the multiple discriminative features and spatial information [8, 19, 26, 31, 42, 43, 50], so it is more helpful to use SSL pre-trained encoder than label-supervised encoder for robustly performing on diverse downstream tasks.",1,related,1,positive
"Consequently, the image equipped with a prompt (prompted image) is constructed as follows:
x̃ = clip(x+ ϵhϕ(x)) hϕ(x) = gϕd(zx, ϕt)
where zx = f(x) is the feature vector of x from the frozen SSL encoder f(·), and ϵ ∈ [0, 1] is a hyperparameter that controls the intensity of visual prompt.",1,related,1,positive
6 confirms that the SSL encoder outperforms the supervised pre-trained or randomly initialized encoder (scratch).,1,related,0,negative
We exploit an SSL pre-trained encoder while we plug the randomly initialized extremely lightweight decoder.,1,related,1,positive
"Concretely, the computation of (2) can directly benefit from just dropping all the masked patches before forwarding them, in a similar manner to masked autoencoder [12].",1,related,1,positive
"(4) Inspired by the principles of self-supervised training [41, 15, 14], we leverage this methodology to train our encoder-decoder architecture.",1,related,1,positive
"Inspired by the advantage of long-range receptive fields from transformer layers, we follow MinD-Vis [6] to adopt the architecture of masked autoencoder [14] as the encoder-decoder model for fMRI signals.",1,related,1,positive
The only difference from MAE is that we finetune on iNaturalist21 rather than iNaturalist17.,1,related,0,negative
"As a base model, we consider the ViT-Base model using the Masked AutoEncoder (MAE) pretraining setup [16], which leads to state-of-the-art results for this general task.",1,related,1,positive
"For iNaturalist, we consider a Masked AutoEncoder (MAE) [16] pre-trained model and finetune it following the same procedure and hyperparameters as the original paper.",1,related,1,positive
"Though the scRNA-seq data is distinct from images, our results show that the performance gains of xTrimoGene are comparable to those of MAE, with more efficient training and better downstream task performance.",1,related,0,negative
"To further tackle the mode collapse challenge in unsupervised learning and scale up the optimization iterations, inspired by recent masked autoencoders [23, 57], we construct a masked point modeling paradigm where both point color reconstruction objective and surfel normal reconstruction objective are proposed to recover the masked color and geometric information of the point cloud respectively.",1,related,1,positive
"To this end, we turn to use MAE [19], a SOTA image encoder pre-trained in an unsupervised manner, similar to MLM.",1,related,1,positive
"To this end, we turn to use MAE [18], a SOTA image encoder pre-trained in an unsupervised manner, similar to MLM.",1,related,1,positive
"We find MAEbase-MLM clearly improves the standard MAEbase on HM with the TS model, but obtains marginal gains with the E2E model.",1,related,1,positive
"We extract the ViT features from the same ViT architecture training in different ways: (b) supervised ViT [17], (c) self-supervised DINO [43], and (d) MAE [44].",1,related,1,positive
using transformer feature representations extracted from pretrained masked autoencoder (MAE) [44].,1,related,1,positive
"Note that other popular image feature extractors [3, 9] can also be applied in our framework.",1,related,1,positive
"Given the simplicity and efficiency of Masked AutoEncoding (MAE) [33], we leverage it as the self-supervised prepretraining approach.",1,related,1,positive
We follow [33] to train MAE models on IG-3B without using any labels.,1,related,0,negative
"In particular, we show that (i) MAE not only scales with model size as shown in [33], but also with the size of the training data (Figure 2).",1,related,1,positive
We first begin with the Masked Autoencoder (MAE) [33] self-supervised learning technique to pre-pretrain vision models without using any labels.,1,related,1,positive
We compare the performance of MAE pretraining on the large scale IG-3B with the original MAE [33] models trained on IN1k for 1600 epochs.,1,related,0,negative
We follow the same hyperparameters used in [33] for pretraining on IN1k.,1,related,1,positive
"We follow SimpleClick [26] to build the interactive segmentation model, which consists of two patch embedding modules for image and click map respectively, a ViT [10] backbone initialized with MAE [16], a simple feature pyramid [21], and an MLP segmentation head.",1,related,1,positive
"As shown in Table 3, the model initialized with MIS pre-training the MAE [16] counterpart especially on the relatively hard dataset (i.e., SBD and DAVIS).",1,related,0,negative
"As shown in Table 3, the model initialized with MIS pre-training the MAE [16] counterpart especially on the relatively hard dataset (i.",1,related,0,negative
"Following the principle of MAE [27], we randomly mask out some tokens, and utilize the EAT encoder to extract high-level latent features.",1,related,1,positive
"Beyond augment-and-compare or mask-and-predict pretext tasks in MV-SSL and MIM, in this paper, we endeavor to investigate another simple yet effective paradigm for selfsupervised visual representation learning.",1,related,1,positive
"Overall, our CIM is a simple yet effective approach that can perform on par or better than existing MV-SSL and MIM methods with both ViT and ResNet models.",1,related,1,positive
"Due to the architectural difference between ViT and CNN models, we observe performance degeneration of some MIM and MV-SSL pretraining methods, such as SimMIM [68], MoCo v2 [10], and SimSiam [11].",1,related,1,positive
2) We demonstrate the advantages of our CIM in learning transferable representations for both ViT and ResNet models that can perform on par or better than the current state-of-the-art MIM and MV-SSL learners while improving model robustness and training efficiency.,1,related,1,positive
"For all ablation experiments, we choose ViT-Base (ViT-B/16) as the default backbone and follow a common setting used in existing works [2, 26]: 300-epoch self-supervised pre-training without labels and 100-epoch supervised end-to-end fine-tuning, to evaluate the quality of learned representations.",1,related,1,positive
"We omit the results in other metrics (NDCG, MAE MAPE) and on other data as their trends are similar.",1,related,1,positive
"We also adopt the mean absolute error (MAE), rooted mean squared error (RMSE) as well as mean absolute percentage error (MAPE) to evaluate the model accuracies.",1,related,1,positive
"The concatenation of unmasked patches’ embedding and masked tokens are processed by the decoder to predict the unmasked patches [17, 43].",1,related,1,positive
Masked autoencoder (MAE) [141] (see Figure 8) simplifies it to an end-to-end denoising framework by predicting the masked patches from the unmasked patches.,1,related,1,positive
Masked Autoencoders (MAE) [30].,1,related,1,positive
"One of the key observations of the MAE work [30] is that the decoder does not need to be very good for the encoder to achieve good performance: by using only a small decoder, MAE successfully trains a ViT in an auto-encoder fashion.",1,related,0,negative
"Compared to a model that uses masked image modeling, the original MAE [27] and to the MaskFeat model [55], our model underperforms by 0.",1,related,1,positive
"First, inspired by [28, 97], we randomly mask some joints and require the model to reconstruct them.",1,related,1,positive
"Starting with the baseline ViT configurations used in the original BEiT series pre-training [5, 92, 123] (∗ in Table 2), we progressively refine the model design and make the following observations: (i) The performance of SwiGLU FFN is mediocre with the random weight initialization method used in BEiT, but works quite well with JAX weight initialization [14, 51] (+1.",1,related,1,positive
"Still, we note that this efficient decoder retains much of the modeling capacity of standard masked decoders [5, 8, 14] that employ full self-attention on context and mask {zL, zM}.",1,related,1,positive
"Since the standard masked modeling [5, 8, 14] of applying a bidirectional transformer on mask and context {zM , zL} leads to a complexity of O((NM + NL)(2)), we opt into an efficient decoder that reduces the cost to linear to mask sizeO(NM ) while preserving as much modeling capacity as possible.",1,related,1,positive
"First of all, we pre-train a Masked Autoencoder (MAE) [13, 36] on a large-scale facial dataset in a selfsupervised manner.",1,related,1,positive
Our framework starts with pre-training a masked autoencoder (MAE) [9] from a set of face databases.,1,related,1,positive
"To verify transfer learning ability on scalable Vision Transformers beyond UNets, we compare ImageNet 2562 pre-trained DiT [43] to MAE pre-trained vanilla ViTs [17] on CIFAR-10
and Tiny-ImageNet.",1,related,1,positive
"Drawing from the connections between diffusion networks and denoising autoencoders (Section 3.1), we infer that DDAEs can produce linear-separable representations at some implicit encoderdecoder interfaces, resembling DAEs and MAEs.",1,related,1,positive
"Following [10, 26], we randomly mask the features of joints after the embedding layer.",1,related,1,positive
"Different from MAE [10] and some existing graph data augmentation methods [12, 23], we feed the topology of the original joints into the encoder.",1,related,1,positive
The great success of MAE [10] makes us rethink data augmentation.,1,related,0,negative
"Surprisingly, when we directly reconstruct masked joints using a method similar to that in MAE [10], the performance degrades instead.",1,related,1,positive
We will discuss the relationship between MAE [10] and graph data augmentation in detail in Sec.,1,related,1,positive
"Inspired by MAE [10], we propose an augmentation framework named MGPose.",1,related,1,positive
"As in MAE [23], the encoded tokens zT (Eqn.",1,related,1,positive
"Thus, this work is based on Masked AutoEncoders (MAE) [23] for pre-training.",1,related,0,negative
"We implemented the 3D MAE using 3D-ResNetBlocks instead of Vision Transformer, different from the previous study [13], due to the constraint of GPU memory.",1,related,1,positive
"Following the previous study [13], we implement an asymmetric design with a lightweight decoder compared to the encoder.",1,related,1,positive
"To avoid information leakage, we use self-supervised MAE for model pre-training on split ImageNet-subset.",1,related,1,positive
One exception is fine-tuning MAE on ImageNet-subset where we found the transformations used for CPP generate inferior results and thus we follow the data transformations used in the original paper [20].,1,related,1,positive
"DualPrompt [59] is originally designed using Transformer structures, so we only change the pre-trained weights to MAE to avoid information leakage.",1,related,1,positive
"To this end, we implement CPP with four up-to-date pre-training methods including ViT [14], Deit [53], Dino [5], and MAE [20] that sweep supervised and self/un-supervised learning as well as discriminative and generative models.",1,related,1,positive
"2) MIM: To handle unpaired images, we leverage the Masked Image Modeling (MIM) paradigm [59].",1,related,1,positive
"Therefore, we use a multiway transformer to extract multi-modal features and two linear layers to solve PLM and MIM tasks, respectively [38], [59].",1,related,1,positive
"Our VLP model achieved the best performance with a higher masking ratio of 85%, which is in contrast to the optimal masking ratio of 75% reported by MAE [59].",1,related,0,negative
"PosterV2-Vit feature: To acquire advanced visual features, we trained a Vision Transformer (ViT) [4] model on the AffectNet7 dataset [21] using unsupervised learning techniques [5].",1,related,1,positive
"In this paper, we revisit deep supervision for masked image modeling (MIM) [15, 11, 48, 2], a self-supervised pretraining strategy for Vision Transformer [12] (ViT).",1,related,1,positive
"Then, similar to MAE [15], we randomly mask a high proportion of patches, yielding a masked image x̃.",1,related,1,positive
"For concreteness, we use MAE [15] to illustrate our underlying approach.",1,related,1,positive
"Then ViT-B is finetuned on ImageNet-1K [9], following common practice [15].",1,related,1,positive
"We follow the same training recipe of MAE [15], more details can be found in supplementary materials.",1,related,0,negative
"FULL 100% 79.1 86.2 59.7 75.0
Addition-based methods
MLP-3 1.60% 73.6 75.2 35.7 61.5 PROMPT-SHALLOW 0.04% 79.9 82.5 37.8 66.7 PROMPT-DEEP 0.23% 76.8 84.5 53.4 71.6 ADAPTER-8 1.18% 81.7 87.3 61.2 76.7 SPT-ADAPTER (ours) 0.33% 83.0 87.3 62.1 77.5
Reparameterization-based methods
baseline method on VTAB-1k benchmark with only 0.26% and 0.08% trainable parameters for MAE and MoCo v3 pretrained backbones, respectively.",1,related,1,positive
"We show more parameter sensitivity patterns for ViT-B/16 with various pre-training strategies (i.e., MAE [20] and MoCo V3 [11]) and datasets sampled from FGVC benchmark [24].",1,related,1,positive
"We conduct experiments on the plain vision Transformer backbone ViT-B/16 [13] that is pre-trained on ImageNet [27] with different pre-training strategies following [24], including supervised pre-training and self-supervised pre-training with MAE [20] and MoCo v3 [11] following [24].",1,related,1,positive
"Visualizations of sampled VTAB-1k datasets with MAE and MoCo V3 pre-trained ViTB/16 are shown in Figures 5, 6, 7.",1,related,0,negative
"For our RFFR model, we adopt a base version of Masked Autoencoder (MAE) [19] and train it on real faces with a batch size of 128.",1,related,1,positive
We find that Data2Vec [3] (row 3) attains similar performance to the MAE [18] (row 4) initialization we use in Sec.,1,related,1,positive
"Specifically, we find that visual representation learning (using masked autoencoding (MAE) [18]) not only improves performance, but also enables model scaling with ViTs.",1,related,1,positive
"Specifically, we propose novel FaceMAE module, a masked autoencoder [25] specialized for FER-W, to achieve the goal of FFL.",1,related,1,positive
"However, we designed FaceMAE, a masked autoencoder [25] specialized for FER-W, by making two major modifications to the original masked autoencoding scheme.",1,related,1,positive
"However, we notice that since MAE’s mask tokens only pass through the decoder [20], a shareddecoder design is critical in our scheme for mask tokens to learn mutual information before performing reconstructions in separate modal-specific decoders.",1,related,1,positive
We follow MAE [20] and Point-M2AE [64] to generate input tokens from images and point clouds.,1,related,1,positive
"Consequently, we design a MAE [20]-structured multi-modal learning framework that incorporates projection alignment for more interactive multi-modal learning.",1,related,1,positive
"For the image branch, we follow [20] to divide images into regular patches with a size of 16 × 16, before the ViT backbone.",1,related,1,positive
"To examine the effectiveness of our method, we perform DPPMask on two representative MIM methods: MAE [26], iBOT [67], which represent two different MIM frameworks: pixel reconstruction and feature contrast.",1,related,1,positive
"For the self-supervised learning models, DINO [5] and MAE [16], we additionally measure the endto-end performance of benchmark models pre-trained on our UnlabelledNAIP.",1,related,1,positive
"DINO [5] based on knowledge distillation and the generative model MAE [16] based on autoencoder, on our FireRisk.",1,related,1,positive
"For the self-supervised architectures, MAE [16] and DINO [5], we use ViT-B/16 [10] as the backbone and fine-tune on FireRisk using latent representa-",1,related,1,positive
"Using transfer learning, we fine-tune ResNet-50 [17], ViT-B/16 [10], as well as DINO [5] and MAE [16] with ViT-B/16 as the backbone, all of which were pre-trained on ImageNet [8], using our",1,related,1,positive
"• To investigate the performance of supervised and self-supervised learning on our FireRisk, we employ ResNet [17], ViT [10], DINO [5], and MAE [16] as benchmark models.",1,related,1,positive
"On FireRisk, we present benchmark performance for supervised and self-supervised representations, with Masked Autoencoders (MAE) [16] pre-trained on ImageNet1k [8] achieving the highest classification accuracy, 65.",1,related,1,positive
"learning, we select two representative self-supervised models for their performance, namely DINO [5] and MAE [16].",1,related,1,positive
"In generating the latent representations, we use ViT-B/16 as the backbone architecture for MAE [16], pre-",1,related,1,positive
"• We gather an unlabelled dataset, UnlabelledNAIP, from the NAIP remote sensing project and utilize it to pre-train novel latent representations of DINO [5] and MAE [16].",1,related,1,positive
"We choose publicly available PTMs, i.e., ResNet18/50/152 [24], ViT-B/16-IN1K/21K, ViT-L/16-IN1K, ViT-B/16-DINO [7], ViT-B/16-SAM [11], ViT-B/16-MAE [23], ViT-B/16CLIP [50] (image encoder) for a holistic evaluation, and report the results in Figure 6.",1,related,1,positive
"Additionally, we report the backbones adopted in the ablation study, including ViT-L/16-IN1K, ViTB/16-DINO [7], ViT-B/16-SAM [11], ViT-B/16-MAE [23], ViT-B/16-CLIP [50] (image encoder), in the table.",1,related,1,positive
"(3) RefCOCO, RefCOCOg, RefCOCO+ [81] 60K MLM with PEVL text encoder [78] Phrase Grounding (1) Flickr30K [79] 32K MLM with PEVL text encoder [78]
Visual Relationship Detection (1) Visual Genome [41] 101K MLM with PEVL text encoder [78] Visual Commonsense Reasoning (1) VCR [84] 100K MLM with PEVL text encoder [78]
Self-Supervised Learning (2) ImageNet-1K [10] 1.3M MoCo-v2 [7], MAE [24] TOTAL (47) 32M
them for specific downstream task.",1,related,1,positive
"Similar to MAE [22] and PointMAE [37], we compute the loss only on masked parts.",1,related,1,positive
"Following MAE [22] and VideoMAE [51], we adopt the asymmetric encoder-decoder design to reduce computation.",1,related,1,positive
"We use 3 alternative models to initialize the feature extractor in the mask generator: (1) the ViT-B network pretrained with MAE [16] (termed as MAE-800), (2) the ViT-B pretrained with iBOT [48] (termed as iBOTB), and (3) the ViT-S pretrained with DINO [3] (termed as DINO-S).",1,related,1,positive
"Also, since the methods
in this section refer to MAE, a comparison test is done between the methods in this section and MAE using the same training method.",1,related,0,negative
"The self-supervised pretraining method in this paper takes reference from MAE, but differs from it in that MAE uses two identical structures of ViT as encoder and decoder, while our method uses a symmetric convolution-deconvolution structure for the autoencoder.",1,related,1,positive
"The resulting data are shown in Table III, from which it can be seen that in each of the four datasets, our method is higher than MAE by more than 3 points in each metric.",1,related,1,positive
"Meanwhile, to better prove the reliability of the method proposed in this chapter, we conducted peer-to-peer experiments using MAE, i.e., we first performed masked self-supervised
learning pre-training, and then selected the one with the lowest training loss model for the pedestrian re-identification task.",1,related,0,negative
"Considering ViT’s flexibility and great potential in masked image modeling [9,14], we explore acceleration algorithms based on the standard ViT.",1,related,1,positive
"We implement a baseline inspired by MIM [2,9].",1,related,1,positive
"Instead of a random formulation [9, 30], we sample a fixed ratio γ of the tokens [M ] (X) to be masked out according to a multinomial distribution related to the patch hand saliency m(X):",1,related,1,positive
"Then this prior is encoded as the token form [9], and a ViT [15] sketcher is trained to disentangle the corresponding structure tokens from partial image patches [30].",1,related,1,positive
"Inspired by this, we implement an MAE by masking the outputs of our encoder, Fo, and then passing the masked encoded features along with the masked tokens to our decoder.",1,related,1,positive
"As illustrated in Figure 1 (b), we adopt a simple Regression head composed of two deconvolution layers and one 1×1 convolution layer on the reshaped Fo following the common setting of the previous works [23].",1,related,1,positive
"We initialize the Temporal ViTPose weights from the pre-trained model of MAE [23], and perform masked region reconstruction with a masking rate of 75%.",1,related,1,positive
"Regarding selfsupervised models, the masked autoencoder (MAE [30]), DINO [7], MoCov3 [10], MSN [2] were selected, because they all include the base ViT (ViT-B/16) for comparison between pretraining schemes (Fig.",1,related,1,positive
"Here, we make a modification to the original MAE (He et al. 2022) by replacing the masked contents with x and processing them in the same way as non-masked ones, instead of discarding them.",1,related,1,positive
"Therefore, we empirically set an even higher combined masking rate of 87.5% (compared to 75% used by He et al. (2022)) in our M3AE to make the self-supervising task nontrivial.",1,related,1,positive
"Here, we make a modification to the original MAE (He et al. 2022) by replacing the masked contents with xsub and processing them in the same way as non-masked ones, instead of discarding them.",1,related,1,positive
"Above all, witnessing the recent success of masked autoencoders in learning rich visual representations (He et al. 2022), we propose multimodal masked autoencoder (M3AE), where a random subset of the modalities and random patches of the remaining ones are masked simultaneously.",1,related,1,positive
"Therefore, we sample a random subset of the modalities for masking to mimic the real situation, in addition to randomly masking 3D patches of the remaining modalities as in the original MAE for natural images.",1,related,1,positive
"Evaluation of Representation: So, why is the Slow Learner such effective, and what accounts for the remaining performance gap? We perform a linear probing experiment [12] to evaluate the performance of the representation layer.",1,related,1,positive
"In addition to supervised pre-training, we consider representative self-supervised paradigms that provide pre-trained checkpoints on ViT-B/16, i.e., MoCo v3 [4], BEiT [2] and
MAE [12].",1,related,1,positive
"Considering architectural consistency with previous works of CLPM [43, 42], we select representative self-supervised methods (i.e., MoCo v3 [4], MAE [12] and BEiT [2]) that release checkpoints on ViT-B/16 in our comparisons.",1,related,1,positive
We utilize the mask ratio 25% and 8 decoder blocks following the practices in MAE [4].,1,related,1,positive
"To address the above issues, we propose to Mimic before Reconstruct for Masked Autoencoders, termed as MR-MAE, which is a simple and effective strategy to enhance MAE [4] by regularizing the intermediate representations with pre-trained off-the-shelf feature encoders.",1,related,1,positive
"Inspired by the popular masked image modeling [12, 13], we build a universal masked autoencoder architecture for VAD by embedding random mask tokens to simulate anomalies, which is a simple yet efficient synthetic method that avoids extra data processing in the original normal data.",1,related,1,positive
"The image encoder is initialized with the first 12 layers of MAE-base (He et al. 2022) weight, which is pre-trained on the ImageNet-1k without any labels.",1,related,1,positive
"We also provide a new pipeline achieving data augmentation efficiently for imbalanced image datasets, using cGAN or diffusion models and ResNet or Masked Autoencoder (MAE) classifiers.",1,related,1,positive
Table 6 provides goodness-of-fit (R2) and Mean Absolute Error (MAE) measurements for the function f .,1,related,1,positive
"To justify our proposed metric and pipeline work regardless of the classifier selection, we also provide the results with Masked Autoencoder (MAE) ViT-H128 [14] which shows state-of-the-art results in dataset such as [12].",1,related,1,positive
a ResNet [15] or Masked Autoencoder (MAE) [14] classifier also trained on the original set with sub-class labels are applied on the synthesized images to select qualified ones.,1,related,1,positive
5% 1 State-of-the-art (SOTA) classification validation accuracy with Masked Autoencoder ViT-H448 [14],1,related,0,negative
The high R2 and low MAE values show that the formulation of f is highly effective on modeling the relationship between SSIM-supSubCls and accuracy improvement with our proposed data augmentation pipeline.,1,related,1,positive
"The ResNet18 classifiers are trained for 100 epochs and the MAE for 50 epochs when their validation accuracy converges, with their hyperparameters remaining the same throughout the whole procedure in each case.",1,related,0,negative
Table 7 compares the state-of-the-art (SOTA) classification validation accuracy from [14] and ours on the whole iNaturalist-2019 dataset.,1,related,0,negative
"To certify that this conclusion can be drawn regardless of the classifier selection, we also conduct the experiments with MAE classifier and the same results are obtained.",1,related,0,negative
"Then
a ResNet [15] or Masked Autoencoder (MAE) [14] classifier also trained on the original set with sub-class labels are applied on the synthesized images to select qualified ones.",1,related,1,positive
"From this table, our results with Masked Autoencoder (MAE) is comparable with the SOTA one, even though the SOTA is with 448× 448 while ours is with 128×128 input image resolutions which largely reduce the need of running time and computational resources.",1,related,1,positive
"In our experiments, the deep generative models, ResNet18 and MAE classifiers are first trained on the original imbalanced set with sub-class instead of super-class labels. cGAN models are trained until the Frechet Inception Distance (FID) scores converge.",1,related,1,positive
"Specifically, our efficient centroid-based MIM outperforms the prior tokenbased MIM [2] and pixel-based MIM [21] in equivalent ViT size and epochs.",1,related,1,positive
"Besides, we also consider a baseline strategy (nofill) that drops the masked patches without refilling as in MAE [13].",1,related,1,positive
"Inspired by the work [24], we propose to use a masked autoencoder based on vision transformer.",1,related,1,positive
"Moreover, we have picked up some parameters that have been proven successful in [24].",1,related,0,negative
"Different from conventional autoencoders, the used masked encoderdecoder operates in an asymmetric fashion [24] enabling the encoder to operate on only the partial observed signal (without the masked patches) and the decoder to rebuild the full image based on the representation given by the encoder and the masked patches.",1,related,1,positive
"We use the standard ViT-B [14] as the encoder network and initialize it with the MAE pretrained [18] weights following [43, 88].",1,related,1,positive
BEiT [1] RRC+40% mask ViT+Linear DALLE SimMIM [56] RRC+60% mask ViT+Linear RGB MaskFeat [53] RRC+40% mask ViT+Linear HOG ConvMAE [15] RRC+75% mask ConvViT+MSA RGB MAE [20] RRC+75% mask ViT+MSA RGB,1,related,1,positive
"In the following analysis, we investigate MAE [20] by diagnosing its reconstruction target and input image patches, identifying two important but overlooked bottlenecks that could have hurt the representation quality.",1,related,0,negative
"We then present a careful analysis with the milestone method, MAE [20], to disclose two important but overlooked bottlenecks of most pixel-based MIM approaches (subsection 3.",1,related,1,positive
"However, unlike the MAE, the occlusion positions of samples generated by OIA are randomly sampled, so we need to conduct completion on each instance.",1,related,1,positive
"As mentioned in III-C, our FCD adapts MAE’s notion [25] of restoring entire features using implicit unoccluded features.",1,related,1,positive
We customize the original masking strategy from MAE [22] and design a facial part masking strategy to ensure that the model can learn the consistencies of all facial parts.,1,related,1,positive
Our masked autoencoder is based on an asymmetric encoder-decoder architecture [22].,1,related,1,positive
"To realize this restoration concept, we pretrain a simple yet powerful masked autoencoder (MAE) model [22] solely on real images.",1,related,1,positive
"In this stage, we customize MAE[22] and its masking strategy with modifications to perform self-supervised learning of real faces, so that the model can learn generic facial part consistency features and can also prevent over-fitting to specific forgery patterns.",1,related,1,positive
"Inspired by [22], we propose a novel self-supervised pretraining method that is based on masking and recovering faces, named Mover.",1,related,1,positive
The emergence of the masked autoencoder (MAE) [22] has greatly influenced our community.,1,related,0,negative
"For 8K iterations, we find VPDA32 surpass all the baseline methods, including those pre-trained on mask image modelling [17, 38], contrastive learning [7] and supervised learning [27, 29].",1,related,1,positive
"♠ SpiderCNN (Xu et al., 2018) 69.8 73.7 ♠ DGCNN (Wang et al., 2019) 73.6 78.1 ♠ PointCNN (Li et al., 2018) 75.1 78.5 ♠ GBNet (Qiu et al., 2021) 77.8 80.5 q PointBert (Yu et al., 2022d) - 83.1 q Point-MAE (Pang et al., 2022) - 85.2 q Point-TnT (Berg et al., 2022) 81.0 83.5
♣ PointNet (Qi et al., 2017a) 63.4 68.2 ♣ PointNet++ (Qi et al., 2017b) 75.4 77.9 ♣ BGA-PN++ (Uy et al., 2019) 77.5 80.2 ♣ PointMLP (Ma et al., 2022) 83.9 85.4 ♣ PointMLP-elite (Ma et al., 2022) 81.8 83.8 r PointMLP-CoC (ours) 84.4↑0.5 86.2↑0.8
Context Clusters are a natural fit for point clouds Qi et al. (2017b); Lu et al. (2022).",1,related,1,positive
"1 on ImageNet and do not use dropout when pre-training on the much larger JFT-300M dataset; recent languagesupervised or self-supervised vision models (Radford et al., 2021; He et al., 2021) do not use dropout.",1,related,0,negative
"Unlike MAE He et al. (2022), BEiT Bao et al. (2022), and DIT Li et al. (2022), we do not use patch-level masking strategy for pre-training.",1,related,0,negative
"…8, we compare several tuning methods with our proposed U-Tuning based on different architectures and pre-trained models, i.e., IN-21K (Deng et al., 2009), MAE (He et al., 2022b) and CLIP (Radford et al., 2021) pre-trained ViT/B16 (Dosovitskiy et al., 2020), as well as CLIP pre-trained ViT/L-14.",1,related,1,positive
"In Table 8, we compare several tuning methods with our proposed U-Tuning based on different architectures and pre-trained models, i.e., IN-21K (Deng et al., 2009), MAE (He et al., 2022b) and CLIP (Radford et al., 2021) pre-trained ViT/B16 (Dosovitskiy et al., 2020), as well as CLIP pre-trained ViT/L-14.",1,related,1,positive
", MAE [15]), we use this finding to further reduce the computational complexity of our model.",1,related,1,positive
"Similar to recent works in the computer vision domain (e.g., MAE [15]), we use this finding to further reduce the computational complexity of our model.",1,related,1,positive
"For model fine-tuning with MAE, we adopt the settings in (He et al., 2021).",1,related,1,positive
"For example, we obtain better results by integrating our method into the fine-tuning process of the Masked AutoEncoder (MAE) in (He et al., 2021), achieving improvements on ImageNet by 0.3%, ImageNet-A by 1.8%, ImageNet-R by 1.4% and ImageNet-C by 3.7% on Vision Transformer Large (Dosovitskiy et…",1,related,1,positive
"15 is that we take the results directly from (He et al., 2021), which have no detailed data of different corruptions.",1,related,0,negative
"For example, we obtain better results by integrating our method into the fine-tuning process of the Masked AutoEncoder (MAE) in (He et al., 2021), achieving improvements on ImageNet by 0.",1,related,1,positive
"C.15 is that we take the results directly from (He et al., 2021), which have no detailed data of different corruptions.",1,related,0,negative
"FT follows MAE (He et al., 2022), while Ours uses MoCo v2 contrastive loss and regularization coefficient λ = 0.",1,related,1,positive
"FT follows MAE (He et al., 2022), while Ours uses MoCo v2 contrastive loss and regularization coefficient λ = 0.1.",1,related,1,positive
"FT follows the training in MAE (He et al., 2022) mostly, using SGD for 200 epochs with a cosine learning-rate scheduler, the base learning rate of 0.",1,related,1,positive
"FT follows the training in MAE (He et al., 2022) mostly, using SGD for 200 epochs with a cosine learning-rate scheduler, the base learning rate of 0.06, weight decay 5e-4, momentum 0.9 and batch size 256.",1,related,1,positive
"We adopted the masked auto-encoder (MAE) (He et al., 2022) to process the point cloud data (denoted as MAE-dense) for pre-training, which is close with Point-MAE (Pang et al., 2022).",1,related,1,positive
"To pre-train the MAE-dense and MAE-sampled, we first follow the standard train-test split of ShapeNet (Chang et al., 2015) adopted by Pang et al. (2022); Yu et al. (2022).",1,related,1,positive
It is denoted as MAE-sampled.,1,related,1,positive
"Specifically, we build the classifier by keeping the encoder structure and weights of the pre-trained MAE-dense and MAE-sampled models, followed by max-pooling as well as a fully connected layer of dimension [256, 256, 40] to map the global token of a dimension of 256 to the 40 categories.",1,related,1,positive
"3, our proposed MAE-sampled outperformed all state-of-the-art methods on 3 out of 4 settings, while MAE-sampled consistently outperformed MAE-dense.",1,related,1,positive
"We adopted the masked auto-encoder (MAE) (He et al., 2022) to process the point cloud data (denoted as MAE-dense) for pre-training, which is close with Point-MAE (Pang et al.",1,related,1,positive
"To evaluate the effectiveness of our claimed contribution, we replace the dense-attention layer in MAE-dense with our sampled-attention layer (Fig.",1,related,1,positive
"To maintain a fully self-supervised pre-training paradigm, we initialize with weights obtained by self-supervised ImageNet pre-training [19].",1,related,1,positive
"Similar to MAE [19], we normalize the output patches as well as the target patches prior to computing the loss, which we found to empirically improve the performance.",1,related,1,positive
2) MAE-unsupIN→SN [19] ViT ImageNet+ScanNet 54.,1,related,1,positive
6) MAE-unsupIN→SN [19] ViT ImageNet+ScanNet 63.,1,related,1,positive
"Although similar to MAE, we choose SimMIM because it adopts the backbone of Swin Transformer, which performs better than ViT adopted in MAE, as shown in the experiment.",1,related,1,positive
"It is shown that the robust curves of the models with the same
Springer Nature 2021 LATEX template
ARES-Bench 3
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
VGGs VGG13_Normal VGG16_Normal VGG19_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
ResNets ResNet50_Normal ResNet50_MOCO ResNet101_Normal ResNet152_Normal Wide-ResNet50_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
DenseNets DenseNet121_Normal DenseNet161_Normal DenseNet201_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(d)
ConvNexts ConvNextS_Normal ConvNextS_21K ConvNextB_Normal ConvNextB_21K ConvNextL_Normal ConvNextL_21K
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(e)
ViTs ViTS_Normal ViTS_21K ViTB_Normal ViTB_21K ViTB_MAE ViTL_Normal ViTL_21K ViTL_MAE
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(f)
XciTs XciTS_Normal XciTM_Normal XciTL_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (g)
T2Ts T2T14_Normal T2T19_Normal T2T24_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(h)
Swins SwinS_Normal SwinS_21K SwinB_Normal SwinB_21K SwinL_21K
Fig.",1,related,1,positive
"Springer Nature 2021 LATEX template
ARES-Bench 13
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
Normal Models VGG19_Normal ResNet152_Normal DenseNet161_Normal ConvNextL_Normal ViTL_Normal XciTL_Normal T2T24_Normal SwinB_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
Pre-trained Models ResNet50_Normal ResNet50_MOCO ViTB_Normal ViTB_21K ViTB_MAE ConvNextL_Normal ConvNextL_21K SwinB_Normal SwinB_21K
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
AT Models ResNet152_Normal ResNet152_AT ConvNextL_Normal ConvNextL_AT ViTB_Normal ViTB_AT XciTL_Normal XciTL_RB SwinB_Normal SwinB_AT",1,related,1,positive
"IN-Val IN-V2 IN-Real ON IN-A IN-R IN-V IN-C SIN IN-Sketch
ViTS
Normal 74.4 61.6 80.0 13.1 8.8 30.4 11.2 32.0 9.1 19.9 34.0 Pre-train 81.4 70.3 86.8 22.7 27.3 45.7 16.6 47.1 15.8 32.5 44.6 AT 70.2 57.3 77.9 11.5 6.1 46.0 8.5 27.8 16.8 29.8 35.2
ViTB
Normal 75.8 61.6 80.9 13.2 11.4 32.8 13.3 34.3 10.9 23.7 35.8 Pre-train 84.6 73.9 88.8 27.4 44.5 56.8 19.4 57.5 22.6 43.0 51.9 MAE 83.6 73.1 88.1 24.9 37.7 49.8 18.2 49.4 20.2 36.4 48.1 AT 73.4 60.4 80.5 12.7 8.9 50.7 9.4 36.6 22.2 35.7 39.1
ViTL
Normal 75.2 60.7 79.8 11.2 11.3 33.3 13.4 35.4 9.3 25.0 35.4 Pre-train 85.8 76.0 89.2 30.5 56.1 64.2 25.5 65.3 30.1 51.8 57.4 MAE 85.1 75.6 89.0 27.3 50.6 60.0 21.5 56.2 24.1 46.4 53.6
XciTS Normal 82.4 71.5 86.8 23.7 31.3 45.0 17.0 50.1 19.5 32.9 46.0
RB 73.3 60.5 80.6 12.7 6.3 45.7 9.7 28.5 18.4 31.2 36.7
XciTM Normal 82.6 71.0 86.8 23.4 33.3 44.7 17.7 50.5 20.3 33.1 46.3
RB 74.1 61.7 81.3 13.6 7.0 47.1 9.5 30.2 19.7 32.6 37.7
XciTL Normal 83.0 72.0 86.9 23.7 36.2 46.2 17.9 50.2 20.4 34.4 47.1
RB 75.1 62.7 81.7 13.4 8.8 49.0 10.7 32.0 19.9 34.4 38.7
T2T14 Normal 81.6 70.9 86.8 22.3 24.1 44.7 16.7 46.8 17.7 32.2 44.4
T2T19 Normal 82.3 71.6 87.2 23.2 29.0 47.3 18.0 50.2 20.9 34.4 46.4
T2T24 Normal 82.4 71.7 87.2 22.9 29.7 47.9 18.0 52.0 20.8 35.1 46.8
SwinS
Normal 83.2 72.1 87.5 24.7 33.0 44.9 19.3 45.1 16.8 32.0 45.8 Pre-train 83.3 73.5 88.6 28.1 43.9 54.8 21.3 50.6 17.2 41.2 50.3 AT 75.8 63.3 82.6 15.3 10.6 52.5 10.8 37.1 21.1 37.1 40.6
SwinB
Normal 83.4 72.3 87.6 25.5 35.8 46.6 20.2 45.6 17.9 32.4 46.7 Pre-train 85.1 75.2 89.1 28.8 51.8 59.1 22.7 56.4 19.6 45.1 53.3 AT 76.8 64.5 83.4 15.5 13.1 53.5 11.8 39.3 22.7 39.3 42.0
SwinL Pre-train 86.3 77.0 89.6 31.6 61.0 63.6 26.4 61.3 23.4 48.8 56.9
AT 78.7 66.9 84.9 18.2 18.1 57.3 11.6 43.4 25.2 42.9 44.7
increases from 34.1% of ResNet50 to 36.8% of ResNet101, and finally to 38.0% of ResNet-152.",1,related,1,positive
"C V
] 2
8 Fe
b 20
23
2 ARES-Bench
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
VGGs VGG13_Normal VGG16_Normal VGG19_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
ResNets ResNet50_Normal ResNet50_MOCO ResNet50_AT ResNet50_RB ResNet50_RL ResNet101_Normal ResNet101_AT ResNet152_Normal ResNet152_AT ResNet152_FD Wide-ResNet50_Normal Wide-ResNet50_AT Wide-ResNet50_RB
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
DenseNets DenseNet121_Normal DenseNet161_Normal DenseNet201_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(d)
ConvNexts ConvNextS_Normal ConvNextS_21K ConvNextS_AT ConvNextB_Normal ConvNextB_21K ConvNextB_AT ConvNextL_Normal ConvNextL_21K ConvNextL_AT
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(e)
ViTs ViTS_Normal ViTS_21K ViTS_AT ViTB_Normal ViTB_21K ViTB_MAE ViTB_AT ViTL_Normal ViTL_21K ViTL_MAE
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(f)
XciTs XciTS_Normal XciTS_RB XciTM_Normal XciTM_RB XciTL_Normal XciTL_RB
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (g)
T2Ts T2T14_Normal T2T19_Normal T2T24_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(h)
Swins SwinS_Normal SwinS_21K SwinS_AT SwinB_Normal SwinB_21K SwinB_AT SwinL_21K SwinL_AT
Fig.",1,related,1,positive
"Specifically, according to the pioneering work [19], given a natural image from an unlabeled dataset X, we divide it into N regular image patches, denoted as x ∈ RN×S where S denotes the patch size (e.",1,related,1,positive
"The training settings are the same as MAE [19], we adopt the same encoder-decoder structure to perform the MIM task.",1,related,0,negative
"For a fair comparison, we fully follow the hyperparameters of MAE [19] in our image classification experiments.",1,related,1,positive
"DeiT[14] proposes an effective receipt to train ViT with limited data, and MAE[15] adopts a masked autoencoder to pre-train the ViT.",1,related,0,negative
"We adopt the recipe in vanilla ViT[13], DeiT III[14], and MAE[15] to train ViTs.",1,related,0,negative
"Follow MAE (He et al., 2021), we evaluate the performance of the proposed Layer Grafted Pre-training with different number of fixing blocks.",1,related,0,negative
"…first step of Layer Grafted Pre-training, since it identically follows the original MIM pipeline, we directly adopt the pre-trained model of MAE (He et al., 2021) and conduct the second step, where we initialize the network with the MIM pre-trained model and train with Moco V3 (Chen et al.,…",1,related,1,positive
"For other settings such as data augmentation, LR scheduling or weight decay, we identically follow He et al. (2021).",1,related,1,positive
"…65.3 -
C-MAE (Huang et al., 2022) 73.9 65.3 77.3 MimCo (Zhou et al., 2022) 70.2 62.7 - Layer Grafted Pre-training (Ours) 77.7 65.5 77.8
ViT-L/16 MAE (He et al., 2021) 75.8 55.2 78.7
Moco V3 (Chen et al., 2021) 77.6 - - Layer Grafted Pre-training (Ours) 81.0 69.3 80.1
Our method also demonstrates…",1,related,1,positive
"For fine-tuning, we train with AdamW (Loshchilov & Hutter, 2017) for 100 epochs following MAE (He et al., 2021).",1,related,1,positive
"We by default adopt MAE (He et al., 2021) and Moco V3 (Chen et al., 2021) as our MIM and CL frameworks, respectively.",1,related,1,positive
"Then, the image with minimal augmentation would be utilized for computing MIM loss following MAE He et al. (2021).",1,related,1,positive
"In contrast, we employ MAE (He et al., 2021), an image modeling framework that is free from strong augmentations.",1,related,1,positive
"In this paper, we propose to predict the masked edges in Emask in training, inspired by the success of masked autoencoding in computer vision [23].",1,related,1,positive
"As shown in Figure 4, we adopt two reconstruct heads of simple linear layers to respectively recover the masked 2D pixels and 3D coordinates following MAE [He et al., 2021] and Point-MAE [Pang et al.",1,related,1,positive
"Additionally, we compared our method with some recent self-supervised methods, including MoCoV2 [32], MAE [13], and ConvMAE [33].",1,related,1,positive
"In this paper, we present VoxFormer, a strong camera-based 3D semantic scene completion (SSC) framework composed of (1) class-agnostic query proposal based on depth estimation and (2) class-specific segmentation with a sparse-to-dense MAE-like design.",1,related,1,positive
"Our framework is a two-stage cascade composed of class-agnostic proposals and class-specific segmentation similar to [68]: stage-1 generates class-agnostic query proposals, and stage-2 uses an MAE-like architecture to propagate information to all voxels.",1,related,1,positive
"Motivated by reconstruction-before-hallucination and sparsity-in-3D-space , we build a two-stage framework: stage-1 based on CNN proposes a sparse set of voxel queries from image depth to attend to images since the image features correspond to visible and occupied voxels instead of non-visible and empty ones; stage-2 based on Transformer uses an MAE-like architecture to first strengthen the featurization of the proposed voxels by voxel-to-image cross-attention, and then process the full set of voxels with self-attention to enable the voxel interactions.",1,related,1,positive
We name such learnable parameter as mask token [3] for conciseness since unselected from Q is analogous to masked from Q.,1,related,1,positive
Auto Encoder (MAE) [108] pre-trained model to initialize the ViT backbone.,1,related,1,positive
"Algorithmically, the Figure 2: Illustration of representative SSL methods: SimCLR [9], MoCo V3 [9], BYOL [15], and the Masked Auto-Encoder [10].",1,related,1,positive
"We further evaluate our defense under other popular SSL training algorithms and different model structures and datasets, e.g., ResNet-18 and ViT-Small/16 trained using SimCLR, MoCO V3, BYOL, MAE over CIFAR-10 or the ImageNet (Appendix 6.3).",1,related,1,positive
"Here we use self-supervised learning methods consistent with those in Section 4.3, including the contrastive learning method SimCLR, MoCO V3, BYOL, and the masked-model training method MAE.",1,related,1,positive
"For Case-1, we incorporate four state-of-the-art SSL training methods, i.e., SimCLR [9], MoCo V3 [14], BYOL [15], and the MAE [10], for evaluation.",1,related,1,positive
"For example, the accuracies of our model pre-trained with MAE were 8.68% and 5.16% higher than those of ResNet101 pre-trained with SimSiam in the five-class and binary classification tasks.",1,related,0,negative
"By leveraging a simple SSL framework, MAE, we alleviated the problem of training classification models without sufficient high-quality labeled OCT images.",1,related,1,positive
[35] as the initial weights of our model.,1,related,1,positive
"In the model pre-training stage, we transfer the weights of the MAE encoder and decoder pre-trained on the ImageNet-1 K dataset [35] (see the upper part of Fig.",1,related,1,positive
"For the classification task of cervical OCT images, we are
the first to propose a ViT-based image classification model pre-trained with a non-contrastive SSL framework, MAE, which can help ease the burden of insufficient labeled image data on the model’s prediction performance.",1,related,1,positive
A few essential parameters were configured in the selfsupervised model pre-training with MAE.,1,related,0,negative
"1(a) presents the self-supervised pre-training of our model with MAE, which includes a ViT encoder and a lightweight Transformer decoder.",1,related,1,positive
"For example, compared with the supervised MViT-B (the SOTA model) with the weights transferred from the ImageNet-1 K dataset, the five-class and binary classification accuracies of ViT-B (224) pre-trained with MAE were increased by 2.65% and 1.52%, respectively.",1,related,1,positive
"However, note that our approach is general and easily extends to self-supervised settings. e.g. (Chen et al., 2020; He et al., 2021; Chen et al., 2021).",1,related,1,positive
We only take the unmasked patches as the input of the encoder similar to MAE [9].,1,related,1,positive
"(Yang et al., 2022)
Oriented RCNN ViTAE 81.24 Use MAE (He et al., 2022) to pretrain the plain ViTAE transformer.",1,related,1,positive
"With the pretraining method MAE andRVSA, the detector outperformed all previousmethods, achieving 81.24% and 71.05%mAP onDOTA-V1.",1,related,0,negative
"First, it trains a ViT-based encoder fθ(·) on all images in X via self-supervised methods such as MAE [28].",1,related,1,positive
"For feature extraction, we use MAE [7].",1,related,1,positive
"For pre-training Edge MAE, we tuned the learning rate as 1.5e-4, weight decay as 0.05, the mask ratio as 1/3, and batch size as 2048.",1,related,0,negative
"From the two tables, we can see that, Edge Transformer outperforms almost all baseline models, and the results can be further improved by pre-training Edge MAE on unlabeled edges.",1,related,1,positive
"Furthermore, we propose a novel Edge Transformer model and pre-train the model via Masked Auto-Encoders (MAE).",1,related,1,positive
"In this section, we evaluate our Edge Transformer and Edge MAE in two settings.",1,related,1,positive
"We can see the proposed Edge MAE achieves the best results, and link prediction models perform better as they model the whole return process entirely.",1,related,1,positive
"4.2 Parameter Settings For Edge Transformer and Edge MAE, we set the latent vector size 𝐷 as 256, the dropout rate as 0.0, and the number of attention heads as 3.",1,related,1,positive
"Inspired by MAE [7] in CV which pre-trains masked auto-encoders on unlabeled images, we propose to improve Edge Transformer with an Edge MAEmodel.",1,related,1,positive
"For the MAEViT-Base (He et al., 2022) baseline, we report additional results for λ values 1, 10, and 50.",1,related,1,positive
"When η = 1.0, it means the number of features in the baseline is the same as the number of features learned using PISCO and as a result, similar for CIFAR-10 results in §D.2, we observe the in-distribution performance of PISCO in this case being almost the same as that of baseline methods even for higher values of λ.
MAE-ViT-Base results for λ values 1, 10, and 50: Results for additional values of λ when MAE-ViT-Base is the baseline are in Table 19.",1,related,1,positive
"We also report analogous results for another popular feature extractor, MAE-ViT-Base (He et al., 2022), in Table 5.",1,related,1,positive
"We train two types of auto-encoders (AEs) to reconstruct the images in the collection D, namely denoising AEs [73] and masked AEs [28].",1,related,1,positive
We employ Adam [36] and the mean squared error (MSE) to train both AEs.,1,related,1,positive
"First of all, we propose four novel pre-retrieval predictors, namely (i) the magnitude of the reconstruction error of denoising [73] or masked [28] auto-encoders trained on the database, (ii) the density of the k-means cluster to which the query image embedding is assigned, (iii) the confidence distribution of a classification head attached to the embedding layer of the retrieval model, and (iv) the score predicted by a fine-tuned ViT model [20].",1,related,1,positive
"We compare the soups to a nominal model, the l∞-robust classifier used in the soups, their ensemble, the Masked Autoencoders of [18], AdvProp [54], PyramidAT [24], and the ensemble obtained by averaging the output (after softmax) of the four models included in the soups.",1,related,1,positive
"Finally, while the capability of selecting a model specific to each image distribution is a main point of our model soups, we also show that it is possible to jointly select a soup for average performance across several IMAGENET variants to achieve better accuracy than adversarial and self-supervised baselines [18,24].",1,related,1,positive
Other alternatives carefully tweak R per dataset and architectures e.g. to only compute the reconstruction loss on parts of the data as with BERT Devlin et al. (2018) or MAEs He et al. (2022).,1,related,1,positive
"|D<t) where k ∈ [1, . . . ,K] on ImageNet for 4 different architecture and pre-training methods: ResNet50 + SimCLR, ResNet50 + BYOL, ViT/B16 + DINO and ViT/B16 + MAE.",1,related,1,positive
"For ResNet-50, we pre-train on supervised SimCLR and BYOL losses; for ViT/B16, we pre-train on supervised (Touvron et al., 2020; He et al., 2021) DINO and MAE losses.",1,related,1,positive
"Natural # Special # Structured #
ViT/B16 DINO 1.57 1 1.00 1 2.63 3 ResNet50 BYOL 2.71 2 3.00 2 2.13 1 ViT/B16 MAE 5.71 6 3.25 3 2.38 2 ViT/B16 SUP 2.86 3 3.25 4 5.75 6 ResNet50 SimCLR 4.29 5 5.75 6 3.38 4 ResNet50 SUP 3.86 4 4.75 5 4.75 5
ranks in Table 1b.",1,related,1,positive
"Following the command in Masked AutoEncoder [9], we use a lightweight decoder, which has only 10% computation per token compared with the encoder.",1,related,1,positive
"After the pre-training & fine-tune method [18] was proposed, more andmore experiments demonstrated its superior performance, so here, we also used the Masked AutoEncoder pre-trained ViT-base model.",1,related,0,negative
"As for the structure of our decoder, inspired by Masked AutoEncoder [9], Our decoder is also based on the attention algorithm, which has the same structure as the encoder(ViT-base) but is only 10% of its size.",1,related,1,positive
Proposed Masked AutoEncoder.,1,related,0,negative
"(2022), Z is obtained by clustering the latent features produced by a Masked Autoencoder (He et al., 2022).",1,related,1,positive
"For example, in Liu et al. (2022), Z is obtained by clustering the latent features produced by a Masked Autoencoder (He et al., 2022).",1,related,1,positive
"To further improve the robustness of SGLDM to different sketches, we adopt the arbitrarily masking conditional training strategy in training inspired by Masked AutoeEncoder [7], which masks random patches of the input and let the model reconstruct it automatically.",1,related,1,positive
"2022), we aim to adapt the data-efficient masked image autoencoder (MAE) (He et al. 2022) to multi-modal and small-scale RGB-D datasets for better representation learning.",1,related,1,positive
"Driven by the good self-supervised learning performance of MAE (He et al. 2022) in images, we design a multi-modal masked autoencoder (MM-MAE) in RGBD data.",1,related,1,positive
"Unlike the original MAE (He et al. 2022), our CoMAE presents a shared encoder and decoder among RGB and depth modalities and acts as a kind of regularizer to guide pre-training.",1,related,0,negative
"Inspired by the results in VideoMAE (Tong et al. 2022), we aim to adapt the data-efficient masked image autoencoder (MAE) (He et al. 2022) to multi-modal and small-scale RGB-D datasets for better representation learning.",1,related,1,positive
"In addition, our CoMAE instantiated with ViT-B also achieves
competitive and even better results of the same ViT-B model pre-trained by ImageMAE (He et al. 2022), this reveals the data efficiency advantage of our CoMAE pre-training.",1,related,1,positive
"Vision models We include 14 computer vision models (VMs) in our experiments, representing three families of models: SegFormer (Xie et al., 2021), MAE (He et al., 2022), and ResNet (He et al., 2016).",1,related,1,positive
We also compare M2A2E with the symmetric multimodal MAE [3] when finetuning on all downstream modality settings.,1,related,1,positive
"Compared with the vanilla MAE [18], M(2)A(2)E adopts the same masked strategy in unimodal ViT encoder but targeting at multimodal reconstruction with multiple unshared ViT decoders.",1,related,1,positive
"We find that 1) leveraging local feature descriptors benefits the ViT on IR modality; 2) partially finetuning or using adapters can achieve reasonable performance for ViT-based multimodal FAS but still far from satisfaction; and 3) mask autoencoder [3, 18] pre-training cannot provide better finetuning performance compared with ImageNet pre-trained models.",1,related,1,positive
"Following MAE [21], we use the normalized pixel values as targets in the reconstruction task.",1,related,1,positive
"Following MAE [21], we only feed the visible, unmasked patches to the online encoder Es.",1,related,0,negative
"For MIM tasks, we apply augmentations and masking strategy following MAE [21] to the input of the online encoder.",1,related,1,positive
"5, which is lower than the best configuration reported in (He et al., 2022).",1,related,0,negative
"The masking ratio used during pre-training was set to 0.5, which is lower than the best configuration reported in (He et al., 2022).",1,related,0,negative
"For our experiments, we use the CNN feature map (Santoro et al., 2017; Zambaldi et al., 2018) for end-to-end learned fixedregion representations and the mini-patch representations from pre-trained Masked AutoEncoder (MAE) (He et al., 2022) for pre-trained fixed-region representations.",1,related,1,positive
"The pre-training of the MAE encoder was conducted using a batch size of 128, a learning rate of 0.001, and a weight decay of 0.05.",1,related,0,negative
"In this work, we develop our pretraining objective based on a masked image modeling approach like [41, 18].",1,related,1,positive
We perform OOD detection with MIM pretext task with each metric – the results are shown in Tab.,1,related,1,positive
"For one-class OOD detection, we pre-train the MIM model and finely tune it on ImageNet-21k [49], as recommended by BEiT [2].",1,related,0,negative
"Specifically, we adopt the masked image modeling (MIM) [2, 11, 20] as our self-supervised pretext task, which has been demonstrated to have great potential in both natural language processing [11] and computer vision [2,20].",1,related,1,positive
"In our method, we pre-train the model with Masked Image Modeling (MIM) pretext [11] on a large dataset and fine-tune it on the ID dataset.",1,related,1,positive
"Specifically, we adopt the masked image modeling (MIM) [2,11,20] as our self-supervised pretext task, which has been demonstrated to have great potential in both natu-ral language processing [11] and computer vision [2,20].",1,related,1,positive
We compare the performance of MIM and contrastive learning pretext task MoCov3 [8] in Tab.,1,related,1,positive
"In the MIM task, we split images into patches and randomly mask a proportion of image patches before feeding the corrupted input to the vision transformer.",1,related,1,positive
"For multi-class OOD detection, we pre-train the MIM model, intermediately use fine-tuning on ImageNet-21k [49], and apply fine-tuning again on the ID dataset.",1,related,1,positive
The self-supervised pretext task in our framework is Masked Image Modeling (MIM).,1,related,1,positive
"Inspired by the tremendous success of the masked autoencoding paradigm in NLP [19] and Computer Vision [32], we propose to incorporate a new masked autoencoder model into our generalized framework.",1,related,1,positive
"paradigm in NLP [19] and Computer Vision [32], we propose to incorporate a new masked autoencoder model into our generalized framework.",1,related,1,positive
"Notably, whenγ = 1, our masked autoencoder is equivalent to MAE for vision [32].",1,related,1,positive
"We also hope to incorporate selfsupervised methods that learn representations by training on a union of unlabeled data from source and target via proxy tasks like reconstruction (Gidaris et al., 2018; He et al., 2022) and contrastive learning (Caron et al.",1,related,1,positive
"We also hope to incorporate self-supervised methods that learn representations by training on a union of unlabeled data from source and target via proxy tasks like reconstruction (Gidaris et al., 2018; He et al., 2022) and contrastive learning (Caron et al., 2020; Chen et al., 2020).",1,related,1,positive
"Pretrained MAE and CLIP with world model (MAE+WM, CLIP+WM)
Masked autoencoder (MAE; He et al. 2021) learns visual representation in a self-supervised manner by training a vision Transformer (ViT; Dosovitskiy et al. 2021) to reconstruct masked patches.",1,related,1,positive
"Convolutional feature embedding Unlike prior work that masks random pixel patches (He et al., 2021), we embed camera observations into convolutional feature maps and mask these features following the design of Seo et al. (2022a).",1,related,1,positive
"Specifically, we consider CLIP (Radford et al., 2021) and MAE (He et al., 2021) representations as they have recently shown to be effective for robotic manipulation (Shridhar et al., 2021; Radosavovic et al., 2022).",1,related,1,positive
"We hypothesize this is because spatial information redundancy (He et al., 2021) is more significant in visual observations from
manipulation tasks than natural images.",1,related,1,positive
"We hypothesize this is because spatial information redundancy (He et al., 2021) is more significant in visual observations from manipulation tasks than natural images.",1,related,1,positive
"We also incorporate the idea of a prior work (Seo et al., 2022a) that masks convolutional features instead of pixel patches (He et al., 2021) and predicts rewards to learn representations capturing fine-grained details required for visual control.",1,related,1,positive
"Convolutional feature embedding Unlike prior work that masks random pixel patches (He et al., 2021), we embed camera observations into convolutional feature maps and mask these features following the design of Seo et al.",1,related,1,positive
"R O
] 3
1 M
ay 2
02 3
experiments on RLBench (James et al., 2020) show that our method outperforms single-view representation learning baselines (Radford et al., 2021; He et al., 2021; Seo et al., 2022a) and a multi-view representation learning baseline (Sermanet et al., 2018).",1,related,1,positive
", 2020) show that our method outperforms single-view representation learning baselines (Radford et al., 2021; He et al., 2021; Seo et al., 2022a) and a multi-view representation learning baseline (Sermanet et al.",1,related,1,positive
"We then perform extensive experiments on CIFAR10 and CIFAR100 datasets with various representative pre-trained vision models (He et al., 2016; Dosovitskiy et al., 2021; Chen et al., 2020d;c; Radford et al., 2021; Xie et al., 2022; He et al., 2022), which are trained in supervised or self-supervised manners.",1,related,1,positive
"1, we empirically analyze the main assumptions of our theory in various deep vision models (Dosovitskiy et al., 2021; He et al., 2016; Radford et al., 2021; Chen et al., 2020d;c; He et al., 2022; Xie et al., 2022).",1,related,1,positive
"com/facebookresearch/mae (He et al., 2022) License https://github.",1,related,0,negative
"We adopt pre-trained checkpoint in (He et al., 2022).",1,related,0,negative
"(3) (4)Note that for methods like MAE (He et al., 2022; Pang et al., 2022), the teacher network is an identity mapping with no parameters, and therefore φ is an empty set.",1,related,1,positive
"(3) 4Note that for methods like MAE (He et al., 2022; Pang et al., 2022), the teacher network is an identity mapping with no parameters, and therefore φ is an empty set.",1,related,1,positive
"The masked ratio can be even higher than the original configuration described in MAE (He et al., 2022) (e.",1,related,1,positive
", 2021) and MAE (He et al., 2022), we adopted AdamW (Loshchilov & Hutter, 2017) for pre-training, fine-tuning.",1,related,0,negative
"According to the setup in MoCo v3 and MAE (He et al., 2022), both adopted a batch size of 4,096.",1,related,0,negative
"To address the representation deficiency issue in MLM, we propose a simple framework MAE-LM, which pretrains bidirectional Transformer encoders using the MLM objective, but based on the Masked Autoencoder (He et al., 2022) structure.",1,related,1,positive
"To address the representation deficiency issue, we propose a simple text encoder pretraining method, MAE-LM, which conducts MLM pretraining based on the Masked Autoencoder architecture (He et al., 2022).",1,related,1,positive
"AE + SR 16 denotes a baseline experiment with a auto-encoder architecture as in He
et al. (2022).",1,related,1,positive
"In Table 3, we compare our method against DINO Caron et al. (2021), MoCo-V3 Chen et al. (2021), MaskFeat Wei et al. (2021), BEiT Bao et al. (2021), iBOT Zhou et al. (2022), and MAE He et al. (2022).",1,related,1,positive
"Unlike recent methods Zhou et al. (2022); He et al. (2022), we do not perform exhaustive searches for the optimal hyperparameters such as learning rates.",1,related,1,positive
The result of MAE He et al. (2022) with 400 epochs is based on our reimplementation.,1,related,0,negative
"(2021a); Ho et al. (2020) with ↓ (x) = √γx + √ 1− γε, with ε ∼ N (0, I) and γ uniformly sampled as γ ∼ U(0, 1).",1,related,1,positive
"We follow the details presented in MAE He et al. (2022) and implement an asymmetric
Methods GPUs × H Acc.",1,related,1,positive
"To show the effectiveness of noisy image modeling in visual feature learning and adversarial defense, we adopt two simple, representative MIM methods, SimMIM [24] and MAE [10] for comparison.",1,related,1,positive
"In our study, the random sampling masking ratio is 75% [37].",1,related,0,negative
"Referring to the research about the decoder in pretraining in MAE [37], combined with the task of our work, we design an extremely lightweight encoder.",1,related,0,negative
"Because the reconstruction part appears only in pretraining and is not used in the sequence-based inference, we follow He et al. (2022) and use a lightweight decoder with fewer channels in Revolution.",1,related,1,positive
"Inspired by the work of Masked Autoencoder [20], our transformer can be divided into an encoder part and a decoder
1 3
part.",1,related,1,positive
"Inspired by the work of Masked Autoencoder [20], our transformer can be divided into an encoder part and a decoder Fig.",1,related,1,positive
"Inspired by these, we inherit the recent success of spectrogram SSL in the frequency domain, which guarantees efficient compression and high-level semantic understanding.",1,related,1,positive
"Following He et al. (2022), we split low-resolution radiograph into nonoverlapping image patches, where 75% patches are randomly masked.",1,related,1,positive
"In Figure 2, we provide relation maps of three samples from ImageNet, using 10 checkpoints of MAE-Large [13].",1,related,1,positive
2 Results and analysis ImageNet We measure the label error detection performance on ImageNet with the synthetic label noise by training an MAE-Large model [13].,1,related,1,positive
"We measure the detection performance with MAE-Large [13], BEIT-Large [1], and ConvNeXt-Large [29] models.",1,related,1,positive
"For ADE20K, the input size is set to 512×512 following previous works (Bao et al., 2021; He et al., 2021; Chen et al., 2022a; Zhou et al., 2021).",1,related,1,positive
"…reconstruct the targets.
methods:
`MIM(Decoder(Regressor(Encoder(Rv))),Target(Rm)), (3) where Target(Rm) is a function to map the masked patches to the targets, e.g., d-VAE (Ramesh et al., 2021) token used in CAE and BeiT (Bao et al., 2021), or normalized RGB values used in MAE (He et al., 2021).",1,related,1,positive
"The main tracker only consists of a ViT backbone and a box estimation head, we test both ViTBase and ViT-Large, and the ViT parameters are initialized with MAE (He et al. 2022) pre-trained model.",1,related,1,positive
"Our approach is inspired by the previous MIM method (Xie et al. 2022; He et al. 2022), but we have to deal with two fundamental problems in the tracking framework: (1) Visual tracking is a downstream vision task that generally does not have the pre-train process to apply the MIM strategy.",1,related,0,negative
We only use the pretrained encoder part to extract the image features [32].,1,related,1,positive
"Note, we have not demonstrated it here, but Zorro can also be trained using unimodal self-supervised methods such as MAE [25] and DINO [12] separately on the audio and visual streams.",1,related,0,negative
"Motivated by the great success of other MAE-style approaches (He et al., 2021; Feichtenhofer et al., 2022; Hou et al., 2022), we also adopt an asymmetric design that the encoder only operates visible tokens after applying masking on input embedding, and a lighter decoder processes encoded tokens…",1,related,1,positive
"It is hypothesized and summarized in (He et al., 2021; Feichtenhofer et al., 2022) that the masking ratio is related to the information density and redundancy of the data, which has an immense impact on the performance of the autoencoders.",1,related,1,positive
Our encoder/predictor architecture is reminiscent of the generative masked autoencoders (MAE) [35] method.,1,related,1,positive
"Methods without view data augmentations data2vec [7] ViT-L/16 1600 73.3
MAE [35] ViT-L/16 1600 67.1 ViT-H/14 1600 71.5
trained with I-JEPA requires less computational effort than a ViT-Small/16 trained with iBOT.",1,related,0,negative
I-JEPA outperforms MAE while requiring less pretraining epochs when using a similar encoder architecture.,1,related,0,negative
"Methods without view data augmentations data2vec [7] ViT-L/16 1600 77.3
MAE [35] ViT-B/16 1600 68.0 ViT-L/16 1600 76.0 ViT-H/14 1600 77.2
CAE [21] ViT-B/16 1600 70.4 ViT-L/16 1600 78.1
Closest to our work is data2vec [7] and Context Autoencoders [24].",1,related,1,positive
We draw inspiration from MAE [19] for this design.,1,related,0,negative
We draw inspiration from MAE [20] for this design.,1,related,0,negative
"Our decoder is a lighter-weight 8-layer 512-dimensional transformer, following MAE [19].",1,related,1,positive
Our decoder follows the decoder design from MAE [20].,1,related,1,positive
"For the image reconstruction loss, we normalize the ground-truth per-patch, following [38].",1,related,1,positive
"During finetuning, we use a layer-wise learning rate decay, following [17, 38, 3].",1,related,1,positive
"We find that a combination of two state of the art approaches: masked auto-encoders, MAE [38] and contrastive language image pre-training, CLIP [69] provides a benefit over CLIP when trained on a corpus of 11.",1,related,1,positive
"For all models which rely on masking, we use a 75% masking ratio, consistent with [38, 31], as we did not find an alternative that improved downstream results.",1,related,0,negative
"As in [38] we experiment with predicting both normalized and un-normalized patch values, finding that predicting the normalized patch value slightly improves the performance of our MAE implementation.",1,related,1,positive
"For evaluation on CIFAR-10, CIFAR-100, Tiny Imagenet-100 and CBIS-DDSM datasets, we use the linear probing strategy (He et al., 2021).",1,related,1,positive
"For evaluation on CIFAR-10, CIFAR-100, Tiny Imagenet-100, and chest X-ray datasets, we use the linear probing strategy (He et al., 2021).",1,related,1,positive
"3 exhibits the performance of PARSeq with CLIPTER, when leveraging the vision-based image encoders of DiNO, ViT-MAE and OWL-ViT, and when using the visionlanguage models of CLIP, BLIP and GIT.",1,related,1,positive
"When transferring the representation to ImageNet-1K [18], we follow the widely used fine-tuning recipe introduced by [5], [24].",1,related,1,positive
"For single-modal SSL methods, we choose MoCoV3 [14] and SimCLR [11] as representative discriminative methods, and MAE [24] as a representative generative method.",1,related,1,positive
"During pre-training, input images are resized to 224× 224 and we set random mask ratio to 75% following [28].",1,related,1,positive
"Among numerous architecture designing spaces, without loss of generalization, we adopt an asymmetric encoderdecoder architecture following MAE [28] and a dualencoder architecture following CLIP [47] for their flexibility.",1,related,1,positive
We follow most of setups in [28] for fine-tuning.,1,related,1,positive
"Meanwhile, following MAE [28], we randomly mask a large portion of image patches and leave the remaining patches to be",1,related,1,positive
We choose MAE [28] and CLIP [47] as representative methods of masked image modeling and vision-language contrastive learning.,1,related,1,positive
"Inspired by MAE [12], we propose a naive background reconstruction method (NBR).",1,related,1,positive
"Therefore, we utilize a mature but large Vision Transformer (MAE-ViT-large [12]) for background reconstruction.",1,related,0,negative
"3) Comparison With Feature Extraction: We use pre-trained MAE10 [72] and SimCLR11 [73] models to extract features for the NUS-WIDE dataset and obtain 768- and 2,048dimensional feature representations respectively.",1,related,1,positive
"3) Comparison With Feature Extraction: We use pre-trained MAE10 [72] and SimCLR11 [73] models to extract features for the NUS-WIDE dataset and obtain 768- and 2,048- dimensional feature representations respectively.",1,related,1,positive
"We compute the loss only on masked parts, similar to BERT (Devlin et al., 2019) and MEA (He et al., 2021), so the model output on unmasked parts is qualitatively worse.",1,related,1,positive
"Our research is based on masked autoencoding, which is a form of more general denoising autoencoding (He et al., 2021).",1,related,1,positive
"We also observe calculating loss values only on masked patches gives higher accuracy (row 6), which is consistent with He et al. (2021).",1,related,1,positive
"We try to conclude that in essence, the difficulty is rooted in the gap between language and vision in terms of data processing (Bateman, 2014; He et al., 2021).",1,related,1,positive
"One may therefore ask: what exactly is preventing the application of BERT to convnets? We try to conclude that in essence, the difficulty is rooted in the gap between language and vision in terms of data processing (Bateman, 2014; He et al., 2021).",1,related,1,positive
"We then remove the hierarchical design (row 4), which results in a single-scale masked modeling that is commonly used
for transformers (Devlin et al., 2018; Bao et al., 2021; He et al., 2021; Xie et al., 2021).",1,related,1,positive
"As vision encoder, we consider (1) ViT-B/16 [7] (patch size of 16×16 pixels) with pre-trained weights from self-supervised MoCo-v3 [5], DINO [2] and MAE [10], all trained on IN-1K but without any labels.",1,related,1,positive
"For the downstream task of OAR segmentation, we employed the ViT backbone and UperNet [20] decoder as the encoder and decoder parts of the segmentation model, following the implementation described in a previous work [10].",1,related,1,positive
"Specifically, we replace the multi-crop strategy with a random masked sampling strategy, consistent with pioneering SSL approaches that use masked image modeling with ViT in a patch-wise manner [9], [10].",1,related,1,positive
"Then for enhancing the learning of visual representations, we introduce a masked autoencoder (MAE)[8] branch, which is parallel to the SSL framework and they share the same encoder.",1,related,1,positive
"Therefore, we investigate other MIM methods besides MAE[8] and observe that LoMaR[18] can further boost the model performance by 0.",1,related,1,positive
Therefore we introduce a masked autoencoder (MAE)[8] branch to enforce the visual representation learning and help ViT generate more accurate pseudo,1,related,1,positive
"As for the MAE branch, we follow the default settings of [8].",1,related,1,positive
"For data augmentation, we follow the settings in MAE [18].",1,related,1,positive
"In this work, we adopt MAE [18] as the MIM model due to its popularity and simplicity.",1,related,1,positive
"Therefore, instead of following the common transferring assumption, we revisit the old good idea of training with indomain egocentric data only, but this time in light of the development of recent data-efficient training methods, such as masked autoencoders [33, 63, 28] as well as the scale growth of egocentric data collections (e.",1,related,1,positive
Our method applies the original MAE [33] and video MAE [28] algorithms.,1,related,1,positive
"4, we visualize the MAE [33, 28] reconstruction results on a few Ego4D [31] examples with a ViT-B [25] trained for 200 epochs without per-patch normalization.",1,related,1,positive
"In image representation, MAE [24] and SimMIM [25] use the random masking strategy to discard or replace the selected patches and in this paper, we adopt the former approach for selected frames.",1,related,1,positive
"It is worth noting that, unlike MAE [24] or SimMIM [25], we do notmake predictions for themasked sequences at the pixel level, choosing to reconstruct the input at the representation level in an implicit way and ensuring that the pair of masked sequences can be as close as possible in the representation.",1,related,1,positive
"Inspired by the masked auto-encoders [24], we consider the masking mechanism as a way of data augmentation and combine it with contrastive learning framework; the frame-",1,related,1,positive
"Compared to the plain ViT pre-trained with MAE [31], our approach performs similarly up to the Large model regime, despite using much fewer parameters (198M vs 307M).",1,related,0,negative
"We compare our approach to previous masked auto-encoder methods [3, 31, 77], which were all designed for transformer-based models.",1,related,1,positive
"To perform this analysis, we randomly select 1,000 images from different classes in the ImageNet-1K validation set and extract the high-dimensional features from each layer of different models, including the FCMAE models, the ConvNeXt supervised model [52] and the MAE pretrained ViT model [31].",1,related,1,positive
We retrained the CSWin transformer without redesign and original transformer [9] separately and compared them with our redesigned,1,related,0,negative
"In this work, we tackle an active visual exploration problem using a vision transformer model [15] as the architectural backbone.",1,related,1,positive
"Table 3: MAE Attention as glimpse: We use CLS attention of pre-trained MAE[15] as the glimpse map and use locations corresponding to the max value, min value, median value, and random value as the next glimpse location.",1,related,1,positive
We study the use of CLS attention map as an alternative for the learned glimpse map through a set of heuristics to use the base MAE model for active visual exploration.,1,related,1,positive
"Since the pretext task for training MAE, i.e. random masking and pixel value prediction for masked regions, resembles the partial observability constraint that we are trying to solve, we find MAE encoder to be best suited for our context extractor module and the pre-trained weights to be transferable for our use case.",1,related,1,positive
"We build our context encoder on the mask-auto-encoder (MAE) [15] model, pre-trained as a masked-image-model for the image reconstruction pretext task.",1,related,1,positive
"In this work, we evaluate our method on the widely studied task of image reconstruction,
We first compare against a baseline where the base MAE model with random glimpse selection is finetuned on the SUN360 and ADE20K datasets, denoted by ‘Random glimpse’ in Table 1.",1,related,1,positive
"Next, in addition to finetuning the task module and the context extractor, initialized with MAE weights, we train the glimpse selection module to predict the loss of the task module (i.e reconstruction loss).",1,related,1,positive
"While the random selection outperforms other heuristics for base MAE, we see that random selection of glimpse performs inferior to a learned glimpse selection policy, Table 1.",1,related,1,positive
Analysis for the importance of self-supervised feature: Our encoder and decoder are initialized with pretrained self-supervised weights [15].,1,related,1,positive
"As we intend to evaluate the use of base MAE for active vision, we do not finetune it on SUN360 dataset.",1,related,0,negative
9% on Imagenet 1000 class classification [15].,1,related,0,negative
"We show that vision transformer mod-
els, and in particular MAE, trained on large unlabelled data can replace contemporary CNN-based counterparts.",1,related,1,positive
We therefore use MAE’s decoder as our task module.,1,related,1,positive
Our context extractor is a ViT [12] initialized with MAE’s encoder weights.,1,related,1,positive
"2) Generating Input Tokens: According to ViT [52], a trainable and lightweight projection should be applied to embedding patches, denoted as F .",1,related,1,positive
"Baselines We benchmark the performance of our two baselines - Average Pairwise Feature Distance (Distance) and Cluster Assignment Distribution (Cluster) using five different feature extractors for each: pixels (φ ), three intermediate ResNext-101 [32] convolutional feature layers (φ, φ, and φ), and features from the encoder output of a Masked Autoencoder [13] (φ ).",1,related,1,positive
"Finally, φ extracts the per-patch features produced by the encoder of a large Transformer model (we used Masked AutoEncoder [13]).",1,related,1,positive
"Finally, ϕT extracts the per-patch features produced by the encoder of a large Transformer model (we used Masked AutoEncoder [13]).",1,related,1,positive
"Baselines We benchmark the performance of our two baselines - Average Pairwise Feature Distance (Distance) and Cluster Assignment Distribution (Cluster) using five different feature extractors for each: pixels (ϕP ), three intermediate ResNext-101 [32] convolutional feature layers (ϕR(1), ϕR(2), and ϕR(3)), and features from the encoder output of a Masked Autoencoder [13] (ϕT ).",1,related,1,positive
"Specifically, we pre-train models based on MAE [14] for 100 epochs on ImageNet-1k [17] with uniform masking and plot the corresponding convergence curve on the validation set during fine-tuning.",1,related,1,positive
"We pre-train models on ImageNet1K [17] following the settings of MAE [14], where the decoder TABLE I ABLATION STUDY ON THE ADAPTIVE LEARNING RATE SCALE RULE.",1,related,1,positive
1) Disjoint Masking: We evaluate DM based on MAE [14] with normalized pixels as reconstruction targets and only pretrain 100 epochs for fast ablation study on ImageNet-1K.,1,related,1,positive
"Following a standard SSL evaluation recipe [14], [21], we conduct end-to-end fine-tuning or linear probing for classification on ImageNet-1K and transfer learning for object detection/instance segmentation on COCO [56] and semantic segmentation on ADE20k [57].",1,related,1,positive
"In Table III, we evaluate the training efficiency of DM based on several MIM methods with their original pre-training recipes, including MAE [14], BEiT [10], SimMIM [15], and MaskFeat [16].",1,related,1,positive
"Another line of work is completionbased [25, 31, 36, 55, 58, 60] methods, which get inspiration from Masked Autoencoders [14].",1,related,1,positive
"Particularly, in the self-supervised setting, e.g., as in MAE (He et al., 2022), we identify a condition number that is pivotal to solving downstream tasks.",1,related,1,positive
", as in MAE (He et al., 2022), we identify a condition number that is pivotal to solving downstream tasks.",1,related,1,positive
Scale-MAE is a selfsupervised pretraining framework based on the Masked Autoencoder (MAE) [25].,1,related,1,positive
"As a result, we develop an asymmetric encoder-decoder architecture similar to MAE [15].",1,related,1,positive
"Therefore, in this framework, we construct a semi-supervised cross-algorithm ensemble method for lesion identification in UW-OCTA DR images based on MAE, ConvNeXt, and SegFormer algorithms[10,24,13,5,6].",1,related,1,positive
"In this framework, first, we pre-train the MAE algorithm [10] on the DR grade assessment dataset in the diabetic retinopathy analysis challenge (DRAC) 2022.",1,related,0,negative
"After the pre-training, we would extract the backbone of the MAE encoder (also considered as pre-trained MAE-ViT) [9,10].",1,related,0,negative
"Then, the MAE encoder [10] performs feature extraction on the visible patches subset of the image.",1,related,1,positive
"We conduct experiments using MVTN with ResNet-50 [37] and ViT [21] as backbone networks, starting from scratch or using weights from ImageNet [73], Dino [9], and Masked Autoencoders (MAE) [36] as initial weights.",1,related,1,positive
"Figure 1: Architecture of the Masked Autoencoder (MAE) in [8], with Vision Transformer (ViT) backbone.",1,related,1,positive
We divide an image into regular non-overlapping patches as in [8] and then calculate the gradient sum of each patch and rank them by its gradient sum.,1,related,1,positive
"Inspired by He et al[7], we propose a full convolutional neural network[16] based cross-modal text feature extractor(TFE).",1,related,1,positive
"One is a decoder (He et al., 2022) which reconstructs the input image, the other is a linear classifier.",1,related,1,positive
"Secondly, we compared the classical MIM-based methods such as MAE (ViT-B) (He et al., 2022) and SimMIM (Swin-B) (Xie et al.",1,related,1,positive
"We denote the methods with * when they are adapted to Point Transformer backbone, e.g., “PointMAE [39]* + PointTrans”.",1,related,1,positive
"With the same consistency loss, PointMAE gets 71.0%, which is 0.9% lower than ours.",1,related,0,negative
"In our experiments, RGMIM and MAE used the ViT-Base model.",1,related,0,negative
"Compared with the vision transformer-based methods RGMIM [69] and MAE [70], although we use the traditional and straightforward ResNet model, our method outperformed them, especially when the amount of annotated data was significantly reduced.",1,related,1,positive
"We employ the vanilla ViT model (Dosovitskiy et al., 2021) as the backbone of our audio SSL models without heavy
structure engineering, and apply the speed-up technique proposed in He et al. (2022).",1,related,1,positive
"A learnable corruption embedding e[M] is used to replace the masked position, with which the corrupted representation ZM = 1(M) e[M] + 1(1−M) T is input to encoder (Devlin et al., 2019) or decoder (He et al., 2022b)2.",1,related,1,positive
"We find that this leads to an inferior result, consistent with the observation in 2D that data of low semantics requires a non-trivial decoder for modeling purpose (He et al., 2022b).",1,related,1,positive
"The objective is to train the a student encoder fS to predict/reconstruct the output from a teacher encoder fT , where the teacher could be a discrete variational autoencoder (dVAE) (Bao et al., 2022) or simply identity mapping (He et al., 2022b).",1,related,1,positive
"D (7)
With this parameter-efficient prompt tuning strategy, we are able to tune the pretrained foundational Transformer while preserving as much pretrained knowledge as possible (He et al., 2022a).",1,related,1,positive
"To comprehensively compare our proposed model with other state-of-the-art methods, We use four widely used metrics to evaluate our method: structuremeasure (Sm) [6], E-measure (Em) [7], weighted F-measure (Fωβ ) [19], and mean absolute error (MAE).",1,related,1,positive
"We note that this budget constraint is similar to MAE [10] and other random masking schemes, which choose to randomly mask out a fixed 75% of the image.",1,related,1,positive
"To learn robust audio-video representations, we go beyond raw input reconstruction in uni-model MAEs [2, 4, 55], multimodal MAE [56], and CAV-MAE [41].",1,related,1,positive
"Following MAE [2], we only keep the pre-trained encoders and use the average-pooled top-layer outputs for classification.",1,related,0,negative
We notice a concurrent and independent study CAV-MAE [41] uses inter-modal contrastive objective and MAE [2] to reconstruct raw inputs.,1,related,1,positive
"For the visual branch, we either randomly initialize it or initialize it with the self-supervised MAE [2] pre-trained on ImageNet (compared in Table 6).",1,related,1,positive
"Furthermore, inspired by He et al. (2021), we explore enhancing the visual encoder via randomly masking the input image tokens and then reconstructing them, which can help reduce the computation cost during training and boost visual embedding by maintaining low-level visual information.",1,related,1,positive
MAE means MAE unsupervised pretraining [30] on the MillionAID [54].,1,related,0,negative
"We followed the data augmentation scheme of the MAE[21] fine-tuning phase, and the rest of the settings were kept consistent with the pre-training phase.",1,related,0,negative
"In Table 11, we compare our results with previous supervised pretraining methods[18, 29], self-supervised MIM methods [2, 10, 3, 7, 5] and CLIP-based MIM methods [25, 31, 13, 24] (i.",1,related,1,positive
We extend the masked-autoencoding framework [41] to learn audiovisual feature representations that can be leveraged for both multimodal and unimodal downstream tasks.,1,related,1,positive
"In the masked autoencoding framework [41], the input, x, is tokenised following previous supervised learning setups [6, 26, 33].",1,related,1,positive
"In the masked autoencoding framework [10, 29, 41], the decoder is another transformer that reconstructs the masked tokens given the encoded tokens as context.",1,related,1,positive
"Additional standardisation may also be applied to x̃ [41, 76].",1,related,0,negative
"Our approach is inspired by the masked autoencoding framework [10,41], which itself is based on similar masked data modelling approaches in NLP [24] and earlier works ar X iv :2 21 2.",1,related,1,positive
We begin with an overview of masked autoencoders [41] and transformers in vision in Sec.,1,related,1,positive
We follow the training strategy in MAE [31] and VideoMAE [21] for image teachers and video teachers respectively.,1,related,0,negative
"In this paper, we follow the decoupled encoder-decoder transformer architecture in MAE [31] due to its effectiveness and simplicity.",1,related,1,positive
"For obtaining spatial targets, we adopt the vanilla image ViT pretrained by masked image modeling [31] on the image dataset (e.",1,related,1,positive
"To show the feasibility of the concept, we explore the adoption of masked autoencoder (MAE) structure [21] for image reconstruction, which achieves state-of-the-art performance in the self-supervised learning regime.",1,related,1,positive
"1, we compare the pooling strategies based on the scratch training and fine-tuning on pre-trained models by Self-Supervised Learning (SSL), including MAE [27], BeiT [3], SimMIM [77] and Data2Vec [2].",1,related,1,positive
"Every SSL model are pre-trained with ImageNet-1K, except BeiT [3] with † used ImageNet-22K.",1,related,1,positive
We ablate the type of visual representation and prior use by trying an initialization using the VGG16 network [68] (VideoDex-VGG) and the MVP network [7] [69] (VideoDex-MVP) based representation trained for robot learning.,1,related,1,positive
"Inspired by the representation bottleneck and the mask-based method MAE [45], we propose a training pipeline based on the mask to boost data augmentation.",1,related,1,positive
"Inspired by the representation bottleneck and the mask-based method MAE [45], we propose a training",1,related,1,positive
"We also compare Deep Incubation with the recently proposed improved E2E baselines in [15], where a systematically hyper-parameter search is performed on training configurations.",1,related,1,positive
We follow the design choice of MAE [23] and VideoMAE [50] that skips the video mask token [M] in the encoder and then insert it in the decoder.,1,related,1,positive
"Following [23,50], we only apply the encoder on visible video tokens, a small subset (e.",1,related,1,positive
"Inspired by the success of MAE [18] in 2D images, we develop the masked autoencoder for self-supervised learning on LiDAR point clouds, as shown in Figure 4.",1,related,1,positive
"To set a baseline with the transformer decoder, we follow MAE [18] and some existing works [12,18,19,38,73] to design the pipeline, as shown in Figure 5(a).",1,related,1,positive
"Then, we preserve the encoder part of MAE as our shared face encoder and fix it all the time during training.",1,related,0,negative
"Thus, we opt to employ a pre-trained masked autoencoder (MAE) (He et al. 2022) to extract facial features that better capture facial appearances and identity information.",1,related,0,negative
"In contrast to the ViT decoder in MAE, we find the convolutional decoder achieves more realistic results.",1,related,1,positive
"Compared to the compact latent code of StyleGAN2 (Karras et al. 2020) and the identity embedding, the latent space of MAE can better capture facial appearances and identity information, because masked training requires reconstructing masked image patches from visible neighboring patches, thus ensuring each patch embedding contains rich topology and semantic information.",1,related,1,positive
The encoder is designed following MAE (He et al. 2022) and pre-trained on a large-scale face dataset using the masked training strategy.,1,related,0,negative
"As for F swa, we first pre-trained the face encoder following the training strategy of MAE on our face dataset.",1,related,0,negative
"To verify the superiority of using the latent representation of MAE, we train a new model which adopts the identity embedding as the identity representation and employs AdaIN as the injection method.",1,related,1,positive
"We first train our face masked autoencoder, following the same settings in MAE4 (He et al. 2022).",1,related,0,negative
"To demonstrate the robustness of VQT to different pretraining setups, we also evaluate VQT on self-supervised (MAE) [20] and image-language (CLIP) pre-trained [39]",1,related,1,positive
"Moreover, in order to demonstrate the robustness of the compatibility, we evaluate performance on three different pre-trained backbones: self-supervised pre-trained (MAE with ImageNet1K) [20], image-language pre-trained (CLIP) [39] and supervised pre-trained (ImageNet-21K).",1,related,1,positive
"To demonstrate the robustness of VQT to different pretraining setups, we also evaluate VQT on self-supervised (MAE) [20] and image-language (CLIP) pre-trained [39]
backbones.",1,related,1,positive
We make the following modifications to the MAE decoding process to customize it for document image generation and our task unification framework: (4.a) Cross-Attention with Character Embeddings.,1,related,1,positive
"Next, we describe the MAE decoding process.",1,related,1,positive
"For the vision decoder, we adopt the decoder of MAE [14] and directly generate the image pixels with text and layout information.",1,related,1,positive
We adopt the MAE objective [14] for vision selfsupervised learning.,1,related,0,negative
"Thus, here we directly apply a standard masked image modeling (MIM) pipeline [5, 19, 48] for training, illustrated in Figure 2.",1,related,1,positive
"We show that LOCA yields improved performance over state-of-the-art supervised [53,60,65] and unsupervised [13,17,34,84] representation learning methods for ViTs when transferred to 11 diverse and challenging semantic segmentation benchmarks.",1,related,1,positive
"In terms of training efficiency, based on our implementation, one LOCA epoch takes 17.4 minutes while one MAE epoch takes 5.7 minutes.",1,related,0,negative
"Indeed, we have observed that freezing the backbone and training a linear classifier on top of MAE features perform very poorly [34].",1,related,1,positive
"In this section, we compare LOCA to popular state-ofthe-art SSL models for ViTs: DINO [13], MoCo-v3 [17], MAE [34] and iBOT [84].",1,related,1,positive
"Under the longer training schedule (800 epochs), our model reaches 83.9% accuracy, 0.4% higher than MAE (He et al. 2021) and
0.9% higher than RandSAC (Hua et al. 2022) (a concurrent autoregressive work of ours).",1,related,0,negative
"For linear probing, we use the feature of the last block and adopt an extra BatchNorm (Ioffe and Szegedy 2015) layer before the linear classifier following MAE.",1,related,1,positive
"We followed the pre-training parameters in MAE (He et al. 2021) and MaskFeat (Wei et al. 2021) without using color jittering, drop path, and gradient clip.",1,related,1,positive
"Our model pretrained with 300 epochs achieves the same accuracy as MAE (He et al. 2021) pretrained with 1600 epochs, which indicates that our method converges faster in the pretraining stage.",1,related,1,positive
"We observe that our method achieves 47.8 mIoU, which is slightly lower than MAE by 0.3, but higher than all others.",1,related,0,negative
"We note that BEIT and MAE are pretrained with 1600 epochs and use grid-search to find the best hyperparameters, while we only pretrain 800 epochs and don’t tune any parameters in the fine-tune stage due to limited access to computation.",1,related,1,positive
"The patch-based iGPT achieves 82.70 Top1 accuracy, which is higher than DeiT-base (Touvron et al. 2021)(81.8) but still lags behind the MIM method (e.g. 83.6 for MAE).",1,related,0,negative
"While, our proposed stochastic autoregressive image modeling, SAIM, utilizes all the information of the image to generate clear images, and achieve better fine-tuning accuracy than MAE on ImageNet-1K.",1,related,1,positive
"Method Plane Bcycl Bus Car Horse Knife Mcyle Persn Plant Sktb Train Truck Mean
CDAN [48]
R es
N et 85.2 66.9 83.0 50.8 84.2 74.9 88.1 74.5 83.4 76.0 81.9 38.0 73.9 MCC [36] 88.1 80.3 80.5 71.5 90.1 93.2 85.0 71.6 89.4 73.8 85.0 36.9 78.8 SDAT [57] 95.8 85.5 76.9 69.0 93.5 97.4 88.5 78.2 93.1 91.6 86.3 55.3 84.3 MIC (SDAT) 96.7 88.5 84.2 74.3 96.0 96.3 90.2 81.2 94.3 95.4 88.9 56.6 86.9 TVT [87]
V iT 92.9 85.6 77.5 60.5 93.6 98.2 89.3 76.4 93.6 92.0 91.7 55.7 83.9 CDTrans [85] 97.1 90.5 82.4 77.5 96.6 96.1 93.6 88.6 97.9 86.9 90.3 62.8 88.4 SDAT [57] 98.4 90.9 85.4 82.1 98.5 97.6 96.3 86.1 96.2 96.7 92.9 56.8 89.8 SDAT w/ MAE [25] 97.1 88.4 80.9 75.3 95.4 97.9 94.3 85.5 95.8 91.0 93.0 65.4 88.4 MIC (SDAT) 99.0 93.3 86.5 87.6 98.9 99.0 97.2 89.8 98.9 98.9 96.5 68.0 92.8",1,related,0,negative
"Moreover, we compare the image imputation performance of our Edge-MAE with vanilla MAE [24].",1,related,1,positive
"We adopt the partial fine-tuning strategy inspired by [24], i.",1,related,1,positive
"Inspired by [16,23,46], we perform mask sampling on video and text to achieve end-to-end video-text alignment.",1,related,1,positive
"Instead of blindly applying the mask-thenprediction paradigm from MAE, we propose a maskedthen-alignment paradigm, namely Masked Contrastive Pretraining, for efficient video-text alignment.",1,related,1,positive
"Without blindly applying mask-then-prediction paradigm from MAE, we explore the masking contrastive mechanism based on
the video language domain, and propose a mask-thenalignment paradigm to efficiently learn a multimodal alignment.",1,related,1,positive
"Another common tactic is fulfilled with sinusoidal mapping Fsine [20,54], through which p is generated on-the-fly by a fixed function dependent on NH , NW and D. Due to space limitation, we provide explicit expression in Appendix B.1.",1,related,1,positive
We demonstrate the explicit mapping function Fsine for sine-cosine positional embedding p as follows.,1,related,1,positive
The Masked Autoencoder (MAE) method [29] further takes advantage of masking to reduce training time and memory.,1,related,0,negative
"Our fine-tuning implementation follows MAE [29], with the learning rate tuned for each entry.",1,related,0,negative
MAE sparsely applies the ViT encoder [20] to visible content.,1,related,1,positive
"Our work is related to MAE and its vision-language extensions [23, 41, 31, 19].",1,related,1,positive
"We do not use a reconstruction loss, unlike MAE [29].",1,related,0,negative
"Inspired by the sparse computation of Masked Autoencoders (MAE) [29], we randomly remove a large portion of image patches during training.",1,related,1,positive
"By default, we apply our models on intact images at inference-time, similar to [29].",1,related,1,positive
We use MAE pre-training for most experiments by default unless otherwise specified.,1,related,0,negative
"Thus, for the model initialized from MAE pre-training, we increase finetuning iterations to 180k and batch size to 64.",1,related,0,negative
"In experiments, we explore two pre-training schemes: 1) MAE pre-training: The ViT backbone is initialized from the self-supervised MAE [13] trained on ImageNet-1K [7], while the rest of the model parameters are randomly set; 2) GIT pre-training: The ViT backbone and text decoder are initialized from the pre-trained image VL model GIT [33] and the rest are randomly set.",1,related,1,positive
We find MAE pre-training can alleviate overfitting and benefit from more training epochs as discussed in [20].,1,related,0,negative
"Additionally, a special Classification (CLS) token is appended before adding the sinusoidal positional embedding as per convention [4], [5], [7] such that the final dimension of the input data after patchification is (S+1)×D.",1,related,1,positive
"Inspired by the masked auto-encoder (MAE) [21], we modified and applied it to the adversarial environment.",1,related,1,positive
"Similar to MAE [25], we found that applying a batch normalization layer [30] without affine transformations is beneficial for VideoMAE models.",1,related,1,positive
"Unless stated otherwise, we train our models for 500 epochs (for example, training with VMAEB on SSV2 takes 137 minutes with one 3090 GPU) with a batch size of 512 and use all 16 clips.",1,related,0,negative
"Our SCALE linear with SVT backbone beats the previous state of the art (71.8% vs. 71.5% [18]) and SCALE ft can even improve the accuracy of VMAEBft , which is a strong supervised model, from 81.5% to 81.84%.",1,related,1,positive
"We choose ρBYOL for their excellent linear performance, SVT for the usage of ViT [14], and VMAE for showing 1) the applicability of our proposed method to MAE models, 2) the scalability of our method to larger models, and 3) possibility of using supervisedly fine-tuned models as our backbone.",1,related,1,positive
We also used a backbone pretrained and fine-tuned on SSv2 (VMAEBSSv2) for the SSv2 experiment to show the universality of SCALE with respect to the pretraining dataset.,1,related,1,positive
"Since our clip representations are somewhat abstract representations of the video, we expect the optimal masking ratio to be close to NLP models rather than video MAEs.",1,related,1,positive
"Pretrained backbones: We use the pretrained checkpoints of ρBYOL [18], SVT [44], and three variants of VideoMAE [54] (base(B), large(L), and fine-tuned base(FT)).",1,related,1,positive
"With SCALE k-NN, we see a consistent improvement over the baseline and find that pre-trained MAE-based models greatly benefit from our training.",1,related,0,negative
We even improve the supervised model trained on SSv2 (VMAEBSSv2).,1,related,1,positive
"Then, with this frozen visual encoder, we used the same feed forward architecture, Q-function parameterization, and training objective (CQL with C51) as scaled QL to
finetune the MAE network.",1,related,1,positive
"As before, we also evaluate fine-tuning performance using the representations obtained via masked auto-encoder pre-training (He et al., 2021; Xiao et al.).",1,related,1,positive
"In Figure 7, we present our results for offline fine-tuning on 5 games from Lee et al. (2022), ALIEN, MSPACMAN, SPACE INVADERS, STARGUNNER and PONG, alongside the prior approach based on decision transformers (“DT (pre-trained)”), and fine-tuning using pre-trained representations learned from state-of-the-art self-supervised representation learning methods such as contrastive predictive coding (CPC) (Oord et al., 2018) and masked autoencoders (MAE) (He et al., 2021).",1,related,1,positive
"For MAE, we first pretrained a vision transformer (ViT-Base) (Dosovitskiy et al., 2020) encoder with 80M parameters trained via a reconstruction loss on observations from multi-game Atari dataset and freeze the encoder weights as done in prior work (Xiao et al.).",1,related,1,positive
MAE is a more recent self-supervised approach that we find generally outperformed CPC in this comparison.,1,related,1,positive
"For the MAE implementation, we used the Scenic library (Dehghani et al., 2022) with the typical configuration used for ImageNet pretraining, except using 84× 84× 4 sized Atari observations, instead of images of size 224× 224× 3.",1,related,1,positive
We train the MAE for 2 epochs on the entire multi-task offline Atari dataset and we observe that the reconstruction loss plateaus to a low value.,1,related,1,positive
"We train the completion network on the 11,426 complete skeletons using a masked auto-encoder strategy [32] where the missing keypoints are masked at the input and will be predicted using the unmasked keypoints.",1,related,1,positive
", 2022) and masked autoencoders (He et al., 2021) are all using patches as input.",1,related,1,positive
"Similar to [7], we utilized ADAMW [13] with learning of 1.",1,related,1,positive
"1, our framework consists of three asymmetric vision transformer inspired by the work of [7].",1,related,1,positive
"Inspired by these two papers, we add a mask-based random reconstruction module to the input images: x ← g(x), where g is a masked auto-encoder [16], defined by:",1,related,1,positive
"Here, we examine whether an additional reconstruction module can regularize the partial derivatives to make them semantic-aware to produce human-meaningful images [16, 41, 56].",1,related,1,positive
"The pre-training loss is computed on the masked tokens similar to MAE [29], and the final pre-training loss Lpre-train is defined as: Lpre-train = Lnode + Ledge, Lnode = ∑",1,related,1,positive
"We investigated works in NLP [17] and computer vision (CV) [29] where the pre-trained models have been dominantly used, and we find that the key to most successful pre-training models is to design simple but effective tasks that can scale well.",1,related,0,negative
"Inspired by MAE [29], we propose a bi-branch graph masking and reconstruction task for molecular pre-training.",1,related,1,positive
"Similar to MAE [29], we propose a transformer-style asymmetric encoder-decoder architecture for each branch.",1,related,1,positive
"To enable effective expansion, we explore three prior models for guided imagination: DALL-E2 [56] and Stable Diffusion [59] are advanced image generative methods, while MAE [24] is skilled at reconstructing images.",1,related,1,positive
"Thanks to strong image reconstruction abilities, our GIF-MAE applies the MAE-trained model [24] as its prior model.",1,related,0,negative
"As DALL-E2 [56] and SD are powerful in generating images, and MAE [24] excels at reconstructing images, we explore their use as prior models of GIF for data imagination.",1,related,1,positive
"We choose ViT-Base (ViT-B) [24] as the backbone of our framework for both audio and video modalities, due to its stability in performance across different data streams [34, 30, 8].",1,related,1,positive
"Inspired by the recent success of Transformers in different domains [23, 30, 34, 25], we use ViT [24] as the backbone of our framework for both audio and visual modalities.",1,related,1,positive
"Further, we drop the masked tokens x before feeding the input to θae for computational efficiency [3, 34].",1,related,1,positive
"Inspired by the recent success and scalability of pretraining with masked reconstruction in different domains [23, 13, 65, 30, 12, 34, 50, 64], we adopt masked data mod-",1,related,1,positive
"Our suggested method, MAEDAY, addresses FSAD by using Masked AutoEncoder (MAE) [9], a model trained for general image completion based on partial observations.",1,related,1,positive
": To investigate the impact of neighbors on network performance, we set the number of neighbors [2, 8, 16, 32, 64] and compared them in the table II.",1,related,1,positive
"To learn visual representations of images in a self-supervised manner, we apply Masked Autoencoder (MAE) which is
trained to reconstruct the randomly masked image patches.",1,related,1,positive
"A comparison between the attention maps generated by iTPN, the variant without integral pre-training (w/o iPT), and the MIM baseline (MAE [25]).",1,related,1,positive
"We emphasize here that we directly adopted the pre-trained model from masked autoencoder (MAE) (He et al., 2022), instead of training the model ourselves.",1,related,1,positive
"Therefore, our dual transfer learning reduces training time via utilizing the public model from MAE (He et al., 2022).",1,related,0,negative
"Different from other masked prediction variants [12, 9], we found mask loss is not useful in our setting, as our goal is to obtain an scalable decision making model but not only for representation learning.",1,related,1,positive
"• multi-goal reaching: For every trajectory in the validation set, we randomly sample a start state and 5 goal states at random future timesteps from [12, 60).",1,related,1,positive
Our first key observation is that masked token prediction with random masking similar to BERT [9] and MAE [12] provides a general and flexible way for learning from unsupervised data.,1,related,1,positive
"Architecture Our encoder is a Transformer [33] but applied only on visible, unmasked states and actions, similar to MAE [12].",1,related,1,positive
"As in recent unsupervised papers [1, 10, 11, 13], we compare the quality of our unsupervised keypoints to the features obtained by a fullysupervised ImageNet model.",1,related,1,positive
"An exponential moving average (EMA)[20] with α = 0.998 is implemented, and the shadow weight is updated after each training step.",1,related,1,positive
The structural configuration of SIVT follows the design of the MAE-base but we reduce the embedding dimension to 240 for efficient computation.,1,related,1,positive
ST-MAE is proposed in [23] that is based on the MAE [19] utilizing the Siamese encoder,1,related,1,positive
"Co-DETR with Swin-L yields 56.9% and 62.3% AP on LVIS val and minival, surpassing ViTDet with MAE-pretrained [13] ViT-H and GLIPv2 [40] by
+3.5% and +2.5% AP, respectively.",1,related,0,negative
"3% AP on LVIS val and minival, surpassing ViTDet with MAE-pretrained [13] ViT-H and GLIPv2 [40] by Method Backbone enc.",1,related,0,negative
"However, our network’s encoder and decoder are asymmetric, which indicates a significantly smaller decoder [25, 57].",1,related,0,negative
"Method Better Worse ×3, ×4 training warm-start [40] scratch std in normalization from data [25] 1.",1,related,1,positive
Implementation Details We adopt the model architecture of Masked AutoEncoder (He et al. 2022) with BERT (Devlin et al. 2018) word embeddings and language modeling head.,1,related,1,positive
Implementation Details We adopt the model architecture of Masked AutoEncoder (He et al. 2022) with BERT (Devlin et al.,1,related,1,positive
"For test-time training we do not use any augmentation, instead we construct a batch from the single point cloud sample and for Masked Autoencoder reconstruction, we randomly mask 90% of the tokens.",1,related,1,positive
"We test this approach on a diverse set of chromosome aberrations (an intra-chromosomal unbalanced abnormality: del(5q); intra-chromosomal balanced rearrangements: inv(3) and inv(16), and inter-chromosomal translocations: t(9;22), t(9;11), and t(11:19)) commonly seen in",1,related,1,positive
"For example, the normal chromosome 9s from the held out pre-training folds were added to the t(9;11) aberration training set for normal vs aberrant chr9 identification and likewise for the remaining aberration datasets.",1,related,0,negative
"Similarly, (D) shows precision-recall for de novo aberration detection based on distance to N-nearest point (here 50th) for t(9;11), t(11;19), del(5q), and t(9;22), respectively.",1,related,1,positive
"In this section, we first briefly introduce Masked Image Modeling (MIM) for image representation learning and then extend it to our video representation learning scheme, Masked Video Modeling (MVM) (§3.1 and §3.2).",1,related,1,positive
"To resolve this problem, we involve masked autoencoder [36], which applies 50% random masking to the input data.",1,related,1,positive
"Nevertheless, to compare to other pre-training strategies, we consider MAE [29] pre-trained on ImageNet [63], thus with a cosine positional embedding, a ViT-Base encoder, and with a Small decoder that is randomly initialized.",1,related,1,positive
"We observe that CroCo pre-training obtains the lowest errors, significantly outperforming the MAE pre-training and the random initialization.",1,related,0,negative
"Note that these methods, as MAE [29], regress pixel values that are normalized according to the mean and standard deviation inside each patch, we thus apply the inverse transform for
display: this means that the overall color of each patch will be correct, as it comes from the ground-truth values.",1,related,1,positive
The masking implementation follows [19]:,1,related,1,positive
"Different from [19], we reshape xmask into a masked images as input xinput ∈ RH×W×C .",1,related,1,positive
"Specifically, apart from the original optimization target, we exploit the Masked Image Modeling (MIM) [1, 19] task to mask and recover random patches during the training.",1,related,1,positive
"Although we did not achieve the best performance on OSCC and temporal localization tasks in Ego4d Challenge 2022, we believe that, by paying much more
attention to downstream task formulation and optimization, models that are pretrained on egocentric datasets under the settings of VideoMAE will further improve state-of-the-art performance on various Ego4d tasks.",1,related,1,positive
"In this report, we demonstrate that merely pretraining the model on 3rd-person view datasets (e.g. Kinetics 400[7]) under the settings of VideoMAE can achieve state-of-the-art performance on egocentric video understanding tasks including Ego4d object state change classification and Ego4d PNR temporal localization.",1,related,1,positive
"We will show that even with weights obtained on 3rd-person view datasets, VideoMAE shows great generalization ability on egocentric downstream tasks and surpass most existing methods both on OSCC and temporal localization tasks.",1,related,1,positive
"As shown in Table 1 and Table 2, by simply pretraining on Kinetics 400 under the settings of VideoMAE, we ranked 2nd place in both tasks.",1,related,0,negative
"In our experiments, we use ViT as our backbone of which weights are initialized from VideoMAE[9] pretrained on Kinetics-400.",1,related,1,positive
"In order to make this mix strategy compatible with existing pretraining tasks like Masked Image Modeling (MIM) and Image Classification (IC), we split the mask m into patches with p× p size.",1,related,1,positive
"Restrictions apply.
auto-encoder [34, 76], global/dense distillation [33, 81] and masked image modeling (MIM) [4, 5, 14, 30, 87].",1,related,1,positive
"For example, p = 16 is by default used for MIM [5,30].",1,related,1,positive
"Given that the supervision on masked patches is a regularization for the learning of CAE v2, we suppose that it may not be appropriate to adopt a high mask ratio (75% in MAE [27], 40%-50% in BEiT [3], CAE [10], and MVP [49]) for all scales of ViTs.",1,related,1,positive
"The encoder F only receives the visible patches Xv following [10, 27].",1,related,0,negative
"Our findings are different from the common sense in the current MIM methods [3,10,27] that only compute the loss on the masked patches, which is inherited from BERT [15] in the NLP areal and has been verified by most current works.",1,related,1,positive
"Following previous MIM methods [3, 10, 27, 48], CAE v2 first embeds x into a total length of N patches, which are then randomly masked by a specific proportion γ.",1,related,1,positive
Results with Masked Autoencoders (MAE).,1,related,0,negative
"As a representative example, we deploy EfficientTrain on top of MAE [22] in Table 10.",1,related,1,positive
"Additionally, to test transferability, we use the adversarially generated samples using PRIME-ResNet50 and FANVIT as classifiers, and test them on other architectures: (1) ResNet50 [17], (2) MAE [16], a generalizable and scalable asymmetric encoder-decoder architecture, (3) RegNet-
Y [14], a ResNet-type model with a regulatory model to extract complementary features, and (4) data2vec [6], a selfsupervised transformer that predicts contextualized latent representations in a self-distillation setup for any modality.",1,related,1,positive
"Additionally, to test transferability, we use the adversarially generated samples using PRIME-ResNet50 and FANVIT as classifiers, and test them on other architectures: (1) ResNet50 [17], (2) MAE [16], a generalizable and scalable asymmetric encoder-decoder architecture, (3) RegNet-",1,related,1,positive
"…we notice that there are various open-sourced image ViTs (Wightman, 2019; Touvron et al., 2021), which have been well-pretrained on huge web datasets under rich supervision such as image-text contrastive learning (Radford et al., 2021) and mask image modeling (He et al., 2022; Bao et al., 2021).",1,related,1,positive
"…of our UniFormerV2 design, we apply it on the ViTs with different pertaining methods, including supervised learning (Dosovitskiy et al., 2021; Touvron et al., 2022), contrastive learning(Caron et al., 2021; Radford et al., 2021) and mask image modeling (He et al., 2022; Bao et al., 2021).",1,related,1,positive
"We only use standard random cropping and horizontal flipping for data augmentation, following MAE [13], CAE [5], etc.",1,related,1,positive
"To restrain the feature magnitudes of teacher features, we generate the alignment target ỹ by normalizing each level of teacher features as MAE [13] does on pixel values:",1,related,1,positive
"Self-supervised Learning
We pre-trained MAE following the official implementation9 on eight NVIDIA A-100 GPUs and fine-tuned the last layer of its encoder on 16 NVIDIA V-100 GPUs.",1,related,0,negative
"We pretrain a Vision Transformer model, specifically ViT-B [13],
as MAE’s encoder for 200 epochs with a mask ratio of 0.75.",1,related,1,positive
"To verify this idea, we adopt a self-supervised learning method called masked autoencoder (MAE) [21].",1,related,1,positive
"Following MAE, we adopt the ViT as the backbone of the decoder g(·).",1,related,1,positive
"In the objective segmentation, MR SimCLR significantly improves APmask over MAE by 0.4 points (46.9 vs. 46.5).",1,related,0,negative
In this manuscript we use a ViT-B8 with the modifications proposed by He et al. (2022) where the classifier is applied after global average pooling over the vision tokens.,1,related,1,positive
"Following [18], we divide the vectorized voxels into patches which will be subsequently transformed into embeddings using a 1D convolutional layer with a stride equal to the patch size.",1,related,1,positive
"We also adopt an asymmetric architecture as in [18]: the encoder is optimized to learn effective fMRI representations, while the decoder tries to predict the masked patches.",1,related,1,positive
"Commonly, the encoder’s output is averaged, or a cls token is appended to produce a pooled 1D feature vector for downstream tasks [8,18].",1,related,1,positive
"In this work, we benchmark four representative methods MoCo, DINO, MAE, and data2vec on the proposed dataset.",1,related,1,positive
"We find 70% to be the best masking ratio, which is similar to natural images as reported in MAE paper, where 75% is the best.",1,related,0,negative
"This way, we cover a reasonably diverse set of representative methods from each model category: MoCo utilizes contrastive representative, DINO represents a distillation method, MAE is based on masked reconstruction, and data2Vec combines the masking mechanism with a joint-embedding architecture.",1,related,1,positive
We pre-train the MAE models using its default settings following the publicly available repository (https: //github.com/facebookresearch/mae).,1,related,0,negative
"For EO applications we demonstrate SSL4EOS12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec.",1,related,1,positive
"In the first stage, we pretrain the model by borrowing the idea from masked autoencoders (MAE) [11].",1,related,1,positive
Our encoder uses the same settings as ViT-B in MAE [11].,1,related,0,negative
"In the pretraining stage, we apply RandomResizedCrop to augment data, which is similar to MAE.",1,related,1,positive
Masked autoencoders (MAE).,1,related,0,negative
"To better use the pretrained knowledge, different from MAE or ViTSTR [2], which only fine-tune on the pretrained encoder, we fine-tune on both the encoder and the decoder.",1,related,1,positive
"To this end, we adopt a two-stage training strategy to train the model as follows:
In the first stage, we pretrain the model by borrowing the idea from masked autoencoders (MAE) [11].",1,related,1,positive
"However, recognizing texts is beyond the scope of MAE, and we propose a novel language-aware model to deal with it, as shown in Figure 1.",1,related,1,positive
"Different from MAE, our MVLT recognizes scene text in addition to reconstructing the masked patches.",1,related,0,negative
"The encoder of MAE is a ViT, which only operates on xu to learn the visual feature embeddings:
vu = encoder(xu), (1)
where vu ∈ RNu×D1 and D1 is the feature dimension in the encoder.",1,related,1,positive
"When finetuning the detector on COCO, we find that applying learning rate decay [1,4,5,9] for the components of the detector (encoder and decoder) gives a ∼0.",1,related,1,positive
"Then, REGCLR is instantiated by integrating masked autoencoders [12] as a representative example of a contrastive method and enhanced Barlow Twins as a representative example of a regularized method with configurable input image augmentations in both branches.",1,related,1,positive
MAE branch uses weak augmentation and masking to obtain X1 and then follows procedure introduced in MAE [12] to compute reconstructed loss for unmasked patches as LMAE to obtain X ′ 1.,1,related,1,positive
"Our implementation of pretraining is built on the repo provided in MAE [12], and the projector is implemented as a 2 layer multilayer perceptron with 1024 dimensional output.",1,related,1,positive
"For RGMIM and MAE, we employed the same settings in all experiments, except for the masking strategy.",1,related,0,negative
We also studied the masking ratio for RGMIM and MAE using hyperparameters.,1,related,1,positive
"As comparison methods, we used five SOTA selfsupervised learning methods as comparative methods, including masked autoencoder (MAE) [10], self-knowledge distillation based self-supervised learning (SKD) [47], cross-view self-supervised learning (Cross) [48], bootstrap your own latent (BYOL) [20], and simple siamese self-supervised learning (SimSiam) [21].",1,related,1,positive
"As comparison methods, we used five SOTA selfsupervised learning methods as comparative methods, including masked autoencoder (MAE) [10], self-knowledge distillation based self-supervised learning (SKD) [47], cross-view",1,related,1,positive
"In addition, we observe that RGMIM outperforms MAE in terms of robustness, especially when the masking ratio is relatively low, demonstrating the superiority of our proposed method in handling incomplete lung X-ray images.",1,related,0,negative
"Method Architecture 1% 5% 10% 50% 100% RGMIM ViT-Base 0.771 0.893 0.919 0.957 0.962 MAE [10] ViT-Base 0.754 0.875 0.903 0.948 0.956 SKD [47] ResNet-50 0.742 0.812 0.896 0.947 0.957 Cross [48] ResNet-50 0.747 0.795 0.817 0.934 0.953 BYOL [20] ResNet-50 0.683 0.754 0.790 0.933 0.954 SimSiam [21] ResNet-50 0.623 0.700 0.781 0.929 0.949 Transfer ViT-Base 0.689 0.861 0.893 0.940 0.953 Transfer ResNet-50 0.539 0.619 0.665 0.913 0.936 From Scratch ViT-Base 0.413 0.580 0.645 0.810 0.848 From Scratch ResNet-50 0.284 0.496 0.532 0.619 0.774
has layer L = 8, latent vector size D = 512, and the number of heads is 16.",1,related,1,positive
"Although existing Transformer models like ViT [32] and MAE [10] are usually trained on the large-scale dataset, We trained the ST-MAE model on the limited samples from scratch by an AdamW optimizer with a learning rate of 1e-4 and batch size of 8 for 400 training epochs, while the weights",1,related,1,positive
"Note that our ST-MAE is a feature vision Transformer that operates on the deep features of DCNN, which is slightly different from the base ViT [10], its consecutive computational process is roughly demonstrated.",1,related,1,positive
"like the MAE [10] and Intr [36], we adopted the feature-level measurement, the relaxed version of the pixel-level constraints, for better robustness.",1,related,1,positive
"1, in analogy to MAE [10], our ST-MAE has an asymmetric encoder-decoder design that reconstructs the input in feature space, yet our encoder applies a Siamese architecture.",1,related,1,positive
"In our proposed method, MobileNetV3([16]) is adopted as the backbone network.",1,related,1,positive
"In our proposed method, MobileNetV3[16] is adopted as the backbone network.",1,related,1,positive
"1) Black-Box Attack: For the attack model, we adopt a similar structure to the edge model: an MAE decoder [18] is used, and is pretrained on ImageNet.",1,related,1,positive
"We also report results for our MAE implementation, which approximately matches the original numbers reported by He et al. (2022), validating our MAE results on JFT-300M.",1,related,0,negative
For linear probing we list the hyperparameters in Table 10 for which we followed the settings in He et al. (2022).,1,related,1,positive
"For MAE pre-training, we use the same hyperparameters as listed in He et al. (2022), except for the use of Glorot uniform initialization instead of LeCun initialization as done in He et al. (2022).",1,related,1,positive
"Following He et al. (2022), only the T ′ unmasked patches are passed to the ViT encoder, which processes the two views in parallel.",1,related,1,positive
"Corroborating the findings of He et al. (2022), we find it best to only compute the reconstruction loss on masked patches:
Lrec = 1
2n ∑ v=1,2 n∑ i=1 ‖Mvi ◦ (xvi − x̂vi )‖22
where ◦ multiplies all pixels in the tth patch of the residual image xvi − x̂vi by (Mvi )t ∈ {0, 1}.",1,related,1,positive
Decoder architecture: Our decoder architecture is the same as He et al. (2022).,1,related,1,positive
"However, when we apply generative pretrained backbones, e.g. BERT (Kenton and Toutanova, 2019) and MAE (He et al., 2022), to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information for classification (Wallat et al., 2020; Yosinski et al., 2014).",1,related,1,positive
"However, when we apply generative pretrained backbones, e.g. BERT (Kenton and Toutanova, 2019) and MAE (He et al., 2022), to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information…",1,related,1,positive
Our model-based image augmentation method was implemented based on the official codes of the MAE [18] and ViTPose [21].,1,related,0,negative
"To solve the problem of loss and deformation of data acquired by clients, we used a masked image-encoding method that learns image representations corrupted by masking [7, 8].",1,related,1,positive
"Following [13], we apply an end-to-end fine-tuning mechanism instead of linear probing to obtain the highest performance.",1,related,1,positive
"Instead of random initialization, we initialize the ViT encoder using the ImageNet-1k pre-trained encoder released by [13].",1,related,1,positive
"(ii) MAE-IN1k refers to fine-tuned from the ImageNet-1k [14] pre-trained MAE [13], where we use the same fine-tuning settings as that of MAE-Face.",1,related,1,positive
"Among the various techniques proposed for self-supervised visual representation learning, we opt to adopt masked autoencoder (MAE) [13] as our baseline.",1,related,1,positive
"We also test the impact of patch-wise normalization (w/ norm), which has also been studied in [13].",1,related,1,positive
"CONCLUSIONS In this paper, based on MAE[16] and combined with convolution, we propose a transformer image inpainting model to solve the image inpainting problem of irregular missing areas.",1,related,1,positive
"In this paper, based on MAE, we propose a transformerbased inpainting model for irregular missing images.",1,related,1,positive
"By utilizing MAE pretraining, ProContEXT outperforms the recent SOTA method, OStrack [5], by 0.9%, 1.5%, and 2.1% for AO, SR0.",1,related,0,negative
We implement the proposed ProContEXT using PyTorch and utilize the ViT-base backbone that is pre-trained by MAE [28].,1,related,1,positive
Implementation Details We implement the proposed ProContEXT using PyTorch and utilize the ViT-base backbone that is pre-trained by MAE [28].,1,related,1,positive
"As mentioned in Section 1, we follow a similar approach to the work put forth in [7], where masked autoencoders reconstructed images with large amounts of masking.",1,related,1,positive
"Unlike [7], we don’t remove the masked patches from the input to the encoder and provide positional information to the input of the decoder.",1,related,1,positive
"We denote the former as “speech branch” and the latter as “text branch”, where the former can be viewed as a masked autoencoder [28] and the latter is TTS.",1,related,1,positive
"Specifically, we propose an SSL model, MAEEG, inspired by both BENDR and the MAE model proposed in computer vision [He et al., 2022], to compare with BENDR and to broaden our understanding of reconstruction-based SSL for EEG.",1,related,1,positive
This study was inspired by MAE [1] for an MIM and Bootstrap Your Own Latent [12] (BYOL) as a framework for directly learning la-,1,related,1,positive
"While we use the same positional encoding as MAE [1], we tested various masking ratios as discussed in Section 4.",1,related,0,negative
"In this work, we capitalize in particular on the masked autoencoding of [2] and investigate using it for object-centric learning.",1,related,1,positive
"Then, exactly as in MAE [2], we add a learnable mask token at the positions of the masked tokens and the sine-cosine position embeddings.",1,related,1,positive
"We therefore choose the random masking strategy, exactly as in MAE [2].",1,related,1,positive
"Just as in MAE [2], we add fixed sine-cosine postional encodings from [23] to the embeddings of the patches.",1,related,1,positive
Our model has a narrower bottleneck in comparison to MAE [2].,1,related,0,negative
Our model is inspired by the transformer-based masked autoencoder [2] with modifications to enable end-to-end unsupervised multi-object segmentation and representation learning.,1,related,1,positive
"More specifically, we adapt the Masked Autoencoder (MAE) [2] design and modify it to for explicit object-centric representation learning and segmentation.",1,related,1,positive
"Similarly, while ViT has been shown to outperform CNNs on large datasets [17], we observe that ViT outputs contain uneven pixelation when decoding the 16 × 16 patch embeddings, resulting in its low SSIM score.",1,related,1,positive
"For MAE He et al. (2022), a method based on a reconstruction objective, we select an attention-based ViT encoder.",1,related,1,positive
"0 0.5 1
−0.6
−0.4
−0.2
0
0 0.5 1 0 0.5 1
0 0.5 1
−0.6
−0.4
−0.2
0 0.5 1
CLIPPretrained MAEPretrained MLPMixerPretrained1k MLPMixerPretrained21k ResNet50Pretrained1k ResNet50Pretrained21k SimCLRPretrained ViTPretrained1k ViTPretrained21k iBotPretrained1k iBotPretrained21k
Generalization when Lighting varies in training
percent of diverse Lighting training samples percent of diverse Lighting training samples percent of diverse Lighting training samples
percent of diverse Lighting training samples percent of diverse Lighting training samples
t o p - 1
a c c u r a c y d
r o p
t o p - 1
a c c u r a c y d
r o p",1,related,1,positive
"In addition to the finetning results, we include here linear evaluation results for class generalization gaps A11.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Translation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 92.24% 91.37% 92.25% 94.01% 96.33% 77.78% 75.93% 68.52% 68.52% 58.89% 39.82% 57.70% 64.51% 65.45% 66.98% MAEPretrained 52.54% 55.93% 60.44% 67.91% 83.79% 20.37% 27.78% 33.33% 38.89% 27.78% 9.67% 12.91% 14.26% 15.56% 15.18% MLPMixerPretrained1k 94.66% 93.98% 94.58% 95.75% 97.25% 77.78% 74.07% 72.22% 66.67% 48.15% 36.86% 53.70% 59.35% 63.05% 63.46% MLPMixerPretrained21k 94.95% 95.09% 95.19% 96.02% 97.13% 75.93% 75.93% 74.07% 77.78% 70.37% 43.89% 67.36% 71.25% 73.69% 76.18% ResNet50Pretrained1k 95.00% 94.58% 94.83% 95.79% 97.23% 88.89% 90.74% 87.04% 83.33% 70.37% 44.35% 63.26% 68.99% 70.04% 70.01% ResNet50Pretrained21k 95.51% 95.30% 95.90% 96.27% 97.39% 77.78% 72.22% 74.07% 74.07% 70.37% 46.61% 68.04% 73.02% 75.90% 77.13% SimCLRPretrained 96.13% 95.74% 96.22% 96.82% 97.50% 81.48% 79.63% 81.48% 72.22% 70.00% 43.73% 62.96% 69.10% 70.63% 72.02% ViTPretrained1k 95.79% 96.18% 96.37% 96.80% 97.75% 88.89% 83.33% 77.78% 77.41% 75.93% 49.34% 67.92% 72.74% 75.65% 77.22% ViTPretrained21k 95.43% 95.01% 95.62% 96.39% 97.50% 83.33% 83.33% 83.33% 77.78% 72.22% 46.59% 67.21% 71.71% 74.02% 75.17% iBotPretrained1k 96.67% 96.43% 96.49% 97.01% 97.66% 81.48% 81.48% 79.63% 79.63% 72.22% 40.27% 65.23% 73.10% 76.06% 77.33% iBotPretrained21k 96.84% 96.30% 96.44% 96.92% 97.61% 90.74% 85.19% 87.04% 81.48% 72.22% 47.69% 70.55% 76.57% 78.53% 79.04%
Table A1: Position varying linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Rotation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 91.70% 91.43% 92.42% 93.98% 96.28% 81.48% 85.19% 85.19% 79.63% 55.56% 28.64% 41.16% 44.72% 46.17% 48.85% MAEPretrained 53.90% 57.24% 61.28% 68.44% 83.90% 25.93% 44.44% 38.89% 42.59% 29.63% 6.68% 7.93% 8.58% 8.63% 8.36% MLPMixerPretrained1k 94.24% 93.76% 94.74% 95.72% 97.28% 74.07% 74.07% 72.22% 70.37% 46.30% 26.84% 39.47% 43.04% 46.51% 48.73% MLPMixerPretrained21k 94.49% 94.81% 95.03% 95.88% 97.13% 79.63% 77.78% 77.78% 79.63% 72.22% 36.90% 55.46% 61.57% 63.67% 65.50% ResNet50Pretrained1k 94.72% 94.56% 94.89% 95.74% 97.18% 85.19% 87.04% 85.19% 81.48% 68.52% 34.44% 46.22% 49.90% 51.73% 53.24% ResNet50Pretrained21k 95.51% 94.96% 95.69% 96.12% 97.30% 77.78% 75.93% 75.93% 72.22% 66.67% 40.29% 57.68% 61.83% 63.91% 65.04% SimCLRPretrained 95.74% 95.63% 96.16% 96.80% 97.51% 81.48% 83.33% 83.33% 83.33% 62.96% 32.94% 47.35% 52.88% 55.97% 56.91% ViTPretrained1k 95.71% 95.76% 96.09% 96.79% 97.67% 87.04% 79.63% 81.48% 79.63% 59.26% 39.13% 55.09% 60.14% 64.14% 65.42% ViTPretrained21k 95.23% 94.89% 95.68% 96.42% 97.45% 85.19% 83.33% 83.33% 83.33% 66.67% 38.25% 54.50% 58.34% 60.94% 62.28% iBotPretrained1k 96.39% 96.22% 96.41% 96.96% 97.56% 83.33% 81.48% 81.48% 79.63% 72.22% 34.62% 51.88% 58.88% 62.19% 63.11% iBotPretrained21k 96.44% 96.09% 96.42% 96.92% 97.61% 90.74% 88.89% 90.74% 87.04% 72.22% 41.03% 57.48% 63.69% 65.49% 67.34%
Table A2: Pose linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Spot hue_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 92.49% 91.60% 92.46% 94.06% 96.32% 77.78% 75.93% 70.37% 70.37% 59.26% 39.61% 60.78% 66.72% 68.27% 68.64% MAEPretrained 52.26% 55.43% 60.36% 67.78% 83.62% 22.22% 31.48% 37.04% 37.04% 20.37% 11.51% 15.09% 16.10% 18.91% 15.58% MLPMixerPretrained1k 95.17% 94.20% 94.98% 95.86% 97.30% 79.63% 77.78% 70.37% 66.67% 57.04% 40.09% 56.90% 64.43% 67.35% 68.93% MLPMixerPretrained21k 94.89% 95.29% 95.44% 96.13% 97.20% 81.48% 79.63% 72.22% 74.07% 76.30% 44.68% 69.30% 73.32% 76.12% 77.75% ResNet50Pretrained1k 95.26% 94.81% 95.03% 95.80% 97.23% 87.04% 88.89% 87.04% 83.33% 77.78% 44.08% 62.96% 68.67% 71.81% 72.54% ResNet50Pretrained21k 95.91% 95.45% 96.00% 96.28% 97.41% 77.78% 75.93% 75.93% 72.22% 71.11% 44.22% 67.22% 70.95% 72.38% 74.39% SimCLRPretrained 96.30% 95.91% 96.27% 96.84% 97.59% 79.63% 75.93% 74.07% 74.07% 66.67% 43.36% 63.22% 71.32% 73.72% 72.26% ViTPretrained1k 95.99% 96.36% 96.54% 96.95% 97.80% 90.74% 87.04% 83.33% 81.48% 74.07% 52.53% 69.53% 71.81% 76.01% 77.28% ViTPretrained21k 95.57% 95.39% 95.84% 96.51% 97.59% 85.19% 81.48% 83.33% 77.78% 75.93% 46.01% 67.50% 71.97% 75.75% 77.06% iBotPretrained1k 96.50% 96.55% 96.74% 97.12% 97.70% 79.63% 81.48% 81.48% 77.78% 74.07% 42.32% 68.61% 72.75% 76.28% 78.04% iBotPretrained21k 97.01% 96.42% 96.65% 97.04% 97.64% 88.89% 87.04% 83.33% 83.33% 75.93% 48.83% 73.37% 78.09% 80.39% 81.55%
Table A3: Spot hue linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Scale_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 91.81% 91.51% 92.26% 93.90% 96.25% 79.63% 72.22% 74.07% 70.37% 61.11% 36.80% 51.29% 57.05% 56.99% 58.80% MAEPretrained 51.98% 56.15% 60.87% 68.07% 84.12% 25.93% 35.19% 33.33% 37.04% 35.19% 7.01% 10.74% 11.05% 11.29% 11.44% MLPMixerPretrained1k 94.27% 93.55% 94.47% 95.59% 97.17% 79.63% 77.78% 70.37% 68.52% 53.70% 32.50% 46.15% 51.66% 55.30% 56.62% MLPMixerPretrained21k 94.86% 94.98% 95.08% 95.93% 97.09% 79.63% 79.63% 75.93% 75.93% 74.07% 40.55% 60.87% 66.63% 67.52% 70.03% ResNet50Pretrained1k 94.89% 94.57% 94.84% 95.66% 97.16% 87.04% 90.74% 90.74% 83.33% 72.22% 39.22% 54.23% 59.10% 61.27% 60.89% ResNet50Pretrained21k 95.51% 95.28% 95.77% 96.08% 97.35% 81.48% 77.78% 77.78% 74.07% 74.07% 41.11% 60.06% 66.41% 69.69% 70.33% SimCLRPretrained 95.91% 95.53% 96.09% 96.66% 97.48% 83.33% 77.41% 77.78% 72.22% 62.96% 39.79% 54.93% 61.81% 62.93% 62.96% ViTPretrained1k 95.65% 96.01% 96.18% 96.69% 97.69% 87.04% 83.33% 79.63% 75.93% 72.22% 44.10% 58.13% 63.91% 67.85% 68.82% ViTPretrained21k 95.06% 94.87% 95.47% 96.30% 97.38% 83.33% 83.33% 83.33% 81.48% 72.22% 41.40% 58.53% 63.25% 65.43% 65.14% iBotPretrained1k 96.58% 96.37% 96.48% 96.98% 97.60% 84.07% 77.78% 79.63% 77.78% 66.67% 41.34% 58.93% 67.24% 68.66% 69.08% iBotPretrained21k 96.92% 96.18% 96.39% 96.86% 97.53% 85.19% 77.78% 81.48% 81.48% 75.93% 44.59% 63.11% 68.90% 71.45% 70.82%
Table A4: Scale linear eval top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Background path_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 90.37% 91.88% 92.73% 94.36% 96.70% 85.19% 77.78% 77.78% 77.78% 66.67% 41.30% 54.70% 59.96% 61.75% 63.41% MAEPretrained 51.51% 56.16% 62.31% 67.08% 84.32% 27.78% 31.48% 37.04% 37.04% 29.63% 9.47% 12.21% 11.84% 13.30% 12.67% MLPMixerPretrained1k 92.84% 93.82% 94.81% 95.41% 97.38% 75.93% 77.78% 72.22% 74.07% 61.11% 37.39% 49.19% 52.41% 55.75% 56.74% MLPMixerPretrained21k 95.42% 95.33% 95.62% 96.52% 97.55% 83.33% 81.48% 75.93% 75.93% 77.78% 48.30% 64.37% 66.79% 70.15% 70.84% ResNet50Pretrained1k 94.35% 95.12% 94.83% 95.93% 97.46% 90.74% 92.59% 88.89% 88.89% 83.33% 44.89% 59.87% 64.15% 66.70% 67.29% ResNet50Pretrained21k 95.20% 96.40% 95.89% 96.65% 97.67% 81.48% 79.63% 77.78% 77.78% 75.93% 51.21% 65.75% 69.47% 72.01% 73.90% SimCLRPretrained 95.50% 95.98% 96.14% 96.45% 97.52% 83.33% 83.33% 81.48% 77.78% 72.22% 44.33% 59.82% 65.39% 67.93% 68.93% ViTPretrained1k 95.72% 96.42% 96.27% 96.57% 97.95% 94.44% 88.89% 88.89% 87.04% 77.78% 50.62% 64.09% 67.14% 72.33% 73.19% ViTPretrained21k 94.91% 95.22% 96.09% 96.45% 97.71% 83.33% 83.33% 83.33% 83.33% 77.78% 49.36% 64.21% 67.42% 70.77% 72.13% iBotPretrained1k 96.42% 96.58% 96.60% 97.40% 97.28% 88.89% 83.33% 87.04% 81.48% 79.63% 44.58% 62.59% 68.10% 71.20% 73.36% iBotPretrained21k 96.16% 96.14% 96.38% 97.37% 97.27% 88.89% 90.74% 90.74% 83.33% 81.48% 51.20% 68.58% 71.57% 74.62% 75.94%
Table A5: Background path linear eval top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Translation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.49% 97.00% 97.25% 97.55% 96.80% 87.04% 90.74% 77.78% 81.48% 75.93% 45.33% 69.74% 74.53% 77.72% 78.19% MAEPretrained 96.75% 96.63% 97.20% 97.46% 97.91% 83.33% 74.07% 66.67% 64.81% 72.22% 29.17% 51.35% 61.05% 65.16% 70.07% MLPMixerPretrained1k 96.95% 96.73% 97.17% 97.24% 97.88% 88.89% 77.78% 77.78% 74.07% 64.81% 44.65% 64.56% 70.67% 74.34% 75.95% MLPMixerPretrained21k 97.71% 97.69% 97.90% 97.98% 98.35% 85.19% 88.89% 85.19% 81.48% 77.78% 46.53% 70.54% 77.27% 79.78% 81.68% ResNet50Pretrained1k 97.97% 97.92% 97.91% 97.86% 98.27% 87.04% 87.04% 72.22% 81.48% 81.48% 45.05% 64.83% 71.87% 76.56% 79.63% ResNet50Pretrained21k 97.54% 97.62% 97.74% 97.69% 98.20% 88.89% 83.33% 83.33% 83.33% 77.78% 52.22% 70.96% 75.78% 81.10% 82.45% SimCLRPretrained 97.40% 97.57% 97.68% 97.81% 98.12% 90.74% 77.78% 87.04% 83.33% 77.78% 45.21% 68.73% 74.59% 76.55% 79.86% ViTPretrained1k 97.80% 97.88% 98.00% 97.92% 98.28% 90.74% 90.74% 85.19% 81.48% 81.48% 49.30% 71.82% 76.95% 80.39% 82.58% ViTPretrained21k 97.80% 97.59% 97.89% 97.91% 98.25% 87.04% 88.89% 81.48% 81.48% 87.04% 45.83% 71.80% 75.51% 79.96% 81.86% iBotPretrained1k 97.77% 97.55% 97.64% 97.77% 98.06% 88.89% 85.19% 79.63% 75.93% 79.63% 45.69% 68.94% 74.76% 76.63% 79.83% iBotPretrained21k 97.97% 97.83% 97.88% 97.92% 98.13% 88.89% 87.04% 88.89% 81.48% 79.63% 49.84% 70.97% 78.13% 82.53% 84.07%
Table A6: Position finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Rotation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.60% 97.01% 97.30% 97.58% 97.97% 88.89% 88.89% 85.19% 83.33% 72.22% 34.29% 57.01% 62.67% 65.58% 68.45% MAEPretrained 96.87% 96.75% 97.25% 97.52% 97.95% 75.93% 68.52% 62.96% 68.52% 62.96% 22.64% 45.24% 50.87% 54.18% 53.82% MLPMixerPretrained1k 96.75% 96.58% 97.08% 97.25% 97.86% 83.33% 85.19% 83.33% 77.78% 64.81% 33.29% 51.43% 55.55% 58.78% 59.35% MLPMixerPretrained21k 97.68% 97.65% 97.90% 97.90% 98.35% 87.04% 87.04% 77.78% 77.78% 75.93% 38.93% 62.76% 67.97% 71.90% 73.47% ResNet50Pretrained1k 98.05% 97.81% 97.89% 97.93% 98.24% 84.81% 81.48% 81.48% 81.48% 75.93% 33.29% 53.51% 62.56% 64.45% 67.47% ResNet50Pretrained21k 97.52% 97.45% 97.65% 97.61% 98.18% 85.19% 79.63% 83.33% 87.04% 85.19% 37.45% 59.85% 65.39% 70.66% 73.13% SimCLRPretrained 97.37% 97.43% 97.53% 97.74% 98.07% 87.04% 87.04% 83.33% 87.04% 75.93% 35.50% 55.87% 64.34% 66.79% 69.34% ViTPretrained1k 97.94% 97.87% 97.98% 97.95% 98.31% 88.89% 87.04% 85.19% 77.78% 75.93% 40.13% 62.66% 68.53% 71.32% 73.36% ViTPretrained21k 97.68% 97.61% 97.90% 97.97% 98.27% 88.89% 79.63% 83.33% 74.07% 70.74% 39.75% 61.42% 68.39% 71.51% 73.76% iBotPretrained1k 97.49% 97.55% 97.56% 97.75% 98.02% 88.89% 77.78% 83.33% 75.93% 68.52% 36.30% 60.69% 65.98% 68.22% 70.00% iBotPretrained21k 98.00% 97.79% 97.96% 98.01% 98.19% 88.89% 87.04% 83.33% 85.19% 72.22% 38.86% 62.35% 69.18% 71.96% 73.84%
Table A7: Pose finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Spot hue_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.66% 97.13% 97.40% 97.62% 97.98% 90.74% 88.89% 87.04% 87.04% 81.48% 49.99% 70.06% 75.12% 82.36% 82.48% MAEPretrained 97.04% 96.67% 97.20% 97.41% 97.95% 79.63% 77.78% 72.22% 74.07% 68.52% 30.10% 54.63% 60.74% 66.78% 69.34% MLPMixerPretrained1k 97.32% 96.92% 97.20% 97.36% 97.89% 87.04% 83.33% 77.78% 79.63% 72.22% 48.80% 65.60% 70.82% 76.88% 78.59% MLPMixerPretrained21k 97.80% 97.83% 97.99% 97.99% 98.42% 88.89% 87.04% 81.48% 75.93% 79.63% 49.81% 73.15% 75.23% 79.01% 82.35% ResNet50Pretrained1k 98.02% 97.99% 98.03% 97.98% 98.28% 88.89% 87.04% 83.33% 81.48% 77.78% 48.48% 67.72% 74.65% 76.90% 75.83% ResNet50Pretrained21k 97.68% 97.60% 97.87% 97.79% 98.24% 90.74% 87.04% 83.33% 77.78% 75.93% 54.35% 71.69% 76.70% 81.03% 81.86% SimCLRPretrained 97.60% 97.61% 97.72% 97.87% 98.17% 90.00% 76.30% 85.19% 77.78% 74.07% 45.58% 70.36% 79.09% 78.06% 81.75% ViTPretrained1k 97.68% 97.93% NaN 98.00% 98.37% 94.44% 88.89% NaN 81.48% 81.48% 54.38% 70.94% NaN 82.53% 83.94% ViTPretrained21k 97.77% 97.68% 97.94% 98.00% 98.24% 92.59% 85.19% 77.78% 78.15% 81.48% 52.49% 72.55% 76.58% 79.75% 82.39% iBotPretrained1k 97.91% 97.58% 97.76% 97.88% 98.08% 88.89% 81.48% 79.63% 75.93% 74.07% 48.67% 69.24% 75.01% 79.44% 80.81% iBotPretrained21k 98.25% 97.87% 97.88% 97.96% 98.13% 90.74% 83.33% 92.59% 79.63% 83.33% 49.39% 74.42% 80.67% 83.05% 85.02%
Table A8: Spot hue finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Scale_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.68% 97.08% 97.39% 97.63% 97.95% 90.74% 90.74% 87.04% 83.33% 79.63% 45.47% 66.39% 72.46% 75.63% 78.05% MAEPretrained 96.81% 96.64% 97.19% 97.40% 97.91% 81.48% 70.37% 70.37% 70.37% 53.70% 28.70% 51.73% 60.66% 62.49% 64.47% MLPMixerPretrained1k 97.23% 96.60% 97.07% 97.24% 97.82% 87.04% 85.19% 83.33% 74.07% 70.37% 41.62% 60.01% 65.78% 70.24% 71.64% MLPMixerPretrained21k 97.80% 97.80% 97.96% 97.95% 98.37% 85.19% 87.04% 81.48% 72.22% 77.78% 46.02% 68.37% 73.08% 76.51% 77.38% ResNet50Pretrained1k 97.88% 97.94% 97.95% 97.89% 98.24% 87.04% 85.19% 81.48% 75.93% 79.63% 44.47% 62.86% 71.55% 73.81% 75.80% ResNet50Pretrained21k 97.40% 97.58% 97.84% 97.72% 98.18% 90.74% 79.63% 81.48% 79.63% 79.63% 49.37% 68.72% 70.89% 76.85% 79.18% SimCLRPretrained 97.57% 97.54% 97.65% 97.83% 98.10% 88.89% 83.33% 83.33% 83.33% 74.44% 42.25% 65.04% 71.88% 74.89% 76.25% ViTPretrained1k 97.80% 97.92% 98.05% 97.92% 98.34% 88.89% 83.33% 85.19% 79.63% 79.63% 44.17% 65.86% 71.66% 77.46% 78.46% ViTPretrained21k 97.77% 97.71% 97.85% 97.99% 98.24% 85.19% 85.19% 85.19% 79.63% 79.63% 42.63% 67.71% 70.61% 74.96% 76.14% iBotPretrained1k 97.85% 97.61% 97.74% 97.79% 98.04% 90.74% 87.04% 83.33% 83.33% 79.63% 45.14% 64.09% 71.34% 73.90% 76.99% iBotPretrained21k 97.88% 97.75% 97.86% 97.97% 98.12% 88.89% 87.04% 87.04% 81.48% 75.93% 48.70% 65.52% 72.39% 79.16% 80.04%
Table A9: Scale finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Background path_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 96.49% 97.05% 97.52% 97.81% 98.01% 94.44% 88.89% 92.59% 87.04% 81.48% 50.79% 67.24% 76.25% 79.79% 80.92% MAEPretrained 96.20% 96.61% 97.31% 97.63% 97.99% 81.85% 72.22% 77.78% 72.22% 62.96% 30.49% 52.04% 64.77% 66.67% 68.56% MLPMixerPretrained1k 96.16% 97.03% 97.13% 97.66% 97.76% 90.74% 87.04% 81.48% 75.93% 72.22% 47.41% 63.84% 71.85% 73.76% 75.96% MLPMixerPretrained21k 98.08% 98.15% 98.09% 98.33% 98.71% 88.89% 92.59% 90.74% 85.19% 79.63% 51.84% 72.00% 77.05% 80.46% 81.10% ResNet50Pretrained1k 97.71% 97.76% 97.95% 98.04% 98.38% 92.59% 92.59% 90.74% 92.59% 83.33% 46.50% 63.48% 70.90% 73.73% 77.72% ResNet50Pretrained21k 97.38% 98.09% 97.72% 98.33% 98.34% 88.89% 83.33% 87.04% 81.48% 85.19% 50.87% 71.59% 76.21% 79.13% 83.24% SimCLRPretrained 97.38% 97.31% 97.77% 97.55% 98.05% 77.78% 87.04% 88.89% 90.74% 83.33% 43.32% 61.47% 69.94% 76.99% 79.07% ViTPretrained1k 98.12% 97.76% 97.89% 97.73% 98.44% 88.89% 90.74% 87.04% 81.48% 83.33% 54.25% 71.56% 75.73% 80.58% 84.92% ViTPretrained21k 97.77% 97.49% 97.95% 98.04% 98.37% 91.67% 88.89% 90.74% 87.04% 81.48% 55.17% 73.53% 76.50% 82.28% 81.94% iBotPretrained1k 97.47% 97.44% 97.61% 98.15% 97.86% 88.89% 92.59% 90.74% 81.48% 79.63% 51.19% 71.17% 75.32% 78.37% 79.93% iBotPretrained21k 97.69% 97.91% 97.77% 98.17% 97.93% 93.52% 92.59% 90.74% 85.19% 83.33% 55.00% 73.11% 76.62% 83.56% 82.90%
Table A10: Background path finetuning top-1 accuracy across multiple percentages of varying training instances",1,related,1,positive
"0 0.5 1
−0.8
−0.6
−0.4
−0.2
0
0 0.5 1 0 0.5 1
0 0.5 1
−0.6
−0.4
−0.2
0 0.5 1
CLIPPretrained MAEPretrained MLPMixerPretrained1k MLPMixerPretrained21k ResNet50Pretrained1k ResNet50Pretrained21k SimCLRPretrained ViTPretrained1k ViTPretrained21k iBotPretrained1k iBotPretrained21k
Generalization when Lighting varies in training
percent of diverse Lighting training samples percent of diverse Lighting training samples percent of diverse Lighting training samples
percent of diverse Lighting training samples percent of diverse Lighting training samples
t o p - 1
a c c u r a c y d
r o p
t o p - 1
a c c u r a c y d
r o p",1,related,1,positive
"0 50 100 −40
−20
0 50 100 0 50 100 0 50 100 0 50 100
CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k best fit
Percent of varying images across all instances
0 50 100 −40
−20
0
0 50 100 0 50 100 0 50 100 0 50 100",1,related,1,positive
"Using the typical evaluation procedures of self-supervised models (Caron et al., 2021; Dosovitskiy et al., 2021; Chen et al., 2020b) (finetuning and linear evaluation), we examine robustness across a catalog of state-of-the-art vision architectures, such as CLIP (Radford et al., 2021) that have significantly outperformed earlier models on robustness benchmarks such as ObjectNet (Barbu et al., 2019), Masked AutoEncoders (MAE, (He et al., 2022)), or ViTs Dosovitskiy et al. (2021)) among others.",1,related,1,positive
We use pre-trained weights from the official repo of He et al. (2022).,1,related,0,negative
"In our experiment we deploy our Lie operator on top of the Variance-Invariance-Covariance Regularization model (VICReg, (Bardes et al., 2021)), and of the very recent masked autoencoders model (MAE, (He et al., 2021)).",1,related,1,positive
"We incorporate our proposed Lie operator in two recent SSL approaches: masked autoencoders (MAE, (He et al., 2021)), and Variance-Invariance-Covariance",1,related,1,positive
"We incorporate our proposed Lie operator in two recent SSL approaches: masked autoencoders (MAE, (He et al., 2021)), and Variance-Invariance-Covariance
Evaluate Robustness
Self-Supervised Lie Operator
Training Data
Regularization (VICReg (Bardes et al., 2021)) to directly model transformations in…",1,related,1,positive
"In addition to the differentiation of the network structure pre-trained on ImageNet, we include models with a variety of pre-trained strategies, including SimCLR [6], MoCov2 [8] and
1https://pytorch.org/vision/stable/index.html 2https://github.com/rwightman/pytorch-image-models 3https://github.com/open-mmlab
BYOL [21] for ResNet50, MoCov3 [9] and MAE [26] for ViT-B.",1,related,1,positive
The most popular pretraining scheme for ViTs is called Masked Autoencoders (MAE) [45].,1,related,0,negative
"Following this spirit, we take masked autoencoders (MAE) [45] as pre-training task for its simplicity, efficiency, scalability, and compelling performance.",1,related,0,negative
"where LMIM is the MIM loss such as the prediction error used in the MAE [35] when reconstructing missing matches, p[u,v ] is the relative positional embedding with a 2D index of [u, v], g(xi − x j + δx ) and g(yi − y j + δy) are the continuous indexing function for x− and y−coordinate respectively, andM is the index of masked patches.",1,related,1,positive
"Publication date: October 2022.
where L𝑀𝐼𝑀 is the MIM loss such as the prediction error used in the MAE [35] when reconstructing missing matches, p[𝑢,𝑣 ] is the relative positional embedding with a 2D index of [𝑢, 𝑣], 𝑔(𝑥𝑖 − 𝑥 𝑗 + 𝛿𝑥 ) and 𝑔(𝑦𝑖 − 𝑦 𝑗 + 𝛿𝑦) are the continuous indexing function for 𝑥− and 𝑦−coordinate respectively, andM is the index of masked patches.",1,related,1,positive
"Then one adopts a Masked Auto-Encoder (MAE) [35] reconstruction error
L𝑚𝑎𝑒 (xm, x;𝜃 ) = D(I𝜃 (xm), x) with a distance measure D between reconstructed and original images to pre-train the network 𝜃 .",1,related,1,positive
Then one adopts a Masked Auto-Encoder (MAE) [35] reconstruction error,1,related,1,positive
"…al. (2022) requires access to an unlabeled public dataset for all models to measure the distance, and requires the communication of some datasets; He et al. (2021a) experiments with multiple methods to do personalization in decentralized learning, and our clustering-based approach can be viewed…",1,related,1,positive
"For masked autoencoder (He et al., 2021b) experiment on CIFAR-10, we use Vit-Tiny (Dosovitskiy et al., 2020) with AdamW optimizer for 1000 epochs with batch size 256.",1,related,1,positive
"Hence, a high masking ratio (75% (He et al., 2022)) may not be necessary to suppress the amount of information the encoder sees, and we consequently attempt lower ratios of 50%, 60% to introduce more information about the subordinate target.",1,related,0,negative
"In this work, we focus on MAE (He et al., 2022), which proposes to use a high masking ratio and a non-arbitrary ViT decoder.",1,related,1,positive
"We notice that recent work in the literature (He et al., 2022; Bao et al., 2022) performs many experiments in masking strategies, but to the best of our knowledge, we are the first to introduce image mixtures in the pre-training of MIM.",1,related,1,positive
"For better classification performance, we use normalized pixels (He et al., 2022) and a high masking ratio (0.75); for better visual reconstructions, we use a lower masking ratio (0.5) without normalizing target pixels.",1,related,1,positive
"Linear Probing: For linear evaluation, we follow MAE (He et al., 2022) to train with no extra augmentations and use zero weight decay.",1,related,1,positive
"Instead, our patch-dim normalization stresses the relations among patches, which is compatible to the patch-prediction in the MIM scheme.",1,related,1,positive
"We therefore choose a mask-reconstruction [19] SSL scheme for the pretraining of the new model, in which the base model generates reconstruction targets from the full input images and the new model tries to predict base model targets from random masked image input.",1,related,1,positive
We then propose to utilize the self-attention maps as a type of reconstruction targets for MIM to further enhance the semantic relation modeling capability of the new model.,1,related,1,positive
"With the plain backbone pretrained as a MAE [21], our method achieves 4.",1,related,0,negative
"Instead, we simply use the readily available pretrained MAE weights from [21].",1,related,0,negative
"• We show that block-wise masking provides superior performance on Masked Siamese ConvNets to the discrete random masking, commonly used in selfsupervised representation learning frameworks [20, 24, 1].",1,related,1,positive
"Furthermore, instead of using discrete random masking that is applied in [20], we found using block-wise masking will give better performance since it can better preserve global information important for contrastive loss.",1,related,1,positive
"1) Evalutation protocols: Following previous works [14], [28], [31], we adopt three common evaluation protocols, namely fine-tuning evaluation, linear evaluation and k-NN classification [43], to assess the performance of each pretrained model.",1,related,1,positive
"We also collect 7 ViT-S models with different training regimes, including the original ViT training setup [26]6, a stronger data augmentation setup in the Deit paper [89]-36, the training setup with distillation [89]-36, an improved DeiT training setup [90]-36 , and selfsupervised training fashions by MoCo v3 [12]7, MAE [38]8 and BYOL [33]9.",1,related,0,negative
"Following MAE (He et al., 2022) and BEiT v2 (Peng et al., 2022), we test the robustness of MaskDistill on three ImageNet validation sets, i.e., ImageNet-Adversarial (Hendrycks et al., 2021b), ImageNet-Rendition (Hendrycks et al., 2021a) and ImageNet-Sketch (Wang et al., 2019).",1,related,1,positive
We compare between DINO [5] and MAE [11] as a representation for a kNN based anomaly detection algorithm.,1,related,1,positive
"For MAE, we experimented both with kNN and reconstruction error for anomaly scoring and found that the latter works badly, therefore we report just the kNN results.",1,related,0,negative
"A.1 Anomaly detection comparison of MAE and DINO
We compare between DINO [5] and MAE [11] as a representation for a kNN based anomaly detection algorithm.",1,related,1,positive
"In particular, we compare to DINO [14], a state-of-the-art self-supervised method based on instance discrimination, and to MIM methods with MAE [38] and MultiMAE [4].",1,related,1,positive
"We follow the exact same protocol as MAE [38] for that, with global average pooling for CroCo as we did not include a [CLS] token in our model.",1,related,1,positive
"Our model is trained using a simple pixel reconstruction loss over all masked patches, similar to MAE [38].",1,related,1,positive
"Therefore, we used ArcFace [9] to train a projection head composed of two layers, BatchNormalization and Linear. this architecture is based on MAE [13].",1,related,1,positive
"In order to increase robustness to such varying resolution, we utilize up to 2⇥ higher resolution images during training but randomly drop 80% of visual tokens to minimize additional compute overhead (similar to [38, 52]).",1,related,1,positive
"Table 1: Token Merging ablation experiments using ViT-L/16 from MAE (He et al., 2022) on ImageNet-1k evaluated off-the-shelf without training, using r = 8.",1,related,0,negative
"We perform several experiments on ImageNet-1k (Deng et al., 2009) using ViT models trained in four different ways: AugReg (Steiner et al., 2022), MAE (He et al., 2022), SWAG (Singh et al., 2022), and DeiT (Touvron et al., 2021).",1,related,1,positive
"Different from MAE [16], our encoder operates on the full set.",1,related,0,negative
"The experiments reveal that MAE and U-Net are the best shape denoising methods we evaluated for all six
types of noise.",1,related,1,positive
"We evaluate seven shape denoising methods (ASM [6], DBM [8], CDBM [9], EBM [10], U-Net [11], Deeplabv3+ [12], MAE [13]) for denoising shapes corrupted by the six types of noise introduced in Section 2.",1,related,1,positive
"We evaluate seven shape denoising methods (ASM [6], DBM [8], CDBM [9], EBM [10], U-Net [11], Deeplabv3+ [12], MAE [13]) for denoising shapes corrupted by the six types of noise introduced in Section 2.2.",1,related,1,positive
"For the decoder, we use a flexible one following [15].",1,related,1,positive
"We mainly follow the basic setup of MAE [15]: for the encoder, we adopt different variants of ViT [10], i.",1,related,1,positive
"Specifically, MAE adopts a simple mean square error (MSE) loss: LMAE(h) = Ex̄Ex1,x2|x̄ ‖g(f(x1))− x2‖ 2 , (2) where the decoder output x̂2 = h(x1) = g(f(x1)) is assumed to be l2-normalized following the original paper of MAE saying that normalized target x2 yields better performance [15].",1,related,1,positive
We begin by introducing a mathematical formulation of MAE [15].,1,related,1,positive
As inpainter we use a publicly available Masked AutoEncoder (MAE) [21] trained with an adversarial loss.,1,related,0,negative
The MAE [21] we use is based on a ViT architecture and has been pre-trained in an adversarial fashion (as opposed to the standard training with an MSE loss) to output more realistic-looking details,1,related,0,negative
"Following the recent trend of methods for unsupervised object segmentation [7–12, 22], we build our method on top of SSL features, and, in particular, DINO [4] or MAE [21] features.",1,related,1,positive
"This may be somewhat related to the network (transformer) used in MAE, so although we use a masked representation learning approach similar to MAE, we improve the network structure to make it more adaptable to the learning of ECG representations.",1,related,1,positive
"We also compare the performance of those models to instance-based methods such as MAE (He et al., 2021) and data2vec (Baevski et al., 2022), which do not employ volume maximization regularizers.",1,related,1,positive
"We also compare the performance of those models to instance-based methods such as MAE (He et al., 2021) and data2vec (Baevski et al.",1,related,1,positive
Our pretraining follow the standard hyperparameters defined in He et al. (2021).,1,related,1,positive
"Tables 5, 6,7, 8 and 9, and show the results of SimCLR, MSN, VICReg, data2vec and MAE on the CIFAR100, CIFAR100 1%, Place205, Clevr/Count, Clevr/Dist and KITTI downstream tasks.",1,related,1,positive
"We pretrain a ViT-B/16 with data2vec (He et al., 2021) using the AdamW optimizer with a batch size of 2048 for 800 epochs using the official code base, which is publicly available: http: //github.com/facebookresearch/data2vec_vision/tree/main/beit.",1,related,1,positive
"We pretrain a ViT-L/16 with MAE (He et al., 2021) using the AdamW optimizer with a batch size of 1024 for 800 epochs using the official code base, which is publicly available: https:// github.com/facebookresearch/mae.",1,related,0,negative
"Our long-sequence MAE is a simple and minimallychanged extension of MAE [19], which we summarize first.",1,related,1,positive
"To help the model better model the trajectory, we adopt the patch norm method to normalize the trajectory values in each patch into a standard normal distribution, as has also been done in MAE [37].",1,related,1,positive
"For computation efficiency, we follow MAE [37] to only feed the unmasked patches (and their positions) to the encoder.",1,related,1,positive
"We consider two supervised feature backbones: ResNet50 [25] and ViT-B/16 [18], and four selfsupervised backbones: SimCLR [13], MAE [23], MSN [4] and DINO [11].",1,related,1,positive
"We consider two supervised feature backbones: ResNet50 [16] and ViT-B/16 [13], and four self-supervised backbones: SimCLR [9], MAE [17], MSN [2] and DINO [7].",1,related,1,positive
The random mixing and block-wise mixing strategies are inspired by MAE [18] and BEiT [3] and we replace the masking operation with image mixing on patch-level and block-level (both of size 16×16) respectively.,1,related,1,positive
"In particular, we compare to two approaches: R3M [41], which utilizes the Ego4D dataset of human videos to obtain a representation, and MVP [46, 56], which trains a masked auto-encoder [16] on the Bridge Dataset and utilizes the learned latent space as the representation of the new image.",1,related,1,positive
"For a fair comparison, we compare our DMAE ViT-B model with the MAE ViT-B checkpoint released by He et al. (2022) in the linear probing setting on ImageNet.",1,related,1,positive
"Following He et al. (2022); Xie et al. (2022), we use ImageNet-1k as the pre-training corpus which contains 1.28 million images.",1,related,0,negative
"Like MAE (He et al., 2022), we adopt the asymmetric encoder-decoder design for DMAE.",1,related,1,positive
"To enforce the encoder (but not the decoder) to learn robust semantic features, we control the capacity of the decoder by setting a smaller value of the hidden dimension and depth following He et al. (2022).",1,related,1,positive
"For the pre-training of the two DMAE models, we set the masking ratio to 0.75 following He et al. (2022).",1,related,1,positive
"Motivated by the Masked AutoEncoder (MAE) (He et al., 2022), which learns latent representations by reconstructing missing pixels from masked images, we design a new self-supervised task called Denoising Masked AutoEncoder (DMAE).",1,related,1,positive
"In the case of only few labeled data for calibration, the proposed MV-SSTMA is evaluated with the self-supervised method MAE and the supervised methodMD-AGCN.",1,related,1,positive
"In addition, our model outperforms MAE and MD-ADCN in every scenario.",1,related,0,negative
"2 Comparisons of different types of augmentations In this section, we compare the generalization of three canonical augmentations that we analyzed in this work: 1) Gaussian noise injection [21], 2) random mask [16], and 3) random rotation (which we introduced in Section 3.",1,related,1,positive
"In our framework, we allow for cases in which augmentation leads to significant changes in distribution and provide a path to analysis for such OOD augmentations that encompass empirically popular DA [16, 17].",1,related,1,positive
"3, we simulate the biased and unbiased random mask augmentation [16] and test their performance in regression and classification tasks.",1,related,1,positive
"Note that a natural constraint r ≤ m holds, and the original MAE setting [14] can be regarded as the special case when r = m.",1,related,1,positive
"Inspired by MAE [14], we also use a shallow decoder with much fewer Transformer blocks than the encoder (Φdec(.",1,related,1,positive
"We only calculate the loss on the mask tokens, which is consistent with MAE and SimMIM.",1,related,1,positive
"Different from the MAE [8] which utilizes a multilayer converter structure as decoder, we only employ a single-layer MLP is used as decoder.",1,related,1,positive
"Given an image-text pair (𝐼 ,𝑇 ), we random sample 25% of the image patches, the other 75% image patches will not be fed into the models following MAE[11].",1,related,1,positive
"Given an image-text pair (I ,T ), we random sample 25% of the image patches, the other 75% image patches will not be fed into the models following MAE[11].",1,related,1,positive
"The encoder and decoder of PA-former adopt the same structure following [11], in which the encoder transforms the input sub-sampled signal into latent representation and the encoder maps the latent representation back to the full-sampled data.",1,related,1,positive
"For computational efficiency, we adopt an asymmetric MAE architecture (He et al., 2022) based on vanilla Vision Transformers (ViTs) (Dosovitskiy et al.",1,related,1,positive
"For computational efficiency, we adopt an asymmetric MAE architecture (He et al., 2022) based on vanilla Vision Transformers (ViTs) (Dosovitskiy et al., 2020): The encoder operates only on visible patches and the decoder on all the patches.",1,related,1,positive
"For each decoder head, specific positional embeddings are applied following MAE (He et al., 2022).",1,related,1,positive
"Our decoder is another vanilla ViT deployed on the union of the encoded patch set and a set of mask tokens (He et al., 2022).",1,related,1,positive
"Following (He et al., 2022; Feichtenhofer et al., 2022), the encoder is applied only on unmasked patches.",1,related,0,negative
"Our proposed MotionMAE adopts the asymmetric Transformer based masked autoencoder architecture (He et al., 2022), as depicted in Fig.",1,related,1,positive
"MAE uses masked autoencoders (He et al., 2022) and we use its ViT-Large variant.",1,related,1,positive
"B we evaluate the transformer-based models MAE (He et al., 2022) and MOCOv3 (Chen et al.",1,related,1,positive
"B we evaluate the transformer-based models MAE (He et al., 2022) and MOCOv3 (Chen et al., 2021) as well as adversarially fine-tuned versions of MOCOv3 using the same three attacks as for MOCOv2.",1,related,1,positive
"As for segmentation, we use the ViT-base model provided by MMSegmentatation, which is pre-trained by MAE on ImageNet and then finetuned on the ADE20k dataset.",1,related,1,positive
"Note that SimR101, SimR101 and MAEViT stand for Resnet101 pretrained by SimCLRv2, Resnet50 pretrained by SimCLRv2 and ViT-base-16 pretrained by MAE, respectively.",1,related,0,negative
"We craft pre-trained adversarial perturbations (PAPs) for three pre-trained models (i.e., Resnet50 by SimCLRv2, Resnet101 by SimCLRv2, ViT16 by MAE) and evaluate the attack success rates on ten downstream tasks.",1,related,1,positive
We report the results of SimCLR and MAE in Section 4.2.,1,related,0,negative
"Note that Resnet50 and Resnet101 [18] are pre-trained by SimCLRv2 [4], and ViT16 [56] is pre-trained by MAE [21].",1,related,1,positive
"We fine-tune a stateof-the-art vision transformer model pretrained with masked autoencoding (He et al., 2021) for 100 epochs on rotated MNIST (Weiler and Cesa, 2019) (details in Appendix C.5).",1,related,1,positive
"We fine-tune a stateof-the-art vision transformer model pretrained with masked autoencoding (He et al., 2021) for 100 epochs on rotated MNIST (Weiler and Cesa, 2019) (details in Appendix C.",1,related,1,positive
"We use the base model and default finetuning procedure from (He et al., 2021), finetuning for 100 epochs, halving the learning rate on loss plateaus.",1,related,1,positive
"We opted to use a more flexible transformerbased visual architecture, which has recently achieved stateof-the-art results in computer vision tasks [11, 14], and language tasks [10, 31].",1,related,1,positive
125 We pre-train the models via the MAE framework [15].,1,related,1,positive
We leverage the masked 29 autoencoders (MAE) [15] that learn representations by masked prediction.,1,related,1,positive
"An important property of our visual pre-training approach is that uses a self-supervised objec194 tive [15] that makes minimal assumptions about the data distribution and does not rely on human195 designed proxy tasks, like data augmentations.",1,related,1,positive
We pre-train the models via the MAE framework [16].,1,related,1,positive
We train the MAE models for 400 epochs for the combined Ego dataset; 1600 epochs for the HOI dataset; and 1600 epochs for ImageNet dataset.,1,related,1,positive
"Our experiments follow the same architecture, settings, and pre-training recipe as MAE (He et al., 2022), and we find that the simple addition of a teacher (RC-MAE) consistently outperforms MAE in all model sizes (e.",1,related,1,positive
"Method Pre-Train Data APbox APmask Superivsed (He et al., 2022) IN1K w/ labels 47.",1,related,0,negative
"To validate the effectiveness of the EMA Teacher in RC-MAE, we compare RC-MAE to an otherwise identical MAE (He et al., 2022) baseline.",1,related,0,negative
"For the reconstruction target, we use standard normalized patch pixels as done with MAE (He et al., 2022).",1,related,1,positive
", 2021) and implementation details as MAE (He et al., 2022), we fine-tune Mask R-CNN (He et al.",1,related,1,positive
We observe the ASR ranged from 66.34% to 99.18% on both MAE and CAE.,1,related,0,negative
We then take MAE as the target model’s architecture and conduct comprehensive ablation studies to understand the impacts of important backdoor attack components in each supply chain’s phase.,1,related,0,negative
"Concretely, for Type I and Type II attacks, as the adversary does not involve in the pre-training phase, we utilize the public MAE 3 and CAE 4 as our target model.",1,related,0,negative
"We compare the MAE performance of using AdamW, SGD, and LARS as the optimizer and find AdamW reaches the best clean accuracy (see Table 9).",1,related,1,positive
"We consider two MIM architectures as the target models, i.e., Masked Autoencoder (MAE) [15] for end-to-end MIM and Contextual Autoencoder (CAE) [8] for tokenizer-based MIM.",1,related,1,positive
VideoMAE: Masked Autoencoders are DataEfficient Learners for Self-Supervised Video PreTraining.,1,related,0,negative
"To promise the results are comparable, we adopt the same linear probing configurations in all three scenarios for both MAE and CAE.",1,related,1,positive
MAE-AST: Masked Autoencoding Audio Spectrogram Transformer.,1,related,0,negative
", Masked Autoencoder (MAE) [15] for end-to-end MIM and Contextual Autoencoder (CAE) [8] for tokenizer-based MIM.",1,related,1,positive
We find that threshold [3-7] produces the best results which shows that it is important to keep a balance between similar and dissimilar view-pairs.,1,related,1,positive
Our decoder must be initialized randomly because the MAE authors do not share their decoder model parameters.,1,related,1,positive
"For our third specialized model, we first pretrain on in-domain simulated data for 150 epochs, initializing our ViT encoder from MAE’s pretrained model parameters and our ViT decoder randomly.",1,related,1,positive
"We introduce steps 2 and 3 that leverage self-supervised MAE training and finetuning, respectively, using additional adult pose datasets.",1,related,1,positive
"We leverage hierarchical pretraining by continuing to pretrain our ViT encoder on in-domain (i.e., fused depth and pressure) data using the MAE SSL algorithm.",1,related,1,positive
"fied by MAE [34], which we leverage in Section V.",1,related,0,negative
"For our first two specialized models, we pretrain on in-domain—either simulated or real—data for 150 epochs, initializing our ViT encoder from MAE’s pretrained model parameters and our ViT decoder randomly.",1,related,1,positive
We also demonstrate a masked autoencoding (MAE) hierarchical pretraining strategy for ViT that significantly improves accuracy on the SMaL dataset.,1,related,1,positive
"Given that the ViT backbone is amenable to hierarchical MAE pretraining strategy, we can further improve the performance of the best-performing architecture, ViTPose.",1,related,1,positive
"In ViTPose, they initialize with MAE’s encoder; we use this as a baseline to compare with ViTPose models initialized with our three specialized encoders.",1,related,1,positive
"5(left), our method is significantly superior to MAEL, which has the best accuracy of all single deep networks.",1,related,1,positive
"Following the procedure described in previous section, we first constructed object embeddings based on ViT (MAE), CNN (DenseNet) and used kNN classifier (our first approach).",1,related,1,positive
"A. Classifier comparison
In the first round of experiments, we encode RGB and depth modalities of the object separately using ViT (MAE) and CNN (DenseNet) and assessed the performance of seven classifiers, including k-Nearest Neighbors (kNN) [50], Multi-layer Perceptron (MLP) [51], Support Vector Machine (SVM) [52], Decision Tree (DT) [53], Gaussian Process (GP) [54], Random Forest (RF) [55], and Gaussian Naive Bayes (GNB) [56], on the restaurant fine-grained object dataset.",1,related,1,positive
We used MAE (RGB) + DenseNet (RGB-D) to represent each of the objects.,1,related,1,positive
"The accuracy of our multimodal appraoch-II with DeseNet (RGB-D) and MAE (RGB) was 93.51%, which outperformed all single models, CNNs (Dense.+Mnas.)",1,related,1,positive
"For future work, we are interested to explore AttnDistill for knowledge distillation between ConvNets and ViT.",1,related,0,negative
"Observing that the previous SSKD methods focussed on ConvNet do not work well on ViT, we proposed AttnDistill to distill the knowledge from a pretrained teacher model to its student model.",1,related,0,negative
"We draw the following conclusions:
• Based on ViT-T/16 distilled from Mugs(ViT-S/16), our method AttnDistill gets state-of-theart k-NN and Linear probing performance compared with previous knowledge distillation methods based on ConvNet.",1,related,1,positive
"He et al., 2022), we avoid such learnable prefix design with random initialization and propose a parallel attention (PATT) to the original attention module (see Figure 3).",1,related,1,positive
"He et al., 2022) in the NLP domain, we aim to propose a unified model for the vision domain, especially for video-based downstream tasks.",1,related,1,positive
"He et al., 2022; Tong et al., 2022), adding a prefix for a sentence input in NLP can be structurally different from the visual domain.",1,related,1,positive
"Specifically, we use the weight from the original vision MAE model (He et al., 2022) (Weights from https://github.com/ facebookresearch/mae) with only self-supervised learning (SSL) pretraining for all audio, visual, and joint encoder and the decoder.",1,related,1,positive
"Specifically, we use the weights of the original vision MAE He et al. (2022).",1,related,1,positive
"Due to the space limitation, please refer to He et al. (2022); Huang et al. (2022a) for single-modal MAEs.",1,related,1,positive
"Implementation Detail For the image classification problem, we use Vision Transformer pretrained on unlabeled ImageNet dataset with masked auto-encoding (He et al., 2022) and fine-tune it on the downstream task with AdamW optimizer (Loshchilov & Hutter, 2019) for 10,000 steps with batch size 32.",1,related,1,positive
"Firstly, we remove masked auto-encoding objective LMAE and train the model with only distillation loss LDistill before fine-tuning.",1,related,1,positive
"Then we take the copy of the pre-trained model (fθinit , gϕinit) as a student, and match the representations of the student encoder and those of the teacher encoder while optimizing the student with the MAE on the target unlabeled data.",1,related,1,positive
"Masked Auto-Encoding We briefly describe the masked auto-encoding objective (Liu et al., 2019; He et al., 2022) for a language model such as RoBERTA (Liu et al.",1,related,1,positive
"In this work, we focus on the masked auto-encoding (Devlin et al., 2019; He et al., 2022) as a pre-training objective function since its generality compared to other self-supervised methods (Chen et al.",1,related,1,positive
"Masked Auto-Encoding We briefly describe the masked auto-encoding objective (Liu et al., 2019; He et al., 2022) for a language model such as RoBERTA (Liu et al., 2019) and Vision Transformer (ViT) (Dosovitskiy et al., 2021).",1,related,1,positive
"Specifically, we take the pre-trained model with an encoder fθinit and a decoder gϕinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fθ0 and gϕ0 .",1,related,1,positive
"Then we take the copy of the pre-trained initial network gϕinit ◦fθinit as a student and further pre-train the student with masked auto-encoding objective but enforce hidden representation of the encoder of the student fθinit to be close to that of the teacher fθ0 as follows:
(θ1, ϕ1) ∈ argmin θ,ϕ (LMAE(θ, ϕ;Du) + LDistill(θ; θ0,Du))
LDistill (θ; θ0,Du) = 1
n n∑ i=1 ∥∥∥fθ(x(i))− StopGrad(fθ0(x(i)))∥∥∥2 2
(3)
where θ and ϕ are initialized with the pre-trained parameters θinit and ϕinit, respectively and StopGrad denotes the stop-gradient operation which does not back-propagate through the input.",1,related,1,positive
"Then the final objective for masked auto-encoding is defined as follows:
LMAE(θ, ϕ;Du) = 1
n n∑ i=1 Ez(i)∼pγ,T (z)
[ −
K∑ k=1 z (i) k Z(i) · log pθ,ϕ(x(i)k |x̂(i))
] , Z(i) =
K∑ k=1 z (i) k , (2)
where pγ,K(z) denotes a Binomial distribution with its parameters γ for probability that zk = 1 and K for the number of trials.",1,related,1,positive
"In this experiment, we continue to pretrain the encoder of the pre-trained transformer (He et al., 2022) with supervised contrastive loss (Khosla et al.",1,related,1,positive
"Specifically, we take the pre-trained model with an encoder fθinit and a decoder gφinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fθ0 and gφ0 .",1,related,1,positive
"…model with an encoder fθinit and a decoder gϕinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fθ0 and gϕ0 .",1,related,1,positive
"In this work, we focus on the masked auto-encoding (Devlin et al., 2019; He et al., 2022) as a pre-training objective function since its generality compared to other self-supervised methods (Chen et al., 2020b; He et al., 2020; Grill et al., 2020; He et al., 2020; Chen & He, 2021; Caron et al.,…",1,related,1,positive
"In this experiment, we continue to pretrain the encoder of the pre-trained transformer (He et al., 2022) with supervised contrastive loss (Khosla et al.,
2020) while fine-tuning a randomly initialized linear classifier with cross-entropy loss.",1,related,1,positive
"Xiao et al. (2022) demonstrates that masked-autoencoder (He et al., 2022) trained on diverse video data (Goyal et al., 2017; Shan et al., 2020) can be an effective visual embedding for online RL.",1,related,1,positive
"We address this gap by profiling four popular self-supervised methods (BYOL [12], SimCLR [7], DINO [6], and MAE [13]) and two supervised methods (CLIP [27] and standard classification).",1,related,1,positive
"We use the MAE ViT [13] as the backbone in SimCLR, BYOL, and CLIP (using e.",1,related,1,positive
"1 Pre-training Methods and Models We run four common self-supervised methods (BYOL [12], SimCLR [7], DINO [6], and MAE [13]) and two supervised methods (CLIP [27] and standard softmax classification).",1,related,1,positive
"Similar to [13], we ran an exploratory learning rate (LR) search across {1e-5, 5e-5, 1e-4, 5e-4} and weight decay (WD) values {0.",1,related,1,positive
"4.3), we investigate several recent ones (DINO (Caron et al., 2021), MoCo-v3 (Chen et al., 2021), MSN (Assran et al., 2022), MAE (He et al., 2022)).",1,related,1,positive
"For the pre-trained weights, we use the timm library for DINO and third-party releases for MoCo-v35, MSN6, and MAE7.",1,related,0,negative
"To this end, we train DINOSAUR with a ResNet34 encoder (from scratch) on COCO, but reconstruct targets obtained from ViTs pre-trained with different methods: DINO, MoCo-v3, MSN, and MAE.",1,related,0,negative
"In the following, we use block 9 for supervised training, DINO and MSN, and block 12 for MoCo-v3 and MAE.",1,related,0,negative
"Method Ours SKD BYOL SimSiam MAE Transfer From Scratch Accuracy 82.7% 74.2% 68.3% 66.8% 62.3% 53.9% 28.4%
Figure 3: Examples of real and distilled images.",1,related,0,negative
"For each training batch, we compute each objective through a separate forward pass and use the weighted sum of them for the final loss, where λVAM = 1.0 and λMAE = 0.3.
loss = λVAMlossVAM + λMAElossMAE (1)",1,related,1,positive
"(1), we use λVAM = 1.0 and λMAE = 0.3.",1,related,1,positive
"Following MAE-AST [7], we randomly mask 75% of the spectrogram patches.",1,related,0,negative
"We calculate the mean squared error between the reconstructed and original video frames and spectrograms:
lossMAE = 1
NVM ∑ i∈masked ||xVi − x̂Vi ||22 + 1 NAM ∑ j∈masked ||xAj − x̂Aj ||22 (3)
whereNVM andN A M are the number of masked patches for vision and audio, respectively.",1,related,1,positive
"4.2, we use separate decoders (with shared weights) for the vision and audio MAE pretraining objectives.",1,related,0,negative
"Following MAE [26], we randomly mask 75% of the visual patches, and the masking is applied for each video frame independently.",1,related,1,positive
"In addition to the VAM objective to learn cross-modal representation, we also use the masked autoencoding (MAE) objective to improve unimodal representations in the vision-and-language settings, by masking random patches of visual frames and the audio spectrogram, and reconstruct missing inputs as shown in Fig.",1,related,1,positive
"In Figure 3 and Figure 4, we show the reconstruction results with the MAE head, described in the main paper Sec.",1,related,1,positive
"The combination of VAM and MAE further improves the finetuning performance, and we use this configuration as default for TVLT pretraining.",1,related,0,negative
"(a) Using the state-ofthe art reconstruction-based SSL strategy, MAE [8] architecture for pre-training an representation extractor (encoder).",1,related,1,positive
"As indicated in [1, 7, 8] that SSL training requires a large amount of data, we begin with training a MAE on SSL Pre-Training Set.",1,related,0,negative
"1), we utilize a state-of-the-art reconstructed-based strategy, Masked Auto-Encoder (MAE) [8] to learn lowerdimensional semantic feature embedding.",1,related,1,positive
"Motivated by theoretical implication of SSL embedding space, we leverage a Masked Autoencoder [8] for feature extraction.",1,related,1,positive
"In this section, we verify that GPaCo can enhance model generalization ability and robustness on full ImageNet and its variants when compared with MAE [14] models.",1,related,1,positive
"To demonstrate the advantages of GPaCo on out-ofdistribution data, we load MAE [14] pre-trained weights and then fine-tune on full ImageNet with same training strategy as in [14].",1,related,0,negative
"We also experiment with the recently proposed masked auto-encoder framework [60], which is based on the ViT",1,related,1,positive
"Another exception is the masked auto-enconder [60] based on the ViT backbone, where we place SSPCAB and SSMCTB before the first transformer block.",1,related,1,positive
"Mixed Feature Prediction Task Motivated by MAE [12] and Point-BERT [11], we wanted to extend this self-supervised method to point cloud segmentation.",1,related,1,positive
"Mixed Feature Prediction Task
Motivated by MAE [12] and Point-BERT [11], we wanted to extend this self-supervised method to point cloud segmentation.",1,related,1,positive
"We pretrained five encoders, one using our proposed method TOV-VICReg and four using state-of-the-art self-supervised methods: MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021), VICReg (Bardes et al., 2022), and MAE (He et al., 2021).",1,related,1,positive
"For this study we selected DINO (Caron et al., 2021), MoCo (Chen et al., 2021), MAE (He et al., 2021), and VICReg (Bardes et al., 2022) since they are currently considered state-of-the-art, their official implementations are available in
PyTorch, and each represents a different type of approach.",1,related,0,negative
"Lastly, we also consider MAE (He et al., 2021), a masked reconstruction method, which consists in training an auto-encoder based on ViT to reconstruct an image with a set of patches masked.",1,related,1,positive
"The procedures here can be summarized as follow:
y = ClassHead(FF (CLSL))
ViT and other models based on it have found much success in producing state-of-the-art results in image processing domains [23, 24] and have been the building block for many pre-trained models such as, to mention a few, DINO [25], MAE [26], and MOCOv3 [27].",1,related,1,positive
"We take Vit [38], MAE [185], and MoCo [186] in our experiments.",1,related,1,positive
"To further reduce the encoder training time and improve robustness, a pre-trained encoder [27], [28] could be used and fine-tuned on our data.",1,related,0,negative
"Thus, we treat STMM as images and borrow the training approach of MAE [14].",1,related,1,positive
"To this end, we treat STMM as images and borrow the key principle of MAE [14] to process",1,related,1,positive
‘w/ SSL’ denotes training the segmentation network with the model weights pre-trained by a 3D Masked Autoencoder (MAE) SSL method [8].,1,related,1,positive
"Implementation Details To pre-train the ASA model, we use center-cropping augmentation, Xavier uniform initializer [5] for SW-ViT blocks and set the hyper-parameters following [8] (see Table 1(a)).",1,related,1,positive
"To address this problem, the Masked Autoencoder (MAE) self-supervised training method [41] is used to pre-train the proposed model on unlabeled data.",1,related,1,positive
"Further, the Masked Autoencoder pre-training method [41] is introduced.",1,related,0,negative
"By combining MAE [19] pre-training and DAT fine-tuning, our ViT-Huge [20] achieves 31.40 mCE on ImageNet-C [2] and 32.77% top-1 accuracy on Stylized-ImageNet [3].",1,related,1,positive
"Besides, we use DAT to conduct supervised finetuning on downstream ImageNet classification task based on a self-supervised ViTHuge pretrained by MAE [19].",1,related,1,positive
"We perform the pre-training on three largescale medical image-text datasets, i.e., ROCO [40], MedICaT [44], and MIMIC-CXR [21].",1,related,1,positive
"For ROCO and MedICaT, we filter nonradiology samples, and for MIMIC-CXR, we only keep images in the frontal view.",1,related,0,negative
"As for the dataset split, we adopt the official splits of ROCO and MIMIC-CXR.",1,related,1,positive
"For the pretext tasks, we adopt (knowledge-enhanced) MLM, MIM [17], and ITM, where the masking ratios of MLM and MIM are set to 15% and 75%, respectively.",1,related,1,positive
"We first fine-tune a ViT-Large model [9, 14] on Places365 [54], which is dubbed PlacesViT.",1,related,0,negative
The architectural settings strictly follow [19].,1,related,0,negative
"As a special case of MIM, we formulate MKD upon which an empirical investigation is conducted about the influence of different target representations on self-supervised masked autoencoders.",1,related,1,positive
"To answer these questions, we employ the standard masked autoencoder framework [19] to give a system-level study, introduced next.",1,related,1,positive
"To be specific, MIM randomly masks a portion of the input and then reconstructs the masked portion according to the transformed target, formulated as
min θ E x∼D M(T (x (1−M)), fθ(x M)), (1)
where “ ” means element-wise product; M is the patch mask; “x M” represents “unmasked patches” and vice ∗Equal contribution.",1,related,1,positive
"In this work, we paraphrase a term Masked Knowledge Distillation (MKD) to focus our discussion on a special case of MIM where the target is generated by a parameterized network (teacher network), i.e., T (·) = hφ(·).",1,related,1,positive
method data2vec [2] BEiT [3] MAE [19] dBOT,1,related,1,positive
"Similar to other MIM-based SSL work [19, 45], we reconstruct knowledge for those masked patches of input sample x .",1,related,1,positive
"MIM [45], MAE [19], and our MimCo on ImageNet-1K dataset.",1,related,1,positive
"What Semantic PatternsDoesMimCoLearn? To further help reveal what patterns does MIM learn, we follow the visualization of iBOT [47] to explore the learned patterns of the pre-trained models of SimMIM [45], MAE [19], and our MimCo via visualization, respectively.",1,related,1,positive
"We randomly choose 10 classes of ImageNet-1K dataset to visualize for simplicity, the visualization of learned representation shows that our MimCo significantly improves the linear separability of representations compared to SimMIM [45] and MAE [19].",1,related,1,positive
"Furthermore, we choose different generative blocks, including cross-attention block (Chen et al., 2022), self-attention block (He et al., 2022), and convolutional projector (Yang et al., 2022e).",1,related,1,positive
"Different from existing works that focus on MVM for pure vision problems [4, 22, 86], we study MVM as a VidL pre-training task.",1,related,0,negative
"Inspired by the MAE [15], we propose a spatial-temporal masked autoencoder framework for self-supervised 3D skeleton-based action recognition (SkeletonMAE).",1,related,1,positive
The generative SSL trains a generator consisting of an encoder and decoder to reconstruct the input data.,1,related,1,positive
"Like what ImageMAE does in [9], we directly discard a subset (e.g., 50",1,related,1,positive
"Like what ImageMAE does in [9], we directly discard a subset (e.",1,related,1,positive
"We propose the MAE-VQGAN model, which combines ideas from MAE [20] and VQGAN [15].",1,related,1,positive
"…our lexiconbottlenecked masked autoencoder (LexMAE) contains one encoder and one decoder with masked inputs in line with the masked autoencoder (MAE) family (He et al., 2021a; Liu & Shao, 2022), while is equipped with a lexicon-bottlenecked module for document-specific lexicon-importance learning.",1,related,1,positive
"We also compared our model with the few-shot counting sota method Fam-
Net [24], and our model outperforms it with a large advance (15.16 MAE and 24.29 RMSE on Val-COCO and 11.87 MAE and 14.81 RMSE on Test-COCO), which demonstrates the superiority of our model.",1,related,1,positive
"We use two standard metrics to measure the performance of our model, namely, Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).",1,related,1,positive
"MAE = 1 NI
NI ∑ i=1 |Ci−CGTi |, RMSE = √√√√ 1 NI NI ∑ i=1 (Ci−CGTi )2 (6)
Here, NI is the total number of testing images, and Ci and CGTi are the predicted number and ground truth of the ith image.",1,related,1,positive
"In this section, we aim to give the detail of our proposed two-stage training procedure, that is, first pre-train the ViT encoder with MAE [11], and then fine-tune the whole model on supervised object counting.",1,related,1,positive
"We propose a two-stage training scheme, with the transformer-based image encoder being firstly pre-trained with self-supervision via masked image modeling [11], followed by supervised fine-tuning for the task at hand.",1,related,1,positive
"# Shots Val Test
MAE RMSE MAE RMSE
A0 % % % % 0 24.84 86.",1,related,0,negative
"As input for pre-training ViT with MAE, we randomly drop 50% of the visual tokens, and task the model to reconstruct the masked patches with pixel-wise mean square error.",1,related,1,positive
"1 Training Details In this section, we aim to give the detail of our proposed two-stage training procedure, that is, first pre-train the ViT encoder with MAE [11], and then fine-tune the whole model on supervised object counting.",1,related,1,positive
"Specifically, we adopt the recent idea from Masked Autoencoders (MAE), to train the model by image reconstruction with only partial observations.",1,related,1,positive
"In Section 3.2, we further introduce a two-stage training regime, in which the model is firstly pre-trained with self-supervision via masked image reconstruction (MAE), followed by fine-tuning on the downstream counting task.",1,related,1,positive
"The two methods are based on the recent Masked Auto Encoder (MAE) approach [26], where we randomly mask the image ( e.g. , 75",1,related,1,positive
"The two methods are based on the recent Masked Auto Encoder (MAE) approach [29], where we randomly mask the image (e.",1,related,1,positive
"We also exploit more efficiently the input data with Masked Image Modeling, based on the recently proposed Masked Auto Encoder (MAE) [26].",1,related,1,positive
"We also exploit more efficiently the input data with Masked Image Modeling, based on the recently proposed Masked Auto Encoder (MAE) [29].",1,related,1,positive
"We propose a new efficient VLP approach centered on 3 main components; stronger Vision-Language pre-alignment through hierarchical contrastive objective, self supervision via masked image modeling based on MAE, and a new Visual Concepts injection and extraction technique.",1,related,1,positive
"We favor the MAE based, unimodal MIM loss which improves the results by 2.4% RSUM.",1,related,0,negative
"With the most recent representative work in this direction, masked autoencoder (MAE) [11], we are now able to efficiently and effectively pre-train high-capacity Vision Transformers (ViTs) with strong feature representations, leading to state-of-the-art solutions for a wide range of downstream visual tasks.",1,related,1,positive
"Following MAE [11], we first perform self-supervised pre-training on ImageNet-1k [7].",1,related,0,negative
We choose a decoder depth of 8 as the default setting as in [11].,1,related,1,positive
"Following the setting in MAE [7], we add a batch-norm layer without learnable affine parameters before the classifier to avoid adjusting the learning rate for each model.",1,related,1,positive
"CLIP
+
SimCLR
CLIP
+ MAE
Sandwich bread
MaskCLIP
(Ours)
CLIP
Bird
MaskCLIP
(Ours)
CLIP
CLIP
+
SimCLR
CLIP
+ MAE
SnowMountain goats Santa hatBearded Man
BandanaDog",1,related,1,positive
We start from CLIP+MAE and add three components of the distillation loss one by one.,1,related,1,positive
We adopted the MAE structure proposed in [14].,1,related,0,negative
"Following the previous work [9], we take ViTB [57] as the backbone network, which consists of 12 transformer layers and was pre-trained on ImageNet-21K with the self-supervised method MAE [21].",1,related,1,positive
"Inspired by [8], we design a decoder using a lightweight transformer structure [8] and is only used during the pre-training phase.",1,related,0,negative
"For image encoder, we employ a standard ViT-B/16 (Dosovitskiy et al. 2020) with random masking strategy (He et al. 2022).",1,related,1,positive
"Following MAE (He et al. 2022), we randomly divide the patches",1,related,1,positive
"Following (He et al. 2022), we take a 4-layer transformer with 512 dim as our image decoder.",1,related,1,positive
"Unlike MAE (He et al. 2022), we aim to reconstruct the invisible patches with both image and text features to facilitate multi-modal information fusion.",1,related,1,positive
"Motivated by recent generative methods in visual self-supervised learning (He et al. 2022; Chen et al. 2022), we propose to introduce pixel-level reconstruction task to VLP.",1,related,1,positive
"Following MAE (He et al. 2022), we adopt a lightweight vision transformer as our image decoder.",1,related,1,positive
"Following MAE (He et al. 2022), we randomly divide the patches into visible patches { xvisi }N−M i=1
and invisible patches{ xmski }M i=1
according to mask ratio α, where M = αN .",1,related,1,positive
"We compared the performance with other baselines involving Vision Transformers such as ConViT [16], Masked Auto Encoders (MAE) [53], Convolution-enhanced image Transformer (CeiT) [33], LeViT [34].",1,related,1,positive
"Inspired by (He et al. 2022; Wettig et al. 2022) that a large mask rate may be beneficial for pre-training, we employ a mask rate greater than the BERT’s 15% setting.",1,related,0,negative
"Inspired by (He et al. 2022; Wettig et al. 2022) that a large mask rate may benefit pre-training, we adopt up to 30% and 45% mask rate in CoT-MAE’s encoder and decoder, respectively.",1,related,0,negative
"Thus, borrowing insights from masked autoencoders [33], we believe most of the points can be redundant and we randomly masked the points with a probability of 75% per visit.",1,related,1,positive
"In the long series prediction, we employ Mean Squared Error (MSE) and Mean Absolute Error (MAE) as indicators and follow the setting (Zhou et al. 2021).",1,related,1,positive
"The formula is as follows: • Mean Squared Error (MSE):
MSE = 1
n n∑ i=1 (Yi − Ŷi)2
• Mean Absolute Error (MAE):
MAE = 1
n n∑ i=1 |Yi − Ŷi|
In the short series prediction, we employ Root Relative Squared Error (RSE) and Empirical Correlation Coefficient (CORR) as indicators and follow the setting (Lai et al. 2018).",1,related,1,positive
"In the absence of prior work applying masked autoencoding to abstract/synthetic imagery, we explore a large masked pixel percentage (75% of the image), thus forcing the model to attempt to recover the image based only on the unmasked 25% of the image, such high percentages have been shown to work well for natural imagery [30].",1,related,1,positive
%) of ViT-B and ViT-L trained by selfsupervised MAE on ImageNet.,1,related,0,negative
"2) self-supervised settings: we follow the MAE training framework to pretrain and fine-tune ViT-B and ViT-L, and report results in Table 3.",1,related,0,negative
"To obtain the benefits of self-supervised learning in video classification, we utilize VideoMAE pre-trained in Something-Something [8] to extract video features inherent in make-up scenes.",1,related,0,negative
"When self-pretrained, MAE [24] is mainly used, and we directly use their pretrained models.",1,related,0,negative
"When the model is self-pretrained by MAE [24], we first evaluate the fine-tuning performances of MAE on the labeled data only, as the common practice in self/un-supervised learning literature [25, 14, 22], with results shown Table 1.",1,related,1,positive
"Following the training recipe provided by MAE [25], the ViT models pretrained on ImageNet1K dataset serve as the backbone of UperNet [59], and are finetuned together with the segmentation layers.",1,related,0,negative
We use a masked autoencoder architecture similar to MAE [25].,1,related,1,positive
"Using the default 75% masking ratio, our prompting decoder reduces the decoding computation cost by 20% compared to MAE [25].",1,related,1,positive
"Our work takes some inspiration from MAE, however, PatchDropout can be applied to target tasks directly using standard ViTs (unlike MAE).",1,related,0,negative
"To ensure the effectiveness of training, we compute the loss on the common parts of the masked patches of T1 and unmasked patches of T2 (following [29]).",1,related,1,positive
"For simplicity, we take MAE [29] as an representative example of MIM, in which the similarity measurement is simply l2−distance on the masked patches.",1,related,1,positive
"3In original MAE [29], the encoder network only generates tokens of unmasked patches and the decoder only predict the masked patches during training.",1,related,0,negative
"Furthermore, taking MAE [29] as a representative example of MIM, we empirically find the success of MIM models relates a little to the choice of similarity functions, but the learned occlusion invariant feature introduced by masked image – it turns out to be a favored initialization for vision transformers, even though the learned feature could be less semantic.",1,related,1,positive
"Inspired by the above pilot studies, we employ the same MIM pretraining and finetuning routine to investigate the influences of plain ViTs on RS tasks.",1,related,0,negative
"For more details about MAE, please refer to [12].",1,related,0,negative
"Different from these works, we use the representative MAE method [12] for MIM pretraining and specifically focus on advancing plain ViTs for remote sensing tasks.",1,related,0,negative
"Different from these works, we use the representative MAE method [12] for",1,related,1,positive
"To explore the effectiveness of MIM pretraining in remote sensing tasks, we pretrain our model using MAE [12] with the large-scale RSI dataset, i.e., MillionAID [11].",1,related,1,positive
"To explore the effectiveness of MIM pretraining in remote sensing tasks, we pretrain our model using MAE [12] with the large-scale RSI dataset, i.",1,related,1,positive
