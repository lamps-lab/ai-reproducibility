text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"Non-determinism of GPU operations also produces churn even when all initial parameters are the same [Summers and Dinneen, 2021].",1,related,1,positive
"Our findings confirm the work of Summers and Dinneen (2021), who reach similar conclusions.",1,related,0,negative
"In particular we show, confirming the results of Summers and Dinneen (2021), that varying just a single weight at initialization produces only 1% less churn than all three sources of randomness combined.",1,related,1,positive
"We recently became aware of Summers and Dinneen (2021), who reach the same conclusions; our results further confirm their findings via several new experiments.",1,related,0,negative
"During encoding, we suggest to continue to maximize I (Ï† (X) ; X) and apply random data shuffling, a standard trick in real training processes [54, 55], to make the neural network learn samples rather than over-fit sample index.",1,related,1,positive
"While augmentation of data in training and stochastic regularization randomly applied in training (Summers & Dinneen, 2021) also influence irreproducibility, we do not consider these here, as they are in a different category of directly and intentionally adding randomness.",1,related,1,positive
