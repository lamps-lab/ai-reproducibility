text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"If considering s0 > 0, the layerwise sparsity of the initial mask follows the ERK distribution introduced in [31].",1,related,1,positive
"PruneFL starts with a pre-selected node to train a global shared mask function, while FedDIP generates the mask function with weights following the Erdős-Renéyi-Kernel (ERK) distribution [31], as we will discuss in the later sections.",1,related,1,positive
"Similar to the previous studies [25, 31, 32, 34], we use Cosine Annealing fdecay(t, α, Tend) = α2 (1 + cos( tπ Tend )) for topology update.",1,related,1,positive
"We know that uniform initialization assigns equal connections to each layer without considering the layer size and ERK allocates more connections to the layers with more parameters and less connections to the layers with fewer parameters [28, 31].",1,related,1,positive
"Table 1: Classification accuracy of sparse NNs for varying pruning rates κ based on our proposed method ART with L1, L2, and HyperSparse regularization LHS compared to dense models, and masks obtained by SNIP [20], GraSP [42], SRatio [34], LTH [9], IMP [16] and RigL [8].",1,related,1,positive
In addition we evaluate IMP [16] and RigL [8] as dynamic pruning methods.,1,related,1,positive
"After pruning, many PDT methods ([64, 84, 85, 86, 87]) directly obtain the subnetworks and do not require trainingfrom-scratch or fine-tuning process anymore.",1,related,0,negative
"Our method slightly differs from the original pruning and regrowing strategy [5, 10] in that we allow the pruned mask to regrow back in the same round, which allows the amounts of readjusted masks to differ across clients.",1,related,1,positive
"Afterward, each client can readjust its maskmc via pruning and regrowing [5, 10], with a readjustment ratio of αs (Line 10-12).",1,related,1,positive
"In this context, we implemented three leading sparse training methods: Gradual Magnitude Pruning (GMP) [81], RigL [16] and AC/DC [60], which we execute for an increasing number of epochs between 100 (standard) and 1000 (10x).",1,related,1,positive
"In prior work, RigL executed >5x extended training for a 99%-sparse model only [16].",1,related,1,positive
"Therefore, to create a more fair comparison, we consider estimated Floating-Point Operations (FLOPs) necessary for inference; these are computed as in [16].",1,related,1,positive
"Following [37, 39], we apply a cosine decay scheduler to alleviate this problem:",1,related,1,positive
"Ensuring a fair comparison we only consider the dense-to-sparse training paradigm, as opposed to pruning at initialization [46, 66] and dynamic sparse training methods [13, 16].",1,related,1,positive
"As the growing criterion, we choose the uniform random [40] and gradient [12] approaches, as they are widely used in the literature.",1,related,1,positive
2 and gradient growth (we select the gradient-based growth method as it is known to provide good performance in this setup [12]).,1,related,1,positive
We compare the results with the original dense model and static sparse training with ERK initialization.,1,related,1,positive
"DST Settings specific for dynamic sparse training: We use the ER initialization for the distribution of sparsity levels in MLP, and the ERK initialization for the convolutional models.",1,related,1,positive
"We do not delve deeply into this matter at present, but it is unequivocal that unstructured sparsity methods [11, 8, 27] allocate more weights to certain fixed visual points simultaneously.",1,related,1,positive
We follow the widely-used sparse training framework used in [39; 10].,1,related,1,positive
Method Channel Sparsity 10% 20% 30% Standard RigL [10] 76.,1,related,1,positive
"Taking the most representative DST approaches SET [39] and RigL [10] as examples, we measure the number of the Sparse Amenable Channels across layers in Figure 2, with v equals 0%, 20%, 30%, and 40%.",1,related,1,positive
"We report the theoretical FLOPs to be independent of the used hardware, as it is done in the unstructured pruning literature [31, 10].",1,related,1,positive
"1 Performance Comparison with Pruning and Sparse Training Algorithms We compare PALS with a standard during-training pruning approach (GMP [68]), GraNet [31], and a well-known sparse training method (RigL [10]).",1,related,1,positive
"Method Shrink Stable Expand Adaptive Sparsity Schedule Automatically tuning sparsity level RigL [10] ✗ ✓ ✗ ✗ ✗ GMP [68] ✓ ✗ ✗ ✗ ✗ GraNet [31] ✓ ✓ ✗ ✗ ✗ PALS (ours) ✓ ✓ ✓ ✓ ✓ In this work, we take advantage of the successful mechanism of “Shrink” from during-training pruning (such as GraNet [31]) and “Stable” from DST (such as RigL [10]) and propose for the first time the “Expand” mechanism, to design a method to automatically optimize the sparsity level during training without requiring to determine it beforehand.",1,related,1,positive
"GraNet gradually shrinks a network (here, we start from a dense network) during the training to reach a pre-determined sparsity level, while allowing for connection regeneration inspired by DST. GraNet algorithm is described in Appendix B.",1,related,1,positive
PALS GraNet*[31] RigL*[10] GMP*[68] Dataset loss sparsity epochs loss sparsity epochs loss sparsity epochs loss sparsity epochs Electricity 0.,1,related,1,positive
"In this work, we take advantage of the successful mechanism of “Shrink” from during-training pruning (such as GraNet [31]) and “Stable” from DST (such as RigL [10]) and propose for the first time the “Expand” mechanism, to design a method to automatically optimize the sparsity level during training without
requiring to determine it beforehand.",1,related,0,negative
"Algorithm Layer-wise Sparsity
Uniform [54] sl ∀ l ∈ [0, N) ER [12] 1 − n l−1+nl
nl−1×nl
ERK [14] 1 − n l−1+nl+wl+hl
nl−1×nl×wl×hl
Since the graph size of the proposed GE is based on the size of the input data, we selected three datasets with the same data sizes, namely CIFAR-10, CIFAR-100 [28], and the downscaled Tiny-ImageNet (of size 32 × 32 pixels) [11].",1,related,1,positive
"Sources are the following:
• SNIP https://github.com/mil-ad/snip • GraSP https://github.com/alecwangcq/GraSP • SynFlow https://github.com/ganguli-lab/Synaptic-Flow • ProsPR https://github.com/mil-ad/prospr • DSR and SNFS https://github.com/TimDettmers/sparse_learning • ITOP https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization • RigL https://github.com/nollied/rigl-torch9
• Uniform, ER and ERK https://github.com/VITA-Group/Random_Pruning",1,related,1,positive
"We propose a novel DST method, Structured RigL (SRigL), based on RigL (Evci et al., 2021).",1,related,1,positive
"We achieve the desired overall sparsity by distributing the per-layer sparsity according to the Erdős-Rényi-Kernel (ERK) (Evci et al., 2021; Mocanu et al., 2018) distribution, which scales the per-layer sparsity based on the number of neurons and the dimensions of the convolutional kernel, if present.",1,related,1,positive
"We achieve the desired overall sparsity by distributing the per-layer sparsity according to the ERK (Evci et al., 2021; Mocanu et al., 2018) distribution, which scales the per-layer sparsity based on the number of neurons and the dimensions of the convolutional kernel, if present.",1,related,1,positive
"1) Experiment Setup: To answer the RQ3, we select 4 state-of-the-art model compression techniques proposed in the last two years (i.e., LAMP [30], Global [31], Uniform+ [32] and ERK [33]) to see whether our DeepArc framework can speed up the model compression efficiency.",1,related,1,positive
"We use ERK sparsity distribution (Mocanu et al., 2018; Evci et al., 2020) in our ViT experiments.",1,related,1,positive
"Though most results match previous work, we observe a significant improvement for the accuracy achieved by the SET algorithm compared to the implementation done in (Evci et al., 2020).",1,related,1,positive
"We train 80% sparse ResNet-50 models on ImageNet to reproduce previous results reported in the literature (Gale et al., 2019; Evci et al., 2020).",1,related,1,positive
"Using GMP and an extended training schedule, we are able to obtain sparse models that match or outperform the dense baseline, both in terms of accuracy and ROC-AUC values, even at high (≥ 99%) sparsities, while providing substantial improvements in theoretical FLOPs (computed as in [14]), and practical inference speed on CPU when using the DeepSparse inference engine [10].",1,related,1,positive
"For the latter, we implement the sparse training methods (i.e., SET [20], RigL [22]) on SNN models (i.e., SET-SNN, RigL-SNN).",1,related,1,positive
"We adopt the default hyperparameters from RigL (Evci et al., 2020) for dynamic sparsity.",1,related,1,positive
"In this work, we focus on improving the training efficiency (test-accuracy w.r.t training FLOPs) of DNNs.
Recent works (Evci et al., 2020; Jayakumar et al., 2020) have explored using weight sparsity to reduce the FLOPs spent in training.",1,related,0,negative
"%} using three sparse training methods: static sparsity, SET (Mocanu et al., 2018) and RigL (Evci et al., 2020).",1,related,1,positive
"Sparsity Setup For enabling the SIFT transformations, we use the RigL (Evci et al., 2020) algorithm in its default hyperparameter settings (α = 0.3,∆T = 100), with the drop-fraction (α) annealed using a cosine decay schedule for 75% of the training run.",1,related,1,positive
"During training, DST methods periodically update the sparse connectivity of the network; e.g., in Mocanu et al. (2018); Evci et al. (2020) authors remove a fraction ζ of the parameters θs and add the same number of parameters to the network to keep the sparsity level fixed.",1,related,1,positive
We use gradients for weight regrowth Evci et al. (2020).,1,related,1,positive
The starting point of our implementation is based on the sparse evolutionary training introduced as SET in Mocanu et al. (2018)3 to which we added the gradient-based connections growth proposed in RigL Evci et al. (2020).,1,related,1,positive
"In this section, we compare NeuroFS with RigL Evci et al. (2020), which is a DST method mainly designed for classification; it uses gradient for weight regrowth when updating the sparse connectivity in the DST framework.",1,related,1,positive
"We use gradients for weight regrowth Evci et al. (2020). For each hidden layer h(l), NeuroFS performs the following two steps: 1.",1,related,1,positive
"Following a mainstream convention in many sparse training papers (Frankle & Carbin, 2019; Gale et al., 2019; Evci et al., 2020; Lee et al., 2019; Liu et al., 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as…",1,related,1,positive
"Following a mainstream convention in many sparse training papers (Frankle & Carbin, 2019; Gale et al., 2019; Evci et al., 2020; Lee et al., 2019; Liu et al., 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al.",1,related,1,positive
"We have also verified that various during-training pruning methods like GMP [55], RigL [7], and GraNet [32] could improve the performance of OOD detection, which are often more efficient.",1,related,0,negative
"We show the performance of three methods: GMP [55], RigL [7], and GraNet [32].",1,related,1,positive
"Methods are proposed to drop the unimportant weights [13, 55, 7, 32].",1,related,0,negative
"For the growing criterion, we test both random growth SDST(SET) (Mocanu et al., 2018; Liu et al., 2021c) and gradient-based growth SDST(RigL) (Evci et al., 2020).",1,related,1,positive
"For SDST methods, we test both grow methods, i.e., SDST(SET) (Mocanu et al., 2018) which grows connections randomly and SDST(RigL) (Evci et al., 2020) which grows connections via gradient.",1,related,1,positive
"Following Evci et al. (2020), we specify the hyper-parameters of DST through sparsity distribution, update schedule, drop criterion, and grow criterion.",1,related,1,positive
"At initialization, we use the commonly adopted Erdős-Rényi-Kernel (ERK) strategy (Evci et al., 2020; Dettmers & Zettlemoyer, 2019; Liu et al., 2021c) to allocates higher sparsity to larger layers.",1,related,1,positive
"Out of simplicity, we increase the density by growing the connections with the largest gradient magnitude (Evci et al., 2020).",1,related,1,positive
"We compare our approach against Incremental (Zhu & Gupta, 2018), STR (Kusu-pati et al., 2020), Global Magnitude (Singh & Alistarh, 2020), WoodFisher (Singh & Alistarh, 2020), GMP (Gale et al., 2019), Variational Dropout (Molchanov et al., 2017), RIGL (Evci et al., 2020), SNFS (Dettmers & Zettlemoyer, 2020) and DNW (Wortsman et al., 2019 MobileNetV1.",1,related,1,positive
"…wheresoever equally, and yet becomes totally ignorant of nowadays pruning researches like sparsity budget allocation, e.g., Erdős-Rényi (Mocanu et al., 2018) and Erdős-Rényi-Kernel (ERK) (Evci et al., 2020), or commonsense in this area like “Leave at least one path from input through output”.",1,related,1,positive
"This design endows our method with versatility while treating weight wheresoever equally, and yet becomes totally ignorant of nowadays pruning researches like sparsity budget allocation, e.g., Erdős-Rényi (Mocanu et al., 2018) and Erdős-Rényi-Kernel (ERK) (Evci et al., 2020), or commonsense in this area like “Leave at least one path from input through output”.",1,related,1,positive
"Implementations: We follow the settings in (Evci et al., 2020; Sundar & Dwaraknath, 2021).",1,related,1,positive
"We perform a comprehensive empirical evaluation of CigL, comparing it with the popular baseline method RigL (Evci et al., 2020).",1,related,1,positive
Datasets & Model Architectures: We follow the settings in Evci et al. (2020) for a comprehensive comparison.,1,related,1,positive
"Inspired by the widely-used sparse training method RigL (Evci et al., 2020), we believe a larger weight/gradient magnitude implies that the weight is more helpful for loss reduction and needs to be activated.",1,related,0,negative
"Notably, the speedups attained through RigL surpass those obtained through GMP.",1,related,0,negative
"We set the hyper-parameters α = 0.3, Tend = 80, and ∆T = 10 (introduced in the original paper (Evci et al., 2020)).",1,related,1,positive
"Finally, we examine the performance of SparseProp on the RigL method (Evci et al., 2020), a dynamic sparse training technique, to train a sparse ResNet18 model.",1,related,1,positive
"3, Tend = 80, and ∆T = 10 (introduced in the original paper (Evci et al., 2020)).",1,related,1,positive
"We calculate the gradient variance and correlation of the ResNet-50 on CIFAR-100 from RigL (Evci et al., 2020) and SET (Mocanu et al., 2018) at different sparsities including 0%, 50%, 80%, 90%, and 95%.",1,related,1,positive
"Aligned with popular sparse training methods (Evci et al., 2020; Özdenizci & Legenstein, 2021; Liu et al., 2021), we choose piecewise constant decay schedulers for learning rate and weight decay.",1,related,1,positive
"We add our AGENT to three recent sparse training pipelines, namely SET (Mocanu et al., 2018), RigL (Evci et al., 2020), BSR-Net (Özdenizci & Legenstein, 2021) and ITOP (Liu et al., 2021).",1,related,1,positive
"We summarize additional experimental results for the BSR-Net-based Özdenizci & Legenstein (2021), RigLbased Evci et al. (2020), and ITOP-based Liu et al. (2021) models.",1,related,1,positive
"iteratively updated with various criteria (Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021; Özdenizci & Legenstein, 2021).",1,related,0,negative
"Implementations: Aligned with the choice of Evci et al. (2020); Sundar & Dwaraknath (2021); Özdenizci & Legenstein (2021), the parameters of the model are optimized by SGD with momentum.",1,related,1,positive
"In RigL-based results, we follow the settings in Evci et al. (2020); Sundar & Dwaraknath (2021).",1,related,1,positive
"As for the blue curve for our A-RigL, it is always on the top of the green curve for RigL, indicating that the speedup is successful.
methods in RigL-based models Evci et al. (2020).",1,related,1,positive
"L G
iteratively updated with various criteria (Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021; Özdenizci & Legenstein, 2021).",1,related,0,negative
"Crucially, following [11], the updated inverse (HUj+1) −1 can be calculated efficiently by removing the first row and column, corresponding to j in the original H, from the inverse of (HUj ) −1 in (3)For example, structured (column-wise) pruning ResNet50 to 50% structured sparsity without accuracy loss is challenging, even with extensive retraining [30], while unstructured pruning to 90% sparsity is easily achievable with state-of-the-art methods [6, 39].",1,related,1,positive
"…finding a fixed sparse mask at the initialization as we mentioned in introduction, on the other hand, dynamic sparse training allows the sparse mask to be updated during training, e.g., (Mocanu et al., 2018; Mostafa and Wang, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021a,c,d).",1,related,1,positive
"For completeness, comparison is also provided with DNW [38], RIGL [3] and GraNet [25] (see Section 2 for the presentation of those methods), implemented as recommended by their respective authors.",1,related,1,positive
"For other baselines, we select SNIP [10] and GraSP [11] as the static mask training baselines while adopting DeepR [32], SNFS [22], DSR [13], SET [12], RigL [14], MEST [23], RigL-ITOP [1] as the dynamic mask training baselines as shown in Table II.",1,related,1,positive
We use the same sparsity initialization method ERK in the state-of-the-art sparse training method such as RigL [14] and ITOP [1].,1,related,1,positive
"We do this because [20] showed that the layer-wise sparsity obtained by this scheme outperforms the other well-studied sparsity ratios [19, 37, 39].",1,related,1,positive
"Aligned with many prior works [20, 54, 27], we find that the weight magnitude is an effective metric for estimating the connection importance (#4).",1,related,1,positive
The value of pl is decided based on ErdősRényi-Kernel scaling (used in (Evci et al. 2021; Raihan and Aamodt 2020)).,1,related,1,positive
"To confirm this, we trained
GGR only using either gradient-based insertion or random insertion using a sparsity of 90% and ERK.",1,related,1,positive
"We implement two different approaches for selecting the layer density at initialization: Erdős-Rényi-Kernel (ERK) (Mocanu et al., 2018b; Evci et al., 2020) and uniform density.",1,related,1,positive
"We implement two different approaches for selecting the layer density at initialization: Erdős-Rényi-Kernel (ERK) (Mocanu et al., 2018b; Evci et al., 2020) and uniform density.",1,related,1,positive
"The percentage of weights to be redistributed is decreased every epoch using the cosinefunction as suggested by the authors of RigL (Evci et al., 2020).",1,related,1,positive
"We compare with global magnitude (GM) following the same schedule as CAP, as well as the state-of-the-art SViTE [Chen et al., 2021] paper, which trains the sparse model from scratch using a variant of RigL [Evci et al., 2020], but for a total of 600 epochs.",1,related,1,positive
"The only existing prior work on unstructured ViT pruning is SViTE [Chen et al., 2021], which performed careful customization of the RigL pruning method [Evci et al., 2020] to the special case of ViT models.",1,related,1,positive
"We take a simple 5-layer MLP to clarify this operation: its dimensions are [512, 100, 100, 100, 10] with 4 weight matrices w1 ∈ R512×100, w2 ∈ R100×100, w3 ∈ R100×100, and w4 ∈ R100×10.",1,related,1,positive
"In spirit, our effort can be likened to methods like RigL (Evci et al., 2020), which discovers the wiring of a sparse neural network during training rather than pruning a dense network post training.",1,related,1,positive
"Following [15, 9], we apply a cosine decay scheduler to alleviate this problem:",1,related,1,positive
"ER networks rewired with DST: Test Accuracies for an ER(p) VGG16 with a fixed mask and after rewiring edges with RiGL (Evci et al., 2020; Liu et al., 2021) on CIFAR10.",1,related,1,positive
"For sparse to sparse training with DST, we us weight magnitude as importance score for pruning (with prune rate 0.5) and gradient for growth.",1,related,1,positive
"99 an ER network of some initial sparsity and further pruned to a final sparsity (initial → final) while modifying the mask using the RiGL (Evci et al., 2020) algorithm.",1,related,1,positive
"In the DST experiments, we use the same setup as random pruning, and modify the mask every 100 iterations.",1,related,1,positive
"61 In addition to the rewiring experiments shown in Table 2, we use Dynamical Sparse Training to prune an already sparse ER network to a higher sparsity and see if this can achieve the same performance as performing DST starting from a denser network.",1,related,1,positive
"Our main results could also be interpreted as theoretical justification for Dynamic Sparse Training (DST) (Evci et al., 2020; Liu et al., 2021.; Bellec et al., 2018), which prunes random networks of moderate sparsity.",1,related,1,positive
"These can be partially remedied by using the rewiring strategy of Dynamic Sparse Training (DST) (Evci et al., 2020",1,related,1,positive
"Sparse to sparse training with DST Final test accuracy for VGG16 on CIFAR10 is reported where the model is initialized with an ER network of some initial sparsity and further pruned to a final sparsity (initial → final) while modifying the mask using the RiGL (Evci et al., 2020) algorithm.",1,related,1,positive
"Our main results could also be interpreted as theoretical justification for Dynamic Sparse Training (DST) (Evci et al., 2020; Liu et al., 2021.",1,related,1,positive
"Dynamical Sparse Training To improve randomly pruned networks at extremely high sparsities, we employ the RiGL algorithm (Evci et al., 2020) to obtain Table 2.",1,related,1,positive
"We compare Global MP with various popular SOTA algorithms that are well known for pruning, such as SNIP [42], SM [7], DSR [6], DPF [9], GMP [18], DNW [19], RigL [8], and STR [10].",1,related,1,positive
"Using this dataset, we compare Global MP with SOTA algorithms like GMP [18], DSR [6], DNW [19], SM [7], RigL [8], WoodFisher [63], MFAC [62], DPF [9], and STR [10].",1,related,1,positive
Rigging the Lottery (RigL) [8] allocates sparsity based on the number of parameters in a layer.,1,related,1,positive
"We compare the accuracy, training FLOPs, and memory costs of our framework with the most representative sparse training works [2, 3, 53, 54, 4, 19, 5, 1] at different sparsity ratios.",1,related,1,positive
"The comparison of key features between SpFDE and other representative sparse training works, i.e., SNIP [2], GraSP [3], RigL [5], ITOP [6], SET [19], DSR [4], and MEST [1], is provided in Tab.",1,related,1,positive
"We also adapt representative sparse training methods (SNIP (34), RigL (19)) to the CL setting by combining them with DER++ (SNIP-DER++, RigL-DER++).",1,related,1,positive
"We also use the best hyperparameter setting reported in (8; 56) for CL methods, and in (19; 34) for CL-adapted sparse training methods.",1,related,1,positive
"Moreover, we evaluate the training FLOPs (19), and memory footprint (66) (including feature map pixels and model parameters during training) to demonstrate the efficiency of each method.",1,related,1,positive
"For sparse training, we follow a similar drop and grow method that was proposed in RigL [47] to explore the sparse model architecture during the sparse training process.",1,related,1,positive
The sparse training method RigL [7] is applied in our experiment to ensure that the model weights are sparse.,1,related,1,positive
"We now compare our SPDST mask initialization, with that of parameter density distribution evaluated via ERK+ Huang et al. (2022); Evci et al. (2020).",1,related,1,positive
"Hence, we can approximate the number of MAC operations as 2Nc as done in [21].",1,related,1,positive
The prune-redistribute-regrowth cycle is adopted in (Dettmers and Zettlemoyer 2019; Mostafa and Wang 2019; Evci et al. 2020) to change weights according to different criteria dynamically.,1,related,0,negative
5 3 7 – RigL (Evci et al. 2020)∗ Unstructured 50%×1 W – – 76.,1,related,0,negative
"Aware of the sensitivity and negative impact of changing initialization identified by (Evci et al., 2020c), we point that that linear scaling will not hurt the original sparse mask’s initialization, thanks to the BatchNorm layer which will effectively absorb any linear scaling of the weights.",1,related,1,positive
"Code Our code is built upon the TF-Agents (Guadarrama et al., 2018), Dopamine (Castro et al., 2018), and RigL (Evci et al., 2020) codebases.",1,related,1,positive
"Within network sparsity In Figure 4 (left) we turn our attention to the question of distributing parameters within networks and compare two strategies; uniform and ERK (Evci et al., 2020).",1,related,1,positive
"Comparison of our model with SOTA pruning methods, DPF [29], STR[23], LAMP[27], RiGL[11], and SuRP[19].",1,related,1,positive
"4.3, we further show the performance of LBC and traditional unstructured sparsity methods including RigL [7],
GMP [41], STR [17] under similar total compression rates.",1,related,1,positive
"3, we further show the performance of LBC and traditional unstructured sparsity methods including RigL [7],",1,related,1,positive
"Inspired from (Evci et al., 2020), we take similar steps to update the local mask on each client.",1,related,1,positive
"We highlight the contributions we make to extend the traditional sparse training techniques like Rigl (Evci et al., 2020), and Set (Mocanu
et al., 2018) to the proposed decentralized sparse training technique as follows: (i) To deal with the data heterogeneity and to learn different sparse models…",1,related,1,positive
"We highlight the contributions we make to extend the traditional sparse training techniques like Rigl (Evci et al., 2020), and Set (Mocanu",1,related,1,positive
"We highlight the contributions we make to extend the traditional sparse training techniques like Rigl (Evci et al., 2020), and Set (Mocanu
et al., 2018) to the proposed decentralized sparse training technique as follows: (i) To deal with the data heterogeneity and to learn different sparse models for each client, decentralized sparse training operates on the local client instead of operating on the centralized device.",1,related,1,positive
"We follow RigL [7], which regenerates part of the pruned connections based on the gradient magnitude.",1,related,1,positive
"We compressed the pre-trained state-of-the-art deraining models by the same FLOPs, for a fair comparison to the most classical (l1 [13]) and modern (erk [8], lamp [15]) pruning methods.",1,related,1,positive
"Considering that the original data is not available, we mainly compare with the alternative magnitude-based pruning methods, including the most classical l1 regularization [13], and the most modern methods of erk [8] and lamp [15].",1,related,1,positive
The topology evolution in RLx2 is made by adopting the RigL [10] method.,1,related,1,positive
"Our RLx2 algorithm, which contains both topology evolution [10] (using the same scheme in TE) and accurate value estimation (using multi-step TD targets [19]), is able to achieve a performance close to TE+Q∗ without the need for pre-trained expert Q-values.",1,related,1,positive
"Algorithm 1 Topology Evolution [10] 1: Nl: Number of parameters in layer l 2: θl: Parameters in layer l 3: Mθl : Sparse mask of layer l 4: sl: Sparsity of layer l 5: L: Loss function 6: ζt: Update fraction in training step t 7: for each layer l do 8: k = ζt(1− sl)Nl 9: Idrop = ArgTopK(−|θl Mθl |, k) 10: Igrow = ArgTopKi/ ∈θl Mθl\Idrop(|∇θlL, k|) 11: Update Mθl according to Idrop and Igrow 12: θl ← θl Mθl 13: end for",1,related,1,positive
"In particular, we apply a gradient-guided topology search scheme [10] to enable dynamic network evolution.",1,related,1,positive
", 2021b]; one SST method: ERK [Evci et al., 2020]; and one pruning at initialization approach: SNIP [Lee et al.",1,related,1,positive
"To ver-
ify the effectiveness of Sup-tickets, we apply it to various sparse training methods, including 3 DST methods: SET, RigL [Evci et al., 2020], and GraNet [Liu et al., 2021b]; one SST method: ERK [Evci et al., 2020]; and one pruning at initialization approach: SNIP [Lee et al., 2018].",1,related,1,positive
"ify the effectiveness of Sup-tickets, we apply it to various sparse training methods, including 3 DST methods: SET, RigL [Evci et al., 2020], and GraNet [Liu et al.",1,related,1,positive
"Our baselines are iterative magnitude pruning [48], RigL with the Erdos-Renyi-Kernel (ERK) sparsity distribution [16], Soft Threshold Weight Reparameterization (STR) [25], probabilistic masking (ProbMask) [47], OptG [46], and Top-KAST with Powerpropagation and ERK [24; 37].",1,related,1,positive
"For Top-KAST, we exclude the first convolutional layer from pruning (following [37; 16]) and we use fully dense backward passes (i.",1,related,1,positive
"We do not prune biases and batch-normalization parameters, as they only account for a small fraction of the total number of parameters yet are crucial for obtaining well-performing models [19].",1,related,1,positive
"We train the sparse dynamic convolution following an iterative pruning process [11, 13].",1,related,1,positive
"We conjecture that the sparse property [16,18] has reduced the redundancy in high-resolution feature maps in our HRCA and leads to higher performance and efficiency.",1,related,1,positive
"As for random pruning, every layer can be uniformly pruned with the same pre-defined pruning ratio (Mariet and Sra, 2015; He et al., 2017; Gale et al., 2019) or the pruning ratio can be varied for different layers such as Erdö-Rényi (Mocanu et al., 2018) and Erdö-Rényi Kernel (Evci et al., 2020).",1,related,1,positive
"…for random pruning, every layer can be uniformly pruned with the same pre-defined pruning ratio (Mariet and Sra, 2015; He et al., 2017; Gale et al., 2019) or the pruning ratio can be varied for different layers such as Erdö-Rényi (Mocanu et al., 2018) and Erdö-Rényi Kernel (Evci et al., 2020).",1,related,1,positive
"5.3, we discuss IP and SP for more sophisticated sparse training methods, namely LTs [15] and the DST methods SET [49] and RigL [13].",1,related,1,positive
"For training sparse networks, we distinguish between (i) pruning at initialization (PaI) [9,35,66,71,75] which prunes the network at initialization and fixes zeroed parameters during training, (ii) finding the sparse architecture to be finally trained by iterative train-prune-reset cycles, a so called lottery ticket (LT) [14,15], and (iii) dynamic sparse training (DST) [13,39,49] which prunes the network at initialization, but allows the pruning mask to be changed during training.",1,related,1,positive
"We demonstrate this by achieving SOTA results with the application of IP to SOTA standard PaI, LT, DST as well as classical pruning methods.",1,related,1,positive
"Furthermore, we want to check if IP boosts the SOTA methods LT and RigL as well.",1,related,1,positive
"3, we discuss IP and SP for more sophisticated sparse training methods, namely LTs [15] and the DST methods SET [49] and RigL [13].",1,related,1,positive
Related work covers general pruning and pruning before training and DST. Training a sparse model allows to learn non-zero FB coefficients and FBs jointly from scratch.,1,related,1,positive
"5 compare SP and IP on various sparse training and other pruning methods, namely:
DST randomly prunes the model at initialization.",1,related,1,positive
"The pruning mask is updated each 1, 500 iterations for SET and 4, 000 for RigL.",1,related,1,positive
"Table 1 compares IP and SP on the SOTA pruning methods RigL [13], GMP [17] and FT [59].",1,related,1,positive
"Sparse initialization of D(x,θD) 2: G(z,θsG)← ERK(G(z,θ), sG) .",1,related,1,positive
"Output: Sparse Generator G(z,θsG), Sparse discriminator D(x,θsD )
1: D(x,θsD )← ERK(D(x,θ), sD) .",1,related,1,positive
"We consider unstructured sparsity (individual weights are removed from a network) in this paper, not only due to its promising ability to preserve performance even at extreme sparsities [15, 36] but also the increasing support for sparse operations on the practical hardware [42–45].",1,related,1,positive
"Note that newly added connections are not activated in the last sparse topology, and are initialized to zero since it establishes better performance as indicated in (Evci et al., 2020a; Liu et al., 2021b).",1,related,1,positive
"However, our flying bird first removes the parameters with the lowest magnitude, which ensures a small term of the first-order Taylor approximation of the loss and thus limits the impact on the output of networks (Evci et al., 2020a).",1,related,1,positive
"As two major components in the dynamic sparsity exploration (Evci et al., 2020a), we conduct thorough ablation studies in Table 4 and 5.",1,related,0,negative
"Comprehensive results of these subnetworks at 80% and 90% sparsity are reported in Table 1, where the chosen sparsity follows routine options (Evci et al., 2020a; Liu et al., 2021b).",1,related,0,negative
"We compare ASNI-I against its counterparts [45,21,5] where the two last ones are the state-of-the-art methods.",1,related,1,positive
"For instance, RigL [8] re-allocates the removed",1,related,1,positive
"Note that our OptG sorts the weights in a global manner to automatically decide a layer-wise sparsity budget, thus avoiding the rule-of-thumb design [8] or complex hyper-parameter tuning for learning sparsity distributions [16].",1,related,1,positive
"Besides, we compare our OptG with several the state-of-the-arts including SNIP [25], GraSP [45], SET [34], GMP [11], SynFlow [44], DNW [46], RigL [8], GSM [6], STR [22] and GraNet [29].",1,related,1,positive
"We begin on the server by initializing a server network θ(1) and a sparse maskm(1), following the layer-wise sparsity distribution described in (Evci et al. 2020).",1,related,1,positive
"Note that following the convention of (Mocanu et al. 2018; Evci et al. 2020; Liu et al. 2021c), FedDST so far only considers element-wise unstructured sparsity.",1,related,1,positive
"We begin on the server by initializing a server network θ1 and a sparse maskm1, following the layer-wise sparsity distribution described in (Evci et al. 2020).",1,related,1,positive
"As in (Evci et al. 2020; Dettmers and Zettlemoyer 2019), we use a cosine decay update schedule for αr,",1,related,1,positive
"As in (Evci et al. 2020; Dettmers and Zettlemoyer 2019), we use a cosine decay update schedule for αr,
αr = α
2
( 1 + cos ( (r − 1)π Rend )) .",1,related,1,positive
"However, suitable values for α are still smaller than in RigL; for α ∈ [0.001, 0.05], FedDST significantly outperforms other methods.",1,related,1,positive
The client-side mask readjustment procedure is familiar and takes inspiration from RigL (Evci et al. 2020).,1,related,0,negative
"For RandomMask, we randomly sample weights at the server, then perform layer-wise magnitude pruning, following the ERK sparsity distribution (Evci et al. 2020), before the first round, and perform FedAvgM on this sparse network.",1,related,1,positive
"For RigL, we use the PyTorch implementation of Sundar & Dwaraknath (2021).",1,related,1,positive
"Results of 98% sparsity in Table 2 show that RMDA consistently outdoes RigL, indicating regularized training could be a promising alternative to pruning.",1,related,0,negative
"We run RigL with 1000 epochs, as its performance at the default 500 epochs was unstable, and let
RMDA use the same number of epochs.",1,related,1,positive
"We compare RMDA with a state-of-the-art pruning method RigL (Evci et al., 2020).",1,related,1,positive
"RigL aims to sparsify model weights/parameters, so we use it as a baseline in MLP-based models (Mixer).",1,related,1,positive
"On the WikiText-103 1State-of-the-art sparse training methods require up to 5× more training epochs compared to dense models [Evci et al., 2020] 2An unstructured sparse model with 1% nonzero weights can be as slow as a dense model [Hooker, 2020]",1,related,1,positive
"Specifically, we achieve training speed up on both MLP-Mixer and ViT models by up to 2.3× wall-clock time with no drop in accuracy compared to the dense model and up to 4× compared to RigL, BigBird and other sparse baselines.",1,related,0,negative
"On the WikiText-103
1State-of-the-art sparse training methods require up to 5× more training epochs compared to dense models [Evci et al., 2020]
2An unstructured sparse model with 1% nonzero weights can be as slow as a dense model [Hooker, 2020]
language modeling task, we speed up GPT-2 Medium…",1,related,1,positive
"Figure 6: Comparison with a representative sparse training baseline RigL [Evci et al., 2020].",1,related,0,negative
"For a fair comparison, we conduct the experiment on Mixer-S/32 model for 100 epochs because RigL aims for sparsity on weights, while we aim for both weights & attention.",1,related,1,positive
"Additionally, we include the “The Rigged Lottery” (RigL) method [12] with Erdős-Rényi-Kernel (ERK) weight density.",1,related,1,positive
"The top-performing methods we consider here are Soft Threshold Reparametrization (STR) [41], Alternating Compressed/DeCompressed Training (AC/DC) [57] and “The Rigged Lottery” (RigL) [12].",1,related,1,positive
"We also consider the “The Rigged Lottery” (RigL) method [12], which achieves close to state-of-the-art ImageNet results by allowing for dynamic weight pruning and re-introduction with long finetuning periods, to be a regularization method.",1,related,1,positive
"In comparison with the multishot results, we observe that SYNFLOW, SNIP, and MAGNITUDE pruning outperform RIGL on this task for the extreme sparsity levels (compare Fig.",1,related,1,positive
"1Note that Evci et al. (2020) use percentage of pruned parameters for their plots, i.e. 1−sparsity in our paper.",1,related,0,negative
"Note that the version of SNIP used in the original paper is essentially the singleshot approach, which indeed performs worse than RIGL (compare Fig.",1,related,1,positive
"For our benchmark data, we construct similar networks as for the multishot experiments – i.e. depth 6 and width 100 fully connected networks – and run the available implementation of RIGL with default parameters as suggested in the paper, and Adam optimization with the same parameter settings as for all other considered methods.",1,related,1,positive
"Our results on Circlematch those results, with RIGL being able to match ground truth ticket performance for sparsity .5 and .1.",1,related,0,negative
"We show in Appendix B that neither dynamic sparse training (Evci et al., 2020) nor LT fine-tuning techniques (Liu et al.",1,related,1,positive
"The original results reported in Evci et al. (2020) indicate that for their considered (classification) tasks, RIGL outperforms other dynamic sparse training methods, and that for target sparsity levels of .1 and lower, performance quickly deteriorates for all methods.1 In the original paper, there…",1,related,0,negative
"For regression tasks, we see a similar trend, with RIGL performing comparably good as SNIP for sparsity levels ≥ .1, but the performance decreases rapidly for more extreme target sparsitiy levels.",1,related,1,positive
"The original results reported in Evci et al. (2020) indicate that for their considered (classification) tasks, RIGL outperforms other dynamic sparse training methods, and that for target sparsity levels of .1 and lower, performance quickly deteriorates for all methods.1 In the original paper, there was no exploration of the more extreme sparsities considered here, nor a comparison to ticket pruning other than SNIP.",1,related,0,negative
"We show in Appendix B that neither dynamic sparse training (Evci et al., 2020) nor LT fine-tuning techniques (Liu et al., 2021a; Renda et al., 2020) find architectures that are competitive with our constructed ground truth tickets.",1,related,1,positive
"RIGL While the main focus of our paper are lottery tickets, we here briefly discuss results for RIGL (Evci et al., 2020), a state-of-the-art dynamic sparse training approach, which results in sparsified and trained network architectures which are comparable to trained ’weak’ tickets.",1,related,1,positive
"Generally, for the regression tasks we observe that RIGL is outperformed by the state-of-the-art ticket pruner SYNFLOW and iterative MAGNITUDE pruning.",1,related,1,positive
"We note that we follow the advice of Evci et al. (2020) and Dettmers & Zettlemoyer (2019) and do not prune biases and batch-normalization parameters, since they only amount to a negligible fraction of the total weights, however keeping them has a very positive impact on the performance of the…",1,related,1,positive
", RigL [11]) that use gradients of the dense model to find the weights to grow back, we only use sparse gradients to identify less important",1,related,1,positive
introduces a negligible amount of extra FLOPs (over baseline methods) we only show such values in the extended training setting to provide a fair comparison to the setup in [13].,1,related,0,negative
"We orient ourselves primarily on the experimental setup in [13] & [14], both of which present techniques among the strongest in the literature.",1,related,1,positive
"We also provide results using the Erdos-Renyi Kernel [13], a redistribution of layerwise sparsity subject to the same fixed overall budget.",1,related,1,positive
"In particular, we find that gradient-based re-allocation (Evci et al., 2019) results in a collapse of the explored network parameters (Figure 11), which we mitigate through the use of random parameter re-allocation.",1,related,1,positive
"We found that the cosine decay of the pruning
ratio introduced in Evci et al. (2019) outperforms constant pruning schedules and leads to a reduction of the changes in network topology during training.",1,related,1,positive
"In particular, during the re-allocation step of DynSparse training, we use random re-allocation of pruned weights instead of gradient-based techniques as in RigL (Evci et al., 2019).",1,related,1,positive
"To this end, we adopt and investigate DynSparse training techniques (Dettmers & Zettlemoyer, 2019; Evci et al., 2019) for pre-training of BERT bidirectional language encoder (Devlin et al., 2018) based on the highly scalable Transformer architecture (Vaswani et al., 2017).",1,related,1,positive
"To this end, we adopt and investigate DynSparse training techniques (Dettmers & Zettlemoyer, 2019; Evci et al., 2019) for pre-training of BERT bidirectional language encoder (Devlin et al.",1,related,1,positive
"Given that DynSparse training has been primarily developed for vision architectures (Dettmers & Zettlemoyer, 2019; Evci et al., 2019) and did not show competitive performance on the language tasks, we find it necessary to reassess some of the algorithm choices for BERT.",1,related,1,positive
"We additionally compare AC/DC with Top-KAST and RigL, in terms of the validation accuracy achieved depending on the number of training FLOPs.",1,related,1,positive
"GFLOPs Inference EFLOPs Train
Dense 0 76.84 8.2 3.14
AC/DC 95 73.14± 0.2 0.11× 0.53× RigL1× 95 67.5± 0.1 0.08× 0.08× RigL1× (ERK) 95 69.7± 0.17 0.12× 0.13× Top-KAST 95 fwd, 50 bwd 71.96 0.08× 0.22×
STR 94.8 70.97 0.04× - WoodFisher 95 72.12 0.09× -
AC/DC 98 68.44± 0.09 0.06× 0.46× Top-KAST 98 fwd, 90 bwd 67.06 0.05× 0.08×
STR 97.78 62.84 0.02× - WoodFisher 98 65.55 0.05× -
ResNet50 Results.",1,related,1,positive
"GFLOPs Inference EFLOPs Train
Dense 0 76.84 8.2 3.14
AC/DC 80 76.3± 0.1 0.29× 0.65× RigL1× 80 74.6± 0.06 0.23× 0.23× RigL1×(ERK) 80 75.1± 0.05 0.42× 0.42× Top-KAST 80 fwd, 50 bwd 75.03 0.23× 0.32×
STR 79.55 76.19 0.19× - WoodFisher 80 76.76 0.25× -
AC/DC 90 75.03± 0.1 0.18× 0.58× RigL1× 90 72.0± 0.05 0.13× 0.13× RigL1× (ERK) 90 73.0± 0.04 0.24× 0.25× Top-KAST 90 fwd, 80 bwd 74.76 0.13× 0.16×
STR 90.23 74.31 0.08× - WoodFisher 90 75.21 0.15× -
Table 2: ResNet50/ImageNet, high sparsity results.",1,related,1,positive
"For example, the RigL technique [16] randomly ∗Correspondence to Alexandra Peste: alexandra.peste@ist.ac.at
35th Conference on Neural Information Processing Systems (NeurIPS 2021).
removes a large fraction of connections early in training, and then proceeds to optimize over the sparse support, providing savings due to sparse back-propagation.",1,related,1,positive
"Then, our results are quite close to RigL2×, with half the training epochs, and less training FLOPs.",1,related,0,negative
"Moreover, RigL does not prune the first layer and the depth-wise convolutions, whereas for the results reported we do not impose any sparsity restrictions.",1,related,1,positive
"GFLOPs Inference EFLOPs Train
Dense 0 71.78 1.1 0.44
AC/DC 75 70.3± 0.07 0.34× 0.64× RigL1× (ERK) 75 68.39 0.52× 0.53×
STR 75.28 68.35 0.18× - WoodFisher 75.28 70.09 0.28× -
AC/DC 90 66.08± 0.09 0.18× 0.56× RigL1× (ERK) 90 63.58 0.27× 0.29×
STR 89.01 62.1 0.07× - WoodFisher 89 63.87 - -
Semi-structured Sparsity.",1,related,1,positive
"Additionally, we experiment with extending the number of training iterations for AC/DC at 90% and 95% sparsity two times, similarly to Top-KAST and RigL which also provide experiments for extended training.",1,related,1,positive
"For this reason, we focus on gradient-based regeneration proposed in Rigged Lottery ( RigL) [9], i.",1,related,1,positive
"Again, we use the gradient as the importance score for regeneration, same as the regrow method as used in RigL [9].",1,related,1,positive
"We prune the weights with the smallest magnitude, as it has evolved as the standard method when pruning happens during training, e.g., GMP [77, 13] and DST [44, 9, 37].",1,related,1,positive
"All accuracies are in line with the baselines reported in the references [8, 11, 67, 9, 37].",1,related,0,negative
"Please note that models with the non-uniform sparsifying distribution in Table 2 already have the last FC layer pruned, thus the experiment setup is the same as the ones in [16].",1,related,1,positive
"In Table 10, we perform additional experiments to supplement Table 2 by pruning the last FC layer using the S-GaP method and comparing them with [16].",1,related,1,positive
We observe that the improvement over [16] is 1.,1,related,1,positive
We introduce the regurgitating tickets interpretation (RTI) based on the prior work in Evci et al. (2020b) and test its causal relationship with the success of IMP using repellent and attractive losses.,1,related,1,positive
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin. We find that this is not necessarily the case at very low or high sparsities. The LTH has spawned the idea that lottery tickets could be generated in a mostly data-independent fashion without relying on the very costly IMP. This would open up the possibility of fully sparse training without any prior dense training. For example, Morcos et al. (2019) hint at ""the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets."" Our evidence for the RTI indicates that such efforts might be misguided, as the construction of a lottery ticket requires prior knowledge of a dense solution. This is in line with Frankle et al. (2020b), who empirically examine current attempts at data-independent pruning at initialization (PAI), and show that these attempts significantly underperform compared to IMP.",1,related,1,positive
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin. We find that this is not necessarily the case at very low or high sparsities. The LTH has spawned the idea that lottery tickets could be generated in a mostly data-independent fashion without relying on the very costly IMP. This would open up the possibility of fully sparse training without any prior dense training. For example, Morcos et al. (2019) hint at ""the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets.",1,related,1,positive
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin. We find that this is not necessarily the case at very low or high sparsities. The LTH has spawned the idea that lottery tickets could be generated in a mostly data-independent fashion without relying on the very costly IMP. This would open up the possibility of fully sparse training without any prior dense training. For example, Morcos et al. (2019) hint at ""the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets."" Our evidence for the RTI indicates that such efforts might be misguided, as the construction of a lottery ticket requires prior knowledge of a dense solution. This is in line with Frankle et al. (2020b), who empirically examine current attempts at data-independent pruning at initialization (PAI), and show that these attempts significantly underperform compared to IMP. We hope that an improved understanding of IMP and the LTH, will allow future work to design better pruning and sparse optimization algorithms. In summary, our key contributions are as follows: 1. We demonstrate that the lottery ticket hypothesis can be extended to large networks and show that it is possible to train large neural networks from scratch without rewinding by using a sufficiently large batch size. This reinforces the hypothesis of Frankle et al. (2020a) that finding lottery tickets is possible when training is stable with respect to linear mode connectivity. 2. We introduce the regurgitating tickets interpretation (RTI) based on the prior work in Evci et al. (2020b) and test its causal relationship with the success of IMP using repellent and attractive losses.",1,related,1,positive
"To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50.",1,related,0,negative
"In the formulation of the RTI, we informally characterize the successive optima from IMP as similar.",1,related,1,positive
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin.",1,related,1,positive
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50.",1,related,1,positive
"Using the RTI, we can explain the relation of IMP with stability.",1,related,1,positive
"Based upon Frankle et al. (2020a) and Evci et al. (2020b), we hypothesized and tested the regurgitating tickets interpretation (RTI) as an explanation for the lottery ticket hypothesis.",1,related,1,positive
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin. We find that this is not necessarily the case at very low or high sparsities. The LTH has spawned the idea that lottery tickets could be generated in a mostly data-independent fashion without relying on the very costly IMP. This would open up the possibility of fully sparse training without any prior dense training. For example, Morcos et al. (2019) hint at ""the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets."" Our evidence for the RTI indicates that such efforts might be misguided, as the construction of a lottery ticket requires prior knowledge of a dense solution. This is in line with Frankle et al. (2020b), who empirically examine current attempts at data-independent pruning at initialization (PAI), and show that these attempts significantly underperform compared to IMP. We hope that an improved understanding of IMP and the LTH, will allow future work to design better pruning and sparse optimization algorithms. In summary, our key contributions are as follows: 1. We demonstrate that the lottery ticket hypothesis can be extended to large networks and show that it is possible to train large neural networks from scratch without rewinding by using a sufficiently large batch size. This reinforces the hypothesis of Frankle et al. (2020a) that finding lottery tickets is possible when training is stable with respect to linear mode connectivity.",1,related,1,positive
"However, it is not impossible that other methods to find lottery tickets exist which do not suffer from the RTI, and our discussion of RTI should hence not necessarily be taken as an absolute statement on the LTH but only of the LTH in relation to IMP.",1,related,1,positive
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations.",1,related,1,positive
"To do so, we see potential to rely on other forms of sparsity, including dynamic weight sparsity (Evci et al., 2019) and conditional activation sparsity, improving the capability to handle multiple languages and data domains within the same architecture (Fedus et al.",1,related,1,positive
"To do so, we see potential to rely on other forms of sparsity, including dynamic weight sparsity (Evci et al., 2019) and conditional activation sparsity, improving the capability to handle multiple languages and data domains within the same architecture (Fedus et al., 2021).",1,related,1,positive
Our experimental results show that DST brings other favorable advantages to the DRL agents besides memory and computation efficiency.,1,related,0,negative
"In the rest of this section, we will explain the details of our proposed DST method for the TD3 algorithm (DS-TD3).",1,related,1,positive
We follow the method described in [13] to calculate the FLOPs.,1,related,1,positive
"For the · update schedule, it contains: (i) the update interval ∆T, which is the number of training iterations between two sparse topology updates; (ii) the end iteration Tend, indicating when to stop updating the sparsity connectivity, and we set Tend to 80% of total training iterations in our experiments; (iii) the initial fraction α of connections that can be pruned or grow, which is 50% in our case; (iv) a decay schedule of the fraction of changeable connections fdecay(t, α,Tend) = α 2 (1+cos( t×π Tend )), where a cosine annealing is used, following [24, 25].",1,related,1,positive
"To meet this challenging demand, we draw inspirations from the latest sparse training works [24, 25] that dynamically extract and train sparse subnetworks instead of training the full models.",1,related,1,positive
"Infrequent gradient calculation [24] is adopted in our case, which computes the gradients in an online manner and only stores the top gradient values.",1,related,1,positive
"Our SViTE method (and its variants S(2)ViTE and SViTE+) is inspired from state-of-the-art sparse training approaches [24, 25] in CNNs.",1,related,1,positive
"Grow criterion: Similar to [24, 25], we active the new units with the highest magnitude gradients, such as ‖ ) ∂A(l,h) ‖`1 and ‖ ∂L(X) ∂W (l,1) j,· ‖`1 for the hth attention head and the jth neuron of the MLP (W ), respectively.",1,related,1,positive
"For a consistent description, we follow the standard notations in [24, 25].",1,related,1,positive
"We start by demonstrating the efficacy of our method on the ImageNet dataset for image classification, where we train a sparse ResNet-50 as in previous works [7, 10].",1,related,1,positive
"Following Evci et al. (2019); Gale et al. (2019), we conduct experiments to compare this strong baseline with `1-norm filters pruning while employing CLR on CIFAR-10, CIFAR-100 and ImageNet.",1,related,1,positive
We calculate theoretical FLOP requirements in a manner similar to Evci et al. [2020] (exact details in the supplementary material).,1,related,1,positive
"Similar to Evci et al. [2020], we assume that algorithms utilize sparsity during training.",1,related,1,positive
"Additionally, we include comparisons to recent works on weight rewinding and dynamic sparsity, in particular SNIP (Lee et al., 2018), DSR (Mostafa and Wang, 2019), SNFS (Dettmers and Zettlemoyer, 2019), and RiGL (Evci et al., 2020).",1,related,1,positive
"The results in Table 3 and Table 4 show that the proposed method outperform the baseline and previous methods [32, 16, 5].",1,related,0,negative
"The current state-ofthe-art for DST is RigL (Evci et al., 2020), which maintains a fixed layer-wise sparsity distribution, prunes parameters with the smallest magnitude, and re-activates weights with the largest-magnitude gradients.",1,related,1,positive
"…A.12 we compare different heuristics for distributing trainable parameters between network layers – in particular, uniform density per layer (uniform), equal number of parameters per layer (EPL), equal number of parameters per filter (EPF) and the ERK distribution used in (Evci et al., 2020).",1,related,1,positive
"In RigL (Evci et al., 2019a), the authors consider the case of SADt−1:t = 2k, where k is dynamically calculated for each layer during training.",1,related,1,positive
"B. Implementation Details of RigL-ITOP in Section 4.2
In this Appendix, we describe our replication of RigL (Evci et al., 2020a) and the hyperparameters we used for RigLITOP.",1,related,1,positive
"We train sparse ResNet-50 for 100 epochs, the same as Dettmers & Zettlemoyer (2019); Evci et al. (2020a).",1,related,1,positive
"More importantly, our method requires only 2× training time to match the performance of dense ResNet-50 at 80% sparsity, far less than RigL (5× training time) (Evci et al., 2020a).",1,related,0,negative
"We compare ST-RNNs with strong state-of-the-art DST baselines including SET, SNFS, and RigL and a dense-to-sparse method, ISS.",1,related,1,positive
"We can see that, with 33% parameters, all gradient-based methods (SNFS and RigL) fail to match the performance of the dense-to-sparse method (ISS), whereas random-based methods (SET, ST-LSTM) can all outperform ISS and the dense model.",1,related,1,positive
"Note that its variant Erdős-Rényi-Kernel proposed by Evci et al. (2020) scales back to ER for RNNs, as no kernels are involved.",1,related,1,positive
"iterative pruning method in the RNN setting (Evci et al., 2020).",1,related,1,positive
We follow the way of calculating training FLOPs proposed by Evci et al. (2020).,1,related,1,positive
"To address this research gap, we turn our attention to lottery ticket hypothesis (LTH) [20, 27, 31, 50, 76, 81], a fast-rising field that investigates the sparse trainable subnetworks within full dense networks.",1,related,1,positive
"In addition to IMP we analyze two dynamic sparsity methods: Discovering Neural Wirings (DNW) [19] and Rigged Lottery Tickets (RigL) [3], algorithms where the connectivity changes throughout training.",1,related,1,positive
"In comparison to newer dynamic training algorithms including RigL and Top-KAST [3, 11], our modified DNW algorithm is likely still less computationally efficient for most applications.",1,related,1,positive
"[ !&quot;&amp;#&apos;  %% !&amp;#$ ! &amp; !# # 5]): a global magnitude threshold is used to prune weights. (Uniform [5]): a uniform layer-wise sparsity budget is used. (ERK [13]): a layer-wise sparsity budget weighted to prune more weights from larger convolutional layers, as described in Equation17.    !# Soft Threshold Reparameterization with data, as reported in [ ",1,related,1,positive
"(3) Momentum Based Pruning (MoP) (Ding et al., 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020): Notice that as we have non-stationary data and thus it is also reasonable to measure the weight importance by its momentum (calculated with the exponential moving average of gradients).",1,related,1,positive
"An extension of Erdős-Rényi method (originally given by Mocanu et al. (2018)) accounting for convolutional layers, as proposed by Evci et al. (2020).",1,related,1,positive
"An extension of Erdős-Rényi method (originally given by Mocanu et al. (2018)) accounting for convolutional layers, as proposed by Evci et al. (2020).",1,related,1,positive
"However, we don’t know how to find Lottery Tickets (LTs) efficiently; while RigL (Evci et al. 2020), a recent DST method, requires 5× the training steps to match dense NN generalization.",1,related,1,positive
"– In addition, with the help of our proposed distance metric, we confirm and complement the findings from [5] by being able to quantify how different are the sparse and, at the same time, similarly performing topologies obtained with adaptive sparse connectivity.",1,related,1,positive
"Second, by use of dynamic network rewiring rules over training time that keeps network sparsity below a given threshold (Mocanu et al., 2018; Bellec et al., 2018; Yan et al., 2019; Evci et al., 2020).",1,related,1,positive
"One way, is to initialise with a sparse distribution and then train with the periodical interchanging of which parameters are considered pruned and which are ‘grown back ’ [4, 14, 15].",1,related,1,positive
"We compare (gRDA) with the Erdős-Rényi-Kernel of [15], variational dropout [42] and a reinforcement-learning based AutoML method [32].",1,related,1,positive
"The “+ ERK” suffix implies the usage of ERK budget (Evci et al., 2020) instead of the original sparsity budget.",1,related,0,negative
"llowing [14]). We run CS with s 0 2f0:0; 0:01; 0:02; 0:03; 0:05gyielding 5 tickets with varying sparsity levels. Table2summarizes the results achieved by CS, IMP, and state-of-the-art pruning methods [25, 26, 27, 28, 29], where reported sparsity levels are computed for the set of parameters that each method prunes. IMPydenotes IMP run for 12 rounds, i.e. using a larger training budget. Differences in each method’s me",1,related,1,positive
"…dynamic sparse
1 https://github.com/zahraatashgahi/CTRE.
training methods use magnitude as a pruning criterion, weight regrowing approaches are of different types, including random (Mocanu et al., 2018; Mostafa & Wang, 2019) and gradient-based regrowth (Evci et al., 2020; Jayakumar et al., 2020).",1,related,1,positive
"We demonstrate that HYDRA (Sehwag et al., 2020) and Robust-ADMM (Ye et al., 2019) yield better results when used with non-uniform strategies determined by ERK (Evci et al., 2020) and LAMP (Lee et al., 2021) (cf. Section 4.3).",1,related,1,positive
"For link prediction in Ogbl-Collab, we adopt 28-layer ResGCNs.",1,related,1,positive
"To test the scalability of DGLT, we further use a large-scale dataset called Ogbl-Collab Hu et al. (2020) for link prediction. Finally, we examine our algorithm for graph classification on D&D Dobson & Doig (2003) and ENZYMES Borgwardt et al.",1,related,1,positive
"For Ogbl-Collab, in order to simulate a real collaborative recommendation application, we take the cooperation before 2017 as the training edge, the cooperation in 2018 as the validation edge and the cooperation in 2019 as the testing edge.",1,related,1,positive
"To answer RQ2, we conduct experiments on the Ogbl-Collab dataset using ResGCNs as the backbone.",1,related,1,positive
"To test the scalability of DGLT, we further use a large-scale dataset called Ogbl-Collab Hu et al. (2020) for link prediction.",1,related,1,positive
"…finding a fixed sparse mask at the initialization as we mentioned in introduction, on the other hand, dynamic sparse training allows the sparse mask to be updated during training, e.g., (Mocanu et al., 2018; Mostafa and Wang, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021a,c,d).",1,related,1,positive
"We can confirm, using our two random methods (Rand and ERK) on Figure 3, that they always yield σ ≤ 0, no matter the settings.",1,related,1,positive
"Following previous dynamic sparse training algorithms [8, 26, 27, 18], we select the k weights with the smallest magnitudes (i.",1,related,1,positive
"Different from previous work which selects new parameters based on dense gradients [8] or dense weights [18], we select blocks of new parameters directly based on the input value and output gradient of each layer, making our algorithm purely sparse.",1,related,1,positive
"Compared with nonstructured dynamic sparse training (RigL [8]), our DSB has slightly lower accuracy at 0.",1,related,1,positive
"Different from previous work that computes the gradients for all weights and adds the weights with the largest gradients[8, 26, 27], we use dOut and In to estimate the importance of weights.",1,related,1,positive
We use the same sparsity initialization method ERK in the state-of-the-art sparse training method such as RigL (Evci et al. 2020) and ITOP (Liu et al.,1,related,1,positive
We use the same sparsity initialization method ERK in the state-of-the-art sparse training method such as RigL (Evci et al. 2020) and ITOP (Liu et al. 2021b).,1,related,1,positive
"To improve the flexibility, dynamic mask training has been proposed (Mocanu et al. 2018; Mostafa and Wang 2019; Evci et al. 2020; Liu et al. 2021b; Ma et al. 2021), where the sparse mask is periodically updated by drop-andgrow to search for better subnetworks with high accuracy, where in the drop process we deactivate a portion of weights from active states (nonzero) to non-active states (zero), vice versa for the growing process.",1,related,1,positive
"The only existing prior work on unstructured ViT pruning is SViTE [Chen et al., 2021], which applied the general RigL pruning method [Evci et al., 2020] to the special case of ViT models.",1,related,1,positive
"Future work should also be able to extend our pruner to structured compression of ViT models, or employ our oViT pruner inside different, more computationally-intensive pruning algorithms such as RigL [Evci et al., 2020].",1,related,1,positive
"We compare with global magnitude (GM) following the same schedule as oViT, as well as the state-of-the-art SViTE [Chen et al., 2021] paper, which trains the sparse model from scratch using a variant of RigL [Evci et al., 2020], but for a total of 600 epochs.",1,related,1,positive
"After pruning, we grow the pruned parameters with the largest gradient magnitude, like RigL (Evci et al., 2020).",1,related,1,positive
"Dynamical Sparse Training In order to improve the expressiveness of ER networks and achieve extremely sparse WLTs, ER networks can be rewired with the help of DST. Specifically, we use the algorithm RiGL (Evci et al., 2020a).",1,related,1,positive
"This way, we also provide a missing theoretical foundation for dynamic sparse training approaches (Evci et al., 2020a; Liu et al., 2021.",1,related,1,positive
"In spirit, our effort can be likened to methods like RigL (Evci et al., 2020), which discovers the wiring of a sparse neural network during training rather than pruning a dense network post training.",1,related,1,positive
"On the other hand, LTH-based (Chen et al., 2020a; Evci et al., 2020) methods can be borrowed to find the mask by several iterations, but it is prohibitively time-consuming.",1,related,1,positive
"Table 1 compares the inference accuracy, inference FLOPS, and model size of the proposed method with pruning (Gale et al., 2019), and with two sparsity training methods: RigL (Evci et al., 2020) and MEST+EM&S (Yuan et al., 2021).",1,related,1,positive
"Immediately after magnitude pruning, we explore the same number of p-proportion of new weights with the largest magnitude gradients as in [15]:",1,related,1,positive
"Following RigL3 [15], the FLOPs are calculated with the total number of multiplications and additions layer by layer for a given layer Sl. Briefly speaking, with ERK distribution, the training FLOPs of a sparse Wide ResNet28-10 at sparsity S = 0.8 and S = 0.9 are 33.7% and 16.7% of the dense model, respectively.",1,related,1,positive
"To address this problem, we turn our attention to dynamic sparsity [3, 15, 47, 52, 55], a recently emerged sparse training area that enables training sparse neural networks from scratch by dynamically optimizing the sparse connectivities.",1,related,1,positive
"For the convolutional layers, we use the kernel variant, Erdős-Rényi-Kernel (ERK) as introduced in [15].",1,related,1,positive
"In this section, we conduct various experiments to validate the effectiveness of SIS in terms of test accuracy vs. sparsity and inference time FLOPs vs. sparsity by comparing against RigL (Evci et al., 2020).",1,related,1,positive
"…SMAPE FLOPs
Dense 12.2 4.53G 18.6 927.73G 8.3 41.26M
SNIP (Lee et al., 2019) 14.3 2.74G 24.6 398.92G 10.1 21.45M LRR (Renda et al., 2020) 13.7 2.61G 23.1 339.21G 9.3 14.47M RigL (Evci et al., 2020) 13.9 2.69G 22.4 326.56G 10.2 15.13M SIS (Ours) 13.1 2.34G 21.1 290.38G 9.7 14.21M
N-BEATS on M4.",1,related,1,positive
"In order to estimate the layer-wise redundancy, we use the Erdos-Renyi kernel (ERK) [15].",1,related,1,positive
Note that ri can be negative if the current sparsity (si) is larger than the sparsity estimated by ERK.,1,related,1,positive
"3.4 16: end if 17: end if 18: end for 19: end while 20: Output Layer wise sparse scheme N iv
In order to estimate the layer-wise redundancy, we use the Erdos-Renyi kernel (ERK) [15].",1,related,1,positive
"Thus, we use the layer-wise sparsity generated by ERK as a layer-wise redundancy guidance, which can be formulated as:
ri = ei − si
maxi=Li=1 |ei − si| (7)
where ei is the sparsity of layer i decided by ERK.",1,related,1,positive
"We note that we follow the advice of Evci et al. (2019) and Dettmers and Zettlemoyer (2019) and do not prune biases and batch-normalization parameters, since they only amount to a negligible fraction of the total weights, however keeping them has a very positive impact on the performance of the…",1,related,1,positive
"We follow the method described in (Evci et al., 2020) to calculate the FLOPs.",1,related,1,positive
We follow the method described in [11] to calculate the number of FLOPs required for training which is based on the total number of multiplications and additions layer by layer.,1,related,1,positive
"s Following the convention in (Evci et al., 2020), multiplication and addition are counted as two operations.",1,related,1,positive
"For non-uniform sparse ResNet-50 model, the improvement over RigL5× (Evci et al., 2020) is 1.0% and 1.5% at 80% and 90% sparsity, respectively.",1,related,1,positive
"As shown in Table 1, our implementation has slightly higher accuracy, which indicates that our code base in PyTorch is comparable with the original implementation in Evci et al. (2020), and can be used for reproducing and extending the training of RigL without causing fairness issue.",1,related,0,negative
"We observe that the improvement over RigL5× (Evci et al., 2020) is 1.3% (77.9% vs. 76.6",1,related,0,negative
"In Table 10, we perform additional experiments to supplement Table 1 by pruning the last FC layer using the C-GaP method and comparing them with Evci et al. (2020).",1,related,1,positive
"In this section, we provide the implementation details of the RigL (Evci et al., 2020).",1,related,1,positive
"We also implement the RigL and RigL5× (Evci et al., 2020) using our code base on PyTorch (please refer to Appendix B.",1,related,1,positive
"Please note that models with the non-uniform sparsifying distribution in Table 1 already have the last FC layer pruned, thus the experiment setup is the same as the ones in Evci et al. (2020).",1,related,1,positive
"Note that previous works update weights either greedily (e.g., RigL (Evci et al., 2020)) or randomly (e.g., SET (Mocanu et al., 2018) and DSR (Mostafa & Wang, 2019)).",1,related,1,positive
"We also implement the RigL and RigL5× (Evci et al., 2020) using our code base on PyTorch (please refer to Appendix B.2 for details).",1,related,1,positive
"We include the SNIP (Lee et al., 2019) and SET (Mocanu et al., 2018) results in uniform sparsity that are reproduced in Evci et al. (2020).",1,related,1,positive
"2 OUR IMPLEMENTATION DETAILS OF RIGL In this section, we provide the implementation details of the RigL (Evci et al., 2020).",1,related,1,positive
"We observe that the improvement over RigL5× (Evci et al., 2020) is 1.",1,related,1,positive
"From Figure 11, we observe that LTH and RigL are capable of maintaining the generalization ability of dense networks at a sparsity level of 21%.",1,related,1,positive
All results and analyses about RigL are referred to Appendix A2.,1,related,0,negative
"We choose the top-performing algorithm, RigL (Evci et al., 2020; Liu et al., 2021b), which starts from a random sparse network and encourages the connectivity to evolve dynamically based on a grow-and-prune strategy.",1,related,1,positive
