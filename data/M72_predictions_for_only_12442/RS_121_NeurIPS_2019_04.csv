text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"Srlt Franceschi et al. (2019): The abbreviation Srlt comes from the paper title — Unsupervised Scalable Representation Learning for Multivariate Time Series. Inspired by how word2vec Mikolov et al. (2013) is trained, this method proposes a novel triplet loss for modeling the time series data.",1,related,1,positive
"of representing learning on the UTS and the MTS datasets Franceschi et al. (2019). D.Conv consists of layers of dilated convolutions. Compared to the previous work Franceschi et al. (2019), we do not use causal convolutions, so as to incorporate information from both before and after time step i when conducting convolution operations. Time-Series Transformer (TsTransformer): The Time-Series Transformer has proved a success in representing the MTS data type Zerveas et al. (2021). It has the same structure as the original transformer encoder Vaswani et al. (2017), except that it replaces the Layer Normalization layer with the Batch Normalization layer and the embedding layer with the linear projection layer.",1,related,1,positive
"of representing learning on the UTS and the MTS datasets Franceschi et al. (2019). D.Conv consists of layers of dilated convolutions. Compared to the previous work Franceschi et al. (2019), we do not use causal convolutions, so as to incorporate information from both before and after time step i when conducting convolution operations. Time-Series Transformer (TsTransformer): The Time-Series Transformer has proved a success in representing the MTS data type Zerveas et al. (2021). It has the same structure as the original transformer encoder Vaswani et al.",1,related,1,positive
"(2020); Franceschi et al. (2019). Second, under limited computational resources, we chose to do experiments on as many datasets as possible to make conclusions informative and credible.",1,related,0,negative
Srlt Franceschi et al. (2019): The abbreviation Srlt comes from the paper title — Unsupervised Scalable Representation Learning for Multivariate Time Series.,1,related,1,positive
"of representing learning on the UTS and the MTS datasets Franceschi et al. (2019). D.Conv consists of layers of dilated convolutions. Compared to the previous work Franceschi et al. (2019), we do not use causal convolutions, so as to incorporate information from both before and after time step i when conducting convolution operations.",1,related,1,positive
"(2020); Franceschi et al. (2019). Second, under limited computational resources, we chose to do experiments on as many datasets as possible to make conclusions informative and credible. It is worth noting that the model structures presented below all model the time series in both directions. LSTM: Previous work Sagheer and Kotb (2019) shows the usefulness of unsupervised pretraining of LSTM-based autoencoder for MTS prediction tasks.",1,related,1,positive
"(2020); Franceschi et al. (2019). Second, under limited computational resources, we chose to do experiments on as many datasets as possible to make conclusions informative and credible. It is worth noting that the model structures presented below all model the time series in both directions. LSTM: Previous work Sagheer and Kotb (2019) shows the usefulness of unsupervised pretraining of LSTM-based autoencoder for MTS prediction tasks. Here we have simplified the model by constructing a vanilla bidirectional LSTM of two layers. Dilated Convolutional Neural Network (D.Conv): The Convolutional Neural Network performs well on time series forecasting Yue et al. (2022) and demonstrates its strengths",1,related,1,positive
"The evaluation results presented in the aforementioned paper demonstrate that TST outperforms other methods, such as ROCKET [18], dilation-CNN [24], XGBoost [10], and LSTM [39], on a range of datasets, achieving the best average rank across multiple dimensions.",1,related,1,positive
"Using the time series embeddings from [24], it learns embeddings that seamlessly blend with the local context.",1,related,1,positive
"Specifically, for DS and PS, we adopt two layers of LSTM; for C-FID, we adopt ts2vec [24] as the backbone.",1,related,1,positive
"Finally, we follow the same protocol as [10], where a decoder is trained on top of the",1,related,1,positive
"We perform comprehensive experiments on time series classification to assess the classification performance of our approach, in comparison to other unsupervised time series representation models, namely T-Loss [10], TS-TCC [9], TST [49], and TNC [32].",1,related,1,positive
"Similar to the augmentation network, we utilize a neural network with two components, a fully connected layer, and a multi-layer dilated CNN module [25], [61] as the feature extraction encoder.",1,related,1,positive
"During our experiments, we utilize a grid search algorithm to explore the search space of [1, 2, 3, 4, 5, 10, 15, 20, 25, 30] for optimizing the anomaly threshold proportion r.",1,related,1,positive
"We encode each sequence with unsupervised representations learned by (Franceschi, Dieuleveut, and Jaggi 2019).",1,related,1,positive
"Then, we use the triplet loss that is used in [31] as the loss function of the similarity prediction task:",1,related,1,positive
"As mentioned in [102], suppose one anchor x , one positive sample x, and K negative samples x k , k∈1,2,··· ,K are chosen, we expect to assimilate x ref and x and to distinguish between x and x k , i.",1,related,1,positive
"We compare MBrain with state-of-the-art models including one supervised classification model MiniRocket [12] and several self-supervised and unsupervised models: CPC [28], SimCLR [7], Triplet-Loss (T-Loss) [17], Time Series Transformer (TST) [45], GTS [36], TS-TCC [16] and TS2Vec [43].",1,related,1,positive
"For the classification task, we follow the settings in [18] and train an RBF SVM classifier on segment-level representations generated by our baselines.",1,related,1,positive
"Similar to [65], we incorporate both instance-wise loss [18] (Lins) and temporal loss [55] (Ltemp) to model the distance within a couplet.",1,related,1,positive
"To further strengthen our empirical evidence, we additionally implement a K-nearest neighbor classifier equipped with DTW [9] metric, along with T-Loss [18] and TST [66] beside the aforementioned SOTA approaches.",1,related,1,positive
"For the choice of the encoder, we directly adopt the encoder network used in [13].",1,related,1,positive
"As stated in the original paper [13, 33], all of these baselines are independent of the architecture of the encoder.",1,related,0,negative
"For a fair comparison and to ensure that the difference in performance is not caused by the differences in the encoders’ architecture, we followed the convention in [13, 33] and used the same encoder network across all compared baselines.",1,related,0,negative
"In this paper, we implement the encoder as a causal convolution network [13] because it has been proved to be efficient for time series data, and can alleviate the disadvantages of recurrent neural networks (e.",1,related,1,positive
"For a detailed introduction of these loss functions, refer to the original paper [13, 21, 33, 34, 40].",1,related,1,positive
"We compare our CSL with 5 URL baselines specially designed for time series, including TS2Vec [56], T-Loss [9], Table 2: Statistics of used anomaly detection datasets.",1,related,1,positive
"We believe that our method is more general as it does not depend on task-specific assumptions like [9, 44, 56].",1,related,1,positive
"Then, we analyze the classification performance of T-Loss, SelfTime, TS-TCC, TST, and TS2Vec after pre-training on 128 UCR and 30 UEA time series datasets.",1,related,1,positive
"For the time-series classification task, we select T-Loss [134], SelfTime [135], TS-TCC [38], TST [29], and TS2Vec [68] to analyze the performance of TS-PTMs and compare them with the supervised FCN [65] model.",1,related,1,positive
"Since the focus of our work is on the novel negative sampling technique introduced above, we rely on an existing proven encoder architecture as described in [9,2] based on exponential dilated convolutional neural networks.",1,related,1,positive
"For more details on the architecture, we refer to [9].",1,related,0,negative
"Formally, this results in algorithm 1 (for one epoch), inspired by [9].",1,related,1,positive
"[9], we normalize the data by substracting the mean and dividing by the variance of the entire dataset.",1,related,1,positive
"We use these as input to the triplet network with the following objective loss function [9,14]:",1,related,1,positive
"Following the previous setting, we evaluate the quality of representations on time series classification in a standard supervised manner (Franceschi, Dieuleveut, and Jaggi 2019; Yue et al. 2022).",1,related,1,positive
"We compare InfoTS with baselines including TS2Vec (Yue et al. 2022), T-Loss (Franceschi, Dieuleveut, and Jaggi 2019), TSTCC (Eldele et al. 2021), TST (Zerveas et al. 2021), and DTW (Franceschi, Dieuleveut, and Jaggi 2019).",1,related,1,positive
"Architecture The adopted encoder fθ(x) : RT×F → RD consists of two components, a fully connected layer, and a 10-layer dilated CNN module (Franceschi, Dieuleveut, and Jaggi 2019; Yue et al. 2022).",1,related,1,positive
"In this study, we compared our method with the benchmark models, including DTW [45], T-Loss [13], TNC[14], TS-TCC[16], TST [15], and TS2Vec [17] for 30 classification datasets in the UEA archive.",1,related,1,positive
"We follow (Zerveas et al., 2021) to use these baselines for comparison: DTWD which stands for dimension-Dependent DTW combined with dilationCNN (Franceschi et al., 2019), LSTM (Graves, 2012), XGBoost (Chen & Guestrin, 2016), Rocket (Dempster et al., 2020), and a transformer-based TST (Zerveas et…",1,related,1,positive
", 2021) to use these baselines for comparison: DTWD which stands for dimension-Dependent DTW combined with dilationCNN (Franceschi et al., 2019), LSTM (Graves, 2012), XGBoost (Chen & Guestrin, 2016), Rocket (Dempster et al.",1,related,1,positive
applies triplet loss to unsupervised learning through specially designed sampling method [7].,1,related,1,positive
"In our scheme, we choose the deep dilated causal convolutional neural network [12] as the encoder backbone given its high efficiency and outstanding excellence in capturing long-range dependencies [5].",1,related,1,positive
"To evaluate the performance of models on classification, we follow the same protocol Franceschi et al. (2019), where an SVM classifier with RBF kernel is trained on obtained instance-level representations.",1,related,1,positive
"For time series classification tasks, we include more competitive unsupervised representation learning methods: TS2Vec, T-Loss (Franceschi et al., 2019), TS-TCC (Eldele et al.",1,related,1,positive
"The results show that our mthod is superior to RISE, SAXVFSEQL, FS, LRS and USRL-FordA because the 1-to-1 wins numbers for these methods are all more than half of all the datasets.",1,related,1,positive
"Thus, we can conclude the performance of our method on classification accuracy is at the same level with COTE and significantly better than RISE, SAX-VFSEQL, FS, LRS and USRL-FordA.",1,related,1,positive
", [15, 34]), we first encode MOTS in short time windows with a neural network to a spatio-temporal voxel embedding.",1,related,1,positive
implementation of [26] and ResNet follows [77].,1,related,1,positive
"We compare MHCCL with eight state-of-the-art approaches in two categories: 1) instance-wise approaches including SimCLR (Chen et al. 2020), BYOL (Grill et al. 2020), TLoss (Franceschi, Dieuleveut, and Jaggi 2019), TSTCC (Eldele et al. 2021) and TS2Vec (Yue et al. 2022), and 2) cluster-wise approaches including SwAV (Caron et al. 2020), PCL (Li et al. 2021a) and CCL (Sharma et al. 2020).",1,related,1,positive
"It can be seen that our model performed best on 3 out of 4 datasets, achieving an average rank of 1.5, followed by Causal CNN.",1,related,0,negative
FDJNet (Franceschi et al. 2019) combines an encoder based on causal dilated convolutions with triplet loss to embed variable-length and multivariate time series.,1,related,1,positive
"Time-series Meta Features: There are prior works that generated standard time-series features [17], tsfresh [12] (that we used for generating part of our meta-features).",1,related,1,positive
"Regarding the neural networks, we follow [14] and do not perform any hyperparameter optimization on the network parameters.",1,related,1,positive
"to extract the variation information (Franceschi et al., 2019; He & Zhao, 2019).",1,related,0,negative
"(1) Temporal contrastive loss: Similar to [7], we employ triplet loss as the temporal contrastive loss for the ith sample, which is formulated as",1,related,1,positive
"We compare MUSLA with ten representative multivariate time series clustering methods: 1) dimension reductionbased methods, including MC2PCA, SWMDFC, and TCK; 2) classical distance-based methods, including m-kAVG +ED, m-kDBA, m-kShape, and m-KSC; 3) deep learningbased methods, such as USRL and DeTSEC; 4) multiview learning methods, such as multi-view spectral clustering via integrating Nonnegative Embedding and Spectral Embedding (NESE) [53].",1,related,1,positive
"In addition, the performance of MUSLA only on
TABLE 3 Performance Comparisons of MUSLA and Contrast Algorithms in Terms of RI
Data sets n Algorithms MC2PCA SWMDFC TCK m-kAVG+ED m-kDBA m-kShape m-kSC USRL DeTSEC NESE MUSLA ArticularyWordRecognition 0.9891 0.8939 0.9734 0.9522 0.9336 0.7582 0.9510 0.9730 0.9718 0.9756 0.9768 AtrialFibrilation 0.5143 0.7429 0.5524 0.7048 0.6857 0.3810 0.6571 0.2000 0.6286 0.6190 0.7238 BasicMotions 0.7910 0.7013 0.8679 0.7718 0.7487 0.5244 0.7718 1.0000 0.7165 0.7449 1.0000 Epilepsy 0.6126 0.6666 0.7856 0.7684 0.7771 0.5136 0.6044 0.9710 0.8397 0.8897 0.8157 ERing 0.7563 0.7724 0.7724 0.8046 0.7747 0.7701 0.7494 0.1330 0.7701 0.7540 0.8414 HandMovementDirection 0.6272 0.6527 0.6353 0.6968 0.6853 0.5728 0.6920 0.3510 0.6275 0.5920 0.7194 Libras 0.8920 0.8611 0.9171 0.9111 0.9133 0.6605 0.9227 0.8830 0.9070 0.9087 0.9412 NATOPS 0.8818 0.7610 0.8334 0.8525 0.8755 0.6534 0.8348 0.9170 0.7143 0.7637 0.9760 PEMS-SF 0.4239 0.7814 0.1909 0.8172 0.7546 0.7303 0.8039 0.6880 0.8058 0.7842 0.8920 PenDigits 0.9288 0.9110 0.9219 0.9345 0.8807 0.8655 0.9208 0.9850 0.8850 0.9064 0.9455 StandWalkJump 0.5905 0.7238 0.7619 0.7333 0.6952 0.3485 0.6571 0.4020 0.7333 0.6476 0.7714 UWaveGestureLibrary 0.8828 0.8246 0.9130 0.9204 0.8934 0.8015 0.9259 0.8840 0.8790 0.8553 0.9129
Arithmetic Mean "" 0.7409 0.7744 0.7604 0.8223 0.8015 0.6316 0.7909 0.6989 0.7899 0.7868 0.8763 Geometric Mean "" 0.7183 0.7702 0.7115 0.8177 0.7966 0.6092 0.7822 0.5883 0.7827 0.7775 0.8709 Absolute Wins "" 1.00 1.00 0.00 0.00 0.00 0.00 1.00 2.50 0.00 0.00 6.50 MUSLA 1-to-1a 11/0/1 11/0/1 11/1/0 11/0/1 12/0/0 12/0/0 11/0/1 9/1/2 11/0/1 11/0/1 - Wilcoxon-Holmb # 6.6667 7.1250 5.1250 3.9167 5.7500 10.2083 5.6250 6.4583 6.5000 6.8333 1.7917 Rank Mean Rank Std # 6.67 2.78 7.17 2.73 5.08 2.47 4.00 1.87 5.75 2.0910.25 1.305.58 2.756.50 3.976.42 2.336.83 2.271.75 1.09 a “MUSLA 1-to-1” indicates the number of MUSLA 1-versus-1 wins/draws/losses. b “Wilcoxon-Holm” indicates the Wilcoxon rank test and Holm’s alpha correction.",1,related,1,positive
"As for the patient-level epileptic wave detection task, we use the following multivariate time series classification models as baselines: EEGNet [23], TapNet [40], MLSTM-FCN [20] and NS [15].",1,related,1,positive
"Baselines we select contain end-to-end models, contrastive learning based models (CoST [18], TS2Vec [22], TNC [24], MoCo [51], Triplet [52], CPC [53], TST [54], TCC [55]) and a feature engineered model (TSFresh package).",1,related,1,positive
"In particular, we propose a novel adaptation of unsupervised time series representations (Franceschi et al., 2019) yielding environment-invariant embeddings.",1,related,1,positive
"While we extend Franceschi et al. (2019) for practical reasons such as speed of experimentation and general robustness, we remark our approach carries over also to other architectures, such as Zerveas et al. (2021).",1,related,1,positive
"The basic building block of our embedding network architecture consists of stacked temporal dilated causal convolutions (Bai et al., 2018) following (Franceschi et al., 2019).",1,related,1,positive
"Specifically, the actor πϕ(st) adopts the dilated causal convolutions (Franceschi, Dieuleveut, and Jaggi 2019) as the basic encoder structure to extract latent time series features, and uses a rank embedding table to extract the base model features.",1,related,1,positive
"One baseline named Scalable Representation Learning [12] is not included in our results, as it requires a much longer running time and we failed to produce its results in several days.",1,related,0,negative
"named Scalable Representation Learning [12] is not included in our results, as it requires a much longer running time and we failed to produce its results in several days.",1,related,0,negative
(4) TS-TCC+: It applies TaT as the encoder like W2V+ while training with the pretext task of TS-TCC.,1,related,1,positive
"W2V and our framework are suitable for both short and long time length datasets, and our performances can significantly surpass W2V.",1,related,1,positive
(2) W2V+: It applies our proposed two tower Transformer-based model as the encoder while training with the pretext task of W2V.,1,related,1,positive
The detailed information and the reason why we choose these methods are as followed: (1) W2V [15]: This method employs the idea of word2vec.,1,related,1,positive
"(6) TST+: It applies TaT as the encoder like W2V+ while training with the
pretext task of TST. (7) NVP+CS: To compare with the regression, in our framework we replace Next Trend Prediction with Next Value Predict (NVP) and regard it as a new strong baseline.",1,related,1,positive
"Data Preprocessing Following Franceschi et al. (2019); Zhou et al. (2021), for univariate time series classification task, we normalize datasets using z-score so that the set of observations for each dataset has zero mean and unit variance.",1,related,1,positive
"Data augmentations used are described in Appendix C.
Triplet (Franceschi et al., 2020) Triplet proposes a time series self-supervised learning approach by taking positive samples to be substrings of the anchor, and negative samples to be randomly sampled from the dataset.",1,related,1,positive
We use a loss function that is close to [7] while allowing forK positive and negative samples for each anchor.,1,related,1,positive
Software implementation was extended from code published by [7] with Python 3.,1,related,1,positive
"In a special case which we generalize in this paper, xpos is a subinterval within xref [7].",1,related,1,positive
"We reported the performance of the baseline results from the original papers [8], [9], [10], [21], [22], [24], [44] and [11], respectively.",1,related,0,negative
"In the UEA archive experiment, we compared our proposed method with eight different methods, including three benchmarks [44], a bag-of-pattern based approach [8], and deep learning-based methods [9], [10], [11], [21], [22], [24].",1,related,1,positive
Dataset EDI DTWI DTWD MLSTM WEASEL NS TapNet ShapeNet TST TS2Vec ODE-RGRU [44] [44] [44] -FCNs [21] +MUSE [8] [22] [9] [10] [24] [11] (Ours),1,related,1,positive
"Unlike the usual supervision-based triplet selection [16,7], we propose a novel unsupervised triplet selection for 2-D time-series images.",1,related,1,positive
"We make the logical assumption similar to other time series approaches (Franceschi et al., 2019; Tonekaboni et al., 2021) that in the input D the smaller the distance (in time) is between two points Dt and Dt′ , the more related they are.",1,related,1,positive
"For all configurations, we use the Adam optimizer with a learning rate of 0.001 (Bo et al. 2020; Franceschi et al. 2019; Madiraju et al. 2018; Mukherjee et al. 2019) at the exception of theDRNN architecture wherewe use the SGDoptimizer with exponential decay, a learning rate of 0.1, and a decay…",1,related,1,positive
"On certain datasets, our results approach other recent unsupervised approaches in which a simple linear classifier is trained on top of a complex unsupervised feature extractor [43, 70, 71].",1,related,1,positive
"Our encoder is a one layer causal dilated encoder with skip connections, an architecture recently shown to provide strong time series classification performance [71].",1,related,1,positive
"This conclusion is also demonstrated in EEG/MEG applications (SelfRegulationSCP1), with 10% labeled samples, SMATE obtained a higher accuracy (0.781) than fully supervised 1NNDTW-D (0.775), USRL (0.771), Semi-TapNet (0.739) and MTL (0.730).",1,related,1,positive
", from 10% labeled training set to fully labeled one, the accuracy of SMATE varies only by 0.046, compared to INN-DTW-D (0.264), USRL (0.286), Semi-TapNet (0.151) and MTL(0.225), showing that SMATE is capable of learning a class-separable representation under weak supervision.",1,related,1,positive
"For comparison, we applied one classic model 1NN-DTW-D [3] and three recently proposed semi-supervised deep learning models: USRL [15], Semi-TapNet [20] and MTL [21].",1,related,1,positive
"USRL [15]; TapNet [20]; MLSTM-FCN [12]; CA-SFCN [6]; SMATENR: SMATE without supervised Regularization, instead, a Softmax layer is applied on the embedding.",1,related,1,positive
"USRL [15]; TapNet
[20]; MLSTM-FCN [12]; CA-SFCN [6]; SMATENR: SMATE without supervised Regularization, instead, a Softmax layer is applied on the embedding.",1,related,1,positive
"Besides, USRL and SMATENR perform much worse than SMATE with the same SVM classifier, confirming the reliability of our supervised regularization on the embedding space.",1,related,1,positive
"As no label information is utilized to learn the representation [15], there is a risk that it deviates from the true features, thus affecting the classifier performance.",1,related,0,negative
"We use an algorithm [10] that outperforms the previously mentioned methods, also tested on many standard datasets for unsupervised representation learning for time series.",1,related,1,positive
"Overall, the training procedure consists of traveling through the training dataset for several epochs (possibly using mini-batches), picking tuples (x, x,(xk neg )k) at random as detailed in Algorithm 1 of franceschi [10], and performing a minimization step on the",1,related,1,positive
"[Methods] We evaluated the SEAnet-generated DEA and its applications in data series similarity search against PAA and DEA generated by SEAnet-nD (a simplified version of SEAnet), and our adaptations of FDJNet [17], TimeNet [31], and InceptionTime [15].",1,related,1,positive
"Although encoder-only architectures is the popular choice [17], we argue (and experimentally verify) that the decoder is necessary in similarity search applications in order to regularize the DEAs, so that they are distinguishable among each other.",1,related,1,positive
"In contrast to existing convolutional autoencoders for series embedding [17], SEAnet comprises both an encoder and a decoder.",1,related,1,positive
"Additionally, we suggest a Frechet Inception distance-like score that is based on unsupervised time series embeddings (Franceschi et al., 2019).",1,related,1,positive
"1) Our model is different from traditional TCN in works [3], [7] in two aspects.",1,related,1,positive
"Different from works [3], [7] that take the whole MTS X(∗, t1 : tn−1) ∈ Rm×(n−1) as input, our TCN takes X(i, t1 : tn−1) ∈ R1×(n−1) for i = 1, 2, .",1,related,1,positive
"We replace our proposed contextual consistency, including the timestamp masking
and random cropping, into temporal consistency (Tonekaboni, Eytan, and Goldenberg 2021) and subseries consistency (Franceschi, Dieuleveut, and Jaggi 2019).",1,related,1,positive
"We then follow the same protocol
as T-Loss (Franceschi, Dieuleveut, and Jaggi 2019) where an SVM classifier with RBF kernel is trained on top of the instance-level representations to make predictions.",1,related,1,positive
"We conduct extensive experiments on time series classification to evaluate the instance-level representations, compared with other SOTAs of unsupervised time series representation, including T-Loss, TS-TCC (Eldele et al. 2021), TST (Zerveas et al. 2021) and TNC (Tonekaboni, Eytan, and Goldenberg 2021).",1,related,1,positive
"We chose to consider as neighbors samples from a patient that are close in time motivated by (Banville et al., 2020) and (Franceschi et al., 2019) works.",1,related,1,positive
"Table 2 shows the results of our proposed method Pattern Discovery with Byte Pair Encoding (PD-BPE), time series embedding proposed in [10] shown here as TS-Embed, and Rocket [7], for IGTB constructs.",1,related,1,positive
"…θ, θ̃) 12: θ ′ ← θ − η∇θJ (θ) 13: end while
lize an original sampling strategy combined with the consistency training [Bachman et al., 2014; Laine and Aila, 2017; Franceschi et al., 2019; Xie et al., 2020] on numerous unlabeled time series to constrain model predictions to be invariant to…",1,related,1,positive
"We recall that Franceschi et al. (Franceschi, Dieuleveut, and Jaggi 2019) followed the principle from word2vec (Mikolov et al. 2013), which makes the assumption that the representation of a word should meet two requirements: (i) the representation should be close to those near its context (Goldberg and Levy 2014), and (ii) it should be distant from those in a randomly chosen context, since they are probably different from the original word’s context.",1,related,1,positive
"Comparison with other methods The experimental accuracies of the baseline results are all taken from the original papers (Bagnall et al. 2018), (Franceschi, Dieuleveut, and Jaggi 2019) and (Zhang et al. 2020), respectively.",1,related,0,negative
"We recall that Franceschi et al. (Franceschi, Dieuleveut, and Jaggi 2019) followed the principle from word2vec (Mikolov et al. 2013), which makes the assumption that the representation of a word should meet two requirements: (i) the representation should be close to those near its context (Goldberg…",1,related,1,positive
"In order to learn these representations for driving time series, we have calculated a triplet loss function inspired by [21], [22] for driving time series, which is fully unsupervised.",1,related,1,positive
"After ignoring MCNN on Face Detection dataset, the average Accuracy score of NFS is 0.595, exceeding that of AgnoS by 0.05.",1,related,0,negative
"To validate the general learning capability of NSF, we consider four public MTS datasets, namely OhioT1DM, Favorita, PhysioNet 2012 and Face Detection, from varying domains.",1,related,1,positive
We do not find consistent representations with USTR.,1,related,1,positive
"In all experiments, we compare Seq2Graph to three time series representation methods: a sequential autoencoder (SAE) [17], to the unsupervised scalable time series representation (USTR) [9] and to the sequence-to-VAR (Seq2VAR) [22].",1,related,1,positive
"From top to bottom: USTR [9], SAE [17], Seq2VAR [22], Seq2Graph.",1,related,1,positive
"We see that Seq2Graph outperforms both SAE, USTR and Seq2VAR for unsupervised representation learning, when meaningful information is fully contained in the causality.",1,related,1,positive
"In the current paper, we use SAE, USTR and Seq2VAR as comparative models in the experiment part.",1,related,1,positive
"Triplet Loss4 (Franceschi et al., 2019) We download the authors’ official source code and use the same backbone as SelfTime, and set the number of negative samples as 10.",1,related,1,positive
"Dataset Existing SOTA USRLFordA[13] InceptionTime[12] Combined (1NN)[13] OSCNN[53] Best: fcn-lstm[19] Vanilla:RNTransformer [18, 55] ResNetTransformer1 [18] ResNetTransformer2 [18] ResNetTransformer3 [18] Ours",1,related,1,positive
"Dataset Classes SeriesLength Existing SOTA USRLFordA[13] InceptionTime[12] Combined (1NN)[13] OSCNN[53] Best: lstm-fcn [19] Vanilla:RNTransformer [18, 55] ResNetTransformer1 [18] ResNetTransformer2 [18] ResNetTransformer3 [18] Ours",1,related,1,positive
"We can see the results in a comparison against two baselines and a novel SOTA deep-learning architecture by Franceschi, [8].",1,related,1,positive
"…of comparisons with related work We next provide a comprehensive comparison between the proposed framework and other state-of-the-art methods, including (WaRTEm (Mathew et al., 2019), DTCR (Ma et al., 2019), USRLT (Franceschi et al., 2019) and BeatGAN (Zhou et al., 2019)), as shown in Table 1.",1,related,1,positive
"Summary of comparisons with related work We next provide a comprehensive comparison between the proposed framework and other state-of-the-art methods, including (WaRTEm (Mathew et al., 2019), DTCR (Ma et al., 2019), USRLT (Franceschi et al., 2019) and BeatGAN (Zhou et al., 2019)), as shown in Table 1.",1,related,1,positive
"[8], we built an architecture composed of several causal convolutional blocks, transforming the 12× L-sized ECG data to 216 L-dimensional feature maps, where each point in a feature map is based on a history of 383 sample points, including itself.",1,related,1,positive
"To this end, we investigated the utility of an unsupervised representation learning model proposed by (Franceschi, Dieuleveut, and Jaggi 2019), which can be trained on a large amount of unlabeled data to learn potentially useful feature representations.",1,related,1,positive
"To this end, we investigated the utility of an unsupervised representation learning model proposed by (Franceschi, Dieuleveut, and Jaggi 2019), which can be trained on a large amount of unlabeled data to learn potentially useful feature representations.",1,related,1,positive
"time series analysis [30], we reduced each feature of a clip",1,related,1,positive
"In this paper, we propose an embedding algorithm using a state-of-the-art deep unsupervised dilated, causal convolutional encoder model [11] to find informative embeddings from continuous vital sign time series hemorrhage data of six vital signs.",1,related,1,positive
"We first utilized the same input layer and a stacked dilated CNN module [Franceschi et al., 2019; Yue et al., 2021] for both g(x) and h(x), respectively.",1,related,1,positive
"We compare our method with 8 state-of-the-art baselines, including TS2Vec [Yue et al., 2022], T-Loss [Franceschi et al., 2019], TNC [Tonekaboni et al., 2021], TS-TCC [Eldele et al., 2021b], TST [Zerveas et al., 2021], DTW [Chen et al., 2013], TF-C [Zhang et al., 2022] and InfoTS [Luo et al., 2023].",1,related,1,positive
"Unsupervised time-series embedders have been proposed [29, 30] to deal with label scarcity.",1,related,0,negative
We used [29] for our stream data embedder since it is more flexible to different time-series lengths and generates good representations for anomaly data compared to [30].,1,related,1,positive
"We follow (Zerveas et al., 2021) to use these baselines for comparison: DTWD which stands for dimension-Dependent DTW combined with dilation-CNN (Franceschi et al., 2019), LSTM (Graves, 2012), XGBoost (Chen & Guestrin, 2016), Rocket (Dempster et al., 2020), and a transformer-based TST (Zerveas et…",1,related,1,positive
", 2021) to use these baselines for comparison: DTWD which stands for dimension-Dependent DTW combined with dilation-CNN (Franceschi et al., 2019), LSTM (Graves, 2012), XGBoost (Chen & Guestrin, 2016), Rocket (Dempster et al.",1,related,1,positive
"Following the previous work in [72], we embed a one-nearestneighbor (1-NN) classifier into the DCRLS-based classifica-",1,related,1,positive
"We compare our performances with stateof-the-art approaches CPC, SRL, TS-TCC and TNC.",1,related,0,negative
"Following [14, 30], we fit an SVM classifier on the learned representations for classification.",1,related,1,positive
3 Triplet loss Our triplet loss formulation is based on word2vec [19] and T-Loss [14].,1,related,1,positive
"We compare TS-Rep with the self-supervised methods [14, 13, 30], and a VAE-based method Incr-VAE [4].",1,related,1,positive
"1 Network Architecture Our network is identical to T-Loss [14], and further details about the network are given in Appendix D.",1,related,1,positive
"One baseline named Scalable Representation Learning [15] is not included in our results, as it requires much longer running time and we failed to produce its results in several days.",1,related,0,negative
"We compare TS2Vec with other SOTAs of unsupervised time series representation, including T-Loss [6] and TNC [5], on time series classification tasks.",1,related,1,positive
"We adopt the same evaluation protocol as [6], which trains a SVM with RBF kernel on top of the instance-level representations to predict the label of an instance.",1,related,1,positive
[1] ein rein Encoder-basiertes Netzwerk in Kombination mit einer sogenannten Triplet-Loss-Funktion für die Klassifikation auf diversen Referenzzeitreihen nutzen.,1,related,0,negative
"Methoden des Representation Learning haben sich in jüngster Zeit im Bereich der Bild- und Sprachverarbeitung etabliert und ermöglichen eine unüberwachte Featureextraktion, welche die Prädiktionsgüte gängiger Verfahren der manuellen und automatisierten Featureextraktion im Fall von nur wenigen vorhandenen gelabelten Daten übertrifft [1].",1,related,1,positive
"Triplet Loss5 (Franceschi et al., 2019) We download the authors’ official source code and use the same backbone as SelfTime, and set the number of negative samples as 10.",1,related,1,positive
"To this end, we use the triplet loss introduced in [2] that follows the word2vec’s intuition.",1,related,1,positive
We will pre-train our encoder in a self-supervised manner using the so-called triplet loss defined in [2].,1,related,1,positive
"Our results are consistent across all datasets that we trained on: while the benchmark performed just under the state-of-the-art results of Franceschi et al [2], the Transformer approach failed to leverage the representation learned in the pre-training and is overfitting early in the training process.",1,related,0,negative
"Based on the recent success of Transformers [4] on machine translation, we try to combine the unsupervised representation learning of [2] with a Transformer to improve the accuracy on the standardized datasets of the UCR archive1.",1,related,1,positive
"We observe that while our benchmark EmbConv performs just under the state-of-the art results of Franceschi et al. [2], the simplified BERT BenchBert fails to produce viable results and is prone to over-fitting, even when performing a pre-training.",1,related,0,negative
"Notice that EmbConv reaches an accuracy of 69% on the testing set, where the state-ofthe-art method presented in [2] reaches an accuracy of 74%.",1,related,0,negative
We also experiment with a recent general-purpose unsupervised timeseries representation proposed in [5].,1,related,1,positive
"In this work, we have presented a replication study of the work by [1] and found that most of the results are replicable.",1,related,0,negative
