text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"We apply an incremental fine-tuning algorithm based on learning rate rewinding (Renda et al., 2020) to the baseline model to obtain the accuracy values corresonding to the following sparsity levels: 50%, 75%, 87.",1,related,1,positive
"To initiate the IMP procedure, we initialize model parameters θ with pre-trained dense weights θ 0 and set the binary pruning mask m to all ones 1 , where m ∈ { 0 , 1 } | θ | .",1,related,1,positive
"For the pruning step, we simply raise the sparsity level and prune from all weights in θ n as opposed to pruning from weights in m ⊙ θ T in the IMP procedure.",1,related,1,positive
"When monolingual data is used in IMP, this procedure yields a language-specific pruning mask m l for a language l .",1,related,1,positive
"Within the framework of the IMP procedure, we introduce a mask adaptation step denoted as n (where n   T ).",1,related,1,positive
Our proposed Stage (2) modified the IMP and the LTH language-specific pruning methods in Stage (2) and achieved a consistent 5.3% relative WER reduction averaged across languages.,1,related,0,negative
The IMP procedure is illustrated as follows: Repeat 2.,1,related,1,positive
"In addition, unstructured pruning [14] is further used to remove unimportant weights and reduce computational costs.",1,related,0,negative
"…(lottery ticket rewinding was used when required, see appendix B) was used for our experiments as it provides an effective procedure to find subnetworks with nontrivial sparsities that have low test error [Frankle and Carbin, 2019, Frankle et al., 2020, Blalock et al., 2020, Renda et al., 2020].",1,related,1,positive
"In LR only the learning rate is reset to its pre-trained value, leaving the unpruned weights to be re-trained from their values at the end of the initial training phase [4].",1,related,0,negative
"This is in stark contrast with the behavior of SNNs on the image classification task, where LTH can gracefully preserve the matching performance even at very extreme sparsities (>95% on CIFAR-10/100 (Yin et al., 2022) and >80% on ImageNet (Renda et al., 2020)).",1,related,1,positive
"Following a mainstream convention in many sparse training papers (Frankle & Carbin, 2019; Gale et al., 2019; Evci et al., 2020; Lee et al., 2019; Liu et al., 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al., 2020) and Knowledge Distillation (Hinton et al., 2015) in our main evaluations, even if we observe that they help to alleviate accuracy drops as in Appendix C.
• Lottery Ticket Hypothesis (LTH) (Frankle & Carbin, 2019) is a strong post-training pruning baseline that iteratively adopts magnitude pruning after training to produce binary masks and re-train together with weights from step t.",1,related,1,positive
", 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al., 2020) and Knowledge Distillation (Hinton et al.",1,related,1,positive
"…in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al., 2020) and Knowledge Distillation (Hinton et al., 2015) in our main evaluations, even if we observe that they help to alleviate accuracy…",1,related,1,positive
"Specifically, we follow Kurtic et al. (2022) and replace the magnitude pruning criterion of the LTH framework with the second-order oBERT criterion; adopt Learning Rate Rewinding (LRR) (Renda et al., 2020) and Knowledge Distillation (KD) (Hinton et al., 2015) during each pruning iteration; and keep the embeddings and classification heads dense.",1,related,1,positive
"…(2022) and replace the magnitude pruning criterion of the LTH framework with the second-order oBERT criterion; adopt Learning Rate Rewinding (LRR) (Renda et al., 2020) and Knowledge Distillation (KD) (Hinton et al., 2015) during each pruning iteration; and keep the embeddings and classification…",1,related,1,positive
"(2022) and replace the magnitude pruning criterion of the LTH framework with the second-order oBERT criterion; adopt Learning Rate Rewinding (LRR) (Renda et al., 2020) and Knowledge Distillation (KD) (Hinton et al.",1,related,1,positive
"Lastly, we highlight that all LR schedules used, including SILO, are rewound to the initial state at the beginning of each pruning cycle, which is the same as the LR rewinding in (Renda et al., 2019).",1,related,0,negative
"We refer to the standard implementation reported in (Renda, Frankle, and Carbin 2019; Frankle and Carbin 2019) (i.e., SGD optimizer (Ruder 2016), 100 training epochs and batch size of 128, learning rate warmup to 0.03 and drop by a factor of 10 at 55 and 70 epochs) and compute the static DNR and…",1,related,1,positive
"21: end if
We train the network using SGD with momentum = 0.9 and a weight decay of 1e-4 (same as (Renda, Frankle, and Carbin 2019; Frankle and Carbin 2019)).",1,related,1,positive
"Another convincing experimental argument in favor of our method lies in the fact that, when combined with iterative and thus quite complex (due to multiple rounds of training cycles) post-training pruning solutions proposed in [6, 32], our ST-3 defines a novel SoA accuracy/sparsity trade-off.",1,related,1,positive
"We note that this learning rate schedule is different from prior work on pruning, which typically uses a single decay cycle [Kusupati et al., 2020, Singh and Alistarh, 2020, Peste et al., 2021], or dynamic learning rate rewinding, e.g. [Frankle et al., 2019, Renda et al., 2020].",1,related,1,positive
"In line with LTH, weight rewinding (Renda et al., 2020) is adopted to retrain the identified positive soft prompts.",1,related,0,negative
"Following the idea in LTH, we adopt the weight rewinding technique (Renda et al., 2020) to re-train the soft prompts after the two-level hierarchical structured pruning.",1,related,1,positive
"In this work, if a matching subnetwork is found better than the winning ticket obtained by the same method that follows the original LTH setup [18, 19], we will also call such a matching subnetwork a winning ticket throughout the paper.",1,related,1,positive
"Let τ denote a training step from which IMP-WR works (i.e., the onset of linear mode connectivity).",1,related,1,positive
"IMP with Weight Rewinding (IMP-WR) is described in Algorithm 1 (Frankle et al., 2020).",1,related,1,positive
"We also study two variants of IMP with different retraining strategies in Section 3.4: IMP with LR rewinding (IMP-LRR) and IMP with finetuning (IMP-FT) (Renda et al., 2020).",1,related,0,negative
"IMP-FT is similar to IMP-LRR but instead of repeating the entire LR schedule, we continue training at the final low LR for the same number of steps: w(L) = AT (m(L) w(L−1), T ).",1,related,1,positive
"In the IMP-WR framework proposed by Frankle et al. (2020) after each pruning step the network is rewound to an early rewind point wτ , and from that point on the network is retrained with the new sparsity pattern.",1,related,1,positive
"Why does pruning a larger fraction in one iteration destroy the actionable information in the mask? Fourth, why does retraining allow us to prune more weights? A variant of IMP that uses a different retraining strategy (learning rate rewinding) also successfully identifies matching subnetworks while another variant (finetuning) fails (Renda et al., 2020).",1,related,1,positive
"The axial subspace associated with mask m(L) is a colored subspace, the pruned rewind point m(L) wτ is the circle in this subspace, the level L solution w(L) obtained from
Algorithm 1: Iterative Magnitude Pruning-Weight Rewinding (IMP-WR) (Frankle et al., 2020)
1: Initialize a dense network w0 ∈ Rd and a pruning mask m(0) = 1d. 2: Train w0 for τ steps to wτ .",1,related,1,positive
"have varying potential for complexity reduction, and we apply an iterative pruning and fine-tuning approach based on the lottery ticket hypothesis (LTH)[8] to operate the learned NLC methods at their optimal complexity.",1,related,1,positive
"For this, we account for the fact that different NN models have varying potential for complexity reduction, and we apply an iterative pruning and fine-tuning approach based on the lottery ticket hypothesis (LTH)[8] to operate the learned NLC methods at their optimal complexity.",1,related,1,positive
"Then, to effectively retrain the pruned model to compensate for the removed weights, we apply LTH[8], wherein we rewind the learning rate schedule before fine-tuning.",1,related,1,positive
"We use a pruning approach similar to[5], but rather than applying an absolute threshold, we gradually prune the relatively small weights and rewind the learning rate schedule before fine tuning[17].",1,related,1,positive
"We use unstructured weight pruning, which 140 can achieve higher sparsities than structured prun- 141 ing (Renda et al., 2020), and has comparatively 142 standard implementations.",1,related,1,positive
"We use unstructured weight pruning, which can achieve higher sparsities than structured pruning (Renda et al., 2020), and has comparatively",1,related,1,positive
"We introduce these settings because previous works (Renda et al., 2020; Le & Hua, 2021; Wang et al., 2021a; 2023) have showed that retraining LR has a great impact on the final performance.",1,related,0,negative
"When the context is clear, we simply write S.
Parameter magnitude is an effective importance metric for model pruning (Han et al., 2015b;a; Paganini & Forde, 2020; Zhu & Gupta, 2018; Renda et al., 2020; Zafrir et al., 2021).",1,related,1,positive
"• Learning rate rewinding: retrains the pruned model for T − t epochs from the final parameters, but reuse the learning rate schedule from the iteration t at the early training phase (Renda et al., 2020).",1,related,1,positive
"After that, LR-Rewinding is applied.",1,related,0,negative
"Once the final pruning rate is reached, the network is re-trained following a warm-restart schedule, which can be called LR-Rewinding [17].",1,related,0,negative
"In the standard pruning scenario (Renda et al., 2020; Han et al., 2015a), training simply resumes with the remaining weights after each iteration of pruning.",1,related,0,negative
We put the emphasis on the efficiency of BN-folding for DNN acceleration by comparing the cost in terms of accuracy to reach similar acceleration with pruning and quantization methods.,1,related,1,positive
"Pruning consists in removing elements of the graph defined by the DNN [Renda et al., 2020].",1,related,1,positive
"• Rewinding has minor impact: Unlike [15, 49], we find that ”late rewinding” technique does not have a notable effect on style transfer subnetworks.",1,related,1,positive
"Then, we prune individual weights with the lowest-magnitudes globally throughout the network [22, 49].",1,related,1,positive
"Once the procedure is finished, we identify the least important graph frequencies and remove them from the model, then we retrain the remaining parameters using the same scheduler as for baseline architectures, what is referred to as LR-rewinding [11] in the literature.",1,related,1,positive
"Once the procedure is finished, we identify the least important graph frequencies and remove them from the model, then we retrain the remaining parameters using the same scheduler as for baseline architectures, what is referred to as LR-rewinding [12] in the literature.",1,related,1,positive
"We tested our method against the following baselines: dense weight training and four pruning algorithms: (i) IMP [10], (ii) Learning rate rewinding [34], denoted by Renda et al.",1,related,1,positive
", 90%, 95%) can be achieved without sacrificing the test accuracy; (ii) the located winning ticket maintains undamaged expressive power as its dense counterpart, and can be easily trained from scratch or early-epoch weights (Renda et al., 2020; Frankle et al., 2020a) to recover the full performance.",1,related,0,negative
"%) can be achieved without sacrificing the test accuracy; (ii) the located winning ticket maintains undamaged expressive power as its dense counterpart, and can be easily trained from scratch or early-epoch weights (Renda et al., 2020; Frankle et al., 2020a) to recover the full performance.",1,related,0,negative
"In this paper, we mainly follow the routine notations in (Frankle & Carbin, 2019; Renda et al., 2020).",1,related,1,positive
"Our baseline is iterative L1-norm weight-based pruning technique (Li et al. 2016; Renda, Frankle, and Carbin 2020) applied iteratively with rewinding.",1,related,1,positive
"Model pruning adversaries first prune the victim model using some pruning methods, then finetune the model using a small set of data [26], [35].",1,related,1,positive
"Our results indicate that state-of-the-art LT pruning methods achieve in general sub-optimal sparsity levels, and are not able to recover LTs that are competitive with a planted ground truth.",1,related,1,positive
"We utilize our planting framework to answer the question whether LT pruning algorithms that identify subnetworks of randomly initialized neural networks are able to identify highly sparse LTs, ideally in a strong sense but we also analyze weak LTs. Hypothetically, it could be possible that pruning algorithms for weak LTs only have to resort to training the identified LT because a highly sparse strong LT does not exist with high probability.",1,related,1,positive
"Similarly, if we insist on finding extremely sparse architectures, it might be necessary to give up the search for initial LTs (Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a).",1,related,1,positive
"To answer the question whether state-of-the-art pruning algorithms can find sparse strong LTs in the setting of standard benchmark data, we plant a trained weak LT in a randomly initialized (VGG
like) neural network.",1,related,1,positive
"If this were true, we should be able to find highly sparse LTs with the original pruning algorithm if we ensure the existence of a solution by planting.",1,related,1,positive
We analyze the performance of tickets before training to assess whether they qualify as strong LTs and after training to evaluate whether at least pruning for weak LTs is feasible and can identify LT of sparsities that can compete with our planted ground truth.,1,related,0,negative
"In addition, we identify an opportunity to improve state-of-the-art pruning algorithms in order to find strong LTs of better sparsity.",1,related,1,positive
"L G
] 2
2 N
ov 2
02 1
ing LT pruning algorithms solely on standard benchmark datasets (Frankle et al., 2021), but demand the comparison with known ground truth LTs.",1,related,1,positive
"We show in Appendix B that neither dynamic sparse training (Evci et al., 2020) nor LT fine-tuning techniques (Liu et al., 2021a; Renda et al., 2020) find architectures that are competitive with our constructed ground truth tickets.",1,related,1,positive
"[43] only reports ResNet results (see the paragraph below) on image classification, we implement one-shot and iterative unstructured magnitude pruning baselines [11,43] using PyTorch’s pruning API4.",1,related,1,positive
* Renda et al. 2020.,1,related,0,negative
"AQCompress consistently outperforms or achieves competitive performances compared to recently proposed pruning and quantization methods [11, 16, 36, 43, 52].",1,related,0,negative
"Many of our experiments compare AQCompress to unstructured pruning [43], which is a well-studied and widely adopted DNN compression method.",1,related,1,positive
"We compare AQCompress with state-of-the-art unstructured pruning [11,36,43] and quantization [16,52] approaches that are designed to compress DNNs.",1,related,1,positive
"In this paper, we compare AQCompress with iterative unstructured pruning [43] to demonstrate the high compression ratios attained by AQCompress.",1,related,1,positive
(iterative) * Renda et al. 2020.,1,related,0,negative
"As a baseline performance for a pruned network, we will use the approach suggested by Renda et al. (2020) as it serves as a good benchmark for the current potential of IMP.",1,related,1,positive
"While many details of this procedure are not specified or elaborated on in the original paper, Renda et al. (2020) suggested the following complete approach: train a network for T epochs and then iteratively prune 20% percent of the remaining weights and retrain for Trt = T epochs until the desired…",1,related,1,positive
"We then study to what degree the retraining phase of IMP can be shortened in the iterative setting compared to the recommendations of Renda et al. (2020) when using an
appropriate learning rate schedule in Section 3.2.",1,related,0,negative
"In its full iterative form, for example formulated by Renda et al. (2020), IMP can require the original train time several times over to produce a pruned network, resulting in hundreds of retraining epochs on top of the original training procedure and leading to its reputation for being…",1,related,0,negative
We have relied on the simple exponential pruning schedule suggested by Renda et al. (2020) for BIMP while GMP relies on a particular schedule defined by a cubic polynomial that effectively leads to pruning larger amounts initially and progressively smaller amounts later in training when compared to…,1,related,1,positive
"We empirically find that the results of Li et al. (2020) regarding the Budgeted Training of Neural Networks apply to the retraining phase of IMP, providing further context for the results of Renda et al. (2020) and Le & Hua (2021).",1,related,1,positive
"This agrees with the results stated in prior works (Renda, Frankle, and Carbin 2020).",1,related,0,negative
"We chose to focus on iterative pruning of ReLU-based networks for two reasons: (i) Iterative pruning tends to provide better pruning performance than one-shot pruning as reported in the literature (Frankle and Carbin 2019; Renda, Frankle, and Carbin 2020).",1,related,1,positive
"For the proposed S-
Cyc, we evaluate its performance using SGD with momentum = 0.9 and a weight decay of 1e-4 (same as (Renda, Frankle, and Carbin 2020; Frankle and Carbin 2019)).",1,related,1,positive
"Similarly, F is defined via the refinement procedure used, making it dependent on the choice of optimizer and whether or not the DNN parameters are left alone or “rewound” to their value at a previous point during training (Frankle et al., 2020a; Renda et al., 2020).",1,related,0,negative
"We apply unstructured pruning that removes more weights, more precisely LR rewinding [39], to prune the teacher model.",1,related,1,positive
"Unstructured Pruning We use the standard iterative magnitude pruning method for unstructured pruning in our framework, specifically learning rate rewinding (LR rewinding) (Renda, Frankle, and Carbin 2020).",1,related,1,positive
The right plot shows the student’s accuracies when different pruning algorithms (LR rewinding [39] and SynFlow [48]) are applied to the teacher.,1,related,1,positive
"In Section 5, we apply LR rewinding [39] to prune the model, and apply the vanilla KD [20] to distill the pruned teacher.",1,related,1,positive
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets (Zagoruyko and Komodakis [2016]). The new experiments led us to discover the limitations of learning rate rewinding which can worsen pruning results on large architectures. Methodology We implemented the code ourselves in Python with TensorFlow 2, basing our implementation of the paper alone and without consulting the source code provided by the authors. We ran two sets of experiments. In the reproduction set, we have striven to exactly reproduce the experimental conditions of Renda et al. [2020]. We have also conducted additional experiments, which use other network architectures, effectively showing results previously unreported by the authors.",1,related,1,positive
"2 Scope of reproducibility Renda et al. [2020] formulated the following claims: Claim 1: Widely used method of training after pruning: finetuning yields worse results than rewinding based methods (supported by figures 1, 2, 3, 4 and Table 5) Claim 2: Newly introduced learning rate rewinding works as good or better as weight rewinding in all scenarios (supported by figures 1, 2, 3, 4 and Table 5, but not supported by Figure 5) Claim 3: Iterative pruning with learning rate rewinding matches state-of-the-art pruning methods (supported by figures 1, 2, 3, 4 and Table 5, but not supported by Figure 5)",1,related,1,positive
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods.",1,related,1,positive
"For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones.",1,related,0,negative
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019].",1,related,1,positive
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets (Zagoruyko and Komodakis [2016]).",1,related,1,positive
"For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in He et al. [2016a]. Additionally, to verify the general usefulness of pruning and retraining methods proposed in Renda et al.",1,related,1,positive
"For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in He et al. [2016a]. Additionally, to verify the general usefulness of pruning and retraining methods proposed in Renda et al. [2020] we extend the list of tested network architectures to much larger wide residual networks from Zagoruyko and Komodakis [2016].",1,related,1,positive
"We compare our proposed approach with the latest state-ofthe-art in trojan mitigation techniques, including fine-tuning, bridge mode connectivity (BMC), Neural Attention Distillation (NAD), Maxup and Cutmix augmentation [18], [19], and our own version of fine-pruning based on learning rate rewinding [20], which we refer to as Learning-Rate rewinding and Compression, or LRComp.",1,related,1,positive
"To design the progressive pruning schedule, we develop a straightforward heuristic design, following the commonly used schedule in most pruning works [17, 26, 35].",1,related,1,positive
"For CNN compression, the general procedure can be largely summarized as: (i) train a full model; (ii) identify and prune the redundant structures to build a slimmer model based on various criteria, including (structured) sparsity (58; 85; 14; 56; 102; 27; 102; 62; 91), Bayesian pruning (101; 65; 59; 81), ranking importance (54; 60; 41; 36; 57; 100), reinforcement learning (37; 7), adversarial robustness (76), scientific control (79), lottery ticket (23; 24; 72), joint quantization learning (80; 90), etc.",1,related,1,positive
"We distinguish our definition of the lottery ticket hypothesis from the weight rewinding technique [5, 12].",1,related,1,positive
"In this paper, we follow the notations from [1, 5].",1,related,1,positive
IMP(·) prunes 20% of remaining weights per iteration until arriving at target sparsity s [5].,1,related,0,negative
We find the weight rewinding technique [5] consistently improves the subnetwork accuracy.,1,related,1,positive
"For IMP(·), we follow the settings in [1, 5] that 20% of the weights are pruned in each iteration.",1,related,1,positive
"We experiment with four models of increasing size (ResNeXt-29, ResNet-18, ResNet-50, WideResNet-18-2), three data augmentation methods (clean, AugMix, Gaussian), two sparsity levels (90%, 95%), and six compression methods (LTH, LRR, EP (layerwise and global), BP (layerwise and global)).",1,related,1,positive
"To test the CARD hypothesis, we use: five models (VGG [12, 46] and ResNet [19] style architectures of varying size), five sparsity levels (50%, 60%, 80%, 90%, 95%), and six model compression methods (FT, GMP, LTH, LRR, EP, BP).",1,related,1,positive
"Our best performing 6-CARD-Deck using LRR WideResNet-18 models (53.58 MB) sets a new state-of-the-art for CIFAR-10 and CIFAR-10-C accuracies of 96.8% and 92.75%, respectively.",1,related,0,negative
"Notably, we found a single LRR CARD (a WideResNet-18 at 96% sparsity) trained with AugMix can attain 91.24% CIFAR-10-C accuracy, outperforming dense ResNeXt-29 trained with AugMix (a state-of-the-art among methods that do not require non-CIFAR-10 training data) by more than 2 percentage points simply by pruning a larger model, i.e., WideResNet-18.",1,related,0,negative
"For a comprehensive analysis of existing pruning methods, we introduce a framework inspired by those in [43, 51] that covers traditional-through-emerging pruning methodologies.",1,related,1,positive
"We also examined the effectiveness of IMP with different rewinding starting points as studied in [29, 69], and found rewinding initializations bear minimal effect on downstream ASR.",1,related,0,negative
"It was pointed out by Renda et al. (2020) that subnetworks found by IMP and rewound early in training can be trained to achieve the same accuracy at the same sparsity as subnetworks found by the standard pruning, providing a possibility that rewinding can also help GAN subnetworks.",1,related,0,negative
"We adopt the weight rewinding technique in Renda et al. (2020): We reset the parameters of the winning tickets to their values in the pre-trained weights, and subsequently fine-tune the subnetwork with the original learning rate schedule.",1,related,1,positive
"• With the insights from dynamical isometry and OrthP, we unveil two mysteries in pruning: why a larger finetuning LR can improve the final performance significantly [40, 24] (Sec.",1,related,1,positive
"3 answers why a larger finetuning LR can improve the final performance in pruning [40, 24]; Sec.",1,related,0,negative
"Concretely, we propose the following plausible explanation to the effect of a larger LR in finetuning [40, 24]: A larger LR helps the network converge faster, thus the dynamical isometry (measured by mean JSV) recovers faster.",1,related,1,positive
"As the performance of larger learning rate schedules such as LRW, SLR, and CLR are rather similar, we select CLR for use in this experiment.",1,related,1,positive
"To verify this conjecture empirically, we conduct experiments with different learning rate schedules including learning rate rewinding (Renda et al., 2020) while varying pruning algorithms, network architectures and datasets.",1,related,1,positive
"To analyze the effect of retraining a pruned network, we based on learning rate rewinding (Renda et al., 2020) and experiment with different retraining settings.",1,related,1,positive
"We selected these works because they do not utilize a similar learning rate value as LRW in the original implementation, i.e., the authors applied a smaller value then the heuristic of LRW.",1,related,1,positive
"Renda et al. (2020) found that learning rate rewinding usually saturate at half of original training, thus, we perform on retraining for 80 epochs on CIFAR-10 and 45 epochs on ImageNet.",1,related,0,negative
"We first experiment with iterative `1-norm filters pruning on CIFAR-10 and report the results in Figure 4(a,b), we can observe that SLR and CLR also perform comparable or better than LRW in this setting.",1,related,1,positive
"In our results, cyclic learning rate restarting (CLR) is slightly more efficient than scaled learning rate restarting (SLR) and learning rate rewinding (LRW).",1,related,1,positive
"Finally we review another line of research on Lottery Tickets Hypothesis, SuperMask and Foresight Pruning.",1,related,1,positive
"We choose six representative methods PBW (Pruning by Weight, [10]), MLPrune [39], RIGL [5], STR [18], DNW[25], GMP [41]) as baselines.",1,related,1,positive
• We shed light on weight and learning-rate rewinding methods of re-training [22].,1,related,0,negative
"…parameters for removal; we focus on randomly selected parameters (random pruning) and removing the smallestmagnitude parameters (magnitude pruning), though other approaches exist in the literature (Han et al., 2015; Renda et al., 2020; Lee et al., 2019; Wang et al., 2020; Blalock et al., 2020).",1,related,1,positive
"There are many approaches to selecting parameters for removal; we focus on randomly selected parameters (random pruning) and removing the smallestmagnitude parameters (magnitude pruning), though other approaches exist in the literature (Han et al., 2015; Renda et al., 2020; Lee et al., 2019; Wang et al., 2020; Blalock et al., 2020).",1,related,1,positive
"Here we focus specifically on parameter pruning: the selective removal of weights based on a particular ranking [4, 5, 7, 45, 57, 58].",1,related,1,positive
"We perform preliminary experiments to investigate the potential for incorporating compression techniques such as pruning [22, 41, 52] as part of our tuning framework.",1,related,0,negative
"We create pruned models (using the approach in [52]) for MobileNet, VGG16, and ResNet18 on CIFAR-10.",1,related,1,positive
"…ResNet-50 network generated by unstructured pruning can achieve a 5.96× compression ratio, with the same accuracy as the original network, but it can only achieve 1× com-
∗The first two authors equally contribute to this paper.
pression in the case of structured sparsity (Renda et al., 2020).",1,related,1,positive
"Specifically, we employ the magnitude-based pruning method (Renda et al., 2020; Gale et al., 2019) during the forward process.",1,related,1,positive
"Therefore, we tested the elementwise pruning on the relatively high compression rates (×2, ×3, ×4) compared to the filter pruning [39].",1,related,1,positive
"Subnetworks (mTi , θ0) and (mTi , θ5%) are found on the task Ti with the random initialization θ0 [31] and an early rewinding weights θ5% [69].",1,related,1,positive
"We use the default implementations and hyperparameters [69, 10, 40, 16, 7, 51, 3].",1,related,1,positive
", applying At ) and then removing a portion of weights with the globally smallest magnitudes [38, 69].",1,related,1,positive
"We also investigate the efficacy of methods introduced by [12, 38, 13, 42] such as iterative magnitude pruning, late resetting, early bird training, and layerwise pruning in the context of object recognition.",1,related,1,positive
We prune the model using unstructured pruning at a sparsity of 81% and the rewind algorithm [57] (see Section 2 for a more detailed description of rewinding).,1,related,1,positive
"Inspired by this finding, we rewound the unimportant component to the initialization values Frankle and Carbin (2019; Renda et al. (2020) and fine-tune
them together with the other trained components for a few more steps.",1,related,1,positive
"Network Pruning On the other hand, our work is also related to the studies on network pruning, including but not limited to [39, 40, 41, 42, 43, 44, 45, 46].",1,related,1,positive
"In light of the aforementioned related works, we observed that most claims and conclusions are based on experiments with unstructured pruning, with the exception of Renda et al. (2020) [20] who present preliminary experiments with structured pruning.",1,related,1,positive
"…iteratively pruned networks found by weight rewinding, learning rate rewinding and
2The percentage of remaining parameters was calculated from the reported compression ratio values of Renda et al. (2020) [20].
their randomly initialized counterparts and considering local and global pruning.",1,related,1,positive
"2) Learning Rate Rewinding: In Figure 5, we present the results from applying the learning rate (LR) rewinding retraining technique as proposed by Renda et al. (2020) [20].",1,related,1,positive
"For comparison, Renda et al. (2020) [20] apply structured pruning with learning rate Rewinding for a small range of compression levels, from approximately 86.96% to 58.82% of the remaining parameters on ResNet-56 and 92.60% to 79.36% of the remaining parameters on ResNet-34.",1,related,1,positive
"Another line of research loosely connected to our work is to reduce inference time via pruning less significant weights and/or converting the model to low-precision (aka quantization) (Han et al., 2016; Howard et al., 2017; Iandola et al., 2016; Renda et al., 2020; Frankle and Carbin, 2019).",1,related,1,positive
"Another line of research loosely connected to our work is to reduce inference time via pruning less significant weights and/or converting the model to low-precision (aka quantization) (Han et al., 2016; Howard et al., 2017; Iandola et al., 2016; Renda
et al., 2020; Frankle and Carbin, 2019).",1,related,1,positive
"And as pointed out in [37], learning rate rewinding usually surpasses weights rewinding, so we mostly focus on learning rate rewinding.",1,related,1,positive
"In addition, we also find a very recent pruning method in ICLR 2020 [37] for obtaining “partiallytrained tickets” can pass our sanity checks.",1,related,0,negative
"In this section, we study pruning methods in a very recent ICLR 2020 paper [37], which is classified as partially-trained tickets (Section 2.",1,related,0,negative
"We then apply our sanity checks on the pruning methods in four recent papers from ICLR 2019 and 2020 [23, 9, 41, 37].",1,related,0,negative
We then combine our insights of random tickets with these partially-trained tickets and propose a method called “hybrid tickets” (Figure 2) which further improves upon [37].,1,related,1,positive
"After pruning at step t of training, we subsequently train the network further by repeating the entire learning rate schedule from the start (Renda et al., 2020).",1,related,0,negative
"We
∗Equal contribution.
prune in an iterative, lottery-ticket fashion to identify Transformers at competitive sparsities with no drop in task performance (Renda et al., 2020; Yu et al., 2020; Brix et al., 2020).",1,related,1,positive
"In order to find an efficient weight-to-approximate mode mapping and reduce the number of evaluated solutions, we employ a four-step methodology based on the concepts of layer significance and weight magnitude [42], [43] (Figure 5).",1,related,1,positive
"[18] show that, in other settings, IMP subnetworks rewound early in training reach the same accuracies at the same sparsities as subnetworks found by this standard pruning procedure.",1,related,0,negative
"Practically speaking, this would allow us to replace a pre-trained BERT with a smaller subnetwork while retaining the capabilities that make it so popular for NLP work.",1,related,0,negative
We use standard hyperparameters for several downstream NLP tasks as shown in Table 1.,1,related,1,positive
"• Unlike previous work in NLP, we find these subnetworks at (pre-trained) initialization rather after some amount of training.",1,related,0,negative
We conclude that the lottery ticket observations from other computer vision and NLP settings extend to BERT models with a pre-trained initialization.,1,related,1,positive
"To do so, we adopt a strategy in which we iteratively prune the 10% of lowestmagnitude weights and train the network for a further t iterations from there (without any rewinding) until we have reached the target sparsity [41, 40, 18].",1,related,1,positive
"Claim 4: When matching subnetworks are found, they reach the same accuracies at the same sparsities as subnetworks found using standard pruning [18].",1,related,1,positive
"As pre-training becomes increasingly central in NLP and other areas of deep learning [7, 8], our results demonstrate that the lottery ticket observations—and the tantalizing possibility that we can train smaller networks from the beginning—hold for the exemplar of this class of learning algorithms.",1,related,1,positive
"…Pruning (MP) (Han et al., 2015), is a popular pruning criterion in which the saliency is simply based on the norm of the parameter:
sMPk = θ 2 k (4)
Despite its simplicity, MP works extremely well in practice (Gale et al., 2019), and is used in current state-of-the-art methods (Renda et al., 2020).",1,related,1,positive
"We hypothesize that this could be one of the reasons behind the success of the Lottery Ticket and Rewinding experiments (Frankle & Carbin, 2018; Frankle et al., 2019; Renda et al., 2020).",1,related,0,negative
"This method is a standard way to prune (Han et al., 2015) that gets stateof-the-art tradeoffs between error and unstructured density (Gale et al., 2019; Renda et al., 2020).",1,related,1,positive
"For IMP, we use a practice called weight rewinding (Frankle et al., 2020; Renda et al., 2020), in which the values of unpruned weights are rewound to their values earlier in training (in our case, epoch 10) and the training process is repeated from there to completion.",1,related,1,positive
"We use per-weight magnitude pruning because it is generic, well-studied (Han et al., 2015), and produces stateof-the-art tradeoffs between density and error (Gale et al., 2019; Blalock et al., 2020; Renda et al., 2020).",1,related,1,positive
"We implemented structured pruning and LR factorization based on the original papers – (Renda et al., 2020) for pruning and (Tai et al., 2016) for LR factorization.",1,related,1,positive
"We implemented structured pruning and LR factorization based on the original papers – (Renda et al., 2020) for pruning and (Tai et al.",1,related,1,positive
"As baselines for comparison, we apply pruning using LRR (Renda et al., 2020) or LR factorization using the technique in (Tai et al.",1,related,1,positive
"As baselines for comparison, we apply pruning using LRR (Renda et al., 2020) or LR factorization using the technique in (Tai et al., 2016) to all the candidate neural network architectures with the constraint to retain the same accuracy as the original model, and pick the most efficient pruned…",1,related,1,positive
"…on ImageNet dataset and VGG-19 on Tiny-ImageNet dataset, comparing our method with PFB (Liebenwein et al. (2019)), HRank (Lin et al. (2020)), ILP (Renda et al. (2020)), Sparse Structure Selection (SSS) (Huang & Wang (2018)), NN Slimming (Liu et al. (2017)), and Eigendamage (Wang et al. (2019)).",1,related,1,positive
"For example, on ResNet-56, with 1% of accuracy loss, the amount of parameter reduction achieved by our method is higher than that of ILP (Renda et al. (2020)) by 34.",1,related,1,positive
"We compare our method with the state-of-the-art works, including SCOP (Tang et al. (2020)), HRank (Lin et al. (2020)), ILP (Renda et al. (2020)), PPR (Zhuang et al. (2020)), RFR (He et al. (2017)), GAL (Lin et al. (2019)), DCP (Zhuang et al. (2018)), GBN (You et al. (2019)), CP (He et al. (2017)),…",1,related,1,positive
"For ResNet-50 on ImageNet, the learning rate increases to 0.256 in a warmup mechanism during the first 5 epochs, and decays with a factor of 0.1 at epochs 30, 60, 80 (Renda et al. (2020); Frankle et al. (2019)).",1,related,1,positive
"For example, on ResNet-56, with 1% of accuracy loss, the amount of parameter reduction achieved by our method is higher than that of ILP (Renda et al. (2020)) by 34.51%; and with 75% FLOPs reduction, the accuracy achieved by our method is higher than PPR (Zhuang et al. (2020)) by 0.87%.",1,related,1,positive
"For example, on ResNet-56, with 1% of accuracy loss, the amount of parameter reduction achieved by our method is higher than that of ILP (Renda et al. (2020)) by 34.51%; and with 75% FLOPs reduction, the accuracy achieved by our method is higher than PPR (Zhuang et al. (2020)) by 0.",1,related,1,positive
"To make matters worse, since we know that there are often multiple winning tickets available from the training process (Renda et al., 2020), picking one ticket but not the other will potentially yield a different set of preferred clustering schemes for the network.",1,related,0,negative
"This technique is referred as weights rewinding and we denote the range of [k1, k2] as the tickets window (Renda et al., 2020).",1,related,1,positive
"We address the first challenge by consulting model-generated information — in this case, the empirical findings on Lottery Ticket Hypothesis (LTH) and related literature on weights shifting — to develop a scoring system that identifies the optimal clustering scheme among options per each convolutional layer (Frankle & Carbin, 2019; Renda et al., 2020).",1,related,1,positive
Our proposed method can also be applied to experiments outside of Renda et al. (2020).,1,related,0,negative
"In addition, our method consults empirical findings on the lottery ticket hypothesis and its derived literature regarding weights shifting (Frankle & Carbin, 2019; Renda et al., 2020; Zhou et al., 2019), and we propose a novel greedy kernel pruning algorithm that is again simple, efficient, yet effective — more on this in Section 3.",1,related,1,positive
"Thus, we can decide which model we will prune on, identify its tickets window by consulting experiment results from Renda et al. (2020), and truncate some epochs in such window to conduct multiple evaluations.",1,related,1,positive
"…method consults empirical findings on the lottery ticket hypothesis and its derived literature regarding weights shifting (Frankle & Carbin, 2019; Renda et al., 2020; Zhou et al., 2019), and we propose a novel greedy kernel pruning algorithm that is again simple, efficient, yet effective — more…",1,related,1,positive
"We address both challenges by relying on the same finding from Renda et al. (2020), which demonstrates that for any k where 0 < k1 ≤ k ≤ k2 < t, W ′
k can be a winning ticket.",1,related,1,positive
"We introduce these settings because previous works [47,23,56] showed that finetuning LR has a great impact on the final performance.",1,related,0,negative
"…on a datasetD (i.e., applyingADt ); (2) eliminating a portion of insignificant weights with the globally smallest magnitudes (Han et al., 2015; Renda et al., 2020) so that the model only has si% of weights remaining (i.e., the sparsity); (3) rewinding model weights to θ (θ = θ0, the original…",1,related,1,positive
"We note that this learning rate schedule is different from prior work on pruning, which typically uses a single decay cycle [Kusupati et al., 2020, Singh and Alistarh, 2020, Peste et al., 2021], or dynamic learning rate rewinding, e.g. [Frankle et al., 2019, Renda et al., 2020].",1,related,1,positive
"Clearly, we see that SLR benefits from a larger weight decay, while the approach by Renda et al. (2020) is suffering from an increased penalty on the weights.",1,related,0,negative
"As a baseline performance for a pruned network, we will use the approach suggested by Renda et al. (2020) as it serves as a good benchmark for the current potential of IMP.",1,related,1,positive
"For the respective results of the algorithm by Renda et al. (2020) we observe test accuracies of 92.91% (±0.39%), 93.08% (±0.54",1,related,1,positive
"…is repeatedly removing only a small fraction of the parameters followed by extensive retraining, is said to achieve results on the Pareto frontier (Renda et al., 2020), its iterative nature is also considered to be computationally tedious, if not impractical: “iterative pruning is computationally…",1,related,1,positive
"Renda et al. (2020) for example suggested the following approach: train a network for T epochs and then iteratively prune 20% percent of the weights and retrain for Trt = T epochs using LRW, i.e., use the same learning rate scheme as during training, until the desired sparsity is reached.",1,related,1,positive
"…baseline in the case of a 1e-4 weight decay within the given retraining time frame, we note that SLR easily outperforms the LRW-based proposal by Renda et al. (2020) when considering the weight decays that also lead to the best performing dense model, which is a strong indicator that it is…",1,related,1,positive
"For a fair comparison we also include LRR (Renda et al., 2020) which uses a pre-trained network and multiple rounds of pruning and retraining by leveraging learning rate rewinding.",1,related,0,negative
"…N-BEATS
WER FLOPs PPL FLOPs SMAPE FLOPs
Dense 12.2 4.53G 18.6 927.73G 8.3 41.26M
SNIP (Lee et al., 2019) 14.3 2.74G 24.6 398.92G 10.1 21.45M LRR (Renda et al., 2020) 13.7 2.61G 23.1 339.21G 9.3 14.47M RigL (Evci et al., 2020) 13.9 2.69G 22.4 326.56G 10.2 15.13M SIS (Ours) 13.1 2.34G 21.1…",1,related,1,positive
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets (Zagoruyko and Komodakis [2016]). The new experiments led us to discover the limitations of learning rate rewinding which can worsen pruning results on large architectures. Methodology We implemented the code ourselves in Python with TensorFlow 2, basing our implementation of the paper alone and without consulting the source code provided by the authors. We ran two sets of experiments. In the reproduction set, we have striven to exactly reproduce the experimental conditions of Renda et al. [2020]. We have also conducted additional experiments, which use other network architectures, effectively showing results previously unreported by the authors.",1,related,1,positive
"2 Scope of reproducibility Renda et al. [2020] formulated the following claims: Claim 1: Widely used method of training after pruning: finetuning yields worse results than rewinding based methods (supported by figures 1, 2, 3, 4 and table 5) Claim 2: Newly introduced learning rate rewinding works as good or better as weight rewinding in all scenarios (supported by figures 1, 2, 3, 4 and table 5, but not supported by figure 5) Claim 3: Iterative pruning with learning rate rewinding matches state-of-the-art pruning methods (supported by figures 1, 2, 3, 4 and table 5, but not supported by figure 5)",1,related,1,positive
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods.",1,related,1,positive
"For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones.",1,related,0,negative
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019].",1,related,1,positive
"Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets (Zagoruyko and Komodakis [2016]).",1,related,1,positive
"For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in He et al. [2016a]. Additionally, to verify the general usefulness of pruning and retraining methods proposed in Renda et al.",1,related,1,positive
"For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in He et al. [2016a]. Additionally, to verify the general usefulness of pruning and retraining methods proposed in Renda et al. [2020] we extend the list of tested network architectures to much larger wide residual networks from Zagoruyko and Komodakis [2016].",1,related,1,positive
"As the specific rewind points, we follow the setting in [3].",1,related,1,positive
We use Pytorch[18] to implement all experiments and follow hyperparameters identically in [3].,1,related,1,positive
"Similar to previous studies [1, 3], we perform experiments on CIFAR-10, Tiny-ImageNet and ImageNet.",1,related,1,positive
"(2019) further scale up LTH by rewinding (Frankle et al., 2020a; Renda et al., 2020).",1,related,0,negative
"In the algorithm, we prune the language model using gradual pruning (Liu et al., 2018b; Renda et al., 2019).",1,related,1,positive
(1) Unstructured pruning : Rewind Renda et al. (2020).,1,related,0,negative
"For some experiments, we also modified line 6 for techniques such as late rewinding and learning rate rewinding (Renda et al., 2020).",1,related,1,positive
"We ran late rewinding experiments from Renda et al. (2020), where instead of rewinding the weights to θ0 in line 6 of Algorithm A1, we set the weights to their values at epoch 2.",1,related,1,positive
"Our main experiments are mainly based on the lottery ticket procedure from Frankle & Carbin (2018) along with follow up work (Renda et al., 2020) for unstructured pruning.",1,related,0,negative
"Our learning rate rewinding experiments also encapsulate the fine-tuning experiments (Renda et al., 2020).",1,related,1,positive
"1 MODIFIED PRUNING PROCEDURES Our main experiments are mainly based on the lottery ticket procedure from Frankle & Carbin (2018) along with follow up work (Renda et al., 2020) for unstructured pruning.",1,related,0,negative
"More Technical Details of Top-down Pruning In our implementation, we set p 1
n(i) = 20% as (Frankle & Carbin, 2019; Renda et al., 2020) and adjust {n(i)} to control the pruning schedule of IMP over sequential tasks.",1,related,1,positive
"…rate schedules other than compressing the original learning rate schedule from the fist phase of training; it is plausible that other learning rate schedules better tuned to the technique or network could train even faster (Renda et al., 2020), but we do not evaluate these alternative schedules.",1,related,0,negative
"We also do not evaluate learning rate schedules other than compressing the original learning rate schedule from the fist phase of training; it is plausible that other learning rate schedules better tuned to the technique or network could train even faster (Renda et al., 2020), but we do not evaluate these alternative schedules.",1,related,0,negative
"For a fair comparison, we follow the standard implementations and hyperparameters in (Renda et al., 2020) for OMP, LTH, RP, and PI experiments, as shown in Table 1.",1,related,1,positive
