text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"Nevertheless, to the best of our knowledge, all of these differentiable PnP layers have a common property: They first solve the PnP problem to obtain either the pose [3, 4, 6] or the posterior pose distribution [7], and then compute the error to be backpropagated based on a dedicated loss funcar X iv :2 30 3.",1,related,1,positive
"We also compare our loss function with the state-of-the-art differentiable PnP methods, namely, BPnP [6] and EPro-PnP [7].",1,related,1,positive
"Note that BPnP [6] does not fully constrain the weights, thus we remove the scale branch as stated in Sec.",1,related,1,positive
We also design a unique training scheme for this network by introducing a Back-propagated PnP (BPnP) layer [2] so that reprojection error can be adopted as the loss function.,1,related,1,positive
"We employ reprojection error as loss function which is enabled by building on the recent progress on the PnP [1], [2], [28] problem.",1,related,1,positive
"To obtain extrinsics and enable the network end-to-end training, we connect the network with a BPnP layer [2] to estimate [W R, C W t] from the predicted K.",1,related,1,positive
"(12) Following [5], we construct a constrain function F to employ the implicit function theorem:",1,related,1,positive
"Since the optimal so-lution T c ∗ b is a local minimum for the objective function O ( o , p , T cb , K ) , a stationary constraint of the optimization process can be constructed by taking the first order derivative of the objective function with respect to T cb : Following [5], we construct a constrain function F to employ the implicit function theorem: Substituting the Eq.",1,related,1,positive
"We used the EPnP [28] implementation from Pytorch3D [44], since we found it to be faster and more stable than the methods based on declarative layers [8, 16].",1,related,1,positive
"Then, we employ the backpropagatable PnP algorithm from [5] to retrieve the estimated rotation matrix R pnp and translation vector t i pnp.",1,related,1,positive
"For DGECN, we replace the DG-PnP in our architecture with PnP variants [6,16,42].",1,related,1,positive
BPnP [10] is not included as it adopts a different train/test split.,1,related,0,negative
(10) instead of the reprojection-metric pose loss in BPnP [10].,1,related,1,positive
EP:Baseline EP:PY EP:RHaug PVNet[44] CDPN[38] BPnP[11] RNNPose[55] DFPN-6D[12] 95.,1,related,1,positive
"After estimating these correspondences, PnP is commonly employed to solve for the 6D pose.",1,related,1,positive
"To backpropagate through L-BFGS we use the implicit function theorem as described in [41, 42].",1,related,1,positive
"(7)
To further utilize the 3D structure information while reducing prediction errors, we employ BPnP [3] to compute the object pose from the predicted 2D keypoints, and then re-project the 3D keypoints on a CAD model back to 2D image space using the computed pose.",1,related,1,positive
"To further utilize the 3D structure information while reducing prediction errors, we employ BPnP [3] to compute the object pose from the predicted 2D keypoints, and then re-project the 3D keypoints on a CAD model back to 2D image space using the computed pose.",1,related,1,positive
"End-to-End Training Incorporating the PnP backpropagation approach in [5], we apply smooth L1 loss on the Euclidean errors of estimated translation vector and yaw angle.",1,related,1,positive
"For PointNet-like PnP, we extend the PointNet in [19] to account for dense correspondences.",1,related,1,positive
"Additionally, replacing the scale-invariant δz in tSITE with the absolute distance tz or directly regressing the object center (ox, oz)
3https://github.com/BoChenYS/BPnP
leads to inferior poses w.r.t. translation (B0 vs. E1, E2).",1,related,1,positive
"For BPnP [8], we replace the Patch-PnP in our framework with their implementation of BPnP3.",1,related,1,positive
"We demonstrate the effectiveness of the image-like geometric features (M2D-3D,MSRA) by comparing our Patch-PnP with traditional PnP/RANSAC [28], the PointNet-like [41] PnP from [19], and a differentiable PnP (BPnP [8]).",1,related,1,positive
"As BPnP was originally designed for sparse keypoints, we further adapt it appropriately to deal with dense coordinates.",1,related,1,positive
We use the implementation from [3] for differentiable PnP.,1,related,1,positive
"Restrictions apply.
a feature extraction network and Heatmap [30] is proposed to accurately calculate the matched pixels of 3-",1,related,1,positive
2 Results beyond the BPnP paper Apart from the experiments conducted by the authors in [1] we provide additional to further support the main claims.,1,related,0,negative
"We communicated with the authors of [1] through GitHub, and we would like to thank them as they provided a fast and detailed response.",1,related,0,negative
Our results support the claims presented by both authors in [1] and [2] respectively.,1,related,0,negative
"BPnP: BPnP focuses on the Pose Retrieval stage, and following [1] we trained our model under the 3 different schemes used in the original work as well: 1We apply the proposed module in the object pose estimation task, while authors originally demonstrated it for the human-pose estimation task, but its concept still applies in our case as well.",1,related,1,positive
"End-to-End Training Incorporating the PnP backpropagation approach in [5], we apply smooth L1 loss on the Euclidean errors of estimated translation vector and yaw angle.",1,related,1,positive
"Regarding differentiable PnP, we generally follow the approach in BPnP [5], with the code completely reimplemented for higher efficiency and uncertainty awareness.",1,related,1,positive
"77736 VOLUME 9, 2021
Noticeable in Table 2 where the ADD metric is compared, our performance will become the best without considering support from additional refinement procedure (e.g., DPOD [26]) or more training samples (e.g., BPnP [50] which uses 67% and 33% of samples for training and testing, in contrast to ours which uses 15% and 85% for training and testing).",1,related,0,negative
