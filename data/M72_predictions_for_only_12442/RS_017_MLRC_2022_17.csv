text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"However, we note that ignoring points where non-differentiability may occur in the domain could introduce errors in some iterations during training, as it may also happen with ReLU [1].",1,related,1,positive
"At zero error, these librairies fix arbitrarily the value of the function error gradient to zero [31].",1,related,1,positive
"Abusing notation slightly, we write dPC2 dz ∣∣∣∣ z=z̄ = diag(c̃(z)) ∈ ∂PC2 (z̄) This aligns with the default rule for assigning a sub-gradient to ReLU used in the popular machine learning libraries TensorFlow[1], PyTorch [39] and JAX [16], and has been observed to yield networks which are more stable to train than other choices [11].",1,related,1,positive
