text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"This quantity is very similar to that given in [Aitchison, 2019], except that it allows for a slightly more general proposal, QMP, which allows for dependencies between the K samples for a single latent variable, z(1) i , .",1,related,1,positive
"We can see that the TMC (orange) [Aitchison, 2019] performs considerably worse than massively parallel VI (red) and IWAE (blue) [Burda et al.",1,related,1,positive
"To sample all K copies of the full joint latent space, TMC [Aitchison, 2019] uses an IID distribution over the K samples, z(1) i , .",1,related,1,positive
"…log 1
L L∑ ℓ=1 pθ(zℓ, x) qϕ(zℓ|x)
] , (2)
which has been extensively used as an objective function (Burda et al., 2015; Sønderby et al., 2016; Aitchison, 2019; Lopez et al., 2020) and, importantly, as a metric for estimating the marginal log-likelihood, log pθ(x), in VAEs (e.g.,…",1,related,1,positive
"Applications of the product-form estimators it builds on can be found peppered throughout the Monte Carlo literature [64, 49, 1, 62, 45], almost always unnamed and specialized to particular contexts.",1,related,1,positive
"With the setting of complete matching, IPF becomes tensor Monte Carlo (TMC) (Aitchison, 2019) for SSMs.",1,related,1,positive
"2021) obtained by combining product-form estimators with SMC and Tensor Monte Carlo (Aitchison 2019) obtained by merging the estimators with variational autoencoders. When choosing among the resulting (and at times bewildering) constellation of estimators, we recommend following one simple principle: pick estimators that somehow ‘resemble’ or ‘mirror’ the target. Good examples of this are well parametrized Gibbs samplers which generate new samples using the target’s exact conditional distributions and, consequently, often outperform other Monte Carlo algorithms (e.g., Sect. 3.4).While formany targets these conditional distributions cannot be obtained (nor are good parametrizations known), their (conditional) independence structure is usually obvious [e.g., see Gelman and Hill (2006), Gelman (2006), Koller and Friedman (2009), Hoffman et al. (2013), Blei et al. (2003), and themany references therein] and can bemirrored using product-form estimators within one’s methodology of choice.",1,related,1,positive
"2021) obtained by combining product-form estimators with SMC and Tensor Monte Carlo (Aitchison 2019) obtained by merging the estimators with variational autoencoders. When choosing among the resulting (and at times bewildering) constellation of estimators, we recommend following one simple principle: pick estimators that somehow ‘resemble’ or ‘mirror’ the target. Good examples of this are well parametrized Gibbs samplers which generate new samples using the target’s exact conditional distributions and, consequently, often outperform other Monte Carlo algorithms (e.g., Sect. 3.4).While formany targets these conditional distributions cannot be obtained (nor are good parametrizations known), their (conditional) independence structure is usually obvious [e.g., see Gelman and Hill (2006), Gelman (2006), Koller and Friedman (2009), Hoffman et al. (2013), Blei et al.",1,related,1,positive
"2021) obtained by combining product-form estimators with SMC and Tensor Monte Carlo (Aitchison 2019) obtained by merging the estimators with variational autoencoders. When choosing among the resulting (and at times bewildering) constellation of estimators, we recommend following one simple principle: pick estimators that somehow ‘resemble’ or ‘mirror’ the target. Good examples of this are well parametrized Gibbs samplers which generate new samples using the target’s exact conditional distributions and, consequently, often outperform other Monte Carlo algorithms (e.g., Sect. 3.4).While formany targets these conditional distributions cannot be obtained (nor are good parametrizations known), their (conditional) independence structure is usually obvious [e.g., see Gelman and Hill (2006), Gelman (2006), Koller and Friedman (2009), Hoffman et al.",1,related,1,positive
"(2021) study their usewithin sequential Monte Carlo (SMC), and Aitchison (2019) builds on them to obtain tensor Monte Carlo (TMC), an extension of importance weighted variational autoencoders. The latter article is the aforementioned exception: its author defines the estimators in general and refers to them as ‘TMC estimators,’ but does not study them theoretically. To the best of our knowledge, there has been no previous systematic exploration of the estimators (2), their theoretical properties, and uses, a gap we intend to fill here. Furthermore, while in simple situations with fully, or almost-fully, factorized test functions [e.g., those in Tran et al. (2013) or Schmon et al. (2020)] it might be clear to most practitioners that employing a product-form estimator is the right thing to do, it may not be quite so immediately obvious how much of a difference this can make and that, in rather precise ways (cf. Theorems 2 and 4), judiciously using product-form estimators is the best thing one can do within Monte Carlo when tacklingmodelswith known independence structure but unknown conditional distributions (a common situation in practice). We aim to underscore these points through our analysis and examples. Lastly, we remark that product-form estimators are reminiscent of classical product cubature rules (Stroud 1971). These are obtained by taking products of quadrature rules and, consequently, require computing sums over NK points much like for product-form estimators [except for fully, or partially, factorized test functions φ where the cost can be similarly lowered, e.g., p. 24 in Stroud (1971)].",1,related,1,positive
"…to estimate intractable acceptance probabilities for similarmodels, Lindsten et al. (2017) andKuntz et al. (2021) study their usewithin sequential Monte Carlo (SMC), and Aitchison (2019) builds on them to obtain tensor Monte Carlo (TMC), an extension of importance weighted variational autoencoders.",1,related,1,positive
"(2021) study their usewithin sequential Monte Carlo (SMC), and Aitchison (2019) builds on them to obtain tensor Monte Carlo (TMC), an extension of importance weighted variational autoencoders. The latter article is the aforementioned exception: its author defines the estimators in general and refers to them as ‘TMC estimators,’ but does not study them theoretically. To the best of our knowledge, there has been no previous systematic exploration of the estimators (2), their theoretical properties, and uses, a gap we intend to fill here. Furthermore, while in simple situations with fully, or almost-fully, factorized test functions [e.g., those in Tran et al. (2013) or Schmon et al. (2020)] it might be clear to most practitioners that employing a product-form estimator is the right thing to do, it may not be quite so immediately obvious how much of a difference this can make and that, in rather precise ways (cf.",1,related,1,positive
"(2021) study their usewithin sequential Monte Carlo (SMC), and Aitchison (2019) builds on them to obtain tensor Monte Carlo (TMC), an extension of importance weighted variational autoencoders. The latter article is the aforementioned exception: its author defines the estimators in general and refers to them as ‘TMC estimators,’ but does not study them theoretically. To the best of our knowledge, there has been no previous systematic exploration of the estimators (2), their theoretical properties, and uses, a gap we intend to fill here. Furthermore, while in simple situations with fully, or almost-fully, factorized test functions [e.g., those in Tran et al. (2013) or Schmon et al.",1,related,1,positive
2021) obtained by combining product-form estimators with SMC and Tensor Monte Carlo (Aitchison 2019) obtained by merging the estimators with variational autoencoders.,1,related,1,positive
"We note in our work, that the intentions of [6] were not to achieve state-of-the-art performances, but to compare the proposed the model to the baseline used in the TMC paper.",1,related,0,negative
"Mathematically, as is shown in [6], the number of evaluated importance samples grows exponentially with the layers, so if we reduce the number of layers to one, we effectively evaluateK(1) = K samples in the TMC, the same as for the IWAE.",1,related,1,positive
"To compute the tensor inner-product in a numerically stable way, the author provides a method referred to as logmmexp ([6]; see Appendix A).",1,related,1,positive
"In order to average over all different combinations ofmarginal log-likelihoods, Aitchison ([6]), defines the new marginal likelihood estimator as",1,related,1,positive
"In this work, we reproduce what we believe are the most important results presented in the Tensor Monte Carlo paper ([6]), where we also provide our reimplementation code.",1,related,1,positive
"Since the effects of TMC becomes apparent only when we have intermediate layers ([6]), we expect the IWAE and TMC to produce approximately the same results.",1,related,1,positive
