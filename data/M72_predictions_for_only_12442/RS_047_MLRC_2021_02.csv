text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"Similar to the interpretation of image generation process [10], we consider the",1,related,1,positive
"We adopt this hypothesis from Schölkopf et al. [2021], positing that distribution changes typically affect only a sparse or local subset of factors, rather than all factors simultaneously.",1,related,1,positive
", n}, X := g(Z) (5) U ∼ pU, Zi ∼ pZi ; ∀Zi ∈ Zcnf , Zj := fj(pa(Zj)); ∀Zj ̸∈ Zcnf , X := g(Z) (6) U ∼ pU, Zi ∼ pZi ; ∀Zi ∈ Zcnf ∪ {Z0}, Zj := fj(pa(Zj)); ∀Zj ̸∈ Zcnf ∪ {Z0}, X := g(Z) (7) As explained in § 2, counterfactual generative networks (CGN) [43] generates counterfactual images by simulating causal model in Eqn 7 above, performing interventions on all of {Z0} ∪ Zcnf .",1,related,1,positive
"X Y Gdo({Z0}∪Zcnf ) [43, 15] Gdo(X) [16, 57, 10, 58] Gdo(Zcnf ) [52, 13] Gdo(Z0)",1,related,1,positive
"Recall that we use CGN [19] to generate counterfactual samples for each training and testing samples (cf. Section II-B) by intervening the texture of the object (i.e., T represents texture).",1,related,1,positive
"For both Animal and Vehicle datasets, we use two widely used data augmentation strategies: 1) randomly cropping
2We adopt this initialization for fair comparison since the CGN used in L2D is trained on ImageNet [22].
the images with random retain ratio in [0.8, 1.0]; and 2) randomly applied horizontal flipping with 50% probability.",1,related,1,positive
"Unlike our work which calculates the consensus for every class and utilizes the result for correction, CGN [19] simply adds these samples to the training set and GCM-CF [10] provides binary information about seen/unseen of an image, but do not interfere the inference.",1,related,1,positive
Recall that we use CGN [19] to generate counterfactual samples for each training and testing samples (cf.,1,related,0,negative
"(4); 5: Return θ̂, CGN, η̂, and ω̂.",1,related,1,positive
"Among the existing models, we find that Counterfactual Generative Network (CGN) [19] can well support our requirement, which accounts for the mediator between X and Y and consists of components to model TY=y and TX=x.",1,related,1,positive
"Considering that CGN is not a perfect generative network, which shows high cognitive uncertainty, we run four repeats for each x∗y′ .",1,related,1,positive
"We thus directly use CGN as the CI module in the L2D framework to generate the counterfactual samples for all candidate classes: {x∗y′ |y′ ∈ [1, C]}.",1,related,1,positive
"/* Testing */ 6: Infer y = f(x|θ̂); 7: for y′ = 1 → C do 8: Infer x∗y′ = CGN(x, y
′); 9: end for
10: Calculate zy (Eq.",1,related,1,positive
"In particular, given a sample x, we generate a counterfactual image x∗y′ by feeding CGN with the common textual of class y′ where we enumerate all possible classes y′.",1,related,1,positive
"Following the rules [19], we generate the same amount of counterfactual samples as the training set to train models.",1,related,0,negative
"For specific implementations and results in this property, readers can refer to [64, 74, 150, 72, 101, 90].",1,related,1,positive
"The generator function in CGN is formally described as:
xgen = C(m,f , b) = m⊙ f + (1−m)⊙ b (5)
where m represents the mask, f is the foreground, and b stands for the background.",1,related,1,positive
Figure 1: (a) causal data generating process considered in this paper (CONIC = Ours); (b) causal data generating process considered in CGN [34].,1,related,1,positive
"We compare CONIC with various baselines including traditional Empirical Risk Minimizer (ERM), Conditional GAN (CGAN) (Goodfellow et al., 2014a), Conditional VAE (CVAE) (Kingma & Welling, 2013), Conditional-β-VAE (C-βVAE) (Higgins et al., 2017), AugMix (Hendrycks et al., 2020), CutMix (Yun et al., 2019), Invariant Risk Minimization (IRM) (Arjovsky et al., 2019), and Counterfactual Generative Networks (CGN) (Sauer & Geiger, 2021).",1,related,1,positive
"In this work, we begin with quantifying confounding in observational data that is generated by an underlying causal graph (more general than considered by CGN) of the form shown in Figure 1(a).",1,related,1,positive
"We compare CONIC with various baselines including Empirical Risk Minimizer (ERM), Conditional GAN (CGAN) [12], Conditional VAE (CVAE) [20], Conditional-β-VAE (C-βVAE) [16], AugMix [15], CutMix [43], Invariant Risk Minimization (IRM) [1], and Counterfactual Generative Networks (CGN) [34].",1,related,1,positive
"We also note that the deterministic models such as CGN fail when they are applied to a different task where the number and type of generative factors are not fixed and are difficult to separate (e.g., CelebA).",1,related,1,positive
"When we increase the number of counterfactual instances, performance of CGN reduces further.",1,related,1,positive
"As shown in Table 2, the time required to run our method to generate counterfactual images w.r.t. a generative factor Zj is significantly less than CGN that learns deterministic causal mechanisms as discussed in Section 2.",1,related,1,positive
"L G
] 1
0 D
ec 2
using generative modeling when there are confounders is challenging (Sauer & Geiger, 2021; Reddy et al., 2022; Funke et al., 2022).",1,related,1,positive
"Using Grad-CAM (Selvaraju et al., 2016) to explain a ResNet-50 (He et al., 2016) domain classifier (bottom-left) does not lead to actionable insights.",1,related,1,positive
"com/usaito/counterfactual-cv CGN[70] MNIST,ImageNet Pytorch https://github.",1,related,0,negative
"A simple way of (approximately) achieving counterfactualinvariant predictors is via counterfactual data augmentations (CDA) [Lu et al., 2020, Kaushik et al., 2019, Sauer and Geiger, 2021], where one augments the training data with inputs generated from different spurious features.",1,related,1,positive
"For example, do(ai+A′) where A′ ∈ {15, 20} and a ∈ [60, 70] results in xcf with the highest median VED, since ai+A′ ∈ [65, 90] is generally outside the learned range of ages.",1,related,1,positive
"Motivated by the findings from the prior works [9, 17], we designed the shape-focused augmentation introduced in Sec.",1,related,1,positive
"Regarding the superior performance of the model trained with shape-focused augmentation over the model trained on counterfactual images across all OOD datasets, we supposed that it results from the inherent limitations that the model architecture in CGN [17] has.",1,related,1,positive
"By utilizing CGN [17]’s method, we generate a set of the counterfactual images where the alteration of texture makes them semantically not resembled with the images of the identical label from the original dataset as shown in Fig.",1,related,1,positive
We implemented the experiment to validate the effectiveness of shape-focused augmentation by comparing it with the prior method [17] in terms of the accuracy on both the original dataset (ImageNet-100) and out-of-distribution samples (OOD dataset [6]).,1,related,1,positive
"We use two datasets where the target attribute has a correlation strength of over 90% with the confounding factor, following the challenging settings of the latest work on visual bias [73, 17, 61, 65].",1,related,1,positive
"In addition, we will investigate generation module for language understanding with unsupervised generative techniques (Sauer and Geiger, 2021).",1,related,1,positive
"A set of methods condition the generative model on attributes annotated in the dataset by using a conditional Generative Adversarial
Network (GAN) (Joshi et al., 2018; Liu et al., 2019; Sauer & Geiger, 2021; Van Looveren et al., 2021; Yang et al., 2021).",1,related,1,positive
Our experimental setup is largely based on the description provided by Sauer and Geiger [22].,1,related,0,negative
69 Section 5 concludes this work by discussing our experience with reproducing the research by Sauer and Geiger [22].,1,related,0,negative
"In summary, this work makes the following contributions: 44 • We reproduce the main experiments conducted by Sauer and Geiger [22] to identify which parts of the 45 experimental results supporting their claims can be reproduced, and at what cost in terms of resources (e.",1,related,0,negative
FlexTENet[131] # ! # # SCP[132] # ! # # CGN[74] ! ! # # SyncTwin[101] # # ! #,1,related,1,positive
"com/trends/explore?date=all&q=mnist [125, 198, 74, 199, 200, 201, 202, 203, 204] ADNI www.",1,related,0,negative
"com/usaito/counterfactual-cv CGN[74] MNIST,ImageNet Pytorch https://github.",1,related,0,negative
"…we also find that our approach offers the ability to effectively interpolate between OOD face images and more importantly, manipulate specific attributes of interest (e.g., non-smiling → smiling), thus validating its utility in semantic editing and counterfactual reasoning (Axel Sauer, 2021).",1,related,1,positive
"Such data can be manipulated, interpolated or composed [12, 13, 28, 43, 44] with dedicated operators in their latent space, and further used for counterfactual reasoning [56, 65, 72, 92].",1,related,1,positive
