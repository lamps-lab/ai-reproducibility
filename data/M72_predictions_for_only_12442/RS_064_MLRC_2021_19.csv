text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"We compare ENG-based attacks with four variants of anchor attack (AA), a recent heuristic generic poisoning attack on classical fair machine learning (Mehrabi et al., 2021).",1,related,1,positive
"We follow previous attacks on classical fair machine learning (Chang et al., 2020; Mehrabi et al., 2021) and suppose a worst-case threat model, which has full knowledge and control of the victim model, as well as the access to part of the training samples.",1,related,1,positive
"We base our research on two separate yet related streams of research: adversarial attacks on GNNs that aim to reduce GNN classification accuracy [7], [23], [36], [39], [44], [45], and attacks on fairness in the context of classical machine learning [5], [26], [27], [30].",1,related,1,positive
"To the best of our knowledge, research in this domain is very scarce for fairness attacks [17, 22] compared to backdoor attacks and BlindSpot is the first technique leveraging fairness attacks for watermarking models.",1,related,1,positive
"For instance, the attacker may poison the training data to degrade the fairness of the model [129, 167] or mislead the GNN explainer model [53].",1,related,0,negative
"the test, we needed different varieties of clients, the cooperative clients who would train less biased models, the uncooperative clients who would intentionally train their models on skewed/imbalanced data to bias their trained models (Mehrabi et al. 2021b; Solans, Biggio, and Castillo 2020; Zhang and Zhou 2019) and normal clients.",1,related,0,negative
• random anchor (RA0/RA1): these adversaries follow the protocol introduced in Mehrabi et al. (2021b).,1,related,1,positive
We conducted a reproducibility study of the paper Exacerbating Algorithmic Bias through Fairness Attacks [11].,1,related,1,positive
"Although the response was259 fairly quick, we were prompted to check the existing code in-depth, while further communication was discouraged.260
7 Conclusion261
In this reproduction study, we extensively reviewed the paper Exacerbating Algorithmic Bias through Fairness Attacks.262 We provided a clear foundation, upon which we described the proposed data poisoning attacks, namely the influence263 attack on fairness and the anchoring attack, as well as the experimental setup of the original paper.",1,related,1,positive
"Figure 1: Impact on performance and fairness of a logistic regression classifier, using the attacks proposed in [11] and other state-of-the-art methods, for increasing ε values.",1,related,1,positive
"[Re] Exacerbating Algorithmic Bias through Fairness Attacks
Anonymous Author(s) Affiliation Address email
Reproducibility Summary1
Scope of Reproducibility2
We conducted a reproducibility study of the paper Exacerbating Algorithmic Bias through Fairness Attacks [11].3 According to the paper, current research on adversarial attacks is primarily focused on targeting model performance,4 which motivates the need for adversarial attacks on fairness.",1,related,1,positive
"In the experiment, we use the Drug consumption dataset, as used in (Mehrabi et al., 2020; Donini et al., 2020).",1,related,0,negative
