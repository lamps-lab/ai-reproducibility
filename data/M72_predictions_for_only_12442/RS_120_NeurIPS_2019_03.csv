text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"This loss can be justified as a lower bound on the data log-likelihood [8, 33] or as a variant of denoising score matching [7, 35].",1,related,1,positive
"We will exploit in this paper the connection between DDPMs and LD sampling of EBMs, based upon which we achieve better sampling performance for LEBM compared with short-run LD.",1,related,1,positive
"Specifically, inspired by the connection between MCMC sampling and denoising diffusion process [7, 8, 31], in this paper we propose a diffusionbased amortization method suitable for long-run MCMC sampling in learning latent space EBMs.",1,related,1,positive
"In scorebased models [63, 64], ∇φ log pt(φ) is named as the score of each sample.",1,related,1,positive
"Because the σ ( s ) has a 1:1 correspondence to a particular time step s , NCSNs can also be written as F θ ( x ; s ) .",1,related,1,positive
"This is the essence of DMs as proposed by [1, 2].",1,related,1,positive
"To address the above issue, we propose Light Field Diffusion (LFD), a novel framework for single-view novel view synthesis based on DDPM [11].",1,related,1,positive
"For the super-resolution module, we train a DDPM [11] with T = 1000 noising steps and a linear noise schedule (1 e − 4 to 2 e − 2 ) as a refiner.",1,related,1,positive
We train a DDPM [11] as a refiner to enhance the image quality by addressing potential artifacts and imperfections that may arise during the upsam-pling process.,1,related,0,negative
"Different from DDPM [11], we find that predicting the target image instead of predicting noise is much more appropriate for our model.",1,related,1,positive
We implement our method based on Denoising Diffusion Probabilistic Model (DDPM) [11].,1,related,1,positive
"Then we can get the light field encoding L by a positional encoding function γ : Light Field Diffusion is implemented based on the De-noising Diffusion Probabilistic Model (DDPM) [11], which can estimate complex data distributions by iteratively de-noising noisy samples.",1,related,1,positive
"Specifically, we take noise level (γt = ∏t i=1 αi) as input to the denoising model gθ, similar to [13; 43; 47; 41].",1,related,1,positive
"For concrete instantiations of the forward and reverse SDE ( (1) and (2) respectively), we use an alternative form of the well-known variance-preserving stochastic differential equation (VESDE) [15] inspired by [11], and adapt it to obtain the drift and diffusion coefficients as follows",1,related,1,positive
"For concrete instantiations of the forward and reverse SDE ( (1) and (2) respectively), we use an alternative form of the well-known variance-preserving stochastic differential equation (VESDE) [15] inspired by [11], and adapt it to obtain the drift and diffusion coefficients as follows
f(st) = −γst, g(t) = σmax (σmax σmin
)t √
2 log (σmax σmin ) , (4)
where γ is a constant parameter, and σmin and σmax are parameters defining the noise schedule of the Wiener process.",1,related,1,positive
"We first train the score model on the DSM loss [7, 8, 19] and fine-tune the score model on a predictive loss which we denote in the following as correcting the reverse process (CRP) loss.",1,related,1,positive
"Given Y (1), we apply N steps of ALS to sample from the posterior distribution pH̄|Y (H̄|Y) [14].",1,related,1,positive
"The second term within this objective corresponds to the weighted denoising score matching objective [14], and it can be further simplified by substituting ∇  ̃̄ Hp  ̃̄ Ht|H̄  ̃̄ Ht|H̄) = −Zt/σ(2) t .",1,related,1,positive
"Harnessing the power of score-based generative models to be optimized for multi-modal landscapes [14], we propose joint estimation of h0 and hk through a single network (i.",1,related,1,positive
"For the DM setting, we choose σ(t) to be a geometric series, following [27].",1,related,1,positive
92 Score based and Diffusion method NCSN [29] 25.,1,related,0,negative
"In contrast to Song and Ermon [31], Urain et al. [7], our diffusion kernel Pt|0(g|g0, O(s), O(b)) in Equation (16) is not the Brownian kernel.",1,related,1,positive
"Although Equation (21) is a straightforward adaptation of the MSE minimizer formula [31], we still provide the derivation in Appendix C.",1,related,1,positive
"From the U-net architecture, we replace its 2-dimensional convolutional layers with 1-dimensional ones and follow the miscellaneous structures in [29, 30].",1,related,1,positive
"Moreover, while Song and Ermon [46] optimizes the denoising score matching loss at different noise levels jointly, we adopt a sequential learning procedure by gradually decreasing the noise level of the training samples.",1,related,1,positive
"from the non-equilibrium thermodynamics [20]: if we know the actual step-wise Gaussian noise imposed in the forward process, we are able to restore the real charging time-series distribution of through a series of iterative denoising procedures.",1,related,1,positive
"To deal with the abovementioned problems, we propose our Diffusion-VAE (D-Va) model, which combines deep hierarchical VAEs [30, 51, 56] and diffusion probabilistic [20, 50, 52] techniques to do seq2seq stock prediction (see Figure 1).",1,related,1,positive
"To do so, we first follow the denoising score-matching (DSM) process of a standard diffusion probabilistic model [36, 52].",1,related,1,positive
"Our TOSM method outperforms the FDK, FBPConvNet, DSM, and DiffusionMBIR in 3D volumetric reconstruction.",1,related,1,positive
"2, for the diffusion model, we use the common U-net architecture [21] with adaptations [16], which consists of multiple 2D convolution layers.",1,related,1,positive
"A score function (where the score is the gradient of the log probability of the underlying density function∇x log p(x)) is then learnt to reverse this forward diffusion process, meaning we can sample new data from the tractable prior N (0, I) (Song & Ermon, 2019).",1,related,1,positive
"While there are various options for λ(t) (as discussed in (Song et al. 2021a)), we adopt in this work the approach introduced in (Song and Ermon 2019).",1,related,1,positive
"Our proposed SDM method gives better results than the DDPM method in terms of both metrics, COV-R and COV-P.",1,related,1,positive
"To this end, we propose another variant of the diffusion model which utilizes the score-based diffusion model (SDM) [15] to solve this problem because it does not scale the structure of the molecule.",1,related,1,positive
"After estimating the score function, we can solve the reverse process of the diffusion model with an iterative procedure called Langevin dynamics[1, 2, 12, 14].",1,related,1,positive
"Instead of creating a new GNN that can effectively learn the 3D structure of the steps in the Diffusion model, we propose another variation of the Diffusion method called SDM.",1,related,1,positive
"To learn that distribution, we introduce at a high level how to apply the score-based diffusion model (SDM) to the task of conformer generation.",1,related,1,positive
"Restrictions apply.
gradient of interatomic distances by the score-based diffusion model (SDM) [12].",1,related,1,positive
"Then, learning the reverse process is equivalent to learning the score of the noised distributions [Vin11; SE19; Son+21b].",1,related,1,positive
"In order to approximate the score function ∇xt log qt(xt), DMs minimize the following score matching objective function [16, 42, 37]: LDM θ = Et,x0,xt [ γt ∥sθ(xt; t)−∇xt log q(xt|x0)∥(2) ] , (5) where x0 ∼ q(x0), xt ∼ q(xt|x0), t is sampled from a distribution over [0, T ], and γt is a positive weighting term.",1,related,1,positive
We consider Gaussian diffusion models initially proposed by [19] and further improved by [20] and [8].,1,related,1,positive
"Song and Ermon [22] show that κ √ η1 should be sufficiently small (e.g., 0.04 in LDM [11]) to ensure that q(x1|x0,y0) ≈ q(x0).",1,related,1,positive
"Similar to the motivation of Song and Ermon (2019), we sample along the reverse diffusion, taking a number of conditional Langevin steps at each time.",1,related,1,positive
"The network is trained as a diffusion model [4, 5] to incrementally reverse a manually constructed noising process that gradually perturbs the object point clouds until they match a distribution PO ) ∼ p ) O (· | PS), which we can efficiently sample from during deployment to begin de-noising at test time.",1,related,1,positive
"We overcome this difficulty by training the predictor as a diffusion model [4, 5] to perform iterative de-noising.",1,related,1,positive
"(1)
Besides, built upon the learning strategy of the noise conditional score network (NCSN) [45], we formulate our loss function as follows by choosing λ(t) = σ(t)2:
L = EU(t;0,1) [ λ(t)||sθ(x(t), t) +
x(t)− µ σ2
||22 ] (2)
= EU(t;0,1) [ ||σ(t)sθ(x(t), t) + z||22 ] , (3)
in which z stands for random noise vector z ∼ N(0, 1) and sθ is the pre-trained score matching network. σ represents the variance mentioned in Eq.",1,related,1,positive
"Besides, built upon the learning strategy of the noise conditional score network (NCSN) [45], we formulate our loss function as follows by choosing λ(t) = σ(t)(2):",1,related,1,positive
"To train the score model, we follow the most common operations in denoising score matching framework [42].",1,related,1,positive
"…at, and, motivated by Proposition 2.1, parameterising the backward drift as bt = at − st, we recover the SGM objectives in Hyvärinen and Dayan (2005); Song and Ermon (2019); Song et al. (2021) from D = DKL; when −→ P µ,a = ←− P ν,b, the variable drift component st will represent the score σ2∇…",1,related,1,positive
"Then we compared the quality of our generated images with DDPM [14], PGGAN [20], NCSN [26], GSN [27], IDEAS [28] and Denoising GAN [15] shown in Table.",1,related,1,positive
"Following [39], we introduce noise-conditioned score function s(x, σ) := ∇x log pσ(x;D) [39] and aim to optimize the following objective given some sequence of annealed smoothing parameters σk, min θ ∑ k σ 2 kEx∼pσk (x;D) [ ∥sθ(x, σk)−∇x log pσk(x;D)∥(2) ] , (9) which has been shown to be equivalent to the denoising-score-matching loss [56].",1,related,1,positive
"ii) In addition, we show that score matching with annealed perturbations [39] gives gradients that stably drive decision variables to land exactly on data when uncertainty is minimized with gradient-based optimization, a property we term data stability.",1,related,1,positive
"To end this, we adopted the score-based thick proposed by Song et al.[23, 24].",1,related,0,negative
"Specifically, we employ a score-based diffusion model [24; 23; 13] to estimate the conditional distribution pdata(p|O).",1,related,1,positive
"If L corresponds to a Markov process corresponding to a continuous version of simulated tempering, we show the corresponding generalized score matching loss is a Gaussian-convolution annealed score matching loss, akin to the one proposed in Song and Ermon (2019).",1,related,1,positive
"To implement the score-based filter, we use the sliced score-matching method (see [37, 38]) to solve the diffusion model problem and train the score model with a 50 neuron - 2 layer neural network.",1,related,1,positive
"For mathematical details, please see [41, 23].",1,related,1,positive
"We follow the settings in [17, 18, 25], utilize the weighted loss, and set W = G(2)(t) for better performance.",1,related,1,positive
"4 Backdoor Attacks on Score-Based Models We trained the score-based model: NCSN [53, 51, 52] on the CIFAR10 dataset with the same model architecture as the DDPM [14] by ourselves for 800 epochs and set the learning rate as 1e-4 and batch size as 128.",1,related,1,positive
"Compared to [8], which only studies one DM (DDPM) on unconditional generation with image triggers, our method can generalize to various DMs, including DDPM [14] and the score-based models [51, 52, 53].",1,related,1,positive
"We trained the score-based model: NCSN [53, 51, 52] on the CIFAR10 dataset with the same model architecture as the DDPM [14] by ourselves for 800 epochs and set the learning rate as 1e-4 and batch size as 128.",1,related,0,negative
"We also consider three kinds of DMs, DDPM [14], LDM [42], and NCSN [51, 52, 53], to examine the effectiveness of our unified framework.",1,related,1,positive
"(a) (Stop Sign, Hat) FID (b) (Stop Sign, Hat) MSE (c) (Stop Sign, Hat) MSE Threshold Figure 10: FID and MSE scores of various samplers and poison rates for the score-based model (NCSN) [51, 52, 53] and the CIFAR10 dataset.",1,related,0,negative
"To show the generalizability of our framework, we discuss two major branches of DMs: DDPM [14] and Score-Based Models [51, 52, 53].",1,related,1,positive
"(1)
SMLD.",1,related,0,negative
"In this work, we will systematically study these four types of diffusion models: DDPM, SMLD, VPSDE, and VESDE.",1,related,1,positive
"We do not show the attack performance on the ODE sampler and DPM sampler for SMLD and VESDE models, because both samplers do not support these models.",1,related,1,positive
"SMLD minimizes the following loss:
Lθ = Et∼[1,T ],x∼pdata,xt∼q(xt |x)[λ(σt)||sθ(xt ,σt)−∇xt log q(xt |x)|| 2],
(2) where λ(σt) is a coefficient function and ∇xt log q(xt |x) = − xt−xσ2t .",1,related,1,positive
"We use four types of diffusion models: DDPM, SMLD, VPSDE, and VESDE, as the target models, which have been introduced in Section 2.",1,related,1,positive
"We explore four different types of diffusion models, including the discrete variance preserving (VP) model — DDPM [14], the discrete variance exploding preserving (VE) model — SMLD [40], the continuous VP model — VPSDE [41] and the continuous VE model — VESDE [41].",1,related,1,positive
"We only study PC samplers for VESDE and SMLD, because they do not support deterministic samplings.",1,related,1,positive
"As in [20], efficiently sampling from the noise-altered distribution q(xt) is achieved through a closed-form expression to generate arbitrary time-step xt:
xt = √ ᾱtx0 + √ 1− ᾱtϵ, where ϵ ∼ N (0, I), αt = 1− βt, ᾱt = t∏ s=1 αs (1)
The denoiser, a time-conditioned denoising neural network sθ(x, t) with trainable parameters θ, is trained to reverse the diffusion process by minimizing re-weighted evidence lower bound (ELBO) [52, 20], adapting to the noise as follows:
Et,x0,ϵ [ ||∇xt log p(xt|x0)− sθ(xt, t)||22 ] (2)
In essence, the denoiser learns to recover the gradient that optimizes the data log-likelihood.",1,related,1,positive
x0 ∼ pθ∗(x0) ≈ p(x0) whose distribution pθ∗(x0) conforms to the same distribution as the original one p(x0) [16].,1,related,1,positive
"As following [48], since Lσ relies on the scale of σ, we use unified objective with all σ ∈ {σi}i=1 as follows:",1,related,1,positive
"We extract DIFT from two commonly used, open-sourced image diffusion models: Stable Diffusion 2-1 (SD) [71] and Ablated Diffusion Model (ADM) [18].",1,related,1,positive
", [50]) could be used; our pipeline is agnostic to this choice.",1,related,0,negative
"…we validate training GANs with score-matching and flow-minimizing costs, using results from normalizing flows (Papamakarios et al., 2021) and NCSNs (Song & Ermon, 2019) on unimodal and multimodal Gaussians, and latent-space matching on image, akin to Wasserstein autoencoders (Tolstikhin et al.,…",1,related,1,positive
Experiments on NCSN were built atop a publicly available implementation (URL: https://github.com/Xemnas0/ NCSN-TF2.,1,related,0,negative
"As a proof of concept, we validate training GANs with score-matching and flow-minimizing costs, using results from normalizing flows (Papamakarios et al., 2021) and NCSNs (Song & Ermon, 2019) on unimodal and multimodal Gaussians, and latent-space matching on image, akin to Wasserstein autoencoders (Tolstikhin et al., 2018) and latent diffusion (Rombach et al., 2022).",1,related,1,positive
"Considering gradient-regularized Wasserstein GAN losses, we show that the optimal generator is the one that minimizes a smoothed score-matching difference term, where the scores are conditioned by means of the kernel associated with the RKHS from which the IPM discriminator is drawn, akin to noise conditioned score networks (NCSN) (Song & Ermon, 2019).",1,related,1,positive
"Based on the links between score-based approaches and the GANs, we
consider the approach presented in noise-conditioned score networks (NCSNv1) (Song & Ermon, 2019), with γt = √ 2αt.",1,related,1,positive
"…we show that the optimal generator is the one that minimizes a smoothed score-matching difference term, where the scores are conditioned by means of the kernel associated with the RKHS from which the IPM discriminator is drawn, akin to noise conditioned score networks (NCSN) (Song & Ermon, 2019).",1,related,1,positive
"Here, we present experiments on the two-component Gaussian mixture originally considered by Song & Ermon (2019): pd(x) = 15N (−51, I)+ 45N (51, I).",1,related,1,positive
"We compare discriminator-guided Langevin sampler, with αt = α0 = 10, with and without noise perturbations zt, against the base NCSN model, owing to the links to the score-based results derived in ScoreGANs and FloWGANs.",1,related,1,positive
"As discussed in the main paper, we directly utilize the U-shape denoiser from DDPM [42].",1,related,1,positive
We follow the UNet [64] denoiser architecture as DDPM [42].,1,related,1,positive
"We note that the theory developed in our paper (Theorems 1 to 4) is agnostic to the loss function [2] or the optimization strategy used [4, 5, 14, 23].",1,related,1,positive
"We consider the baseline models, including NCP-VAE [2] and VAEBM [43], which also recruit NVAE as their backbone model, and other powerful deep generative models, such as GANs [3, 20], score-based models [18, 36] and EBMs [7,8,14,45] on data space.",1,related,1,positive
The sampling during the reverse timestep can be represented by [36]:,1,related,1,positive
"Similarly, we show the attained bias, variance, and MSE for d = [5, 10] and N = 64 in Tab.",1,related,1,positive
"Then, the question we address in this paper is: can we borrow the strengths of score-based DMs to improve likelihood-based GMs, without paying the price of costly sample generation? One distinctive element of score-based DMs is data mollification, which is typically achieved by adding Gaussian noise [66] or, in the context of image data sets, by blurring [59].",1,related,1,positive
"Inspired by the recent success of the diffusion model [30, 89, 90, 91], we introduce DiffMatch, a conditional diffusion-based framework designed to explicitly model the matching field distribution.",1,related,1,positive
"[5], as we build upon their proof to show that the score can be estimated by using a conditional denoising score matching objective [44, 38].",1,related,1,positive
"More specifically, we focus on score-based diffusion models [25, 7, 26] and adopt them for our purpose.",1,related,1,positive
"In NCSN [11], they leverage the Langevin MCMC to run M steps to produce a sample for each pσi(x) sequentially:
x̃mi = x̃ m−1 i + ϵisθ∗(x̃ m−1 i , σi) + √ 2ϵiz m i , m = 1, 2, ...,M, (2)
where ϵi > 0 is the step size, and zmi is standard normal.",1,related,1,positive
"In NCSN [11], they leverage the Langevin MCMC to run M steps to produce a sample for each pσi(x) sequentially: x̃i = x̃ m−1 i + εisθ∗(x̃ m−1 i , σi) + √ 2εiz m i , m = 1, 2, .",1,related,1,positive
"(7) requires knowing the score function ∇x log pt (x), which can be estimated by training a neural network with score matching (Song & Ermon, 2019; Vincent, 2011).",1,related,1,positive
"To estimate the score function ∇x log pt(x), one effective solution is to train a score model sθ(x, t) on samples with score matching (Hyvärinen & Dayan, 2005; Song & Ermon, 2019; Vincent, 2011).",1,related,1,positive
"To generate samples that follow the distribution implicitly defined by the score, we use Langevin dynamics, which is similar in the score-based generative models [14].",1,related,1,positive
"Here we use denoising score matching with noise conditional score networks [Song and Ermon, 2019, 2020]:
min θ
1
L L∑ i=1 λ (σi)
[ 1
2 Ep(x)Eq(z|x)Ez̃∼N (z,σ2i ,I) ∥∥∥∥sθ(z̃, σi) + z̃− zσ2i ∥∥∥∥2 2 ] .",1,related,1,positive
"The original PolyMNIST introduced by Sutter et al. [2021] has five modalities and in order to study the behavior of the methods on a larger number of modalities, we extended the number of modalities to ten.",1,related,1,positive
"To achieve this, we recall the following equation [34]: ∇xt log q(xt) = − 1 √ 1− ᾱt εθ(xt, t) (2)",1,related,1,positive
"To solve this issue we propose a unifying probabilistic framework, where we model matching as a non-stationary diffusion process [64, 45, 23, 46] using a Markov chain formulation.",1,related,1,positive
"We use the U-Net++ as model for the NCSN, and train this model from scratch for each dataset with a Denoising Score Matching loss.",1,related,1,positive
"We use the standard U-Net architecture [31, 27] with a single fixed noise level σ = 0.",1,related,1,positive
"Although Song et. al. state that CD is capable of implementing the new state-of-the-art FID 3.55 under the condition of a single NFE, there still are some constraints on this:
Constraint I. CD and CT may not be effective for certain popular DPMs such as Rectified flow [25], NCSN++, and DDPM [41].",1,related,1,positive
"The relevant experimental results are presented in Tables 2 and 3, and the remaining supplementary results are given in Appendix I. Considered as a one-session training approach, we first compare CUD with other one-session training methods, such as the train-free accelerated sampling techniques, namely “DPM-solver” and DDIM, as well as a variety of DPMs, specifically NCSN++, DDPM, Rectified Flow, and Curvature, which serves as the baseline.",1,related,1,positive
"First, we improve the stability of the training and image generation process by training a probabilistic model instead of GANs to fit the distribution of private data.",1,related,1,positive
"Secondly, one can learn an EBM by matching the first derivatives of the density function and the data distribution (Song & Ermon, 2019; Song et al., 2020).",1,related,1,positive
"where pt(x) is the marginal probability density at timestep t, and the only unknown part ∇x log pt(x) can be modelled as so-called score function sθ(x, t) with score matching methods [26, 52].",1,related,1,positive
"For better positioning our method in the literature of generative models, we also list results of some representative works, including VAE (Kingma & Welling, 2013), PixelCNN (Van den Oord et al., 2016), EBM (Du & Mordatch, 2019) and NCSN (Song
& Ermon, 2019).",1,related,1,positive
"Instead, Song & Ermon (2019); Ho et al. (2020) suggest a simple
2We follow the convention to use Rx to denote applying group actions R on x, which formally is calculated as xRT .
surrogate objective up to irrelevant constant terms:
LDM = Ex0, ∼N (0,I),t [ w(t)|| − θ(xt, t)||2 ] , (3)
where xt =…",1,related,1,positive
We use a modified version of annealed Langevin dynamics [1] to sample from the posterior distribution.,1,related,1,positive
"…is to process the observed data with the forward transition kernel q(xk|x0) = N (xk;x0, σ2kI), with σ2k being a set of increasing noise levels for k = 1, ...,K, and then jointly estimate the Stein scores for the noise density distributions qσ1(x), qσ2(x), ..., qσk(x) (Song and Ermon, 2019).",1,related,1,positive
Our objective is to bound the Lipschitz constant of the Noise Conditional Score U-net [52] to jointly estimate the scores of data distributions while also ensuring the convergence condition for the Diffusion through BSDEs.,1,related,1,positive
"To achieve this, we apply spectral normalization [39] and noise conditioning [52] techniques on a conventional U-net.",1,related,1,positive
"and the denoising score-matching objective [41, 42], which we omit here, are used.",1,related,1,positive
"…Zhao <batmanfly@gmail.com>.
et al., 2014), VAE (Kingma & Welling, 2014), and flowbased models (Dinh et al., 2017), diffusion models offer several desirable properties such as distribution coverage, a stationary training objective, and easy scalability (Song & Ermon, 2019; Dhariwal & Nichol, 2021).",1,related,1,positive
"(5)
This objective is equal to optimizing a reweighted VLB on the data log-likelihood and has a connection to generative score matching (Song & Ermon, 2019; Song et al., 2020).",1,related,1,positive
"…in Sections 1–3, we discuss here how our approach is compared with scorebased diffusion modeling, mainly the representative works (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021), and with the diffusion recovery likelihood for EBM learning (Gao et al., 2021).",1,related,1,positive
"Following the recent process in generative models [33, 34, 24, 30, 27], we formulate our local 3D prior as a denoising diffusion probabilistic model (DDPM) [9], which iteratively denoises a voxelized 32×32×32 cube of occupancy x.",1,related,1,positive
"Our method relies on a conditional Denoising Diffusion Implicit Model (DDIM) [59], which is a variant of diffusion models [58, 19, 60].",1,related,1,positive
"The score function is optimized by the Fisher divergence between sθ and the score of real samples as previous work suggested [47]: LFisher = Et [∥sθ (vu (t)) − ∇vu logpt (vu (t) |vu (0))∥(2)2], (19) where t ∼ U (0,T ) is uniformly sampled.",1,related,1,positive
"In particular, our learned scores can be used to augment
.
the sampling procedure using Langevin Dynamics (Song & Ermon, 2019a).",1,related,1,positive
"Inspired by the empirical success of denoising score matching (Vincent, 2011; Song & Ermon, 2019a), we present constrained denoising score matching (CDSM).",1,related,1,positive
"A score model can then be trained with denoising score matching (Song & Ermon, 2019).",1,related,0,negative
"We defer training details to Song & Ermon (2019; 2020), though it suffices to say that at generation time an annealed version of Langevin MCMC is used where the noise magnitude σ progressively becomes smaller as the number of timesteps increases.",1,related,0,negative
We note that a modern SBGM can be constructed by simply replacing the contrastive-based formulation of pθ(x) in the decoupled COMs variant (Section 2.3) with one that has been trained with score matching as per Song & Ermon (2019).,1,related,1,positive
"…when p(x) is replaced with q(x), the difference is negligible for small values of σ.
Recently, score-based generative models (SBGMs) were proposed (Song & Ermon, 2019; 2020), which can be thought of as an improved version of the denoising score matching EBM but with the added intention of…",1,related,1,positive
"Following Song and Ermon [2019], we can modify our loss to train a Noise Conditioned Score Network with L noise levels i.",1,related,0,negative
"The key hyper-parameters for each baseline method are listed below:
BLENDED, we use DDPM (Song & Ermon, 2019b) sampler with 250 sampling steps.",1,related,1,positive
"Note that −v(x, t) recovers the score function in (Song & Ermon, 2019; Song et al., 2020), i.e.,∇x log p(x, t).",1,related,1,positive
"The neural network is trained using the denoising score matching objective [44, 39], where we add Gaussian noise to",1,related,1,positive
"Given that ∇xt log p(xt) = Ep(x0|xt)[∇xt log p(xt|x0)] we can learn an approximation to the score with a neural network parameterised by θ, sθ(xt, t) ≈ ∇ log p(xt) (Song and Ermon, 2019), by minimising a reweighted variant of the ELBO (Eq.",1,related,1,positive
"For standard ScoreBased Models with At = I, the seminal work of [48] guarantees that the true score is learned by denoising scorematching.",1,related,1,positive
We find that this method can achieve comparable performance as DDPM for VDM in our preliminary experiments.,1,related,0,negative
"Conditioned on z0 and e, denoising model ϵθ(st, t, z0, e) is trained to predict the added noise ϵ in st based on a conditional 3D U-Net with the following loss:
LDM = Et∼U(1,T ),s0∼q(s0),ϵ∼N (0,I)[||ϵ− ϵθ(st, t, z0, e)|| 2] ,
(7) where time step t is uniformly sampled from {1, . . . , T}. ϵθ is further used in DDPM reverse sampling process to output ŝ0 = cat[f̂ K 1 , m̂ K 1 ] with the size of K×Hz×Wz×3, where f̂K1 = {f̂1, . . . , f̂K} and m̂K1 = {m̂1, . . . , m̂K} are synthesized latent flow and occlusion map sequences.",1,related,1,positive
We first compare the results sampled by 10-step DDIM and 100-step DDIM against 1000-step DDPM (default setting).,1,related,1,positive
"2 shows, for a given image x0 and condition y, the image encoder Φ encodes x0 as latent map z0 and pretrained BERT represents y as embedding e. Conditioned on z0 and e, a randomly sampled Gaussian noise volume n with the size of Kz×Hz×Wz×3 is gradually denoised by ϵθ through the DDPM reverse sampling process to generate the latent flow sequence f̂K1 and occlusion map sequence m̂K1 .",1,related,1,positive
"Our proposed LFDM includes four trainable modules: an image encoder Φ, an image decoder Ω, a flow predictor F , and a denoising model ϵθ from DDPM.",1,related,1,positive
"Given a sample from the data distribution s0 ∼ q(s0), the forward process of DDPM produces a Markov chain s1, . . . , sT by progressively adding Gaussian noise to s0 according to a variance schedule β1, . . . , βT , that is:
q(st|st−1) = N (st; √ 1− βtst−1, βtI) , (1)
where variances βt are held constant.",1,related,1,positive
"For sampling, we use 1000-step DDPM for LDM and LFDM.",1,related,1,positive
"Our proposed LFDM is built on denoising diffusion probabilistic models (DDPM) [25, 67, 70].",1,related,1,positive
"Since DDPM sampling is very slow in the large latent space of VDM (40×64×64×3), we employ 200-step DDIM [68] to accelerate the sampling process.",1,related,1,positive
"Unless otherwise specified, we apply T = 1000 step DDPM to sample 40- frame 32× 32× 2 f̂ and 32× 32× 1 m̂ and finally produce 40-frame videos x̂ with 128× 128 frame resolution.",1,related,1,positive
"…. . . , 1 do 3: z ∼ N (0, I)
4: xt−1 = 1√ αt
( xt − βt√
1−ᾱt θ(xt,t)
) + σtz, see Equation (9)
5: end for 6: return x0 as DSyn
To create novel data, we followed Song & Ermon (2019)’s score-based generative method using Langevin dynamics with modifications on Equation (6), sampling z ∼ N (0, I)…",1,related,1,positive
"As an example, we point out that the FID score in this paper (14.15) is significantly lower than the one reported by Song and Ermon (2019) (25.32) which was obtained using annealed Langevin MCMC with multiple noise scales.",1,related,0,negative
We have presented the data sample points of the 25- Gaussians Example in Figure 8 in the main paper.,1,related,1,positive
"For the 25-Gaussians example, we use multi-layer perceptron (MLP) networks for the generator and the elastic discriminator.",1,related,1,positive
"To achieve this, we use a deep generative model [21, 22, 23] to learn the expression distributions of cell types k1 and k2 from the scRNA-seq reference data, denoted as p (x1 | k1) and p (x2 | k2).",1,related,1,positive
"Based on Langevin dynamics [24, 22], we can obtain the decomposition by sampling X = [x1;x2] from the posterior distribution p (X | y, k1, k2),",1,related,1,positive
"where dw corresponds to the standard Wiener process running backward and the only unknown part ∇xt log pt(xt) can be modeled as the so-called score function sθ(xt, t) with denoising score matching methods, and this score function can be trained with the following objective [11, 37]:",1,related,1,positive
"The scores of the conditional distribution ∇m̃ log pt(m̃|x) can be estimated using either techniques from score matching [8, 20, 21] or from implicit score estimation such as DDPM [19, 6].",1,related,1,positive
"Model IS ↑ FID ↓ M-EBM(K=1)* 6.02 35.7 M-EBM(K=2) 6.72 27.1 M-EBM(K=5) 7.14 22.7 M-EBM(K=10) 7.08 20.4 M-EBM(K=20) 7.20 21.1
Explicit EBM(Unconditional)
ShortRun(K=100) [16] 6.72 32.1 IGEBM(K=60) [4] 6.78 38.2 f-EBM(K=60) [22] 8.61 30.8 CF-EBM(K=50) [24] - 16.7 KL-EBM(K=40) [3] 7.85 25.1 DiffuRecov(K=30) [5] 8.31 9.58
Regularized Generator
GEBM [1] - 23.02 VAEBM(K=6) [20] 8.43 12.19
Other
SNGAN [14] 8.59 21.7 NCSN [18] 8.91 25.3 StyleGAN2-ADA [12] 9.74 2.92 DDPM [11] 9.46 3.17 * M-EBM diverges with K = 1, and we report the best FID before diverging.",1,related,1,positive
"Hence, we suppose the gradients ∇xE(x) are defined (almost) everywhere in such manifolds and thus can reduce the perturbation with noise which is originally explained in NCSN [18].",1,related,1,positive
"Our work also related to denoising auto-encode [39] and denoise score matching [38], which was further developed by [23] and [36] for learning from data samples corrupted with multiple levels of noise.",1,related,1,positive
"where ηt denotes the time-dependent step size [8, 9].",1,related,1,positive
"In this setting, we show that score-based generative models (Song & Ermon, 2019; Ho et al., 2020) actually learn a graduated non-convexity (GNC) scheme (Blake & Zisserman, 1987).",1,related,1,positive
"We also denote in the name whether the models have been trained with the Variance Preserving (VP) Song et al. (2021b); Ho et al. (2020) or the Variance Exploding Song et al. (2021b); Song and Ermon (2020, 2019), e.g. we write EDM-VP.",1,related,1,positive
"By choosing some sequence 0 < σT ≤ · · · ≤ σ1 as well as At = I , μt = N(0, σ 2 tC) in (23), and ht = σ 2 t /σ 2 T for some > 0 and self-adjoint, positive, trace-class C, we recover an infinite-dimensional generalization to the NCSN algorithm of Song and Ermon [2019]. In particular, our sample vT is approximately distributed according to the measure νT = μ ∗N(0, σ(2) TC) where σ(2) T 1 is small.",1,related,1,positive
"We train with a combined loss defined by (16) where we re-scale the the noise by σ−1 t and the score by σt, following Song and Ermon [2019]. In particular, our model learns to approximate v 7→ σ−2 t ( RDHμtΦ(u, t) − v ) .",1,related,1,positive
"We remark that this assumption can make precise the “manifold hypothesis” in Song and Ermon [2019] that is used to justify the perturbation since Hμ0 is a proper subspace of H and, in fact, μ0(Hμ0) = 0; see Section 6 of Stuart [2010] for more details.",1,related,1,positive
"We train with a combined loss defined by (16) where we re-scale the the noise by σ−1 t and the score by σt, following Song and Ermon [2019]. In particular, our model learns to approximate v 7→ σ−2 t ( RDHμtΦ(u, t) − v ) . Note that the σ−2 t term is canceled by the adaptive time-step in Algorithm 1, however, as in Song and Ermon [2019], we find that this re-scaling significantly improves performance for all models.",1,related,1,positive
"We also provide results using variance preserving (VP) and variance exploding (VE) diffusion models, originally inspired by DDPM (Ho et al., 2020) and SMLD (Song & Ermon, 2019).",1,related,1,positive
Song & Ermon (2019) showed that we can learn gradient of log likelihood (called score functions) and use it to generate images.,1,related,1,positive
"We introduce Noise2Music, a diffusion-based (SohlDickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020) method of generating music from text prompts and demonstrate its capability by generating 30-second long 24kHz music clips.",1,related,1,positive
"Denote θ ∈ Θ the parametrization of s, then, following Song and Ermon [50], Song et al. [53], we have s(x̃, t) = sθ∗(x̃, t), where
θ∗ = arg min θ∈Θ
Et∼U(0,1) [ ξ(t)Ex∼p(x), x(t)|x [ ∥sθ(x(t), t)−∇x log pt(x(t)|x)∥2 ]] (53)
= arg min θ∈Θ Et∼U(0,1)
[ ξ(t)Ex∼p(x), x(t)|x [∥∥∥∥sθ(x(t), t) + x(t)− xσ(t) ∥∥∥∥2 ]] , (54)
where ξ(t) ∝ σ2(t) and U(0, 1) is the uniform distribution over [0, 1].",1,related,1,positive
"We implement both denoising functions Φ and F via U-Net (Ronneberger et al., 2015) with modifications suggested in (Song et al., 2021b; Saharia et al., 2021).",1,related,1,positive
"In Stage III, inspired by (Song & Ermon, 2019; Chen et al., 2021), we train F to condition on ᾱt, t ∼ U(1, T ).",1,related,1,positive
"[27] showed that we can generate images by estimating the score, i.",1,related,1,positive
"We employ a diffusion-based [61, 17] generative model trained via denoising score-matching [63] to learn the prior.",1,related,1,positive
"While the prior score ∇xt log p(xt) can be readily computed using a pre-trained score network such as NCSN (Song & Ermon, 2019) or NCSNv2 (Song et al., 2020), the likelihood score∇xt log p(y | xt) is generally intractable.",1,related,1,positive
"Specifically, for MNIST, the NCSNv2 (Song & Ermon, 2020) was trained on the MNIST training dataset with a similar training set up as CIFAR-10 in Song & Ermon (2020), while for CIFAR-10, and CelebA, we use the pre-trained models available in this Link.",1,related,0,negative
"By combining the pseudo-likelihood score (11) approximated via EP and the prior score from SGM, we readily obtain an enhanced version of QCS-SGM, dubbed as QCS-SGM+, using the annealed Langevin dynamics (ALD) (Song & Ermon, 2019).",1,related,1,positive
"For more details of QCS-SGM and SGM, please refer to Meng & Kabashima (2023) and Song & Ermon (2019); Song et al. (2020), respectively.",1,related,1,positive
This finding potentially connects our pretraining method with genitive models Song et al. (2021a); Ho et al. (2020); Song & Ermon (2019); Song et al. (2021b).,1,related,1,positive
"We also include the results of several popular generative models (Karras et al., 2020; Ho et al., 2020; Song & Ermon, 2019; Xu et al., 2022b) for reference.",1,related,1,positive
"Recent papers [1,34] have suggested a method to correct original samples xt in order to create better representatives of the marginal densities pt(x) (required by our estimators),",1,related,1,positive
Exploding [62] q(x1) x1 σ1−t × × × Var.,1,related,1,positive
"With different options of S, one can flexibly devise the approximator following score-based conditioning trick (Song et al., 2020b; Song & Ermon, 2019) as follows:
ψ(xt, z S , t) = θ(xt, t)− ∑ c∈S √ 1− ᾱtGcψ(xt, zc, t).",1,related,1,positive
"One way to express µθ(xt, t) is to employ a noiseconditioned score network sθ(xt, t) := ∇xt log pθ(xt) that approximates the score function ∇xt log qαt(xt): µθ(xt, t) =
1√ 1−βt (xt + βtsθ(xt, t)) (Song & Ermon, 2019; Song et al., 2020b).",1,related,1,positive
"In practice, we train our energy model via SE(3) denoising score matching (DSM) [8, 9] as maximum likelihood training is intractable.",1,related,1,positive
"Considering their excellent performance in image generation, we choose DDPM [15], SMLD [38], VPSDE [39] and VESDE [39] as our target models.",1,related,1,positive
"We evaluate our methods on four state-of-the-art diffusion models: DDPM [15], SMLD [38], VPSDE [39] and VESDE [39].",1,related,1,positive
"We also the effects of different diffusion models used in DTB, including improved DDPMs (WeatherDiffusion [23] and SR3 [26]), and the standard DDPM [13].",1,related,1,positive
"In this paper, we propose a novel unsupervised learning paradigm based on denoising diffusion probabilistic models (DDPM), called RainDiffusion, to tackle the unfavorable prevailing problem of real-world image deraining.",1,related,1,positive
"Since
Algorithm 1 RainDiffusion training Input: Unpaired clean image x and rainy image y 1: repeat 2: Randomly sample a binary patch mask Pi 3: xi = Crop(Pi ◦ x), yi = Crop(Pi ◦ y) 4: x′i = Crop(Pi ◦GAφ (x)), x′′i = Crop(Pi ◦GBφ (x′)) 5: y′i = Crop(Pi ◦GBφ (y)), y′′i = Crop(Pi ◦GAφ (y′)) 6: tA, tB ∼ Uniform ({1, ..., T}) 7: A, B ∼ N (0, I) 8: Take gradient descent step on 9: ∇θ,φ[|| A − Aθ ( √ α̂tAxi + tA √ 1− α̂tA , x′i, tA)||2
10: +|| B − Bθ ( √ α̂tBx ′ i + tB √ 1− α̂tB , x′′i , tB)||2
11: +|| B − Bθ ( √ α̂tByi + tB √ 1− α̂tB , y′i, tB)||2
12: +|| A − Aθ ( √ α̂tAy ′ i + tA √ 1− α̂tA , y′′i , tA)||2 13: +λcycLcyc] 14: until converged 15: return θA, θB(optional)
real-world rainy benchmarks usually consist of images with various sizes, we adopt the path-based DDPM [23] as our backbone for size-agnostic image deraining.",1,related,1,positive
"We propose a new data set, MTTN(read mutton), for generating prompts that can be used in diffusion models[1][2][3].",1,related,1,positive
"Since U-net has achieved various good results for previous generative tasks (Song and Ermon 2019; Song et al. 2021), we modify its 2-dimensional convolution layers to 1-dimensional ones for handling time-series observations.",1,related,1,positive
"For a more detailed discussion behind these equations, and how they are derived, please see [24, 61, 64].",1,related,1,positive
"We compare our method to “BASIS NCSN”, using the pre-trained NCSN model (Song and Ermon 2019) on CelebA.",1,related,1,positive
"While the annealed Langevin dynamics of [Song and Ermon, 2019] was originally framed via discrete iteration, we can recast it in continuous time with the SDE",1,related,1,positive
"Instead of reversing the forwards time diffusion in a non-equilibrium manner, we can also use the learned time-dependent score function ∇x log pt(x) (expressed in terms of the optimal denoiser x̂θ(x, t)) to do slow, approximately equilibrated sampling with annealed Langevin dynamics [Song and Ermon, 2019].",1,related,1,positive
We added conditional instance normalization as in [14] for decoder network modules to take condition on t.,1,related,1,positive
"The sampling during the reverse timestep can be represented by [36]: zt−1 ← 1 √ 1− βt (zt − βtsθ(zt, x, t)) + σ(2) t η, (2)",1,related,1,positive
"Similar to classifier guidance techniques [26, 30, 32, 56], we rewrite pθ(xt|y, z) = pθ(y|xt, z)pθ(xt|z)/pθ(y|z), and we adopt classifier-free guidance to model pθ(xt|z), then we can obtain a modified score function [55] as below:",1,related,1,positive
"[19], we parameterize μθ(zt) with a neural network θ(zt, t) (called the score model [49, 50]) and fix Σθ to be a constant.",1,related,1,positive
"As a result, we obtain the DMPS in the form of SMLD (in particular, the NCSNv2 [51]) as shown in Algorithm 2.",1,related,1,positive
"Algorithm 2: DMPS in the form of SMLD [50, 51, 53] Input: y,A, σ2, {βt}Tt=1, , K,λ Initialization: x0T ∼ N (0, I)
1 for t = T to 1 do 2 αt ← β2t /β21 3 for k = 1 to K do 4 Draw zkt ∼ N (0, I) 5 xkt−1 = x k−1 t + αtsθ(x k−1 t , βt) + √ 2αtz k t 6 Compute∇xt log p̃(y | xt) as (24) 7 xkt−1 = x k t−1 + λαt∇xt log p̃(y | xt)
Output: x0",1,related,1,positive
"Various kinds of DM [19, 28, 40, 50, 51] have been proposed recently and in the following we specialize our focus on the most prominent DDPM [28], in particular the ablated diffusion model (ADM) in [19], though other variants of DM can also be easily adapted within our method with proper modifications, e.g., we also provide the corresponding algorithm in the case of SMLD in Appendix C.
Specifically, given data samples x0 ∼ p(x0), the forward diffusion process x0 → x1 → · · · → xT is defined as [28]
p(xt|xt−1) = N (xt; √ 1− βtx0, βtI), t = 1 · · ·T, (3)
where 0 < β1 < β1 < · · · < βT < 1 is prescribed perturbed noise variances so that approximately xT ∼ N (0, I).",1,related,1,positive
"According to [35, 37], λ(t) ∝ 1/E[ ∥∥∇y(t) log pt(y(t)|y(0))∥∥22], and λ(t) is chosen as λ(t) = g(t)(2).",1,related,1,positive
"Intuitively, our family of SDEs combines the diffusion g of the Variance Exploding (VE) SDE [11], [24] with an added drift term f that pulls xt towards the corrupted image y.",1,related,1,positive
"We follow previous works [22, 25, 26] in averaging the DSM loss over various scales σ(t), here indexed by a continuous time variable t ∈ [0, 1], and conditioning the score model on this time index, sθ(x, t) = θ(x, t)/σ(t), where θ is the neural network.",1,related,1,positive
"As shown in Song & Ermon (2019), when K →∞ and αt → 0 for all t, the final sample xKT will become an exact sample from pβmin(x̃) ≈ pdata(x) under some regularity conditions.",1,related,1,positive
"As both DDPM and SMLD estimate, implicitly or explicitly, the score (i.e., the gradient of the log probability density w.r.t. to data), they are also referred to together as score-based generative models (SGM) (Song et al., 2020).",1,related,1,positive
"In practice, we leverage a score network sθ : R → R that is trained to minimize the Fisher divergence between qθ(x) and pdata(x) [23, 26].",1,related,1,positive
"Similar to [26], we perturb the image with different levels of discrete (Categorical) noise, and train the models at different noise levels with annealing.",1,related,1,positive
"Our work builds on the body of literature on score matching [21, 22] and scorebased generative modeling [23, 26, 16].",1,related,1,positive
", large graph diameters), we can further employ Langevin MCMC [46] like score-based models [25], [29] to improve the sample quality at each discretization step.",1,related,1,positive
"DIFFUSION-BASED SOURCE SEPARATION
We propose a framework for applying SGM to the source separation problem.",1,related,1,positive
"We omit the details of diffusion model for brevity (see Song and Ermon [15], Ho et al.",1,related,1,positive
"We omit the details of diffusion model for brevity (see Song and Ermon [15], Ho et al. [16] for more information).",1,related,1,positive
"We propose a deep generative audio model based on score-matching with variance exploding diffusion [14, 15].",1,related,1,positive
"Mean and Covariance:
dµSMLD(t)
dt = 0 =⇒ µSMLD(t) = µ(0)
dΣSMLD(t)
dt = Ex
[√ d[σ2(t)]
dt
√ d[σ2(t)]
dt
] = d[σ2(t)]
dt
E Non-isotropic SMLD (NI-SMLD)
E.1 Score for NI-SMLD
qSMLDσi (xi | x) = N (xi | x, σ 2 iΣ) =⇒ xi = x + σi
√ Σ =⇒ = √ Σ−1 xi − x σi
(93)
=⇒ ∇xi log qSMLDσi (xi | x) = −Σ −1 xi − x σ2i = − √ Σ−1 σi (94)
E.2 Objective function for NI-SMLD
The objective function for SMLD at noise level σ is:
`NI−SMLD(θ;σi) , 1
2 Eqσi (xi|x)p(x) [ ∥∥∥∥sθ(xi, σi) + Σ−1 xi − xσ2i ∥∥∥∥2 2 ] (95)
= 1
2 Eqσi (xi|x)p(x) [ ∥∥∥∥sθ(xi, σi) + 1σi√Σ−1 ∥∥∥∥2
2
] (96)
E.3 Expected value of score for NI-SMLD
E [∥∥∇xi log qNI−SMLDσi (xi | x)∥∥22] = E [∥∥∥∥−Σ−1 xi − xσ2i ∥∥∥∥2 2 ]
= E ∥∥∥∥∥Σ−1σi √ Σ
σ2i
∥∥∥∥∥ 2
2
 = 1 σ2i Σ−1E [ ‖ ‖22 ] = 1 σ2i Σ−1 dim( ) (97)
E.4 Overall objective function for NI-SMLD
=⇒ λ(σi) = σ2iΣ
LNI−SMLD(θ; {σi}Li=1) , 1
2L L∑ i=1 Eqσi (xi|x)p(x) [ ∥∥∥∥σi√Σsθ(xi, σi) +√Σ−1 (x̃− x)σi ∥∥∥∥2 2 ]
= 1
2L L∑ i=1 Eqσi (xi|x)p(x) [ ∥∥∥σi√Σsθ(xi, σi) + ∥∥∥2 2 ] (98)
E.5 Unconditional NI-SMLD score estimation
An unconditional score model is:
sθ(xi, σi) = − √ Σ−1 1
σi θ(xi) (99)
In this case, the overall objective function changes to:
LGFF−SMLD(θ; {σi}Li=1) , 1
2L L∑ i=1 Eqσi (xi|x)p(x) [ ‖ − θ(xi)‖22 ]
= 1
2L L∑ i=1 Eqσi (xi|x)p(x) [ ∥∥∥ − θ(x + σi√Σ )∥∥∥2 2 ] (100)
E.6 Sampling in NI-SMLD
i = 0 corresponds to data, and i = L corresponds to noise.",1,related,1,positive
"The DDPM paper retains conditioning of θ on ᾱt, but SMLD omits it.",1,related,0,negative
"We know that:
N (x | x(i), σ21Σ) = 1
(2π)D/2σD1 |Σ|1/2 exp
( − 1
2σ21 (x− x(i))TΣ−1(x− x(i))
)
Ep(i)(x)[r(j)(x)] = ∫ p(i)(x)p(j)(x)∑N
k=1 p (k)(x)
dx ≤ ∫ p(i)(x)p(j)(x)
p(i)(x) + p(j)(x) dx
= 1
2
∫ 2
1 p(i)(x) + 1 p(j)(x)
dx ≤ 1 2
∫ √ p(i)(x)p(j)(x)dx
= 1
2
1
(2π)D/2σD1 |Σ|1/2
∫ exp ( − 1
4σ21
( (x− x(i))TΣ−1(x− x(i)) + (x− x(j))TΣ−1(x− x(j)) ) dx
= 1
2
1
(2π)D/2σD1 |Σ|1/2
∫ exp ( − 1
4σ21
( (x− x(i))TΣ−1(x− x(i)) + (x− x(j))TΣ−1(x− x(i) + x(i) − x(j)) ) dx
= 1
2
1
(2π)D/2σD1 |Σ|1/2
∫ exp ( − 1
4σ21
( (x− x(i))TΣ−1(x− x(i)) + (x− x(j))TΣ−1(x− x(i))
+ (x− x(j))TΣ−1(x(i) − x(j)) ) dx
= 1
2
1
(2π)D/2σD1 |Σ|1/2
∫ exp ( − 1
4σ21
( (x− x(i))TΣ−1(x− x(i)) + (x− x(i) + x(i) − x(j))TΣ−1(x− x(i))
+ (x− x(i) + x(i) − x(j))TΣ−1(x(i) − x(j)) ) dx
= 1
2
1
(2π)D/2σD1 |Σ|1/2
∫ exp ( − 1
4σ21
( (x− x(i))TΣ−1(x− x(i)) + (x− x(i))TΣ−1(x− x(i))
+ (x(i) − x(j))TΣ−1(x− x(i)) + (x− x(i))TΣ−1(x(i) − x(j)) + (x(i) − x(j))TΣ−1(x(i) − x(j)) ) dx
= 1
2
1
(2π)D/2σD1 |Σ|1/2
∫ exp ( − 1
2σ21
( (x− x(i))TΣ−1(x− x(i))
+ (x− x(i))TΣ−1(x(i) − x(j))
+ 1
2 (x(i) − x(j))TΣ−1(x(i) − x(j))
) dx
= 1
2
1
(2π)D/2σD1 |Σ|1/2
∫ exp ( − 1
2σ21
( (x− x(i))TΣ−1(x− x(i))
+ 2(x− x(i))TΣ−1 (x (i) − x(j))
2
+ (x(i) − x(j))
2
T
Σ−1 (x(i) − x(j)) 2 − (x (i) − x(j)) 2
T
Σ−1 (x(i) − x(j))
2
+ 1
2 (x(i) − x(j))TΣ−1(x(i) − x(j))
) dx
= 1
2
1
(2π)D/2σD1 |Σ|1/2
∫ exp ( − 1
2σ21
( (x− x(i) + (x
(i) − x(j)) 2 )TΣ−1(x− x(i) + (x (i) − x(j)) 2 )
+ 1
4 (x(i) − x(j))TΣ−1(x(i) − x(j))
) dx
= 1
2 exp
( − 1
8σ21 (x(i) − x(j))TΣ−1(x(i) − x(j)) ) ∫
1
(2π)D/2σD1 |Σ|1/2 exp
( − 1
2σ21
( (x− (x (i) + x(j))
2 )TΣ−1(x− (x
(i) + x(j))
2 )
) dx
= 1
2 exp
( − 1
8σ21 (x(i) − x(j))TΣ−1(x(i) − x(j)) ) =⇒ 1
σ21 (x(i) − x(j))TΣ−1(x(i) − x(j)) ≈ 1
=⇒ ( √ Σ−1(x(i) − x(j)))T ( √
Σ−1(x(i) − x(j))) ≈ σ21 =⇒ ∥∥∥√Σ−1(x(i) − x(j))∥∥∥ 2 ≈ σ1
=⇒ ∥∥∥σN Real(W−1N KWN (x(i) − x(j)))∥∥∥
2 ≈ σ1 =⇒ ∥∥∥σNW−1N KWNx(i) − σNW−1N KWNx(j)∥∥∥
2 ≈ σ1
For CIFAR10, this σ1 ≈ 20 for NI-SMLD (whereas for SMLD σ1 ≈ 50).",1,related,1,positive
"See Appendix D and Appendix E for the equivalent derivations for Score Matching Langevin Dynamics (SMLD) [16, 17], and our Non-Isotropic SMLD (NI-SMLD).",1,related,1,positive
"In the appendix, we also include further derivations for non-isotropic SMLD models.",1,related,1,positive
"So an unconditional score model is:
sθ(xi, σi) = − 1
σi θ(xi) (87)
In this case, the overall objective function changes to:
LSMLD(θ; {σi}Li=1) , 1
2L L∑ i=1 Eqσi (xi|x)p(x) [ ‖ − θ(xi)‖22 ] (88)
= 1
2L L∑ i=1 Eqσi (xi|x)p(x) [ ‖ − θ(x + σi )‖22 ]
D.6 Sampling in SMLD
i = 0 corresponds to data, and i = L corresponds to noise.",1,related,1,positive
"Using ALS from [16, 17]: xL ∼ N (x | 0, σmaxI)",1,related,1,positive
"Since we do not hope to see Gaussian noise in our final-sampled images (i.e., beyond the level of intrinsic noise in the observations), it is beneficial to decay the noise level, as in score-based models (Song & Ermon, 2019).",1,related,1,positive
"To address all these challenges, we propose a new diffusion-based generative model [44, 17, 45] that is capable of jointly sampling antibody CDR sequences and structures at the atomic resolution.",1,related,1,positive
"For the baseline, we take Annealed Langevin Dynamics (ALD) as considered in (Song & Ermon, 2019).",1,related,1,positive
"For the sake of proper comparison, we consider a baseline model using Annealed Langevin Dynamics (ALD) (Song & Ermon, 2019) and Sliced Score Matching (SSM) (Song et al., 2020a), which estimates the scores of the marginals ∇ log qt without knowledge of the underlying dynamics.",1,related,1,positive
"For the baseline, we managed to generate the images only taking into account the noise variance of the current distribution qt as proposed in (Song & Ermon, 2019).",1,related,1,positive
"Note, that even using the ground truth scores in ALD does not match the performance of Action Matching (see Table 2) since it is itself an approximation to the Metropolis-Adjusted Langevin Algorithm.",1,related,1,positive
"Moreover, for the conditional generation, the ALD+SSM baseline fails to learn any meaningful scores.",1,related,0,negative
"DPMs approximate the score of the data distribution (Song & Ermon, 2019).",1,related,0,negative
"Throughout this paper, we leverage the fact that the trained model ϵθ approximates the score ∇xj log p(x) of the data (Song & Ermon, 2019).",1,related,1,positive
"Our presentation of the material in this section closely follows the discussion of score-based generative models in (Song & Ermon, 2019), adapted appropriately to conditional setting.",1,related,1,positive
"(1)) becomes the following:
dx(t) =
√ dσ2(t)
dt dwt. (22)
A typical instance of a VE SDE is Score Matching of Langevin Dynamics (SMLD) (Song & Ermon, 2019), where
σ(t) := σmin ( σmax σmin )t for t ∈ (0, 1].",1,related,1,positive
"We borrow the idea from the score-matching generative models [25], [26], [27] which propose to estimate the gradient of the ground truth data distribution and then move the initial image from its original distribution pD(x|y0) to the target distribution pD(x|y) iteratively through the Stochastic Gradient Langevin Dynamics (SGLD) [28], [56], [57]:",1,related,1,positive
"Given samples x from a data distribution pdata(x), noise scheduling functions ↵t and t, we train a diffusion model x̂✓, with parameter ✓, via minimizing the weighted mean squared error [4, 36, 39, 40]",1,related,1,positive
"In this work, we will apply our distillation framework to classifier-free guided diffusion models learned on both pixel-space [4, 36, 39] and latent-space [21, 24, 28, 35].",1,related,1,positive
"The model architecture we use is a U-Net model similar to the ones used in [6] for pixel-space diffusion models and [1, 28] for latent-space diffusion models.",1,related,1,positive
"Following [4, 6, 39], we use a U-Net [29, 39] architecture for the baselines, and the same U-Net backbone with the introduced w-embedding for our two-step student models (see Sec.",1,related,1,positive
"(43) Taking λ(t) = σ2t (x1) corresponds to the original Score Matching (SM) loss from Song & Ermon (2019), while considering λ(t) = β(1− t) (β is defined below) corresponds to the Score Flow (SF) loss motivated by an NLL upper bound (Song et al., 2021); st is the learnable score function.",1,related,1,positive
"We consider three options as diffusion baselines that correspond to the most popular diffusion loss parametrizations (Song & Ermon, 2019; Song et al., 2021; Ho et al., 2020; Kingma et al., 2021).",1,related,1,positive
"Like (Song & Ermon, 2019; Ho et al., 2020) and most follow-up work, we optimize the model by minimizing a simple noise-prediction loss:
L(x) = E ∼N (0,I),t∼U(0,1) [ ‖̂θ(zt, λt)− ‖22 ] (2)
where zt = αtx+ σt , and ̂θ(zt, λt) = σ−1t (zt − αtx̂θ(zt, λt)).",1,related,1,positive
"However, running MCMC in the manner of ALD is inefficient since we do not know how many iterations within each noise level is sufficient.",1,related,1,positive
"To this end, we make use of score modeling (also known as diffusion modeling), which has recently emerged as a powerful approach for modeling distributions (SohlDickstein et al., 2015; Ho et al., 2020; Song & Ermon, 2019).",1,related,1,positive
"To accommodate the extra dimension z on NCSNv2, we concatenate the image with an additional constant channel with value z and thus the first convolution layer takes in four input channels.",1,related,1,positive
"In our DDPM++ architecture, we directly borrow the configuration of channels from the NCSN++ architecture [33] in each residual block (PFGM w/ NCSN++ channel).",1,related,1,positive
VE/VP/sub-VP We use the same set of hyper-parameters and the NCSN++/DDPM++ (deep) backbone in and the continuous-time training objectives for forward SDEs in [33].,1,related,1,positive
We also include our preliminary experimental results on a weaker architecture NCSNv2 [32] in Appendix D.2.,1,related,0,negative
"Figure 10: Uncurated samples from Langevin dynamics [31] and PFGM (RK45), both using the NCSNv2 architecture.",1,related,1,positive
Table 9: FID/NFE on CelebA 64 × 64 FID ↓ NFE ↓ NCSN [31] 26.,1,related,1,positive
"We show that PFGM with the RK45 solver has competitive FID/Inception scores with the Langevin dynamics, which was the best model on the NCSNv2 architecture before, and requires 10× less NFE.",1,related,1,positive
We empirically observe the new configurations in Table 7 give better results on the NCSNv2 architecture.,1,related,1,positive
"In this section, we demonstrate the image generation on CIFAR-10 and CelebA 64 × 64, using NCSNv2 architecture [32], which is the predecessor of NCSN++ and DDPM++ [33] and has smaller capacity.",1,related,1,positive
"We provides more samples in Appendix E.
4.2 Failure of VE/VP-ODEs on NCSNv2 architecture
In our preliminary experiments on NCSNv2 architectures, we empirically observe that the VE/VP-ODEs have FID scores greater than 90 on CIFAR-10.",1,related,1,positive
"14), we observe that the samples are diverse and exhibit fewer artifacts than PFGM on NCSNv2.",1,related,0,negative
PFGM consistently outperforms other ODE baselines on DDPM++ (Table 1) or NCSNv2 (Appendix D.2) backbones.,1,related,0,negative
"We implement PFGM on the NCSNv2 [32], DDPM++ [33], and DDPM++ deep [33] architectures, with sight modifications to account for the extra dimension z.",1,related,1,positive
"When there is no filtering matrix, i.e. Ct = I , we recover the DSM objective used in (Song & Ermon, 2019; 2020; Song et al., 2021b).",1,related,1,positive
"For example, we might need to tune the weights w(t) since for the ablations (and the state-of-the-art model), we use w(t) = 1/σ2t (as in Song & Ermon (2019; 2020); Song et al. (2021b)) which might be causing instabilities for low values of noise (Nichol & Dhariwal, 2021).",1,related,1,positive
"Once the model is trained, we start from a sample of the final distribution, q1, and then use the learned score to gradually denoise it (Song & Ermon, 2019; 2020).",1,related,1,positive
"We use the standard geometric scheduling for the noise (Song & Ermon, 2019; 2020; Song et al., 2021b) and use the methodology described in Section 3.3 to select the blur levels.",1,related,1,positive
"Training Objective For all our experiments, we scale the loss at level t with w(t) = 1/σ2t as in Song & Ermon (2019; 2020); Song et al. (2021b).",1,related,1,positive
Here we reinterpret the heat dissipation process as a form of Gaussian diffusion similar to that used by Ho et al. (2020); Sohl-Dickstein et al. (2015); Song & Ermon (2019) and others.,1,related,1,positive
"The training objective of DSM [26] is
Ldsm = 1
L L∑ k=0 Ex,x̂ [∥∥∇x̂Eθ(x̂, k) +∇x̂ logN (x̂|x, σ2kI)∥∥] , (1)
with x ∼ ρD(x) and x̂ ∼ N (x, σkI)1.",1,related,1,positive
"1Note that we learn the energy Eθ instead of the score, as its common in the literature [16], [24], to use it as a cost function and evaluate the quality of our samples in an optimization problem",1,related,1,positive
We train the model to jointly match the Signed Distance Field (SDF) of the object we aim to grasp and predict the grasp energy level by the DSM loss Eq.,1,related,1,positive
"Hence, the new DSM loss function on Lie groups equates to
Ldsm = 1
L L∑ k=0 EH,Ĥ [∥∥∥∥∥DEθ(Ĥ, k)DĤ + D log q(Ĥ|H, σkI)DĤ ∥∥∥∥∥ ] ,
(5)
with H ∼ ρD(H) and Ĥ ∼ q(Ĥ|H, σkI).",1,related,1,positive
"To apply DSM [24], [27], we first perturb the data distribution ρD(x) with Gaussian noise on L noise scales N (0, σkI) with σ1   σ2   · · ·   σL, to obtain a noise perturbed distribution qσk(x̂) = ∫ x
N (x̂|x, σkI)ρD(x)dx.",1,related,1,positive
"Given the energy, we compute the DSM loss Eq.",1,related,1,positive
"To apply DSM [24], [27], we first perturb the data distribution ρD(x) with Gaussian noise on L noise scales N (0, σkI) with σ1 < σ2 < · · · < σL, to obtain a noise perturbed distribution",1,related,1,positive
"(5), to apply the DSM loss, we compute the energy e ∈ R over the grasp poses Ĥ.",1,related,1,positive
"Towards this goal, we leverage the denoising score-matching generative model [58] to model the gradient of its log-probability.",1,related,1,positive
"In the sampling stage, our method gradually converts an initial Gaussian random noise point into a realistic point cloud by progressively applying the score function to remove the noise via Langevin dynamics [58, 72].",1,related,1,positive
"At the training stage, following the noise-conditioned score-matching model [58], we adopt a multi-scale loss function, with a re-weighting factor for the loss at each noise level:",1,related,1,positive
"where zt ∼ N (0, I) and εt is the learning rate, which is usually decreased (annealed) with a schedule [58].",1,related,1,positive
"VE SDE, which is equivalent to SMLD in [71, 72], takes ηt = 0 and hence has αt = 1.",1,related,1,positive
"For SMLD, we use the implementation of NCSN++ in [23].",1,related,1,positive
"Two extreme choices of Q0 stand out: 1) The SMLD initialization can be viewed as the case when we initialize Q with an improper “uniform” prior Q0 = 1, corresponding Qx0 = N (0, v) with v → +∞.",1,related,1,positive
"Then, we dive deeper into what it means to learn the score function, and connect it explicitly with the perspective of Score-based Generative Modeling.",1,related,1,positive
"Fortunately, we can look to another class of generative models, Score-based Generative Models [12, 13, 14], for exactly this intuition.",1,related,1,positive
"As it turns out, we can show that the VDM formulation we have previously derived has an equivalent Score-based Generative Modeling formulation, allowing us to flexibly switch between these two interpretations at will.",1,related,1,positive
"In this work, we propose a method to perform policy regularization using diffusion (or scorebased) models (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Ho et al., 2020).",1,related,1,positive
"In this work, we employ so-called predictor-corrector (PC) samplers proposed by Song et al. [28], which combine singlestep methods for solving the reverse SDE with numerical optimization approaches such as annealed Langevin Dynamics [26].",1,related,1,positive
"We note that versions of diffusion models defined in discrete time do not suffer from such shortcomings as the truncation is embedded in the discretization scheme, see (Song et al., 2021b; Song and Ermon, 2020; 2019; Ho et al., 2020) for instance.",1,related,1,positive
"Finally, we establish the probabilistic decoder via a temporal conditional noise score network (TCNSN) as a score matching method, which aims to learn the gradient field of the target distribution (Song & Ermon, 2019; 2020).",1,related,1,positive
We aim to formulate the latent-space EBMs such that one can easily plug in arbitrary attribute operators to define the latent distribution of interest.,1,related,1,positive
"Given an arbitrary energy function E(x) ∈ R, energy-based models (EBMs) define a Boltzmann distribution:
p(x) = e−E(x)/Z, (1) where Z = ∑
x∈X e −E(x) is the normalization
term (the summation is replaced by integration if x ∈ X is a continuous variable).",1,related,1,positive
"Crucially, we overcome the text control and sampling difficulties in the aforementioned sequence-space methods, by defining the text control operations in a compact latent space, handled by a latent-space EBMs with the ODE solver for efficient sampling.",1,related,1,positive
We are now ready to formulate the latent-space EBMs by plugging in the attribute classifiers.,1,related,1,positive
"In this section, we describe our principled approach for approximating perceptually aligned gradients via Denoising Diffusion Probabilistic Models (DDPMs), which recently emerged as an interesting generative technique (SohlDickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020).",1,related,1,positive
"In addition, we provide in this work a second, principled approach towards creating such PAG vectors, relying on denoising score matching as used in diffusion models (Song & Ermon, 2019).",1,related,1,positive
"d samples {xi ∈ R}i=1 drawn from an unknown distribution pD(x) supported by χ, and we want to learn a score function sθ(x) : R → R from data which approximates pD(x) [9].",1,related,1,positive
"As we only have accessible the sample-based score model sθ(x), which is an approximator of the gradient of the log-density, we use a specific MCMC procedure called Langevin dynamics [26] to iteratively generate a chain of samples, starting from an initial known sample x0 [9]:",1,related,1,positive
Imagine we have a density of the form [9]:,1,related,1,positive
"Score-based representation of data distribution Instead of directly estimating the pdf, which may pose challenge in evaluating the normalisation constant, we could estimate the gradient of the log density which is defined as the score function sθ(x) [9]:",1,related,1,positive
"The conditional denoising estimator (CDE) is a way of estimating p(xt|y) using the denoising score matching approach [22,25].",1,related,1,positive
"Unlike for Euclidean SGM, ∇ log pt|0(xt|x0) is not available in closed-form.",1,related,1,positive
"Algorithm 2: RDSB 1: for n ∈ {0, . . . , L} do 2: while not converged do 3: Sample ti ∼ Uniform([0, T ]) 4: Simulate {Xiti} B i=0, where X i 0 ∼ pdata
5: Compute ˆ̀bn(φ n) using (8) 6: φn ← Gradient Step(ˆ̀bn(φn)) 7: end while 8: while not converged do 9: Sample ti ∼ Uniform([0, T ]) 10: Simulate {Yiti} B i=0, where Y i T ∼ pprior 11: Compute ˆ̀fn+1(θ n+1) using (9) 12: θn+1 ← Gradient Step(ˆ̀fn+1(θn+1)) 13: end while 14: end for 15: Output: (θL+1, φL)
Proof The proof follows De Bortoli et al. (2021, Proposition 6) using De Bortoli et al. (2022, Theorem 1) instead of Cattiaux et al. (2021, Theorem 4.19)
In particular, we have that Q1 is the diffusion process associated with RSGM, i.e. the time-reversal of the Brownian motion initialized at pprior.",1,related,1,positive
"We introduce novel methodology for generative modeling and interpolation on compact Riemannian manifolds, which accelerates and generalizes RSGM.",1,related,1,positive
Close inspection of Figure 1 shows RDSB with 5 IPF iterations exhibits better convergence than RSGM (equivalent to RDSB with 1 IPF iteration) for N = 10 diffusion steps on the earthquake data.,1,related,1,positive
"For evaluating the model agnostic property of our PDS, we test three recent SGMs including NCSN [31], NCSNv2 [32] and NCSN++ [33].",1,related,1,positive
"We observe that this approach works well for accelerating NCSN++ [33], but has less effects on accelerating NCSN [31] and NCSNv2 [32].",1,related,1,positive
We use NCSN [31] as the SGM for the simplest digital image generation (28× 28).,1,related,1,positive
Sampling using NCSN [31] on MNIST (28× 28).,1,related,1,positive
We also train a SGM on the same samples.,1,related,1,positive
"We compare these models with SGMs and show experimentally that SGMs seem to be able to generate correctly multimodal distributions while keeping the Lipschitz constant of the score network relatively small, suggesting that these models do not suffer of such previously mentioned limitations.",1,related,1,positive
We highlight that this observation does not apply to SGMs since in this setting the network is applied multiple times.,1,related,1,positive
"On the other hand, we show that SGMs seem to be able to generate multimodal distributions while keeping the Lipschitz constant of the score network relatively small and thus do not suffer of the same limitation.",1,related,1,positive
"We train a VAE, a GAN and a SGM on two datasets derived from MNIST (LeCun et al., 1998): first, two images of two different digits (3 and 7) are chosen and 10000 noisy versions of theses images are drawn with a noise amount of σ = 0.15, forming a dataset of n = 20002 independent samples drawn from a balanced mixture of two Gaussian distributions in dimension 784 = 28× 28.",1,related,1,positive
"SGMs (also known as diffusion models) proceed as follows: first, noise is progressively added to the data distribution until we reach a standard Gaussian distribution.",1,related,1,positive
We refer to Song et al. (2020) for an introduction on SGMs.,1,related,1,positive
"In what follows, we illustrate the pratical implications of our results by training GANs, VAEs and SGMs on simple bi-modal distributions.",1,related,1,positive
We also use conditional batch normalization (Song and Ermon 2019) to take random noise’s standard deviation level into consideration.,1,related,1,positive
"We adopt the following four model training tricks from [34, 53, 54] to stabilize the score matching training process.",1,related,0,negative
"We hypothesis that running DULA for more steps or with an adaptive stepsize schedule (Song & Ermon, 2019) will improve its performance.",1,related,0,negative
"To realize this goal, we consider the diffusion-based (a.k.a. score-based) generative models (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Ho et al., 2020; Song and Ermon, 2020; Song et al., 2021c) and inject covariate-dependence into both the forward and reverse diffusion chains.",1,related,1,positive
"1) Batch Random Mask Block: In order to obtain more robust features and make the prediction of the network adapt to complex and changeable environments, we propose a batch random mask (BRM) block inspired by the Masked AutoEncoders (MAE) [50], in which the random patches of the image are masked to reconstruct these missing regions by the model.",1,related,1,positive
"Our results in Section 5.2 shows that while off-the-shelf
CLIP representations can be poor (especially for RGB-stacking see Figure 6 Right), adapting them through our proposed adapters results in similar performance as other adapted representations (such as MAE ones).",1,related,1,positive
", 2020) and masked auto-encoder (MAE) for ViT (He et al., 2022).",1,related,1,positive
", 2020) and for ViT we use masked auto-encoder (MAE) (He et al., 2022).",1,related,1,positive
"For NFNet we use CLIP pretraining (Radford et al., 2021), for ResNet we use BYOL (Grill et al., 2020) and for ViT we use masked auto-encoder (MAE) (He et al., 2022).",1,related,1,positive
"In addition to imagenet pretraining across all three architectures, we also evaluate using ALIGN (Jia et al., 2021) for NFNet, BYOL for ResNet (Grill et al., 2020) and masked auto-encoder (MAE) for ViT (He et al., 2022).",1,related,1,positive
"To demonstrate that the performance of the foundation model improves with the increase in the number of model parameters when pretrained using the same number of datasets in the remote sensing field, we pretrain models with different
KEUMGANG CHA et al.: A BILLION-SCALE FOUNDATION MODEL FOR REMOTE SENSING IMAGES 5
numbers of parameters using MAE [5] and the large-scale remote sensing imagery dataset, MillionAID [44].",1,related,1,positive
CV BYOL [1] ResNet200 2x 375 Million SimCLR v2 [2] ResNet152 3x w sk 795 Million DINO [3] ViT Base 84 Million iBOT [4] ViT Large 307 Million MAE [5] ViT Huge 632 Million ALIGN [6] EfficientNet-L2 800 Million CLIP [7] ViT Large 307 Million SEER [8] RegNety-256gf 1.,1,related,1,positive
"In the original MAE, pretraining is applied with 1600 epochs [5].",1,related,0,negative
"In this section, we discuss the details of the model architecture (vision transformer) [43], pretraining dataset (MillionAID) [44], and pretraining method (MAE) [5].",1,related,1,positive
"For our default model, we re-use weights from the publicly available MAE model.",1,related,1,positive
Stage 1: We follow settings from MAE [28].,1,related,0,negative
"Following [28], we adopt an encoder-decoder architecture during training and ask the model to reconstruct patches that are randomly masked out, at the pixel level.",1,related,1,positive
"We use masked input prediction[20, 28, 5, 75] objective for unimodal stages.",1,related,1,positive
"The difference between our approach and [20, 49] is that we follow the encoder-decoder structure in [28], where masked tokens are removed for the encoder and are reconstructed through a separate decoder.",1,related,1,positive
"• RGB mask Following MIM [41, 11, 36, 14, 3], we randomly mask blocks of 2D input images and take an additional UNet-like decoder after the feature extractor to reconstruct the masked regions.",1,related,1,positive
"Following (He et al., 2022), during linear probing we include BatchNorm without the affine transformation before the final classifier.",1,related,1,positive
"For audio, we use a fixed 2D sinusoidal positional encoding as described in (Huang et al., 2022) and (He et al., 2022).",1,related,1,positive
"In many cases, we have access to large-scale unlabeled data containing such corruptions and noise, and would like to exploit them to pre-train a VT that can be effectively adapted to a wide range of downstream tasks [2, 11,20, 61].",1,related,0,negative
"Compared to MAE, MB1 outperforms MAE by 2% in both UF1 and UAR, approximately.",1,related,0,negative
"We utilize the encoder and decoder parts of μ-BERT (without DMA and POI) to train previous SSL methods (MoCo V3 [4], BEIT [1], MAE [9]) and then continue learning the MER task on the large-scale database CASME3 [14].",1,related,1,positive
"We utilize the encoder and decoder parts of µ-BERT (without DMA and POI) to train previous SSL methods (MoCo V3 [4], BEIT [1], MAE [9]) and then continue learning the MER task on the large-scale database CASME3 [14].",1,related,1,positive
", [9] presented a Masked Auto Encoder (MAE) that utilizes the BERT framework.",1,related,1,positive
"6% top-1 [34], which aligns with our observations on fine-tuning the pre-trained ADM.",1,related,0,negative
"Inspired by the work of SimpleClick [35], we employ large models for feature encoding, such as the widely used MAE-pretrained Vision Transformer (ViT) [21].",1,related,1,positive
"As our ViT backbones are pre-trained on 224 × 224 pixel images by MAE [21], we resize the pre-trained absolute positional embeddings to match the size of our images.",1,related,1,positive
"In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 [19] (for MAE [16], we use ViT-base [11].) encoder Eg for 200 epochs.",1,related,1,positive
"In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 [19] (for MAE [16], we use ViT-base [11].",1,related,1,positive
"• MAE: Masked Autoencoders [15]: ViT-B16, ViTL16.",1,related,0,negative
"Motivated by scalability and powerful pretraining methods, we use an MAE [47] pre-trained Vision Transformer (ViT) [33] minimally adapted to process high resolution inputs [62].",1,related,1,positive
"Unless otherwise specified: (1) SAM uses an MAE [47] pre-trained ViT-H [33] image encoder and (2) SAM was trained on SA-1B, noting that this dataset includes only automatically generated masks from the final stage of our data engine.",1,related,0,negative
"Motivated by scalability and access to strong pre-training, we use an MAE [47] pre-trained Vision Transformer (ViT) [33] with minimal adaptations to process high resolution inputs, specifically a ViT-H/16 with 14×14 windowed attention and four equally-spaced global attention blocks, following [62].",1,related,1,positive
We initialize SAM from an MAE [47] pre-trained ViT-H.,1,related,0,negative
"While our model is initialized with a selfsupervised technique (MAE [47]), the vast majority of its capabilities come from large-scale supervised training.",1,related,0,negative
"For a comprehensive comparison, we also compare TDMR with MRKD which means a simple combination of model reprogramming (MR) and knowledge distillation (KD), and transfer learning method linear probing [7], [80].",1,related,1,positive
We compare finetuned ViT-B models trained with MoCo-v3 and MAE in Table 5.,1,related,0,negative
"We find that MoCo-v3 defended with PatchSearch and i-CutMix is better than MAE both in terms of Acc and FP for 1% labeled
finetuning data, but MAE quickly catches up in the 10% regime.",1,related,0,negative
"Restrictions apply.
from [44] indicate that MAE is robust against backdoor attacks.",1,related,0,negative
"Following previous pre-training approaches [14, 25], we use the default image input size of 224×224.",1,related,1,positive
Comparison with Hiera [52]: We show class-level performance (average precision and relative gain) of Hiera [52] (pre-trained on using MAE [24]) and ours.,1,related,1,positive
"Overall our method has a gain of +2.8 mAP compared to Video MAE [15, 49].",1,related,0,negative
"To further explicitly supervise motion feature generation, inspired by MAE [18], we design a decoder (Figure 3(b)) to reconstruct pixel motion dynamics.",1,related,1,positive
"The detailed pre-training hyper-parameters are in Table 7, which mainly follows the training settings used in the original MAE [37].",1,related,0,negative
The masked autoencoder (MAE) model [37] consists of an encoder and a decoder.,1,related,1,positive
"As proven in [10], a narrow decoder is enough for the MAE task, so we set L′′ to 1.",1,related,1,positive
"In this work, we use one such pre-training algorithm (MAE (He et al., 2021)) to explore scaling and adapting pre-trained visual representations (PVRs).",1,related,1,positive
"We now ask: how much does the relevance and diversity of the pre-training dataset and the model size matter? To study this, we fix the pre-training objective – MAE (He et al., 2021) – and then vary the composition of the pre-training dataset and the size of the visual backbone (ViTB with 86M parameters and ViT-L with 307M parameters).",1,related,1,positive
"Experiment Details of Training PVRs
To train the MAE models, we use the official codebase released by the authors on GitHub (He et al., 2021) and use the default hyperparameters provided by the repo to train the ViT-B and ViT-L models.",1,related,0,negative
"Specifically, in MAE adaptation we continue training the backbone network with the MAE (He et al., 2021) pre-training objective on task-specific data.",1,related,0,negative
"We train vision transformers (ViT-B and ViT-L) (Dosovitskiy et al., 2020) on these 4 datasets using Masked AutoEncoding (MAE) (He et al., 2021), and systematically analyze their performance on CORTEXBENCH.",1,related,1,positive
"To study this, we fix the pre-training objective – MAE (He et al., 2021) – and then vary the composition of the pre-training dataset and the size of the visual backbone (ViTB with 86M parameters and ViT-L with 307M parameters).",1,related,1,positive
"As the final step in this paper, but as a first step towards this open problem, we study adapting VC-1 with either task-specific training losses or datasets (via MAE (He et al., 2021)) to specialize VC-1 for each domain.",1,related,1,positive
"In this work, we refer to a certain attribute of an element as a field and formulate the various design tasks as a unified masked field prediction, which is inspired by the recent masked autoencoders [9,15] and multi-task models [19,36].",1,related,1,positive
"Results for the Masked Autoencoder (He et al., 2022) (Appendix B.",1,related,0,negative
"Specifically, we apply a linear layer to project the latent features Fl to patch pixels [86], and compute the mean squared error (MSE) between the reconstructed and original images on the masked pixels [29].",1,related,1,positive
"We follow the conventions in [29, 86] and mask random patches with 16× 16 pixels, and adopt a high masking ratio i.",1,related,1,positive
"Following MAE [27], ẑ is then “unmixed” to recover the input batch before mixing by inserting a special [MASK] token with M j .",1,related,1,positive
"In this section, we start by adopting mixing in MAE [27] with a simple baseline in Sec.",1,related,1,positive
"Implementation Details: We follow most of the practices of [1, 8].",1,related,0,negative
"To this end, we closely follow the approach of MAE [8], and employ the Masked Image Modeling strategy.",1,related,1,positive
We tailor the MAE approach for the endoscopic setting with three modifications: Layer Wise Learning Rate Decay: The MAE encoder and decoder consist of several layers.,1,related,1,positive
"This unsupervised learning style normally requires numerous data and computation resources [41], [42], so we put the training of it on the resourceful cloud which can collect a lot of data from multiple edges.",1,related,1,positive
"In this section, we will first revisit the original image-version Masked Autoencoders (MAE) (He et al., 2022) and the video extension of Masked Autoencoders (Feichtenhofer et al., 2022), then detail the tailored MAE for the specific tasks.",1,related,1,positive
"To implement our belief in a regressive manner where the learned loss function can be treated as image prior, the first choice is the masked autoencoders (MAE) proposed by He et al (He et al., 2022).",1,related,1,positive
"In this section, we will first revisit the original image-version Masked Autoencoders (MAE) (He et al., 2022) and the video extension of Masked Autoencoders (Feichtenhofer et al.",1,related,1,positive
"To solve this issue, we adopt an approach inspired by MAE [22], where we only feed the non-masked embeddings to the student .",1,related,1,positive
"For instance, if BERT4ETH follows the original masking ratio (15%) and uses 85% unmasked addresses to predict 15% masked addresses, the masked addresses have a high likelihood of being present in the unmasked addresses, leading to an overly easy prediction task [12].",1,related,1,positive
", 2022f) for example, supports a video masked encoder for MAE (He et al., 2022) losses in addition to a module similar to ALBEF.",1,related,1,positive
"We train the tokenizers of the columns C, the encoder, and the decoder by following the approach of the masked autoencoder (He et al., 2022); that is, we randomly select the columns with a masking ratio and replace the corresponding embeddings with a special embedding called a mask token.",1,related,1,positive
"While the original masked autoencoder in He et al. (2022) uses its decoder only in the pretext task and removes it in the downstream task, our model uses the decoder trained on the pretext task in the downstream tasks.",1,related,1,positive
"Furthermore, we note that our model does not use the [cls] token, unlike the approach by He et al. (2022).",1,related,1,positive
"Given the limited data scale, we leverage mask modeling [32] to make good use of the video data.",1,related,1,positive
"For plain architectures, we adopt the MAE [31] pre-training strategy to provide a good initialization for QFormerp, which has been proven effective in mitigating over-fitting in plain ViTs.",1,related,1,positive
"The official MAE pre-trained weights for the backbone are utilized, and the entire model is finetuned for 100 epochs on the MS COCO dataset.",1,related,0,negative
"For large models, we use plain ViT as the reference architecture and adopt the MAE pretrained [31] weights for initialization.",1,related,1,positive
"The hyperparameter for the proposed regularization term is set to 1, and all other parameters follow the same settings as in [31].",1,related,1,positive
"We use the official MAE pre-trained model to initialize the ViT-B backbone and the default training settings in MMPose, i.e., an input image size of 256×192 and a learning rate of 5e-4.",1,related,0,negative
"For the plain models, we adopt the fine-tuning practice proposed in [31], which involves pre-training the model on the ImageNet-1K dataset for 1,600 epochs using the ImageNet-1K training set, followed by supervised fine-tuning.",1,related,1,positive
We overcome the above-mentioned issues by leveraging generic Masked AutoEncoder (MAE) [19].,1,related,1,positive
We use the publicly available Masked Autoencoders [19] pretrained on ImageNet to assist blind defense.,1,related,0,negative
"Masked Autoencoders (He et al., 2022) are scalable self-supervised learners.",1,related,1,positive
"We use two pretrained Masked Autoencoders (He et al., 2022) that are available from their official repository.",1,related,0,negative
"In terms of supervision, in addition to the standard supervised pre-training on ImageNet-1k, we also consider self-supervised methods MoCo-V3 [17], SwAV [11], DINO [12], MAE [32] trained on ImageNet-1k, the weakly supervised SWAG [76] trained on 3.6B uncurated instagram images and CLIP [59] trained on 400M image-language pairs [71].",1,related,1,positive
"In terms of supervision, in addition to the standard supervised pre-training on ImageNet-1k, we also consider self-supervised methods MoCo-V3 [17], SwAV [11], DINO [12], MAE [32] trained on ImageNet-1k, the weakly supervised SWAG [76] trained on 3.",1,related,1,positive
"Masked auto-encoders assign mask tokens to an encoded latent space [16, 44], and adopt them to masked sequences at the decoder level.",1,related,1,positive
"the performance of our architecture against the BERTbased motion in-painting transformer [10], the encoderdecoder-based ∆-interpolator [31], the RNN-based approach TGcomplete [15], and the masked auto-encoder (MAE) architecture [16].",1,related,1,positive
"Moreover, inspired by MIM [2, 10], we propose a simple yet effective strategy named Masked Pose and Shape Estimation, which helps the global transformer further mine the inter-frame correlation on human motion.",1,related,1,positive
"During the global decoder phase, following [10], we pad the SMPL tokens onto the masked location and send the whole sequence into the global decoder to obtain a long-term representation.",1,related,1,positive
"Discriminative SSL methods [11, 22, 27, 79] learn the embeddings by enforcing closeness and/or distantness on the pairwise distance structure among the augmented training samples.",1,related,1,positive
We conjecture that an SSL pre-trained encoder is desirable to capture the demanding diverse semantics instead of a supervised one learned from pre-defined labels.,1,related,1,positive
"Given a frozen prediction model Pθ(y|x), and perturbed image x̃ with prompt corresponding to the label y, the training objective is formulated as:
argmin ϕ
− logPθ;ϕ(y|x̃)
While VP and AR optimize the input space visual prompt directly, we reparameterize the visual prompt to the prompt generation network hϕ(·) parameterized by ϕ = {ϕd, ϕt} ∈ Rd. Specifically, we build a novel autoencoderstyle network named Coordinator composed of a frozen encoder f(·) which is pre-trained on ImageNet [14] by selfsupervised learning (SSL) objective and followed by an extremely light-weight learnable decoder gϕd(·).",1,related,1,positive
"Therefore, for Coordinator, BlackVIP adopts an SSL encoder (i.e., Masked Auto-Encoder [26]).",1,related,1,positive
"Though the encoder can also be a supervised counterpart or lightweight learnable network, we adopt the SSL pre-trained encoder for the following three reasons: 1) It has been widely substantiated that self-supervised representation contains
the multiple discriminative features and spatial information [8, 19, 26, 31, 42, 43, 50], so it is more helpful to use SSL pre-trained encoder than label-supervised encoder for robustly performing on diverse downstream tasks.",1,related,1,positive
"Consequently, the image equipped with a prompt (prompted image) is constructed as follows:
x̃ = clip(x+ ϵhϕ(x)) hϕ(x) = gϕd(zx, ϕt)
where zx = f(x) is the feature vector of x from the frozen SSL encoder f(·), and ϵ ∈ [0, 1] is a hyperparameter that controls the intensity of visual prompt.",1,related,1,positive
6 confirms that the SSL encoder outperforms the supervised pre-trained or randomly initialized encoder (scratch).,1,related,0,negative
We exploit an SSL pre-trained encoder while we plug the randomly initialized extremely lightweight decoder.,1,related,1,positive
"Concretely, the computation of (2) can directly benefit from just dropping all the masked patches before forwarding them, in a similar manner to masked autoencoder [12].",1,related,1,positive
"(4) Inspired by the principles of self-supervised training [41, 15, 14], we leverage this methodology to train our encoder-decoder architecture.",1,related,1,positive
"Inspired by the advantage of long-range receptive fields from transformer layers, we follow MinD-Vis [6] to adopt the architecture of masked autoencoder [14] as the encoder-decoder model for fMRI signals.",1,related,1,positive
The only difference from MAE is that we finetune on iNaturalist21 rather than iNaturalist17.,1,related,0,negative
"As a base model, we consider the ViT-Base model using the Masked AutoEncoder (MAE) pretraining setup [16], which leads to state-of-the-art results for this general task.",1,related,1,positive
"For iNaturalist, we consider a Masked AutoEncoder (MAE) [16] pre-trained model and finetune it following the same procedure and hyperparameters as the original paper.",1,related,1,positive
"Though the scRNA-seq data is distinct from images, our results show that the performance gains of xTrimoGene are comparable to those of MAE, with more efficient training and better downstream task performance.",1,related,0,negative
"To further tackle the mode collapse challenge in unsupervised learning and scale up the optimization iterations, inspired by recent masked autoencoders [23, 57], we construct a masked point modeling paradigm where both point color reconstruction objective and surfel normal reconstruction objective are proposed to recover the masked color and geometric information of the point cloud respectively.",1,related,1,positive
"To this end, we turn to use MAE [19], a SOTA image encoder pre-trained in an unsupervised manner, similar to MLM.",1,related,1,positive
"To this end, we turn to use MAE [18], a SOTA image encoder pre-trained in an unsupervised manner, similar to MLM.",1,related,1,positive
"We find MAEbase-MLM clearly improves the standard MAEbase on HM with the TS model, but obtains marginal gains with the E2E model.",1,related,1,positive
"We extract the ViT features from the same ViT architecture training in different ways: (b) supervised ViT [17], (c) self-supervised DINO [43], and (d) MAE [44].",1,related,1,positive
using transformer feature representations extracted from pretrained masked autoencoder (MAE) [44].,1,related,1,positive
"Note that other popular image feature extractors [3, 9] can also be applied in our framework.",1,related,1,positive
"Given the simplicity and efficiency of Masked AutoEncoding (MAE) [33], we leverage it as the self-supervised prepretraining approach.",1,related,1,positive
We follow [33] to train MAE models on IG-3B without using any labels.,1,related,0,negative
"In particular, we show that (i) MAE not only scales with model size as shown in [33], but also with the size of the training data (Figure 2).",1,related,1,positive
We first begin with the Masked Autoencoder (MAE) [33] self-supervised learning technique to pre-pretrain vision models without using any labels.,1,related,1,positive
We compare the performance of MAE pretraining on the large scale IG-3B with the original MAE [33] models trained on IN1k for 1600 epochs.,1,related,0,negative
We follow the same hyperparameters used in [33] for pretraining on IN1k.,1,related,1,positive
"We follow SimpleClick [26] to build the interactive segmentation model, which consists of two patch embedding modules for image and click map respectively, a ViT [10] backbone initialized with MAE [16], a simple feature pyramid [21], and an MLP segmentation head.",1,related,1,positive
"As shown in Table 3, the model initialized with MIS pre-training the MAE [16] counterpart especially on the relatively hard dataset (i.e., SBD and DAVIS).",1,related,0,negative
"As shown in Table 3, the model initialized with MIS pre-training the MAE [16] counterpart especially on the relatively hard dataset (i.",1,related,0,negative
"Following the principle of MAE [27], we randomly mask out some tokens, and utilize the EAT encoder to extract high-level latent features.",1,related,1,positive
"Beyond augment-and-compare or mask-and-predict pretext tasks in MV-SSL and MIM, in this paper, we endeavor to investigate another simple yet effective paradigm for selfsupervised visual representation learning.",1,related,1,positive
"Overall, our CIM is a simple yet effective approach that can perform on par or better than existing MV-SSL and MIM methods with both ViT and ResNet models.",1,related,1,positive
"Due to the architectural difference between ViT and CNN models, we observe performance degeneration of some MIM and MV-SSL pretraining methods, such as SimMIM [68], MoCo v2 [10], and SimSiam [11].",1,related,1,positive
2) We demonstrate the advantages of our CIM in learning transferable representations for both ViT and ResNet models that can perform on par or better than the current state-of-the-art MIM and MV-SSL learners while improving model robustness and training efficiency.,1,related,1,positive
"For all ablation experiments, we choose ViT-Base (ViT-B/16) as the default backbone and follow a common setting used in existing works [2, 26]: 300-epoch self-supervised pre-training without labels and 100-epoch supervised end-to-end fine-tuning, to evaluate the quality of learned representations.",1,related,1,positive
"We omit the results in other metrics (NDCG, MAE MAPE) and on other data as their trends are similar.",1,related,1,positive
"We also adopt the mean absolute error (MAE), rooted mean squared error (RMSE) as well as mean absolute percentage error (MAPE) to evaluate the model accuracies.",1,related,1,positive
"The concatenation of unmasked patches’ embedding and masked tokens are processed by the decoder to predict the unmasked patches [17, 43].",1,related,1,positive
Masked autoencoder (MAE) [141] (see Figure 8) simplifies it to an end-to-end denoising framework by predicting the masked patches from the unmasked patches.,1,related,1,positive
Masked Autoencoders (MAE) [30].,1,related,1,positive
"One of the key observations of the MAE work [30] is that the decoder does not need to be very good for the encoder to achieve good performance: by using only a small decoder, MAE successfully trains a ViT in an auto-encoder fashion.",1,related,0,negative
"Compared to a model that uses masked image modeling, the original MAE [27] and to the MaskFeat model [55], our model underperforms by 0.",1,related,1,positive
"First, inspired by [28, 97], we randomly mask some joints and require the model to reconstruct them.",1,related,1,positive
"Starting with the baseline ViT configurations used in the original BEiT series pre-training [5, 92, 123] (∗ in Table 2), we progressively refine the model design and make the following observations: (i) The performance of SwiGLU FFN is mediocre with the random weight initialization method used in BEiT, but works quite well with JAX weight initialization [14, 51] (+1.",1,related,1,positive
"Still, we note that this efficient decoder retains much of the modeling capacity of standard masked decoders [5, 8, 14] that employ full self-attention on context and mask {zL, zM}.",1,related,1,positive
"Since the standard masked modeling [5, 8, 14] of applying a bidirectional transformer on mask and context {zM , zL} leads to a complexity of O((NM + NL)(2)), we opt into an efficient decoder that reduces the cost to linear to mask sizeO(NM ) while preserving as much modeling capacity as possible.",1,related,1,positive
"First of all, we pre-train a Masked Autoencoder (MAE) [13, 36] on a large-scale facial dataset in a selfsupervised manner.",1,related,1,positive
Our framework starts with pre-training a masked autoencoder (MAE) [9] from a set of face databases.,1,related,1,positive
"To verify transfer learning ability on scalable Vision Transformers beyond UNets, we compare ImageNet 2562 pre-trained DiT [43] to MAE pre-trained vanilla ViTs [17] on CIFAR-10
and Tiny-ImageNet.",1,related,1,positive
"Drawing from the connections between diffusion networks and denoising autoencoders (Section 3.1), we infer that DDAEs can produce linear-separable representations at some implicit encoderdecoder interfaces, resembling DAEs and MAEs.",1,related,1,positive
"Following [10, 26], we randomly mask the features of joints after the embedding layer.",1,related,1,positive
"Different from MAE [10] and some existing graph data augmentation methods [12, 23], we feed the topology of the original joints into the encoder.",1,related,1,positive
The great success of MAE [10] makes us rethink data augmentation.,1,related,0,negative
"Surprisingly, when we directly reconstruct masked joints using a method similar to that in MAE [10], the performance degrades instead.",1,related,1,positive
We will discuss the relationship between MAE [10] and graph data augmentation in detail in Sec.,1,related,1,positive
"Inspired by MAE [10], we propose an augmentation framework named MGPose.",1,related,1,positive
"As in MAE [23], the encoded tokens zT (Eqn.",1,related,1,positive
"Thus, this work is based on Masked AutoEncoders (MAE) [23] for pre-training.",1,related,0,negative
"We implemented the 3D MAE using 3D-ResNetBlocks instead of Vision Transformer, different from the previous study [13], due to the constraint of GPU memory.",1,related,1,positive
"Following the previous study [13], we implement an asymmetric design with a lightweight decoder compared to the encoder.",1,related,1,positive
"To avoid information leakage, we use self-supervised MAE for model pre-training on split ImageNet-subset.",1,related,1,positive
One exception is fine-tuning MAE on ImageNet-subset where we found the transformations used for CPP generate inferior results and thus we follow the data transformations used in the original paper [20].,1,related,1,positive
"DualPrompt [59] is originally designed using Transformer structures, so we only change the pre-trained weights to MAE to avoid information leakage.",1,related,1,positive
"To this end, we implement CPP with four up-to-date pre-training methods including ViT [14], Deit [53], Dino [5], and MAE [20] that sweep supervised and self/un-supervised learning as well as discriminative and generative models.",1,related,1,positive
"2) MIM: To handle unpaired images, we leverage the Masked Image Modeling (MIM) paradigm [59].",1,related,1,positive
"Therefore, we use a multiway transformer to extract multi-modal features and two linear layers to solve PLM and MIM tasks, respectively [38], [59].",1,related,1,positive
"Our VLP model achieved the best performance with a higher masking ratio of 85%, which is in contrast to the optimal masking ratio of 75% reported by MAE [59].",1,related,0,negative
"PosterV2-Vit feature: To acquire advanced visual features, we trained a Vision Transformer (ViT) [4] model on the AffectNet7 dataset [21] using unsupervised learning techniques [5].",1,related,1,positive
"In this paper, we revisit deep supervision for masked image modeling (MIM) [15, 11, 48, 2], a self-supervised pretraining strategy for Vision Transformer [12] (ViT).",1,related,1,positive
"Then, similar to MAE [15], we randomly mask a high proportion of patches, yielding a masked image x̃.",1,related,1,positive
"For concreteness, we use MAE [15] to illustrate our underlying approach.",1,related,1,positive
"Then ViT-B is finetuned on ImageNet-1K [9], following common practice [15].",1,related,1,positive
"We follow the same training recipe of MAE [15], more details can be found in supplementary materials.",1,related,0,negative
"FULL 100% 79.1 86.2 59.7 75.0
Addition-based methods
MLP-3 1.60% 73.6 75.2 35.7 61.5 PROMPT-SHALLOW 0.04% 79.9 82.5 37.8 66.7 PROMPT-DEEP 0.23% 76.8 84.5 53.4 71.6 ADAPTER-8 1.18% 81.7 87.3 61.2 76.7 SPT-ADAPTER (ours) 0.33% 83.0 87.3 62.1 77.5
Reparameterization-based methods
baseline method on VTAB-1k benchmark with only 0.26% and 0.08% trainable parameters for MAE and MoCo v3 pretrained backbones, respectively.",1,related,1,positive
"We show more parameter sensitivity patterns for ViT-B/16 with various pre-training strategies (i.e., MAE [20] and MoCo V3 [11]) and datasets sampled from FGVC benchmark [24].",1,related,1,positive
"We conduct experiments on the plain vision Transformer backbone ViT-B/16 [13] that is pre-trained on ImageNet [27] with different pre-training strategies following [24], including supervised pre-training and self-supervised pre-training with MAE [20] and MoCo v3 [11] following [24].",1,related,1,positive
"Visualizations of sampled VTAB-1k datasets with MAE and MoCo V3 pre-trained ViTB/16 are shown in Figures 5, 6, 7.",1,related,0,negative
"For our RFFR model, we adopt a base version of Masked Autoencoder (MAE) [19] and train it on real faces with a batch size of 128.",1,related,1,positive
We find that Data2Vec [3] (row 3) attains similar performance to the MAE [18] (row 4) initialization we use in Sec.,1,related,1,positive
"Specifically, we find that visual representation learning (using masked autoencoding (MAE) [18]) not only improves performance, but also enables model scaling with ViTs.",1,related,1,positive
"Specifically, we propose novel FaceMAE module, a masked autoencoder [25] specialized for FER-W, to achieve the goal of FFL.",1,related,1,positive
"However, we designed FaceMAE, a masked autoencoder [25] specialized for FER-W, by making two major modifications to the original masked autoencoding scheme.",1,related,1,positive
"However, we notice that since MAE’s mask tokens only pass through the decoder [20], a shareddecoder design is critical in our scheme for mask tokens to learn mutual information before performing reconstructions in separate modal-specific decoders.",1,related,1,positive
We follow MAE [20] and Point-M2AE [64] to generate input tokens from images and point clouds.,1,related,1,positive
"Consequently, we design a MAE [20]-structured multi-modal learning framework that incorporates projection alignment for more interactive multi-modal learning.",1,related,1,positive
"For the image branch, we follow [20] to divide images into regular patches with a size of 16 × 16, before the ViT backbone.",1,related,1,positive
"To examine the effectiveness of our method, we perform DPPMask on two representative MIM methods: MAE [26], iBOT [67], which represent two different MIM frameworks: pixel reconstruction and feature contrast.",1,related,1,positive
"For the self-supervised learning models, DINO [5] and MAE [16], we additionally measure the endto-end performance of benchmark models pre-trained on our UnlabelledNAIP.",1,related,1,positive
"DINO [5] based on knowledge distillation and the generative model MAE [16] based on autoencoder, on our FireRisk.",1,related,1,positive
"For the self-supervised architectures, MAE [16] and DINO [5], we use ViT-B/16 [10] as the backbone and fine-tune on FireRisk using latent representa-",1,related,1,positive
"Using transfer learning, we fine-tune ResNet-50 [17], ViT-B/16 [10], as well as DINO [5] and MAE [16] with ViT-B/16 as the backbone, all of which were pre-trained on ImageNet [8], using our",1,related,1,positive
"• To investigate the performance of supervised and self-supervised learning on our FireRisk, we employ ResNet [17], ViT [10], DINO [5], and MAE [16] as benchmark models.",1,related,1,positive
"On FireRisk, we present benchmark performance for supervised and self-supervised representations, with Masked Autoencoders (MAE) [16] pre-trained on ImageNet1k [8] achieving the highest classification accuracy, 65.",1,related,1,positive
"learning, we select two representative self-supervised models for their performance, namely DINO [5] and MAE [16].",1,related,1,positive
"In generating the latent representations, we use ViT-B/16 as the backbone architecture for MAE [16], pre-",1,related,1,positive
"• We gather an unlabelled dataset, UnlabelledNAIP, from the NAIP remote sensing project and utilize it to pre-train novel latent representations of DINO [5] and MAE [16].",1,related,1,positive
"We choose publicly available PTMs, i.e., ResNet18/50/152 [24], ViT-B/16-IN1K/21K, ViT-L/16-IN1K, ViT-B/16-DINO [7], ViT-B/16-SAM [11], ViT-B/16-MAE [23], ViT-B/16CLIP [50] (image encoder) for a holistic evaluation, and report the results in Figure 6.",1,related,1,positive
"Additionally, we report the backbones adopted in the ablation study, including ViT-L/16-IN1K, ViTB/16-DINO [7], ViT-B/16-SAM [11], ViT-B/16-MAE [23], ViT-B/16-CLIP [50] (image encoder), in the table.",1,related,1,positive
"(3) RefCOCO, RefCOCOg, RefCOCO+ [81] 60K MLM with PEVL text encoder [78] Phrase Grounding (1) Flickr30K [79] 32K MLM with PEVL text encoder [78]
Visual Relationship Detection (1) Visual Genome [41] 101K MLM with PEVL text encoder [78] Visual Commonsense Reasoning (1) VCR [84] 100K MLM with PEVL text encoder [78]
Self-Supervised Learning (2) ImageNet-1K [10] 1.3M MoCo-v2 [7], MAE [24] TOTAL (47) 32M
them for specific downstream task.",1,related,1,positive
"Similar to MAE [22] and PointMAE [37], we compute the loss only on masked parts.",1,related,1,positive
"Following MAE [22] and VideoMAE [51], we adopt the asymmetric encoder-decoder design to reduce computation.",1,related,1,positive
"We use 3 alternative models to initialize the feature extractor in the mask generator: (1) the ViT-B network pretrained with MAE [16] (termed as MAE-800), (2) the ViT-B pretrained with iBOT [48] (termed as iBOTB), and (3) the ViT-S pretrained with DINO [3] (termed as DINO-S).",1,related,1,positive
"Also, since the methods
in this section refer to MAE, a comparison test is done between the methods in this section and MAE using the same training method.",1,related,0,negative
"The self-supervised pretraining method in this paper takes reference from MAE, but differs from it in that MAE uses two identical structures of ViT as encoder and decoder, while our method uses a symmetric convolution-deconvolution structure for the autoencoder.",1,related,1,positive
"The resulting data are shown in Table III, from which it can be seen that in each of the four datasets, our method is higher than MAE by more than 3 points in each metric.",1,related,1,positive
"Meanwhile, to better prove the reliability of the method proposed in this chapter, we conducted peer-to-peer experiments using MAE, i.e., we first performed masked self-supervised
learning pre-training, and then selected the one with the lowest training loss model for the pedestrian re-identification task.",1,related,0,negative
"Considering ViT’s flexibility and great potential in masked image modeling [9,14], we explore acceleration algorithms based on the standard ViT.",1,related,1,positive
"We implement a baseline inspired by MIM [2,9].",1,related,1,positive
"Instead of a random formulation [9, 30], we sample a fixed ratio γ of the tokens [M ] (X) to be masked out according to a multinomial distribution related to the patch hand saliency m(X):",1,related,1,positive
"Then this prior is encoded as the token form [9], and a ViT [15] sketcher is trained to disentangle the corresponding structure tokens from partial image patches [30].",1,related,1,positive
"Inspired by this, we implement an MAE by masking the outputs of our encoder, Fo, and then passing the masked encoded features along with the masked tokens to our decoder.",1,related,1,positive
"As illustrated in Figure 1 (b), we adopt a simple Regression head composed of two deconvolution layers and one 1×1 convolution layer on the reshaped Fo following the common setting of the previous works [23].",1,related,1,positive
"We initialize the Temporal ViTPose weights from the pre-trained model of MAE [23], and perform masked region reconstruction with a masking rate of 75%.",1,related,1,positive
"Regarding selfsupervised models, the masked autoencoder (MAE [30]), DINO [7], MoCov3 [10], MSN [2] were selected, because they all include the base ViT (ViT-B/16) for comparison between pretraining schemes (Fig.",1,related,1,positive
"Here, we make a modification to the original MAE (He et al. 2022) by replacing the masked contents with x and processing them in the same way as non-masked ones, instead of discarding them.",1,related,1,positive
"Therefore, we empirically set an even higher combined masking rate of 87.5% (compared to 75% used by He et al. (2022)) in our M3AE to make the self-supervising task nontrivial.",1,related,1,positive
"Here, we make a modification to the original MAE (He et al. 2022) by replacing the masked contents with xsub and processing them in the same way as non-masked ones, instead of discarding them.",1,related,1,positive
"Above all, witnessing the recent success of masked autoencoders in learning rich visual representations (He et al. 2022), we propose multimodal masked autoencoder (M3AE), where a random subset of the modalities and random patches of the remaining ones are masked simultaneously.",1,related,1,positive
"Therefore, we sample a random subset of the modalities for masking to mimic the real situation, in addition to randomly masking 3D patches of the remaining modalities as in the original MAE for natural images.",1,related,1,positive
"Evaluation of Representation: So, why is the Slow Learner such effective, and what accounts for the remaining performance gap? We perform a linear probing experiment [12] to evaluate the performance of the representation layer.",1,related,1,positive
"In addition to supervised pre-training, we consider representative self-supervised paradigms that provide pre-trained checkpoints on ViT-B/16, i.e., MoCo v3 [4], BEiT [2] and
MAE [12].",1,related,1,positive
"Considering architectural consistency with previous works of CLPM [43, 42], we select representative self-supervised methods (i.e., MoCo v3 [4], MAE [12] and BEiT [2]) that release checkpoints on ViT-B/16 in our comparisons.",1,related,1,positive
We utilize the mask ratio 25% and 8 decoder blocks following the practices in MAE [4].,1,related,1,positive
"To address the above issues, we propose to Mimic before Reconstruct for Masked Autoencoders, termed as MR-MAE, which is a simple and effective strategy to enhance MAE [4] by regularizing the intermediate representations with pre-trained off-the-shelf feature encoders.",1,related,1,positive
"Inspired by the popular masked image modeling [12, 13], we build a universal masked autoencoder architecture for VAD by embedding random mask tokens to simulate anomalies, which is a simple yet efficient synthetic method that avoids extra data processing in the original normal data.",1,related,1,positive
"The image encoder is initialized with the first 12 layers of MAE-base (He et al. 2022) weight, which is pre-trained on the ImageNet-1k without any labels.",1,related,1,positive
"We also provide a new pipeline achieving data augmentation efficiently for imbalanced image datasets, using cGAN or diffusion models and ResNet or Masked Autoencoder (MAE) classifiers.",1,related,1,positive
Table 6 provides goodness-of-fit (R2) and Mean Absolute Error (MAE) measurements for the function f .,1,related,1,positive
"To justify our proposed metric and pipeline work regardless of the classifier selection, we also provide the results with Masked Autoencoder (MAE) ViT-H128 [14] which shows state-of-the-art results in dataset such as [12].",1,related,1,positive
a ResNet [15] or Masked Autoencoder (MAE) [14] classifier also trained on the original set with sub-class labels are applied on the synthesized images to select qualified ones.,1,related,1,positive
5% 1 State-of-the-art (SOTA) classification validation accuracy with Masked Autoencoder ViT-H448 [14],1,related,0,negative
The high R2 and low MAE values show that the formulation of f is highly effective on modeling the relationship between SSIM-supSubCls and accuracy improvement with our proposed data augmentation pipeline.,1,related,1,positive
"The ResNet18 classifiers are trained for 100 epochs and the MAE for 50 epochs when their validation accuracy converges, with their hyperparameters remaining the same throughout the whole procedure in each case.",1,related,0,negative
Table 7 compares the state-of-the-art (SOTA) classification validation accuracy from [14] and ours on the whole iNaturalist-2019 dataset.,1,related,0,negative
"To certify that this conclusion can be drawn regardless of the classifier selection, we also conduct the experiments with MAE classifier and the same results are obtained.",1,related,0,negative
"Then
a ResNet [15] or Masked Autoencoder (MAE) [14] classifier also trained on the original set with sub-class labels are applied on the synthesized images to select qualified ones.",1,related,1,positive
"From this table, our results with Masked Autoencoder (MAE) is comparable with the SOTA one, even though the SOTA is with 448× 448 while ours is with 128×128 input image resolutions which largely reduce the need of running time and computational resources.",1,related,1,positive
"In our experiments, the deep generative models, ResNet18 and MAE classifiers are first trained on the original imbalanced set with sub-class instead of super-class labels. cGAN models are trained until the Frechet Inception Distance (FID) scores converge.",1,related,1,positive
"Specifically, our efficient centroid-based MIM outperforms the prior tokenbased MIM [2] and pixel-based MIM [21] in equivalent ViT size and epochs.",1,related,1,positive
"Besides, we also consider a baseline strategy (nofill) that drops the masked patches without refilling as in MAE [13].",1,related,1,positive
"Inspired by the work [24], we propose to use a masked autoencoder based on vision transformer.",1,related,1,positive
"Moreover, we have picked up some parameters that have been proven successful in [24].",1,related,0,negative
"Different from conventional autoencoders, the used masked encoderdecoder operates in an asymmetric fashion [24] enabling the encoder to operate on only the partial observed signal (without the masked patches) and the decoder to rebuild the full image based on the representation given by the encoder and the masked patches.",1,related,1,positive
"We use the standard ViT-B [14] as the encoder network and initialize it with the MAE pretrained [18] weights following [43, 88].",1,related,1,positive
BEiT [1] RRC+40% mask ViT+Linear DALLE SimMIM [56] RRC+60% mask ViT+Linear RGB MaskFeat [53] RRC+40% mask ViT+Linear HOG ConvMAE [15] RRC+75% mask ConvViT+MSA RGB MAE [20] RRC+75% mask ViT+MSA RGB,1,related,1,positive
"In the following analysis, we investigate MAE [20] by diagnosing its reconstruction target and input image patches, identifying two important but overlooked bottlenecks that could have hurt the representation quality.",1,related,0,negative
"We then present a careful analysis with the milestone method, MAE [20], to disclose two important but overlooked bottlenecks of most pixel-based MIM approaches (subsection 3.",1,related,1,positive
"However, unlike the MAE, the occlusion positions of samples generated by OIA are randomly sampled, so we need to conduct completion on each instance.",1,related,1,positive
"As mentioned in III-C, our FCD adapts MAE’s notion [25] of restoring entire features using implicit unoccluded features.",1,related,1,positive
We customize the original masking strategy from MAE [22] and design a facial part masking strategy to ensure that the model can learn the consistencies of all facial parts.,1,related,1,positive
Our masked autoencoder is based on an asymmetric encoder-decoder architecture [22].,1,related,1,positive
"To realize this restoration concept, we pretrain a simple yet powerful masked autoencoder (MAE) model [22] solely on real images.",1,related,1,positive
"In this stage, we customize MAE[22] and its masking strategy with modifications to perform self-supervised learning of real faces, so that the model can learn generic facial part consistency features and can also prevent over-fitting to specific forgery patterns.",1,related,1,positive
"Inspired by [22], we propose a novel self-supervised pretraining method that is based on masking and recovering faces, named Mover.",1,related,1,positive
The emergence of the masked autoencoder (MAE) [22] has greatly influenced our community.,1,related,0,negative
"For 8K iterations, we find VPDA32 surpass all the baseline methods, including those pre-trained on mask image modelling [17, 38], contrastive learning [7] and supervised learning [27, 29].",1,related,1,positive
"♠ SpiderCNN (Xu et al., 2018) 69.8 73.7 ♠ DGCNN (Wang et al., 2019) 73.6 78.1 ♠ PointCNN (Li et al., 2018) 75.1 78.5 ♠ GBNet (Qiu et al., 2021) 77.8 80.5 q PointBert (Yu et al., 2022d) - 83.1 q Point-MAE (Pang et al., 2022) - 85.2 q Point-TnT (Berg et al., 2022) 81.0 83.5
♣ PointNet (Qi et al., 2017a) 63.4 68.2 ♣ PointNet++ (Qi et al., 2017b) 75.4 77.9 ♣ BGA-PN++ (Uy et al., 2019) 77.5 80.2 ♣ PointMLP (Ma et al., 2022) 83.9 85.4 ♣ PointMLP-elite (Ma et al., 2022) 81.8 83.8 r PointMLP-CoC (ours) 84.4↑0.5 86.2↑0.8
Context Clusters are a natural fit for point clouds Qi et al. (2017b); Lu et al. (2022).",1,related,1,positive
"1 on ImageNet and do not use dropout when pre-training on the much larger JFT-300M dataset; recent languagesupervised or self-supervised vision models (Radford et al., 2021; He et al., 2021) do not use dropout.",1,related,0,negative
"Unlike MAE He et al. (2022), BEiT Bao et al. (2022), and DIT Li et al. (2022), we do not use patch-level masking strategy for pre-training.",1,related,0,negative
"…8, we compare several tuning methods with our proposed U-Tuning based on different architectures and pre-trained models, i.e., IN-21K (Deng et al., 2009), MAE (He et al., 2022b) and CLIP (Radford et al., 2021) pre-trained ViT/B16 (Dosovitskiy et al., 2020), as well as CLIP pre-trained ViT/L-14.",1,related,1,positive
"In Table 8, we compare several tuning methods with our proposed U-Tuning based on different architectures and pre-trained models, i.e., IN-21K (Deng et al., 2009), MAE (He et al., 2022b) and CLIP (Radford et al., 2021) pre-trained ViT/B16 (Dosovitskiy et al., 2020), as well as CLIP pre-trained ViT/L-14.",1,related,1,positive
", MAE [15]), we use this finding to further reduce the computational complexity of our model.",1,related,1,positive
"Similar to recent works in the computer vision domain (e.g., MAE [15]), we use this finding to further reduce the computational complexity of our model.",1,related,1,positive
"For model fine-tuning with MAE, we adopt the settings in (He et al., 2021).",1,related,1,positive
"For example, we obtain better results by integrating our method into the fine-tuning process of the Masked AutoEncoder (MAE) in (He et al., 2021), achieving improvements on ImageNet by 0.3%, ImageNet-A by 1.8%, ImageNet-R by 1.4% and ImageNet-C by 3.7% on Vision Transformer Large (Dosovitskiy et…",1,related,1,positive
"15 is that we take the results directly from (He et al., 2021), which have no detailed data of different corruptions.",1,related,0,negative
"For example, we obtain better results by integrating our method into the fine-tuning process of the Masked AutoEncoder (MAE) in (He et al., 2021), achieving improvements on ImageNet by 0.",1,related,1,positive
"C.15 is that we take the results directly from (He et al., 2021), which have no detailed data of different corruptions.",1,related,0,negative
"FT follows MAE (He et al., 2022), while Ours uses MoCo v2 contrastive loss and regularization coefficient λ = 0.",1,related,1,positive
"FT follows MAE (He et al., 2022), while Ours uses MoCo v2 contrastive loss and regularization coefficient λ = 0.1.",1,related,1,positive
"FT follows the training in MAE (He et al., 2022) mostly, using SGD for 200 epochs with a cosine learning-rate scheduler, the base learning rate of 0.",1,related,1,positive
"FT follows the training in MAE (He et al., 2022) mostly, using SGD for 200 epochs with a cosine learning-rate scheduler, the base learning rate of 0.06, weight decay 5e-4, momentum 0.9 and batch size 256.",1,related,1,positive
"We adopted the masked auto-encoder (MAE) (He et al., 2022) to process the point cloud data (denoted as MAE-dense) for pre-training, which is close with Point-MAE (Pang et al., 2022).",1,related,1,positive
"To pre-train the MAE-dense and MAE-sampled, we first follow the standard train-test split of ShapeNet (Chang et al., 2015) adopted by Pang et al. (2022); Yu et al. (2022).",1,related,1,positive
It is denoted as MAE-sampled.,1,related,1,positive
"Specifically, we build the classifier by keeping the encoder structure and weights of the pre-trained MAE-dense and MAE-sampled models, followed by max-pooling as well as a fully connected layer of dimension [256, 256, 40] to map the global token of a dimension of 256 to the 40 categories.",1,related,1,positive
"3, our proposed MAE-sampled outperformed all state-of-the-art methods on 3 out of 4 settings, while MAE-sampled consistently outperformed MAE-dense.",1,related,1,positive
"We adopted the masked auto-encoder (MAE) (He et al., 2022) to process the point cloud data (denoted as MAE-dense) for pre-training, which is close with Point-MAE (Pang et al.",1,related,1,positive
"To evaluate the effectiveness of our claimed contribution, we replace the dense-attention layer in MAE-dense with our sampled-attention layer (Fig.",1,related,1,positive
"To maintain a fully self-supervised pre-training paradigm, we initialize with weights obtained by self-supervised ImageNet pre-training [19].",1,related,1,positive
"Similar to MAE [19], we normalize the output patches as well as the target patches prior to computing the loss, which we found to empirically improve the performance.",1,related,1,positive
2) MAE-unsupIN→SN [19] ViT ImageNet+ScanNet 54.,1,related,1,positive
6) MAE-unsupIN→SN [19] ViT ImageNet+ScanNet 63.,1,related,1,positive
"Although similar to MAE, we choose SimMIM because it adopts the backbone of Swin Transformer, which performs better than ViT adopted in MAE, as shown in the experiment.",1,related,1,positive
"It is shown that the robust curves of the models with the same
Springer Nature 2021 LATEX template
ARES-Bench 3
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
VGGs VGG13_Normal VGG16_Normal VGG19_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
ResNets ResNet50_Normal ResNet50_MOCO ResNet101_Normal ResNet152_Normal Wide-ResNet50_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
DenseNets DenseNet121_Normal DenseNet161_Normal DenseNet201_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(d)
ConvNexts ConvNextS_Normal ConvNextS_21K ConvNextB_Normal ConvNextB_21K ConvNextL_Normal ConvNextL_21K
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(e)
ViTs ViTS_Normal ViTS_21K ViTB_Normal ViTB_21K ViTB_MAE ViTL_Normal ViTL_21K ViTL_MAE
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(f)
XciTs XciTS_Normal XciTM_Normal XciTL_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (g)
T2Ts T2T14_Normal T2T19_Normal T2T24_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(h)
Swins SwinS_Normal SwinS_21K SwinB_Normal SwinB_21K SwinL_21K
Fig.",1,related,1,positive
"Springer Nature 2021 LATEX template
ARES-Bench 13
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
Normal Models VGG19_Normal ResNet152_Normal DenseNet161_Normal ConvNextL_Normal ViTL_Normal XciTL_Normal T2T24_Normal SwinB_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
Pre-trained Models ResNet50_Normal ResNet50_MOCO ViTB_Normal ViTB_21K ViTB_MAE ConvNextL_Normal ConvNextL_21K SwinB_Normal SwinB_21K
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
AT Models ResNet152_Normal ResNet152_AT ConvNextL_Normal ConvNextL_AT ViTB_Normal ViTB_AT XciTL_Normal XciTL_RB SwinB_Normal SwinB_AT",1,related,1,positive
"IN-Val IN-V2 IN-Real ON IN-A IN-R IN-V IN-C SIN IN-Sketch
ViTS
Normal 74.4 61.6 80.0 13.1 8.8 30.4 11.2 32.0 9.1 19.9 34.0 Pre-train 81.4 70.3 86.8 22.7 27.3 45.7 16.6 47.1 15.8 32.5 44.6 AT 70.2 57.3 77.9 11.5 6.1 46.0 8.5 27.8 16.8 29.8 35.2
ViTB
Normal 75.8 61.6 80.9 13.2 11.4 32.8 13.3 34.3 10.9 23.7 35.8 Pre-train 84.6 73.9 88.8 27.4 44.5 56.8 19.4 57.5 22.6 43.0 51.9 MAE 83.6 73.1 88.1 24.9 37.7 49.8 18.2 49.4 20.2 36.4 48.1 AT 73.4 60.4 80.5 12.7 8.9 50.7 9.4 36.6 22.2 35.7 39.1
ViTL
Normal 75.2 60.7 79.8 11.2 11.3 33.3 13.4 35.4 9.3 25.0 35.4 Pre-train 85.8 76.0 89.2 30.5 56.1 64.2 25.5 65.3 30.1 51.8 57.4 MAE 85.1 75.6 89.0 27.3 50.6 60.0 21.5 56.2 24.1 46.4 53.6
XciTS Normal 82.4 71.5 86.8 23.7 31.3 45.0 17.0 50.1 19.5 32.9 46.0
RB 73.3 60.5 80.6 12.7 6.3 45.7 9.7 28.5 18.4 31.2 36.7
XciTM Normal 82.6 71.0 86.8 23.4 33.3 44.7 17.7 50.5 20.3 33.1 46.3
RB 74.1 61.7 81.3 13.6 7.0 47.1 9.5 30.2 19.7 32.6 37.7
XciTL Normal 83.0 72.0 86.9 23.7 36.2 46.2 17.9 50.2 20.4 34.4 47.1
RB 75.1 62.7 81.7 13.4 8.8 49.0 10.7 32.0 19.9 34.4 38.7
T2T14 Normal 81.6 70.9 86.8 22.3 24.1 44.7 16.7 46.8 17.7 32.2 44.4
T2T19 Normal 82.3 71.6 87.2 23.2 29.0 47.3 18.0 50.2 20.9 34.4 46.4
T2T24 Normal 82.4 71.7 87.2 22.9 29.7 47.9 18.0 52.0 20.8 35.1 46.8
SwinS
Normal 83.2 72.1 87.5 24.7 33.0 44.9 19.3 45.1 16.8 32.0 45.8 Pre-train 83.3 73.5 88.6 28.1 43.9 54.8 21.3 50.6 17.2 41.2 50.3 AT 75.8 63.3 82.6 15.3 10.6 52.5 10.8 37.1 21.1 37.1 40.6
SwinB
Normal 83.4 72.3 87.6 25.5 35.8 46.6 20.2 45.6 17.9 32.4 46.7 Pre-train 85.1 75.2 89.1 28.8 51.8 59.1 22.7 56.4 19.6 45.1 53.3 AT 76.8 64.5 83.4 15.5 13.1 53.5 11.8 39.3 22.7 39.3 42.0
SwinL Pre-train 86.3 77.0 89.6 31.6 61.0 63.6 26.4 61.3 23.4 48.8 56.9
AT 78.7 66.9 84.9 18.2 18.1 57.3 11.6 43.4 25.2 42.9 44.7
increases from 34.1% of ResNet50 to 36.8% of ResNet101, and finally to 38.0% of ResNet-152.",1,related,1,positive
"C V
] 2
8 Fe
b 20
23
2 ARES-Bench
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
VGGs VGG13_Normal VGG16_Normal VGG19_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
ResNets ResNet50_Normal ResNet50_MOCO ResNet50_AT ResNet50_RB ResNet50_RL ResNet101_Normal ResNet101_AT ResNet152_Normal ResNet152_AT ResNet152_FD Wide-ResNet50_Normal Wide-ResNet50_AT Wide-ResNet50_RB
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
DenseNets DenseNet121_Normal DenseNet161_Normal DenseNet201_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(d)
ConvNexts ConvNextS_Normal ConvNextS_21K ConvNextS_AT ConvNextB_Normal ConvNextB_21K ConvNextB_AT ConvNextL_Normal ConvNextL_21K ConvNextL_AT
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(e)
ViTs ViTS_Normal ViTS_21K ViTS_AT ViTB_Normal ViTB_21K ViTB_MAE ViTB_AT ViTL_Normal ViTL_21K ViTL_MAE
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(f)
XciTs XciTS_Normal XciTS_RB XciTM_Normal XciTM_RB XciTL_Normal XciTL_RB
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (g)
T2Ts T2T14_Normal T2T19_Normal T2T24_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(h)
Swins SwinS_Normal SwinS_21K SwinS_AT SwinB_Normal SwinB_21K SwinB_AT SwinL_21K SwinL_AT
Fig.",1,related,1,positive
"Specifically, according to the pioneering work [19], given a natural image from an unlabeled dataset X, we divide it into N regular image patches, denoted as x ∈ RN×S where S denotes the patch size (e.",1,related,1,positive
"The training settings are the same as MAE [19], we adopt the same encoder-decoder structure to perform the MIM task.",1,related,0,negative
"For a fair comparison, we fully follow the hyperparameters of MAE [19] in our image classification experiments.",1,related,1,positive
"DeiT[14] proposes an effective receipt to train ViT with limited data, and MAE[15] adopts a masked autoencoder to pre-train the ViT.",1,related,0,negative
"We adopt the recipe in vanilla ViT[13], DeiT III[14], and MAE[15] to train ViTs.",1,related,0,negative
"Follow MAE (He et al., 2021), we evaluate the performance of the proposed Layer Grafted Pre-training with different number of fixing blocks.",1,related,0,negative
"…first step of Layer Grafted Pre-training, since it identically follows the original MIM pipeline, we directly adopt the pre-trained model of MAE (He et al., 2021) and conduct the second step, where we initialize the network with the MIM pre-trained model and train with Moco V3 (Chen et al.,…",1,related,1,positive
"For other settings such as data augmentation, LR scheduling or weight decay, we identically follow He et al. (2021).",1,related,1,positive
"…65.3 -
C-MAE (Huang et al., 2022) 73.9 65.3 77.3 MimCo (Zhou et al., 2022) 70.2 62.7 - Layer Grafted Pre-training (Ours) 77.7 65.5 77.8
ViT-L/16 MAE (He et al., 2021) 75.8 55.2 78.7
Moco V3 (Chen et al., 2021) 77.6 - - Layer Grafted Pre-training (Ours) 81.0 69.3 80.1
Our method also demonstrates…",1,related,1,positive
"For fine-tuning, we train with AdamW (Loshchilov & Hutter, 2017) for 100 epochs following MAE (He et al., 2021).",1,related,1,positive
"We by default adopt MAE (He et al., 2021) and Moco V3 (Chen et al., 2021) as our MIM and CL frameworks, respectively.",1,related,1,positive
"Then, the image with minimal augmentation would be utilized for computing MIM loss following MAE He et al. (2021).",1,related,1,positive
"In contrast, we employ MAE (He et al., 2021), an image modeling framework that is free from strong augmentations.",1,related,1,positive
"In this paper, we propose to predict the masked edges in Emask in training, inspired by the success of masked autoencoding in computer vision [23].",1,related,1,positive
"As shown in Figure 4, we adopt two reconstruct heads of simple linear layers to respectively recover the masked 2D pixels and 3D coordinates following MAE [He et al., 2021] and Point-MAE [Pang et al.",1,related,1,positive
"Additionally, we compared our method with some recent self-supervised methods, including MoCoV2 [32], MAE [13], and ConvMAE [33].",1,related,1,positive
"In this paper, we present VoxFormer, a strong camera-based 3D semantic scene completion (SSC) framework composed of (1) class-agnostic query proposal based on depth estimation and (2) class-specific segmentation with a sparse-to-dense MAE-like design.",1,related,1,positive
"Our framework is a two-stage cascade composed of class-agnostic proposals and class-specific segmentation similar to [68]: stage-1 generates class-agnostic query proposals, and stage-2 uses an MAE-like architecture to propagate information to all voxels.",1,related,1,positive
"Motivated by reconstruction-before-hallucination and sparsity-in-3D-space , we build a two-stage framework: stage-1 based on CNN proposes a sparse set of voxel queries from image depth to attend to images since the image features correspond to visible and occupied voxels instead of non-visible and empty ones; stage-2 based on Transformer uses an MAE-like architecture to first strengthen the featurization of the proposed voxels by voxel-to-image cross-attention, and then process the full set of voxels with self-attention to enable the voxel interactions.",1,related,1,positive
We name such learnable parameter as mask token [3] for conciseness since unselected from Q is analogous to masked from Q.,1,related,1,positive
Auto Encoder (MAE) [108] pre-trained model to initialize the ViT backbone.,1,related,1,positive
"Algorithmically, the Figure 2: Illustration of representative SSL methods: SimCLR [9], MoCo V3 [9], BYOL [15], and the Masked Auto-Encoder [10].",1,related,1,positive
"We further evaluate our defense under other popular SSL training algorithms and different model structures and datasets, e.g., ResNet-18 and ViT-Small/16 trained using SimCLR, MoCO V3, BYOL, MAE over CIFAR-10 or the ImageNet (Appendix 6.3).",1,related,1,positive
"Here we use self-supervised learning methods consistent with those in Section 4.3, including the contrastive learning method SimCLR, MoCO V3, BYOL, and the masked-model training method MAE.",1,related,1,positive
"For Case-1, we incorporate four state-of-the-art SSL training methods, i.e., SimCLR [9], MoCo V3 [14], BYOL [15], and the MAE [10], for evaluation.",1,related,1,positive
"For example, the accuracies of our model pre-trained with MAE were 8.68% and 5.16% higher than those of ResNet101 pre-trained with SimSiam in the five-class and binary classification tasks.",1,related,0,negative
"By leveraging a simple SSL framework, MAE, we alleviated the problem of training classification models without sufficient high-quality labeled OCT images.",1,related,1,positive
[35] as the initial weights of our model.,1,related,1,positive
"In the model pre-training stage, we transfer the weights of the MAE encoder and decoder pre-trained on the ImageNet-1 K dataset [35] (see the upper part of Fig.",1,related,1,positive
"For the classification task of cervical OCT images, we are
the first to propose a ViT-based image classification model pre-trained with a non-contrastive SSL framework, MAE, which can help ease the burden of insufficient labeled image data on the model’s prediction performance.",1,related,1,positive
A few essential parameters were configured in the selfsupervised model pre-training with MAE.,1,related,0,negative
"1(a) presents the self-supervised pre-training of our model with MAE, which includes a ViT encoder and a lightweight Transformer decoder.",1,related,1,positive
"For example, compared with the supervised MViT-B (the SOTA model) with the weights transferred from the ImageNet-1 K dataset, the five-class and binary classification accuracies of ViT-B (224) pre-trained with MAE were increased by 2.65% and 1.52%, respectively.",1,related,1,positive
"However, note that our approach is general and easily extends to self-supervised settings. e.g. (Chen et al., 2020; He et al., 2021; Chen et al., 2021).",1,related,1,positive
We only take the unmasked patches as the input of the encoder similar to MAE [9].,1,related,1,positive
"(Yang et al., 2022)
Oriented RCNN ViTAE 81.24 Use MAE (He et al., 2022) to pretrain the plain ViTAE transformer.",1,related,1,positive
"With the pretraining method MAE andRVSA, the detector outperformed all previousmethods, achieving 81.24% and 71.05%mAP onDOTA-V1.",1,related,0,negative
"First, it trains a ViT-based encoder fθ(·) on all images in X via self-supervised methods such as MAE [28].",1,related,1,positive
"For feature extraction, we use MAE [7].",1,related,1,positive
"For pre-training Edge MAE, we tuned the learning rate as 1.5e-4, weight decay as 0.05, the mask ratio as 1/3, and batch size as 2048.",1,related,0,negative
"From the two tables, we can see that, Edge Transformer outperforms almost all baseline models, and the results can be further improved by pre-training Edge MAE on unlabeled edges.",1,related,1,positive
"Furthermore, we propose a novel Edge Transformer model and pre-train the model via Masked Auto-Encoders (MAE).",1,related,1,positive
"In this section, we evaluate our Edge Transformer and Edge MAE in two settings.",1,related,1,positive
"We can see the proposed Edge MAE achieves the best results, and link prediction models perform better as they model the whole return process entirely.",1,related,1,positive
"4.2 Parameter Settings For Edge Transformer and Edge MAE, we set the latent vector size 𝐷 as 256, the dropout rate as 0.0, and the number of attention heads as 3.",1,related,1,positive
"Inspired by MAE [7] in CV which pre-trains masked auto-encoders on unlabeled images, we propose to improve Edge Transformer with an Edge MAEmodel.",1,related,1,positive
"For the MAEViT-Base (He et al., 2022) baseline, we report additional results for λ values 1, 10, and 50.",1,related,1,positive
"When η = 1.0, it means the number of features in the baseline is the same as the number of features learned using PISCO and as a result, similar for CIFAR-10 results in §D.2, we observe the in-distribution performance of PISCO in this case being almost the same as that of baseline methods even for higher values of λ.
MAE-ViT-Base results for λ values 1, 10, and 50: Results for additional values of λ when MAE-ViT-Base is the baseline are in Table 19.",1,related,1,positive
"We also report analogous results for another popular feature extractor, MAE-ViT-Base (He et al., 2022), in Table 5.",1,related,1,positive
"We train two types of auto-encoders (AEs) to reconstruct the images in the collection D, namely denoising AEs [73] and masked AEs [28].",1,related,1,positive
We employ Adam [36] and the mean squared error (MSE) to train both AEs.,1,related,1,positive
"First of all, we propose four novel pre-retrieval predictors, namely (i) the magnitude of the reconstruction error of denoising [73] or masked [28] auto-encoders trained on the database, (ii) the density of the k-means cluster to which the query image embedding is assigned, (iii) the confidence distribution of a classification head attached to the embedding layer of the retrieval model, and (iv) the score predicted by a fine-tuned ViT model [20].",1,related,1,positive
"We compare the soups to a nominal model, the l∞-robust classifier used in the soups, their ensemble, the Masked Autoencoders of [18], AdvProp [54], PyramidAT [24], and the ensemble obtained by averaging the output (after softmax) of the four models included in the soups.",1,related,1,positive
"Finally, while the capability of selecting a model specific to each image distribution is a main point of our model soups, we also show that it is possible to jointly select a soup for average performance across several IMAGENET variants to achieve better accuracy than adversarial and self-supervised baselines [18,24].",1,related,1,positive
Other alternatives carefully tweak R per dataset and architectures e.g. to only compute the reconstruction loss on parts of the data as with BERT Devlin et al. (2018) or MAEs He et al. (2022).,1,related,1,positive
"|D<t) where k ∈ [1, . . . ,K] on ImageNet for 4 different architecture and pre-training methods: ResNet50 + SimCLR, ResNet50 + BYOL, ViT/B16 + DINO and ViT/B16 + MAE.",1,related,1,positive
"For ResNet-50, we pre-train on supervised SimCLR and BYOL losses; for ViT/B16, we pre-train on supervised (Touvron et al., 2020; He et al., 2021) DINO and MAE losses.",1,related,1,positive
"Natural # Special # Structured #
ViT/B16 DINO 1.57 1 1.00 1 2.63 3 ResNet50 BYOL 2.71 2 3.00 2 2.13 1 ViT/B16 MAE 5.71 6 3.25 3 2.38 2 ViT/B16 SUP 2.86 3 3.25 4 5.75 6 ResNet50 SimCLR 4.29 5 5.75 6 3.38 4 ResNet50 SUP 3.86 4 4.75 5 4.75 5
ranks in Table 1b.",1,related,1,positive
"Following the command in Masked AutoEncoder [9], we use a lightweight decoder, which has only 10% computation per token compared with the encoder.",1,related,1,positive
"After the pre-training & fine-tune method [18] was proposed, more andmore experiments demonstrated its superior performance, so here, we also used the Masked AutoEncoder pre-trained ViT-base model.",1,related,0,negative
"As for the structure of our decoder, inspired by Masked AutoEncoder [9], Our decoder is also based on the attention algorithm, which has the same structure as the encoder(ViT-base) but is only 10% of its size.",1,related,1,positive
Proposed Masked AutoEncoder.,1,related,0,negative
"(2022), Z is obtained by clustering the latent features produced by a Masked Autoencoder (He et al., 2022).",1,related,1,positive
"For example, in Liu et al. (2022), Z is obtained by clustering the latent features produced by a Masked Autoencoder (He et al., 2022).",1,related,1,positive
"To further improve the robustness of SGLDM to different sketches, we adopt the arbitrarily masking conditional training strategy in training inspired by Masked AutoeEncoder [7], which masks random patches of the input and let the model reconstruct it automatically.",1,related,1,positive
"2022), we aim to adapt the data-efficient masked image autoencoder (MAE) (He et al. 2022) to multi-modal and small-scale RGB-D datasets for better representation learning.",1,related,1,positive
"Driven by the good self-supervised learning performance of MAE (He et al. 2022) in images, we design a multi-modal masked autoencoder (MM-MAE) in RGBD data.",1,related,1,positive
"Unlike the original MAE (He et al. 2022), our CoMAE presents a shared encoder and decoder among RGB and depth modalities and acts as a kind of regularizer to guide pre-training.",1,related,0,negative
"Inspired by the results in VideoMAE (Tong et al. 2022), we aim to adapt the data-efficient masked image autoencoder (MAE) (He et al. 2022) to multi-modal and small-scale RGB-D datasets for better representation learning.",1,related,1,positive
"In addition, our CoMAE instantiated with ViT-B also achieves
competitive and even better results of the same ViT-B model pre-trained by ImageMAE (He et al. 2022), this reveals the data efficiency advantage of our CoMAE pre-training.",1,related,1,positive
"Vision models We include 14 computer vision models (VMs) in our experiments, representing three families of models: SegFormer (Xie et al., 2021), MAE (He et al., 2022), and ResNet (He et al., 2016).",1,related,1,positive
We also compare M2A2E with the symmetric multimodal MAE [3] when finetuning on all downstream modality settings.,1,related,1,positive
"Compared with the vanilla MAE [18], M(2)A(2)E adopts the same masked strategy in unimodal ViT encoder but targeting at multimodal reconstruction with multiple unshared ViT decoders.",1,related,1,positive
"We find that 1) leveraging local feature descriptors benefits the ViT on IR modality; 2) partially finetuning or using adapters can achieve reasonable performance for ViT-based multimodal FAS but still far from satisfaction; and 3) mask autoencoder [3, 18] pre-training cannot provide better finetuning performance compared with ImageNet pre-trained models.",1,related,1,positive
"Following MAE [21], we use the normalized pixel values as targets in the reconstruction task.",1,related,1,positive
"Following MAE [21], we only feed the visible, unmasked patches to the online encoder Es.",1,related,0,negative
"For MIM tasks, we apply augmentations and masking strategy following MAE [21] to the input of the online encoder.",1,related,1,positive
"5, which is lower than the best configuration reported in (He et al., 2022).",1,related,0,negative
"The masking ratio used during pre-training was set to 0.5, which is lower than the best configuration reported in (He et al., 2022).",1,related,0,negative
"For our experiments, we use the CNN feature map (Santoro et al., 2017; Zambaldi et al., 2018) for end-to-end learned fixedregion representations and the mini-patch representations from pre-trained Masked AutoEncoder (MAE) (He et al., 2022) for pre-trained fixed-region representations.",1,related,1,positive
"The pre-training of the MAE encoder was conducted using a batch size of 128, a learning rate of 0.001, and a weight decay of 0.05.",1,related,0,negative
"In this work, we develop our pretraining objective based on a masked image modeling approach like [41, 18].",1,related,1,positive
We perform OOD detection with MIM pretext task with each metric – the results are shown in Tab.,1,related,1,positive
"For one-class OOD detection, we pre-train the MIM model and finely tune it on ImageNet-21k [49], as recommended by BEiT [2].",1,related,0,negative
"Specifically, we adopt the masked image modeling (MIM) [2, 11, 20] as our self-supervised pretext task, which has been demonstrated to have great potential in both natural language processing [11] and computer vision [2,20].",1,related,1,positive
"In our method, we pre-train the model with Masked Image Modeling (MIM) pretext [11] on a large dataset and fine-tune it on the ID dataset.",1,related,1,positive
"Specifically, we adopt the masked image modeling (MIM) [2,11,20] as our self-supervised pretext task, which has been demonstrated to have great potential in both natu-ral language processing [11] and computer vision [2,20].",1,related,1,positive
We compare the performance of MIM and contrastive learning pretext task MoCov3 [8] in Tab.,1,related,1,positive
"In the MIM task, we split images into patches and randomly mask a proportion of image patches before feeding the corrupted input to the vision transformer.",1,related,1,positive
"For multi-class OOD detection, we pre-train the MIM model, intermediately use fine-tuning on ImageNet-21k [49], and apply fine-tuning again on the ID dataset.",1,related,1,positive
The self-supervised pretext task in our framework is Masked Image Modeling (MIM).,1,related,1,positive
"Inspired by the tremendous success of the masked autoencoding paradigm in NLP [19] and Computer Vision [32], we propose to incorporate a new masked autoencoder model into our generalized framework.",1,related,1,positive
"paradigm in NLP [19] and Computer Vision [32], we propose to incorporate a new masked autoencoder model into our generalized framework.",1,related,1,positive
"Notably, whenγ = 1, our masked autoencoder is equivalent to MAE for vision [32].",1,related,1,positive
"We also hope to incorporate selfsupervised methods that learn representations by training on a union of unlabeled data from source and target via proxy tasks like reconstruction (Gidaris et al., 2018; He et al., 2022) and contrastive learning (Caron et al.",1,related,1,positive
"We also hope to incorporate self-supervised methods that learn representations by training on a union of unlabeled data from source and target via proxy tasks like reconstruction (Gidaris et al., 2018; He et al., 2022) and contrastive learning (Caron et al., 2020; Chen et al., 2020).",1,related,1,positive
"Pretrained MAE and CLIP with world model (MAE+WM, CLIP+WM)
Masked autoencoder (MAE; He et al. 2021) learns visual representation in a self-supervised manner by training a vision Transformer (ViT; Dosovitskiy et al. 2021) to reconstruct masked patches.",1,related,1,positive
"Convolutional feature embedding Unlike prior work that masks random pixel patches (He et al., 2021), we embed camera observations into convolutional feature maps and mask these features following the design of Seo et al. (2022a).",1,related,1,positive
"Specifically, we consider CLIP (Radford et al., 2021) and MAE (He et al., 2021) representations as they have recently shown to be effective for robotic manipulation (Shridhar et al., 2021; Radosavovic et al., 2022).",1,related,1,positive
"We hypothesize this is because spatial information redundancy (He et al., 2021) is more significant in visual observations from
manipulation tasks than natural images.",1,related,1,positive
"We hypothesize this is because spatial information redundancy (He et al., 2021) is more significant in visual observations from manipulation tasks than natural images.",1,related,1,positive
"We also incorporate the idea of a prior work (Seo et al., 2022a) that masks convolutional features instead of pixel patches (He et al., 2021) and predicts rewards to learn representations capturing fine-grained details required for visual control.",1,related,1,positive
"Convolutional feature embedding Unlike prior work that masks random pixel patches (He et al., 2021), we embed camera observations into convolutional feature maps and mask these features following the design of Seo et al.",1,related,1,positive
"R O
] 3
1 M
ay 2
02 3
experiments on RLBench (James et al., 2020) show that our method outperforms single-view representation learning baselines (Radford et al., 2021; He et al., 2021; Seo et al., 2022a) and a multi-view representation learning baseline (Sermanet et al., 2018).",1,related,1,positive
", 2020) show that our method outperforms single-view representation learning baselines (Radford et al., 2021; He et al., 2021; Seo et al., 2022a) and a multi-view representation learning baseline (Sermanet et al.",1,related,1,positive
"We then perform extensive experiments on CIFAR10 and CIFAR100 datasets with various representative pre-trained vision models (He et al., 2016; Dosovitskiy et al., 2021; Chen et al., 2020d;c; Radford et al., 2021; Xie et al., 2022; He et al., 2022), which are trained in supervised or self-supervised manners.",1,related,1,positive
"1, we empirically analyze the main assumptions of our theory in various deep vision models (Dosovitskiy et al., 2021; He et al., 2016; Radford et al., 2021; Chen et al., 2020d;c; He et al., 2022; Xie et al., 2022).",1,related,1,positive
"com/facebookresearch/mae (He et al., 2022) License https://github.",1,related,0,negative
"We adopt pre-trained checkpoint in (He et al., 2022).",1,related,0,negative
"(3) (4)Note that for methods like MAE (He et al., 2022; Pang et al., 2022), the teacher network is an identity mapping with no parameters, and therefore φ is an empty set.",1,related,1,positive
"(3) 4Note that for methods like MAE (He et al., 2022; Pang et al., 2022), the teacher network is an identity mapping with no parameters, and therefore φ is an empty set.",1,related,1,positive
"The masked ratio can be even higher than the original configuration described in MAE (He et al., 2022) (e.",1,related,1,positive
", 2021) and MAE (He et al., 2022), we adopted AdamW (Loshchilov & Hutter, 2017) for pre-training, fine-tuning.",1,related,0,negative
"According to the setup in MoCo v3 and MAE (He et al., 2022), both adopted a batch size of 4,096.",1,related,0,negative
"To address the representation deficiency issue in MLM, we propose a simple framework MAE-LM, which pretrains bidirectional Transformer encoders using the MLM objective, but based on the Masked Autoencoder (He et al., 2022) structure.",1,related,1,positive
"To address the representation deficiency issue, we propose a simple text encoder pretraining method, MAE-LM, which conducts MLM pretraining based on the Masked Autoencoder architecture (He et al., 2022).",1,related,1,positive
"AE + SR 16 denotes a baseline experiment with a auto-encoder architecture as in He
et al. (2022).",1,related,1,positive
"In Table 3, we compare our method against DINO Caron et al. (2021), MoCo-V3 Chen et al. (2021), MaskFeat Wei et al. (2021), BEiT Bao et al. (2021), iBOT Zhou et al. (2022), and MAE He et al. (2022).",1,related,1,positive
"Unlike recent methods Zhou et al. (2022); He et al. (2022), we do not perform exhaustive searches for the optimal hyperparameters such as learning rates.",1,related,1,positive
The result of MAE He et al. (2022) with 400 epochs is based on our reimplementation.,1,related,0,negative
"(2021a); Ho et al. (2020) with ↓ (x) = √γx + √ 1− γε, with ε ∼ N (0, I) and γ uniformly sampled as γ ∼ U(0, 1).",1,related,1,positive
"We follow the details presented in MAE He et al. (2022) and implement an asymmetric
Methods GPUs × H Acc.",1,related,1,positive
"To show the effectiveness of noisy image modeling in visual feature learning and adversarial defense, we adopt two simple, representative MIM methods, SimMIM [24] and MAE [10] for comparison.",1,related,1,positive
"In our study, the random sampling masking ratio is 75% [37].",1,related,0,negative
"Referring to the research about the decoder in pretraining in MAE [37], combined with the task of our work, we design an extremely lightweight encoder.",1,related,0,negative
"Because the reconstruction part appears only in pretraining and is not used in the sequence-based inference, we follow He et al. (2022) and use a lightweight decoder with fewer channels in Revolution.",1,related,1,positive
"Inspired by the work of Masked Autoencoder [20], our transformer can be divided into an encoder part and a decoder
1 3
part.",1,related,1,positive
"Inspired by the work of Masked Autoencoder [20], our transformer can be divided into an encoder part and a decoder Fig.",1,related,1,positive
"Inspired by these, we inherit the recent success of spectrogram SSL in the frequency domain, which guarantees efficient compression and high-level semantic understanding.",1,related,1,positive
"Following He et al. (2022), we split low-resolution radiograph into nonoverlapping image patches, where 75% patches are randomly masked.",1,related,1,positive
"In Figure 2, we provide relation maps of three samples from ImageNet, using 10 checkpoints of MAE-Large [13].",1,related,1,positive
2 Results and analysis ImageNet We measure the label error detection performance on ImageNet with the synthetic label noise by training an MAE-Large model [13].,1,related,1,positive
"We measure the detection performance with MAE-Large [13], BEIT-Large [1], and ConvNeXt-Large [29] models.",1,related,1,positive
"For ADE20K, the input size is set to 512×512 following previous works (Bao et al., 2021; He et al., 2021; Chen et al., 2022a; Zhou et al., 2021).",1,related,1,positive
"…reconstruct the targets.
methods:
`MIM(Decoder(Regressor(Encoder(Rv))),Target(Rm)), (3) where Target(Rm) is a function to map the masked patches to the targets, e.g., d-VAE (Ramesh et al., 2021) token used in CAE and BeiT (Bao et al., 2021), or normalized RGB values used in MAE (He et al., 2021).",1,related,1,positive
"The main tracker only consists of a ViT backbone and a box estimation head, we test both ViTBase and ViT-Large, and the ViT parameters are initialized with MAE (He et al. 2022) pre-trained model.",1,related,1,positive
"Our approach is inspired by the previous MIM method (Xie et al. 2022; He et al. 2022), but we have to deal with two fundamental problems in the tracking framework: (1) Visual tracking is a downstream vision task that generally does not have the pre-train process to apply the MIM strategy.",1,related,0,negative
We only use the pretrained encoder part to extract the image features [32].,1,related,1,positive
"Note, we have not demonstrated it here, but Zorro can also be trained using unimodal self-supervised methods such as MAE [25] and DINO [12] separately on the audio and visual streams.",1,related,0,negative
"Motivated by the great success of other MAE-style approaches (He et al., 2021; Feichtenhofer et al., 2022; Hou et al., 2022), we also adopt an asymmetric design that the encoder only operates visible tokens after applying masking on input embedding, and a lighter decoder processes encoded tokens…",1,related,1,positive
"It is hypothesized and summarized in (He et al., 2021; Feichtenhofer et al., 2022) that the masking ratio is related to the information density and redundancy of the data, which has an immense impact on the performance of the autoencoders.",1,related,1,positive
Our encoder/predictor architecture is reminiscent of the generative masked autoencoders (MAE) [35] method.,1,related,1,positive
"Methods without view data augmentations data2vec [7] ViT-L/16 1600 73.3
MAE [35] ViT-L/16 1600 67.1 ViT-H/14 1600 71.5
trained with I-JEPA requires less computational effort than a ViT-Small/16 trained with iBOT.",1,related,0,negative
I-JEPA outperforms MAE while requiring less pretraining epochs when using a similar encoder architecture.,1,related,0,negative
"Methods without view data augmentations data2vec [7] ViT-L/16 1600 77.3
MAE [35] ViT-B/16 1600 68.0 ViT-L/16 1600 76.0 ViT-H/14 1600 77.2
CAE [21] ViT-B/16 1600 70.4 ViT-L/16 1600 78.1
Closest to our work is data2vec [7] and Context Autoencoders [24].",1,related,1,positive
We draw inspiration from MAE [19] for this design.,1,related,0,negative
We draw inspiration from MAE [20] for this design.,1,related,0,negative
"Our decoder is a lighter-weight 8-layer 512-dimensional transformer, following MAE [19].",1,related,1,positive
Our decoder follows the decoder design from MAE [20].,1,related,1,positive
"For the image reconstruction loss, we normalize the ground-truth per-patch, following [38].",1,related,1,positive
"During finetuning, we use a layer-wise learning rate decay, following [17, 38, 3].",1,related,1,positive
"We find that a combination of two state of the art approaches: masked auto-encoders, MAE [38] and contrastive language image pre-training, CLIP [69] provides a benefit over CLIP when trained on a corpus of 11.",1,related,1,positive
"For all models which rely on masking, we use a 75% masking ratio, consistent with [38, 31], as we did not find an alternative that improved downstream results.",1,related,0,negative
"As in [38] we experiment with predicting both normalized and un-normalized patch values, finding that predicting the normalized patch value slightly improves the performance of our MAE implementation.",1,related,1,positive
"For evaluation on CIFAR-10, CIFAR-100, Tiny Imagenet-100 and CBIS-DDSM datasets, we use the linear probing strategy (He et al., 2021).",1,related,1,positive
"For evaluation on CIFAR-10, CIFAR-100, Tiny Imagenet-100, and chest X-ray datasets, we use the linear probing strategy (He et al., 2021).",1,related,1,positive
"3 exhibits the performance of PARSeq with CLIPTER, when leveraging the vision-based image encoders of DiNO, ViT-MAE and OWL-ViT, and when using the visionlanguage models of CLIP, BLIP and GIT.",1,related,1,positive
"When transferring the representation to ImageNet-1K [18], we follow the widely used fine-tuning recipe introduced by [5], [24].",1,related,1,positive
"For single-modal SSL methods, we choose MoCoV3 [14] and SimCLR [11] as representative discriminative methods, and MAE [24] as a representative generative method.",1,related,1,positive
"During pre-training, input images are resized to 224× 224 and we set random mask ratio to 75% following [28].",1,related,1,positive
"Among numerous architecture designing spaces, without loss of generalization, we adopt an asymmetric encoderdecoder architecture following MAE [28] and a dualencoder architecture following CLIP [47] for their flexibility.",1,related,1,positive
We follow most of setups in [28] for fine-tuning.,1,related,1,positive
"Meanwhile, following MAE [28], we randomly mask a large portion of image patches and leave the remaining patches to be",1,related,1,positive
We choose MAE [28] and CLIP [47] as representative methods of masked image modeling and vision-language contrastive learning.,1,related,1,positive
"Inspired by MAE [12], we propose a naive background reconstruction method (NBR).",1,related,1,positive
"Therefore, we utilize a mature but large Vision Transformer (MAE-ViT-large [12]) for background reconstruction.",1,related,0,negative
"3) Comparison With Feature Extraction: We use pre-trained MAE10 [72] and SimCLR11 [73] models to extract features for the NUS-WIDE dataset and obtain 768- and 2,048dimensional feature representations respectively.",1,related,1,positive
"3) Comparison With Feature Extraction: We use pre-trained MAE10 [72] and SimCLR11 [73] models to extract features for the NUS-WIDE dataset and obtain 768- and 2,048- dimensional feature representations respectively.",1,related,1,positive
"We compute the loss only on masked parts, similar to BERT (Devlin et al., 2019) and MEA (He et al., 2021), so the model output on unmasked parts is qualitatively worse.",1,related,1,positive
"Our research is based on masked autoencoding, which is a form of more general denoising autoencoding (He et al., 2021).",1,related,1,positive
"We also observe calculating loss values only on masked patches gives higher accuracy (row 6), which is consistent with He et al. (2021).",1,related,1,positive
"We try to conclude that in essence, the difficulty is rooted in the gap between language and vision in terms of data processing (Bateman, 2014; He et al., 2021).",1,related,1,positive
"One may therefore ask: what exactly is preventing the application of BERT to convnets? We try to conclude that in essence, the difficulty is rooted in the gap between language and vision in terms of data processing (Bateman, 2014; He et al., 2021).",1,related,1,positive
"We then remove the hierarchical design (row 4), which results in a single-scale masked modeling that is commonly used
for transformers (Devlin et al., 2018; Bao et al., 2021; He et al., 2021; Xie et al., 2021).",1,related,1,positive
"As vision encoder, we consider (1) ViT-B/16 [7] (patch size of 16×16 pixels) with pre-trained weights from self-supervised MoCo-v3 [5], DINO [2] and MAE [10], all trained on IN-1K but without any labels.",1,related,1,positive
"For the downstream task of OAR segmentation, we employed the ViT backbone and UperNet [20] decoder as the encoder and decoder parts of the segmentation model, following the implementation described in a previous work [10].",1,related,1,positive
"Specifically, we replace the multi-crop strategy with a random masked sampling strategy, consistent with pioneering SSL approaches that use masked image modeling with ViT in a patch-wise manner [9], [10].",1,related,1,positive
"Then for enhancing the learning of visual representations, we introduce a masked autoencoder (MAE)[8] branch, which is parallel to the SSL framework and they share the same encoder.",1,related,1,positive
"Therefore, we investigate other MIM methods besides MAE[8] and observe that LoMaR[18] can further boost the model performance by 0.",1,related,1,positive
Therefore we introduce a masked autoencoder (MAE)[8] branch to enforce the visual representation learning and help ViT generate more accurate pseudo,1,related,1,positive
"As for the MAE branch, we follow the default settings of [8].",1,related,1,positive
"For data augmentation, we follow the settings in MAE [18].",1,related,1,positive
"In this work, we adopt MAE [18] as the MIM model due to its popularity and simplicity.",1,related,1,positive
"Therefore, instead of following the common transferring assumption, we revisit the old good idea of training with indomain egocentric data only, but this time in light of the development of recent data-efficient training methods, such as masked autoencoders [33, 63, 28] as well as the scale growth of egocentric data collections (e.",1,related,1,positive
Our method applies the original MAE [33] and video MAE [28] algorithms.,1,related,1,positive
"4, we visualize the MAE [33, 28] reconstruction results on a few Ego4D [31] examples with a ViT-B [25] trained for 200 epochs without per-patch normalization.",1,related,1,positive
"In image representation, MAE [24] and SimMIM [25] use the random masking strategy to discard or replace the selected patches and in this paper, we adopt the former approach for selected frames.",1,related,1,positive
"It is worth noting that, unlike MAE [24] or SimMIM [25], we do notmake predictions for themasked sequences at the pixel level, choosing to reconstruct the input at the representation level in an implicit way and ensuring that the pair of masked sequences can be as close as possible in the representation.",1,related,1,positive
"Inspired by the masked auto-encoders [24], we consider the masking mechanism as a way of data augmentation and combine it with contrastive learning framework; the frame-",1,related,1,positive
"Compared to the plain ViT pre-trained with MAE [31], our approach performs similarly up to the Large model regime, despite using much fewer parameters (198M vs 307M).",1,related,0,negative
"We compare our approach to previous masked auto-encoder methods [3, 31, 77], which were all designed for transformer-based models.",1,related,1,positive
"To perform this analysis, we randomly select 1,000 images from different classes in the ImageNet-1K validation set and extract the high-dimensional features from each layer of different models, including the FCMAE models, the ConvNeXt supervised model [52] and the MAE pretrained ViT model [31].",1,related,1,positive
We retrained the CSWin transformer without redesign and original transformer [9] separately and compared them with our redesigned,1,related,0,negative
"In this work, we tackle an active visual exploration problem using a vision transformer model [15] as the architectural backbone.",1,related,1,positive
"Table 3: MAE Attention as glimpse: We use CLS attention of pre-trained MAE[15] as the glimpse map and use locations corresponding to the max value, min value, median value, and random value as the next glimpse location.",1,related,1,positive
We study the use of CLS attention map as an alternative for the learned glimpse map through a set of heuristics to use the base MAE model for active visual exploration.,1,related,1,positive
"Since the pretext task for training MAE, i.e. random masking and pixel value prediction for masked regions, resembles the partial observability constraint that we are trying to solve, we find MAE encoder to be best suited for our context extractor module and the pre-trained weights to be transferable for our use case.",1,related,1,positive
"We build our context encoder on the mask-auto-encoder (MAE) [15] model, pre-trained as a masked-image-model for the image reconstruction pretext task.",1,related,1,positive
"In this work, we evaluate our method on the widely studied task of image reconstruction,
We first compare against a baseline where the base MAE model with random glimpse selection is finetuned on the SUN360 and ADE20K datasets, denoted by ‘Random glimpse’ in Table 1.",1,related,1,positive
"Next, in addition to finetuning the task module and the context extractor, initialized with MAE weights, we train the glimpse selection module to predict the loss of the task module (i.e reconstruction loss).",1,related,1,positive
"While the random selection outperforms other heuristics for base MAE, we see that random selection of glimpse performs inferior to a learned glimpse selection policy, Table 1.",1,related,1,positive
Analysis for the importance of self-supervised feature: Our encoder and decoder are initialized with pretrained self-supervised weights [15].,1,related,1,positive
"As we intend to evaluate the use of base MAE for active vision, we do not finetune it on SUN360 dataset.",1,related,0,negative
9% on Imagenet 1000 class classification [15].,1,related,0,negative
"We show that vision transformer mod-
els, and in particular MAE, trained on large unlabelled data can replace contemporary CNN-based counterparts.",1,related,1,positive
We therefore use MAE’s decoder as our task module.,1,related,1,positive
Our context extractor is a ViT [12] initialized with MAE’s encoder weights.,1,related,1,positive
"2) Generating Input Tokens: According to ViT [52], a trainable and lightweight projection should be applied to embedding patches, denoted as F .",1,related,1,positive
"Baselines We benchmark the performance of our two baselines - Average Pairwise Feature Distance (Distance) and Cluster Assignment Distribution (Cluster) using five different feature extractors for each: pixels (φ ), three intermediate ResNext-101 [32] convolutional feature layers (φ, φ, and φ), and features from the encoder output of a Masked Autoencoder [13] (φ ).",1,related,1,positive
"Finally, φ extracts the per-patch features produced by the encoder of a large Transformer model (we used Masked AutoEncoder [13]).",1,related,1,positive
"Finally, ϕT extracts the per-patch features produced by the encoder of a large Transformer model (we used Masked AutoEncoder [13]).",1,related,1,positive
"Baselines We benchmark the performance of our two baselines - Average Pairwise Feature Distance (Distance) and Cluster Assignment Distribution (Cluster) using five different feature extractors for each: pixels (ϕP ), three intermediate ResNext-101 [32] convolutional feature layers (ϕR(1), ϕR(2), and ϕR(3)), and features from the encoder output of a Masked Autoencoder [13] (ϕT ).",1,related,1,positive
"Specifically, we pre-train models based on MAE [14] for 100 epochs on ImageNet-1k [17] with uniform masking and plot the corresponding convergence curve on the validation set during fine-tuning.",1,related,1,positive
"We pre-train models on ImageNet1K [17] following the settings of MAE [14], where the decoder TABLE I ABLATION STUDY ON THE ADAPTIVE LEARNING RATE SCALE RULE.",1,related,1,positive
1) Disjoint Masking: We evaluate DM based on MAE [14] with normalized pixels as reconstruction targets and only pretrain 100 epochs for fast ablation study on ImageNet-1K.,1,related,1,positive
"Following a standard SSL evaluation recipe [14], [21], we conduct end-to-end fine-tuning or linear probing for classification on ImageNet-1K and transfer learning for object detection/instance segmentation on COCO [56] and semantic segmentation on ADE20k [57].",1,related,1,positive
"In Table III, we evaluate the training efficiency of DM based on several MIM methods with their original pre-training recipes, including MAE [14], BEiT [10], SimMIM [15], and MaskFeat [16].",1,related,1,positive
"Another line of work is completionbased [25, 31, 36, 55, 58, 60] methods, which get inspiration from Masked Autoencoders [14].",1,related,1,positive
"Particularly, in the self-supervised setting, e.g., as in MAE (He et al., 2022), we identify a condition number that is pivotal to solving downstream tasks.",1,related,1,positive
", as in MAE (He et al., 2022), we identify a condition number that is pivotal to solving downstream tasks.",1,related,1,positive
Scale-MAE is a selfsupervised pretraining framework based on the Masked Autoencoder (MAE) [25].,1,related,1,positive
"As a result, we develop an asymmetric encoder-decoder architecture similar to MAE [15].",1,related,1,positive
"Therefore, in this framework, we construct a semi-supervised cross-algorithm ensemble method for lesion identification in UW-OCTA DR images based on MAE, ConvNeXt, and SegFormer algorithms[10,24,13,5,6].",1,related,1,positive
"In this framework, first, we pre-train the MAE algorithm [10] on the DR grade assessment dataset in the diabetic retinopathy analysis challenge (DRAC) 2022.",1,related,0,negative
"After the pre-training, we would extract the backbone of the MAE encoder (also considered as pre-trained MAE-ViT) [9,10].",1,related,0,negative
"Then, the MAE encoder [10] performs feature extraction on the visible patches subset of the image.",1,related,1,positive
"We conduct experiments using MVTN with ResNet-50 [37] and ViT [21] as backbone networks, starting from scratch or using weights from ImageNet [73], Dino [9], and Masked Autoencoders (MAE) [36] as initial weights.",1,related,1,positive
"Figure 1: Architecture of the Masked Autoencoder (MAE) in [8], with Vision Transformer (ViT) backbone.",1,related,1,positive
We divide an image into regular non-overlapping patches as in [8] and then calculate the gradient sum of each patch and rank them by its gradient sum.,1,related,1,positive
"Inspired by He et al[7], we propose a full convolutional neural network[16] based cross-modal text feature extractor(TFE).",1,related,1,positive
"One is a decoder (He et al., 2022) which reconstructs the input image, the other is a linear classifier.",1,related,1,positive
"Secondly, we compared the classical MIM-based methods such as MAE (ViT-B) (He et al., 2022) and SimMIM (Swin-B) (Xie et al.",1,related,1,positive
"We denote the methods with * when they are adapted to Point Transformer backbone, e.g., “PointMAE [39]* + PointTrans”.",1,related,1,positive
"With the same consistency loss, PointMAE gets 71.0%, which is 0.9% lower than ours.",1,related,0,negative
"In our experiments, RGMIM and MAE used the ViT-Base model.",1,related,0,negative
"Compared with the vision transformer-based methods RGMIM [69] and MAE [70], although we use the traditional and straightforward ResNet model, our method outperformed them, especially when the amount of annotated data was significantly reduced.",1,related,1,positive
"We employ the vanilla ViT model (Dosovitskiy et al., 2021) as the backbone of our audio SSL models without heavy
structure engineering, and apply the speed-up technique proposed in He et al. (2022).",1,related,1,positive
"A learnable corruption embedding e[M] is used to replace the masked position, with which the corrupted representation ZM = 1(M) e[M] + 1(1−M) T is input to encoder (Devlin et al., 2019) or decoder (He et al., 2022b)2.",1,related,1,positive
"We find that this leads to an inferior result, consistent with the observation in 2D that data of low semantics requires a non-trivial decoder for modeling purpose (He et al., 2022b).",1,related,1,positive
"The objective is to train the a student encoder fS to predict/reconstruct the output from a teacher encoder fT , where the teacher could be a discrete variational autoencoder (dVAE) (Bao et al., 2022) or simply identity mapping (He et al., 2022b).",1,related,1,positive
"D (7)
With this parameter-efficient prompt tuning strategy, we are able to tune the pretrained foundational Transformer while preserving as much pretrained knowledge as possible (He et al., 2022a).",1,related,1,positive
"To comprehensively compare our proposed model with other state-of-the-art methods, We use four widely used metrics to evaluate our method: structuremeasure (Sm) [6], E-measure (Em) [7], weighted F-measure (Fωβ ) [19], and mean absolute error (MAE).",1,related,1,positive
"We note that this budget constraint is similar to MAE [10] and other random masking schemes, which choose to randomly mask out a fixed 75% of the image.",1,related,1,positive
"To learn robust audio-video representations, we go beyond raw input reconstruction in uni-model MAEs [2, 4, 55], multimodal MAE [56], and CAV-MAE [41].",1,related,1,positive
"Following MAE [2], we only keep the pre-trained encoders and use the average-pooled top-layer outputs for classification.",1,related,0,negative
We notice a concurrent and independent study CAV-MAE [41] uses inter-modal contrastive objective and MAE [2] to reconstruct raw inputs.,1,related,1,positive
"For the visual branch, we either randomly initialize it or initialize it with the self-supervised MAE [2] pre-trained on ImageNet (compared in Table 6).",1,related,1,positive
"Furthermore, inspired by He et al. (2021), we explore enhancing the visual encoder via randomly masking the input image tokens and then reconstructing them, which can help reduce the computation cost during training and boost visual embedding by maintaining low-level visual information.",1,related,1,positive
MAE means MAE unsupervised pretraining [30] on the MillionAID [54].,1,related,0,negative
"We followed the data augmentation scheme of the MAE[21] fine-tuning phase, and the rest of the settings were kept consistent with the pre-training phase.",1,related,0,negative
"In Table 11, we compare our results with previous supervised pretraining methods[18, 29], self-supervised MIM methods [2, 10, 3, 7, 5] and CLIP-based MIM methods [25, 31, 13, 24] (i.",1,related,1,positive
We extend the masked-autoencoding framework [41] to learn audiovisual feature representations that can be leveraged for both multimodal and unimodal downstream tasks.,1,related,1,positive
"In the masked autoencoding framework [41], the input, x, is tokenised following previous supervised learning setups [6, 26, 33].",1,related,1,positive
"In the masked autoencoding framework [10, 29, 41], the decoder is another transformer that reconstructs the masked tokens given the encoded tokens as context.",1,related,1,positive
"Additional standardisation may also be applied to x̃ [41, 76].",1,related,0,negative
"Our approach is inspired by the masked autoencoding framework [10,41], which itself is based on similar masked data modelling approaches in NLP [24] and earlier works ar X iv :2 21 2.",1,related,1,positive
We begin with an overview of masked autoencoders [41] and transformers in vision in Sec.,1,related,1,positive
We follow the training strategy in MAE [31] and VideoMAE [21] for image teachers and video teachers respectively.,1,related,0,negative
"In this paper, we follow the decoupled encoder-decoder transformer architecture in MAE [31] due to its effectiveness and simplicity.",1,related,1,positive
"For obtaining spatial targets, we adopt the vanilla image ViT pretrained by masked image modeling [31] on the image dataset (e.",1,related,1,positive
"To show the feasibility of the concept, we explore the adoption of masked autoencoder (MAE) structure [21] for image reconstruction, which achieves state-of-the-art performance in the self-supervised learning regime.",1,related,1,positive
"1, we compare the pooling strategies based on the scratch training and fine-tuning on pre-trained models by Self-Supervised Learning (SSL), including MAE [27], BeiT [3], SimMIM [77] and Data2Vec [2].",1,related,1,positive
"Every SSL model are pre-trained with ImageNet-1K, except BeiT [3] with † used ImageNet-22K.",1,related,1,positive
We ablate the type of visual representation and prior use by trying an initialization using the VGG16 network [68] (VideoDex-VGG) and the MVP network [7] [69] (VideoDex-MVP) based representation trained for robot learning.,1,related,1,positive
"Inspired by the representation bottleneck and the mask-based method MAE [45], we propose a training pipeline based on the mask to boost data augmentation.",1,related,1,positive
"Inspired by the representation bottleneck and the mask-based method MAE [45], we propose a training",1,related,1,positive
"We also compare Deep Incubation with the recently proposed improved E2E baselines in [15], where a systematically hyper-parameter search is performed on training configurations.",1,related,1,positive
We follow the design choice of MAE [23] and VideoMAE [50] that skips the video mask token [M] in the encoder and then insert it in the decoder.,1,related,1,positive
"Following [23,50], we only apply the encoder on visible video tokens, a small subset (e.",1,related,1,positive
"Inspired by the success of MAE [18] in 2D images, we develop the masked autoencoder for self-supervised learning on LiDAR point clouds, as shown in Figure 4.",1,related,1,positive
"To set a baseline with the transformer decoder, we follow MAE [18] and some existing works [12,18,19,38,73] to design the pipeline, as shown in Figure 5(a).",1,related,1,positive
"Then, we preserve the encoder part of MAE as our shared face encoder and fix it all the time during training.",1,related,0,negative
"Thus, we opt to employ a pre-trained masked autoencoder (MAE) (He et al. 2022) to extract facial features that better capture facial appearances and identity information.",1,related,0,negative
"In contrast to the ViT decoder in MAE, we find the convolutional decoder achieves more realistic results.",1,related,1,positive
"Compared to the compact latent code of StyleGAN2 (Karras et al. 2020) and the identity embedding, the latent space of MAE can better capture facial appearances and identity information, because masked training requires reconstructing masked image patches from visible neighboring patches, thus ensuring each patch embedding contains rich topology and semantic information.",1,related,1,positive
The encoder is designed following MAE (He et al. 2022) and pre-trained on a large-scale face dataset using the masked training strategy.,1,related,0,negative
"As for F swa, we first pre-trained the face encoder following the training strategy of MAE on our face dataset.",1,related,0,negative
"To verify the superiority of using the latent representation of MAE, we train a new model which adopts the identity embedding as the identity representation and employs AdaIN as the injection method.",1,related,1,positive
"We first train our face masked autoencoder, following the same settings in MAE4 (He et al. 2022).",1,related,0,negative
"To demonstrate the robustness of VQT to different pretraining setups, we also evaluate VQT on self-supervised (MAE) [20] and image-language (CLIP) pre-trained [39]",1,related,1,positive
"Moreover, in order to demonstrate the robustness of the compatibility, we evaluate performance on three different pre-trained backbones: self-supervised pre-trained (MAE with ImageNet1K) [20], image-language pre-trained (CLIP) [39] and supervised pre-trained (ImageNet-21K).",1,related,1,positive
"To demonstrate the robustness of VQT to different pretraining setups, we also evaluate VQT on self-supervised (MAE) [20] and image-language (CLIP) pre-trained [39]
backbones.",1,related,1,positive
We make the following modifications to the MAE decoding process to customize it for document image generation and our task unification framework: (4.a) Cross-Attention with Character Embeddings.,1,related,1,positive
"Next, we describe the MAE decoding process.",1,related,1,positive
"For the vision decoder, we adopt the decoder of MAE [14] and directly generate the image pixels with text and layout information.",1,related,1,positive
We adopt the MAE objective [14] for vision selfsupervised learning.,1,related,0,negative
"Thus, here we directly apply a standard masked image modeling (MIM) pipeline [5, 19, 48] for training, illustrated in Figure 2.",1,related,1,positive
"We show that LOCA yields improved performance over state-of-the-art supervised [53,60,65] and unsupervised [13,17,34,84] representation learning methods for ViTs when transferred to 11 diverse and challenging semantic segmentation benchmarks.",1,related,1,positive
"In terms of training efficiency, based on our implementation, one LOCA epoch takes 17.4 minutes while one MAE epoch takes 5.7 minutes.",1,related,0,negative
"Indeed, we have observed that freezing the backbone and training a linear classifier on top of MAE features perform very poorly [34].",1,related,1,positive
"In this section, we compare LOCA to popular state-ofthe-art SSL models for ViTs: DINO [13], MoCo-v3 [17], MAE [34] and iBOT [84].",1,related,1,positive
"Under the longer training schedule (800 epochs), our model reaches 83.9% accuracy, 0.4% higher than MAE (He et al. 2021) and
0.9% higher than RandSAC (Hua et al. 2022) (a concurrent autoregressive work of ours).",1,related,0,negative
"For linear probing, we use the feature of the last block and adopt an extra BatchNorm (Ioffe and Szegedy 2015) layer before the linear classifier following MAE.",1,related,1,positive
"We followed the pre-training parameters in MAE (He et al. 2021) and MaskFeat (Wei et al. 2021) without using color jittering, drop path, and gradient clip.",1,related,1,positive
"Our model pretrained with 300 epochs achieves the same accuracy as MAE (He et al. 2021) pretrained with 1600 epochs, which indicates that our method converges faster in the pretraining stage.",1,related,1,positive
"We observe that our method achieves 47.8 mIoU, which is slightly lower than MAE by 0.3, but higher than all others.",1,related,0,negative
"We note that BEIT and MAE are pretrained with 1600 epochs and use grid-search to find the best hyperparameters, while we only pretrain 800 epochs and don’t tune any parameters in the fine-tune stage due to limited access to computation.",1,related,1,positive
"The patch-based iGPT achieves 82.70 Top1 accuracy, which is higher than DeiT-base (Touvron et al. 2021)(81.8) but still lags behind the MIM method (e.g. 83.6 for MAE).",1,related,0,negative
"While, our proposed stochastic autoregressive image modeling, SAIM, utilizes all the information of the image to generate clear images, and achieve better fine-tuning accuracy than MAE on ImageNet-1K.",1,related,1,positive
"Method Plane Bcycl Bus Car Horse Knife Mcyle Persn Plant Sktb Train Truck Mean
CDAN [48]
R es
N et 85.2 66.9 83.0 50.8 84.2 74.9 88.1 74.5 83.4 76.0 81.9 38.0 73.9 MCC [36] 88.1 80.3 80.5 71.5 90.1 93.2 85.0 71.6 89.4 73.8 85.0 36.9 78.8 SDAT [57] 95.8 85.5 76.9 69.0 93.5 97.4 88.5 78.2 93.1 91.6 86.3 55.3 84.3 MIC (SDAT) 96.7 88.5 84.2 74.3 96.0 96.3 90.2 81.2 94.3 95.4 88.9 56.6 86.9 TVT [87]
V iT 92.9 85.6 77.5 60.5 93.6 98.2 89.3 76.4 93.6 92.0 91.7 55.7 83.9 CDTrans [85] 97.1 90.5 82.4 77.5 96.6 96.1 93.6 88.6 97.9 86.9 90.3 62.8 88.4 SDAT [57] 98.4 90.9 85.4 82.1 98.5 97.6 96.3 86.1 96.2 96.7 92.9 56.8 89.8 SDAT w/ MAE [25] 97.1 88.4 80.9 75.3 95.4 97.9 94.3 85.5 95.8 91.0 93.0 65.4 88.4 MIC (SDAT) 99.0 93.3 86.5 87.6 98.9 99.0 97.2 89.8 98.9 98.9 96.5 68.0 92.8",1,related,0,negative
"Moreover, we compare the image imputation performance of our Edge-MAE with vanilla MAE [24].",1,related,1,positive
"We adopt the partial fine-tuning strategy inspired by [24], i.",1,related,1,positive
"Inspired by [16,23,46], we perform mask sampling on video and text to achieve end-to-end video-text alignment.",1,related,1,positive
"Instead of blindly applying the mask-thenprediction paradigm from MAE, we propose a maskedthen-alignment paradigm, namely Masked Contrastive Pretraining, for efficient video-text alignment.",1,related,1,positive
"Without blindly applying mask-then-prediction paradigm from MAE, we explore the masking contrastive mechanism based on
the video language domain, and propose a mask-thenalignment paradigm to efficiently learn a multimodal alignment.",1,related,1,positive
"Another common tactic is fulfilled with sinusoidal mapping Fsine [20,54], through which p is generated on-the-fly by a fixed function dependent on NH , NW and D. Due to space limitation, we provide explicit expression in Appendix B.1.",1,related,1,positive
We demonstrate the explicit mapping function Fsine for sine-cosine positional embedding p as follows.,1,related,1,positive
The Masked Autoencoder (MAE) method [29] further takes advantage of masking to reduce training time and memory.,1,related,0,negative
"Our fine-tuning implementation follows MAE [29], with the learning rate tuned for each entry.",1,related,0,negative
MAE sparsely applies the ViT encoder [20] to visible content.,1,related,1,positive
"Our work is related to MAE and its vision-language extensions [23, 41, 31, 19].",1,related,1,positive
"We do not use a reconstruction loss, unlike MAE [29].",1,related,0,negative
"Inspired by the sparse computation of Masked Autoencoders (MAE) [29], we randomly remove a large portion of image patches during training.",1,related,1,positive
"By default, we apply our models on intact images at inference-time, similar to [29].",1,related,1,positive
We use MAE pre-training for most experiments by default unless otherwise specified.,1,related,0,negative
"Thus, for the model initialized from MAE pre-training, we increase finetuning iterations to 180k and batch size to 64.",1,related,0,negative
"In experiments, we explore two pre-training schemes: 1) MAE pre-training: The ViT backbone is initialized from the self-supervised MAE [13] trained on ImageNet-1K [7], while the rest of the model parameters are randomly set; 2) GIT pre-training: The ViT backbone and text decoder are initialized from the pre-trained image VL model GIT [33] and the rest are randomly set.",1,related,1,positive
We find MAE pre-training can alleviate overfitting and benefit from more training epochs as discussed in [20].,1,related,0,negative
"Additionally, a special Classification (CLS) token is appended before adding the sinusoidal positional embedding as per convention [4], [5], [7] such that the final dimension of the input data after patchification is (S+1)×D.",1,related,1,positive
"Inspired by the masked auto-encoder (MAE) [21], we modified and applied it to the adversarial environment.",1,related,1,positive
"Similar to MAE [25], we found that applying a batch normalization layer [30] without affine transformations is beneficial for VideoMAE models.",1,related,1,positive
"Unless stated otherwise, we train our models for 500 epochs (for example, training with VMAEB on SSV2 takes 137 minutes with one 3090 GPU) with a batch size of 512 and use all 16 clips.",1,related,0,negative
"Our SCALE linear with SVT backbone beats the previous state of the art (71.8% vs. 71.5% [18]) and SCALE ft can even improve the accuracy of VMAEBft , which is a strong supervised model, from 81.5% to 81.84%.",1,related,1,positive
"We choose ρBYOL for their excellent linear performance, SVT for the usage of ViT [14], and VMAE for showing 1) the applicability of our proposed method to MAE models, 2) the scalability of our method to larger models, and 3) possibility of using supervisedly fine-tuned models as our backbone.",1,related,1,positive
We also used a backbone pretrained and fine-tuned on SSv2 (VMAEBSSv2) for the SSv2 experiment to show the universality of SCALE with respect to the pretraining dataset.,1,related,1,positive
"Since our clip representations are somewhat abstract representations of the video, we expect the optimal masking ratio to be close to NLP models rather than video MAEs.",1,related,1,positive
"Pretrained backbones: We use the pretrained checkpoints of ρBYOL [18], SVT [44], and three variants of VideoMAE [54] (base(B), large(L), and fine-tuned base(FT)).",1,related,1,positive
"With SCALE k-NN, we see a consistent improvement over the baseline and find that pre-trained MAE-based models greatly benefit from our training.",1,related,0,negative
We even improve the supervised model trained on SSv2 (VMAEBSSv2).,1,related,1,positive
"Then, with this frozen visual encoder, we used the same feed forward architecture, Q-function parameterization, and training objective (CQL with C51) as scaled QL to
finetune the MAE network.",1,related,1,positive
"As before, we also evaluate fine-tuning performance using the representations obtained via masked auto-encoder pre-training (He et al., 2021; Xiao et al.).",1,related,1,positive
"In Figure 7, we present our results for offline fine-tuning on 5 games from Lee et al. (2022), ALIEN, MSPACMAN, SPACE INVADERS, STARGUNNER and PONG, alongside the prior approach based on decision transformers (“DT (pre-trained)”), and fine-tuning using pre-trained representations learned from state-of-the-art self-supervised representation learning methods such as contrastive predictive coding (CPC) (Oord et al., 2018) and masked autoencoders (MAE) (He et al., 2021).",1,related,1,positive
"For MAE, we first pretrained a vision transformer (ViT-Base) (Dosovitskiy et al., 2020) encoder with 80M parameters trained via a reconstruction loss on observations from multi-game Atari dataset and freeze the encoder weights as done in prior work (Xiao et al.).",1,related,1,positive
MAE is a more recent self-supervised approach that we find generally outperformed CPC in this comparison.,1,related,1,positive
"For the MAE implementation, we used the Scenic library (Dehghani et al., 2022) with the typical configuration used for ImageNet pretraining, except using 84× 84× 4 sized Atari observations, instead of images of size 224× 224× 3.",1,related,1,positive
We train the MAE for 2 epochs on the entire multi-task offline Atari dataset and we observe that the reconstruction loss plateaus to a low value.,1,related,1,positive
"We train the completion network on the 11,426 complete skeletons using a masked auto-encoder strategy [32] where the missing keypoints are masked at the input and will be predicted using the unmasked keypoints.",1,related,1,positive
", 2022) and masked autoencoders (He et al., 2021) are all using patches as input.",1,related,1,positive
"Similar to [7], we utilized ADAMW [13] with learning of 1.",1,related,1,positive
"1, our framework consists of three asymmetric vision transformer inspired by the work of [7].",1,related,1,positive
"Inspired by these two papers, we add a mask-based random reconstruction module to the input images: x ← g(x), where g is a masked auto-encoder [16], defined by:",1,related,1,positive
"Here, we examine whether an additional reconstruction module can regularize the partial derivatives to make them semantic-aware to produce human-meaningful images [16, 41, 56].",1,related,1,positive
"The pre-training loss is computed on the masked tokens similar to MAE [29], and the final pre-training loss Lpre-train is defined as: Lpre-train = Lnode + Ledge, Lnode = ∑",1,related,1,positive
"We investigated works in NLP [17] and computer vision (CV) [29] where the pre-trained models have been dominantly used, and we find that the key to most successful pre-training models is to design simple but effective tasks that can scale well.",1,related,0,negative
"Inspired by MAE [29], we propose a bi-branch graph masking and reconstruction task for molecular pre-training.",1,related,1,positive
"Similar to MAE [29], we propose a transformer-style asymmetric encoder-decoder architecture for each branch.",1,related,1,positive
"To enable effective expansion, we explore three prior models for guided imagination: DALL-E2 [56] and Stable Diffusion [59] are advanced image generative methods, while MAE [24] is skilled at reconstructing images.",1,related,1,positive
"Thanks to strong image reconstruction abilities, our GIF-MAE applies the MAE-trained model [24] as its prior model.",1,related,0,negative
"As DALL-E2 [56] and SD are powerful in generating images, and MAE [24] excels at reconstructing images, we explore their use as prior models of GIF for data imagination.",1,related,1,positive
"We choose ViT-Base (ViT-B) [24] as the backbone of our framework for both audio and video modalities, due to its stability in performance across different data streams [34, 30, 8].",1,related,1,positive
"Inspired by the recent success of Transformers in different domains [23, 30, 34, 25], we use ViT [24] as the backbone of our framework for both audio and visual modalities.",1,related,1,positive
"Further, we drop the masked tokens x before feeding the input to θae for computational efficiency [3, 34].",1,related,1,positive
"Inspired by the recent success and scalability of pretraining with masked reconstruction in different domains [23, 13, 65, 30, 12, 34, 50, 64], we adopt masked data mod-",1,related,1,positive
"Our suggested method, MAEDAY, addresses FSAD by using Masked AutoEncoder (MAE) [9], a model trained for general image completion based on partial observations.",1,related,1,positive
": To investigate the impact of neighbors on network performance, we set the number of neighbors [2, 8, 16, 32, 64] and compared them in the table II.",1,related,1,positive
"To learn visual representations of images in a self-supervised manner, we apply Masked Autoencoder (MAE) which is
trained to reconstruct the randomly masked image patches.",1,related,1,positive
"A comparison between the attention maps generated by iTPN, the variant without integral pre-training (w/o iPT), and the MIM baseline (MAE [25]).",1,related,1,positive
"We emphasize here that we directly adopted the pre-trained model from masked autoencoder (MAE) (He et al., 2022), instead of training the model ourselves.",1,related,1,positive
"Therefore, our dual transfer learning reduces training time via utilizing the public model from MAE (He et al., 2022).",1,related,0,negative
"Different from other masked prediction variants [12, 9], we found mask loss is not useful in our setting, as our goal is to obtain an scalable decision making model but not only for representation learning.",1,related,1,positive
"• multi-goal reaching: For every trajectory in the validation set, we randomly sample a start state and 5 goal states at random future timesteps from [12, 60).",1,related,1,positive
Our first key observation is that masked token prediction with random masking similar to BERT [9] and MAE [12] provides a general and flexible way for learning from unsupervised data.,1,related,1,positive
"Architecture Our encoder is a Transformer [33] but applied only on visible, unmasked states and actions, similar to MAE [12].",1,related,1,positive
"As in recent unsupervised papers [1, 10, 11, 13], we compare the quality of our unsupervised keypoints to the features obtained by a fullysupervised ImageNet model.",1,related,1,positive
"An exponential moving average (EMA)[20] with α = 0.998 is implemented, and the shadow weight is updated after each training step.",1,related,1,positive
The structural configuration of SIVT follows the design of the MAE-base but we reduce the embedding dimension to 240 for efficient computation.,1,related,1,positive
ST-MAE is proposed in [23] that is based on the MAE [19] utilizing the Siamese encoder,1,related,1,positive
"Co-DETR with Swin-L yields 56.9% and 62.3% AP on LVIS val and minival, surpassing ViTDet with MAE-pretrained [13] ViT-H and GLIPv2 [40] by
+3.5% and +2.5% AP, respectively.",1,related,0,negative
"3% AP on LVIS val and minival, surpassing ViTDet with MAE-pretrained [13] ViT-H and GLIPv2 [40] by Method Backbone enc.",1,related,0,negative
"However, our network’s encoder and decoder are asymmetric, which indicates a significantly smaller decoder [25, 57].",1,related,0,negative
"Method Better Worse ×3, ×4 training warm-start [40] scratch std in normalization from data [25] 1.",1,related,1,positive
Implementation Details We adopt the model architecture of Masked AutoEncoder (He et al. 2022) with BERT (Devlin et al. 2018) word embeddings and language modeling head.,1,related,1,positive
Implementation Details We adopt the model architecture of Masked AutoEncoder (He et al. 2022) with BERT (Devlin et al.,1,related,1,positive
"For test-time training we do not use any augmentation, instead we construct a batch from the single point cloud sample and for Masked Autoencoder reconstruction, we randomly mask 90% of the tokens.",1,related,1,positive
"We test this approach on a diverse set of chromosome aberrations (an intra-chromosomal unbalanced abnormality: del(5q); intra-chromosomal balanced rearrangements: inv(3) and inv(16), and inter-chromosomal translocations: t(9;22), t(9;11), and t(11:19)) commonly seen in",1,related,1,positive
"For example, the normal chromosome 9s from the held out pre-training folds were added to the t(9;11) aberration training set for normal vs aberrant chr9 identification and likewise for the remaining aberration datasets.",1,related,0,negative
"Similarly, (D) shows precision-recall for de novo aberration detection based on distance to N-nearest point (here 50th) for t(9;11), t(11;19), del(5q), and t(9;22), respectively.",1,related,1,positive
"In this section, we first briefly introduce Masked Image Modeling (MIM) for image representation learning and then extend it to our video representation learning scheme, Masked Video Modeling (MVM) (§3.1 and §3.2).",1,related,1,positive
"To resolve this problem, we involve masked autoencoder [36], which applies 50% random masking to the input data.",1,related,1,positive
"Nevertheless, to compare to other pre-training strategies, we consider MAE [29] pre-trained on ImageNet [63], thus with a cosine positional embedding, a ViT-Base encoder, and with a Small decoder that is randomly initialized.",1,related,1,positive
"We observe that CroCo pre-training obtains the lowest errors, significantly outperforming the MAE pre-training and the random initialization.",1,related,0,negative
"Note that these methods, as MAE [29], regress pixel values that are normalized according to the mean and standard deviation inside each patch, we thus apply the inverse transform for
display: this means that the overall color of each patch will be correct, as it comes from the ground-truth values.",1,related,1,positive
The masking implementation follows [19]:,1,related,1,positive
"Different from [19], we reshape xmask into a masked images as input xinput ∈ RH×W×C .",1,related,1,positive
"Specifically, apart from the original optimization target, we exploit the Masked Image Modeling (MIM) [1, 19] task to mask and recover random patches during the training.",1,related,1,positive
"Although we did not achieve the best performance on OSCC and temporal localization tasks in Ego4d Challenge 2022, we believe that, by paying much more
attention to downstream task formulation and optimization, models that are pretrained on egocentric datasets under the settings of VideoMAE will further improve state-of-the-art performance on various Ego4d tasks.",1,related,1,positive
"In this report, we demonstrate that merely pretraining the model on 3rd-person view datasets (e.g. Kinetics 400[7]) under the settings of VideoMAE can achieve state-of-the-art performance on egocentric video understanding tasks including Ego4d object state change classification and Ego4d PNR temporal localization.",1,related,1,positive
"We will show that even with weights obtained on 3rd-person view datasets, VideoMAE shows great generalization ability on egocentric downstream tasks and surpass most existing methods both on OSCC and temporal localization tasks.",1,related,1,positive
"As shown in Table 1 and Table 2, by simply pretraining on Kinetics 400 under the settings of VideoMAE, we ranked 2nd place in both tasks.",1,related,0,negative
"In our experiments, we use ViT as our backbone of which weights are initialized from VideoMAE[9] pretrained on Kinetics-400.",1,related,1,positive
"In order to make this mix strategy compatible with existing pretraining tasks like Masked Image Modeling (MIM) and Image Classification (IC), we split the mask m into patches with p× p size.",1,related,1,positive
"Restrictions apply.
auto-encoder [34, 76], global/dense distillation [33, 81] and masked image modeling (MIM) [4, 5, 14, 30, 87].",1,related,1,positive
"For example, p = 16 is by default used for MIM [5,30].",1,related,1,positive
"Given that the supervision on masked patches is a regularization for the learning of CAE v2, we suppose that it may not be appropriate to adopt a high mask ratio (75% in MAE [27], 40%-50% in BEiT [3], CAE [10], and MVP [49]) for all scales of ViTs.",1,related,1,positive
"The encoder F only receives the visible patches Xv following [10, 27].",1,related,0,negative
"Our findings are different from the common sense in the current MIM methods [3,10,27] that only compute the loss on the masked patches, which is inherited from BERT [15] in the NLP areal and has been verified by most current works.",1,related,1,positive
"Following previous MIM methods [3, 10, 27, 48], CAE v2 first embeds x into a total length of N patches, which are then randomly masked by a specific proportion γ.",1,related,1,positive
Results with Masked Autoencoders (MAE).,1,related,0,negative
"As a representative example, we deploy EfficientTrain on top of MAE [22] in Table 10.",1,related,1,positive
"Additionally, to test transferability, we use the adversarially generated samples using PRIME-ResNet50 and FANVIT as classifiers, and test them on other architectures: (1) ResNet50 [17], (2) MAE [16], a generalizable and scalable asymmetric encoder-decoder architecture, (3) RegNet-
Y [14], a ResNet-type model with a regulatory model to extract complementary features, and (4) data2vec [6], a selfsupervised transformer that predicts contextualized latent representations in a self-distillation setup for any modality.",1,related,1,positive
"Additionally, to test transferability, we use the adversarially generated samples using PRIME-ResNet50 and FANVIT as classifiers, and test them on other architectures: (1) ResNet50 [17], (2) MAE [16], a generalizable and scalable asymmetric encoder-decoder architecture, (3) RegNet-",1,related,1,positive
"…we notice that there are various open-sourced image ViTs (Wightman, 2019; Touvron et al., 2021), which have been well-pretrained on huge web datasets under rich supervision such as image-text contrastive learning (Radford et al., 2021) and mask image modeling (He et al., 2022; Bao et al., 2021).",1,related,1,positive
"…of our UniFormerV2 design, we apply it on the ViTs with different pertaining methods, including supervised learning (Dosovitskiy et al., 2021; Touvron et al., 2022), contrastive learning(Caron et al., 2021; Radford et al., 2021) and mask image modeling (He et al., 2022; Bao et al., 2021).",1,related,1,positive
"We only use standard random cropping and horizontal flipping for data augmentation, following MAE [13], CAE [5], etc.",1,related,1,positive
"To restrain the feature magnitudes of teacher features, we generate the alignment target ỹ by normalizing each level of teacher features as MAE [13] does on pixel values:",1,related,1,positive
"Self-supervised Learning
We pre-trained MAE following the official implementation9 on eight NVIDIA A-100 GPUs and fine-tuned the last layer of its encoder on 16 NVIDIA V-100 GPUs.",1,related,0,negative
"We pretrain a Vision Transformer model, specifically ViT-B [13],
as MAE’s encoder for 200 epochs with a mask ratio of 0.75.",1,related,1,positive
"To verify this idea, we adopt a self-supervised learning method called masked autoencoder (MAE) [21].",1,related,1,positive
"Following MAE, we adopt the ViT as the backbone of the decoder g(·).",1,related,1,positive
"In the objective segmentation, MR SimCLR significantly improves APmask over MAE by 0.4 points (46.9 vs. 46.5).",1,related,0,negative
In this manuscript we use a ViT-B8 with the modifications proposed by He et al. (2022) where the classifier is applied after global average pooling over the vision tokens.,1,related,1,positive
"Following [18], we divide the vectorized voxels into patches which will be subsequently transformed into embeddings using a 1D convolutional layer with a stride equal to the patch size.",1,related,1,positive
"We also adopt an asymmetric architecture as in [18]: the encoder is optimized to learn effective fMRI representations, while the decoder tries to predict the masked patches.",1,related,1,positive
"Commonly, the encoder’s output is averaged, or a cls token is appended to produce a pooled 1D feature vector for downstream tasks [8,18].",1,related,1,positive
"In this work, we benchmark four representative methods MoCo, DINO, MAE, and data2vec on the proposed dataset.",1,related,1,positive
"We find 70% to be the best masking ratio, which is similar to natural images as reported in MAE paper, where 75% is the best.",1,related,0,negative
"This way, we cover a reasonably diverse set of representative methods from each model category: MoCo utilizes contrastive representative, DINO represents a distillation method, MAE is based on masked reconstruction, and data2Vec combines the masking mechanism with a joint-embedding architecture.",1,related,1,positive
We pre-train the MAE models using its default settings following the publicly available repository (https: //github.com/facebookresearch/mae).,1,related,0,negative
"For EO applications we demonstrate SSL4EOS12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec.",1,related,1,positive
"In the first stage, we pretrain the model by borrowing the idea from masked autoencoders (MAE) [11].",1,related,1,positive
Our encoder uses the same settings as ViT-B in MAE [11].,1,related,0,negative
"In the pretraining stage, we apply RandomResizedCrop to augment data, which is similar to MAE.",1,related,1,positive
Masked autoencoders (MAE).,1,related,0,negative
"To better use the pretrained knowledge, different from MAE or ViTSTR [2], which only fine-tune on the pretrained encoder, we fine-tune on both the encoder and the decoder.",1,related,1,positive
"To this end, we adopt a two-stage training strategy to train the model as follows:
In the first stage, we pretrain the model by borrowing the idea from masked autoencoders (MAE) [11].",1,related,1,positive
"However, recognizing texts is beyond the scope of MAE, and we propose a novel language-aware model to deal with it, as shown in Figure 1.",1,related,1,positive
"Different from MAE, our MVLT recognizes scene text in addition to reconstructing the masked patches.",1,related,0,negative
"The encoder of MAE is a ViT, which only operates on xu to learn the visual feature embeddings:
vu = encoder(xu), (1)
where vu ∈ RNu×D1 and D1 is the feature dimension in the encoder.",1,related,1,positive
"When finetuning the detector on COCO, we find that applying learning rate decay [1,4,5,9] for the components of the detector (encoder and decoder) gives a ∼0.",1,related,1,positive
"Then, REGCLR is instantiated by integrating masked autoencoders [12] as a representative example of a contrastive method and enhanced Barlow Twins as a representative example of a regularized method with configurable input image augmentations in both branches.",1,related,1,positive
MAE branch uses weak augmentation and masking to obtain X1 and then follows procedure introduced in MAE [12] to compute reconstructed loss for unmasked patches as LMAE to obtain X ′ 1.,1,related,1,positive
"Our implementation of pretraining is built on the repo provided in MAE [12], and the projector is implemented as a 2 layer multilayer perceptron with 1024 dimensional output.",1,related,1,positive
"For RGMIM and MAE, we employed the same settings in all experiments, except for the masking strategy.",1,related,0,negative
We also studied the masking ratio for RGMIM and MAE using hyperparameters.,1,related,1,positive
"As comparison methods, we used five SOTA selfsupervised learning methods as comparative methods, including masked autoencoder (MAE) [10], self-knowledge distillation based self-supervised learning (SKD) [47], cross-view self-supervised learning (Cross) [48], bootstrap your own latent (BYOL) [20], and simple siamese self-supervised learning (SimSiam) [21].",1,related,1,positive
"As comparison methods, we used five SOTA selfsupervised learning methods as comparative methods, including masked autoencoder (MAE) [10], self-knowledge distillation based self-supervised learning (SKD) [47], cross-view",1,related,1,positive
"In addition, we observe that RGMIM outperforms MAE in terms of robustness, especially when the masking ratio is relatively low, demonstrating the superiority of our proposed method in handling incomplete lung X-ray images.",1,related,0,negative
"Method Architecture 1% 5% 10% 50% 100% RGMIM ViT-Base 0.771 0.893 0.919 0.957 0.962 MAE [10] ViT-Base 0.754 0.875 0.903 0.948 0.956 SKD [47] ResNet-50 0.742 0.812 0.896 0.947 0.957 Cross [48] ResNet-50 0.747 0.795 0.817 0.934 0.953 BYOL [20] ResNet-50 0.683 0.754 0.790 0.933 0.954 SimSiam [21] ResNet-50 0.623 0.700 0.781 0.929 0.949 Transfer ViT-Base 0.689 0.861 0.893 0.940 0.953 Transfer ResNet-50 0.539 0.619 0.665 0.913 0.936 From Scratch ViT-Base 0.413 0.580 0.645 0.810 0.848 From Scratch ResNet-50 0.284 0.496 0.532 0.619 0.774
has layer L = 8, latent vector size D = 512, and the number of heads is 16.",1,related,1,positive
"Although existing Transformer models like ViT [32] and MAE [10] are usually trained on the large-scale dataset, We trained the ST-MAE model on the limited samples from scratch by an AdamW optimizer with a learning rate of 1e-4 and batch size of 8 for 400 training epochs, while the weights",1,related,1,positive
"Note that our ST-MAE is a feature vision Transformer that operates on the deep features of DCNN, which is slightly different from the base ViT [10], its consecutive computational process is roughly demonstrated.",1,related,1,positive
"like the MAE [10] and Intr [36], we adopted the feature-level measurement, the relaxed version of the pixel-level constraints, for better robustness.",1,related,1,positive
"1, in analogy to MAE [10], our ST-MAE has an asymmetric encoder-decoder design that reconstructs the input in feature space, yet our encoder applies a Siamese architecture.",1,related,1,positive
"In our proposed method, MobileNetV3([16]) is adopted as the backbone network.",1,related,1,positive
"In our proposed method, MobileNetV3[16] is adopted as the backbone network.",1,related,1,positive
"1) Black-Box Attack: For the attack model, we adopt a similar structure to the edge model: an MAE decoder [18] is used, and is pretrained on ImageNet.",1,related,1,positive
"We also report results for our MAE implementation, which approximately matches the original numbers reported by He et al. (2022), validating our MAE results on JFT-300M.",1,related,0,negative
For linear probing we list the hyperparameters in Table 10 for which we followed the settings in He et al. (2022).,1,related,1,positive
"For MAE pre-training, we use the same hyperparameters as listed in He et al. (2022), except for the use of Glorot uniform initialization instead of LeCun initialization as done in He et al. (2022).",1,related,1,positive
"Following He et al. (2022), only the T ′ unmasked patches are passed to the ViT encoder, which processes the two views in parallel.",1,related,1,positive
"Corroborating the findings of He et al. (2022), we find it best to only compute the reconstruction loss on masked patches:
Lrec = 1
2n ∑ v=1,2 n∑ i=1 ‖Mvi ◦ (xvi − x̂vi )‖22
where ◦ multiplies all pixels in the tth patch of the residual image xvi − x̂vi by (Mvi )t ∈ {0, 1}.",1,related,1,positive
Decoder architecture: Our decoder architecture is the same as He et al. (2022).,1,related,1,positive
"However, when we apply generative pretrained backbones, e.g. BERT (Kenton and Toutanova, 2019) and MAE (He et al., 2022), to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information for classification (Wallat et al., 2020; Yosinski et al., 2014).",1,related,1,positive
"However, when we apply generative pretrained backbones, e.g. BERT (Kenton and Toutanova, 2019) and MAE (He et al., 2022), to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information…",1,related,1,positive
Our model-based image augmentation method was implemented based on the official codes of the MAE [18] and ViTPose [21].,1,related,0,negative
"To solve the problem of loss and deformation of data acquired by clients, we used a masked image-encoding method that learns image representations corrupted by masking [7, 8].",1,related,1,positive
"Following [13], we apply an end-to-end fine-tuning mechanism instead of linear probing to obtain the highest performance.",1,related,1,positive
"Instead of random initialization, we initialize the ViT encoder using the ImageNet-1k pre-trained encoder released by [13].",1,related,1,positive
"(ii) MAE-IN1k refers to fine-tuned from the ImageNet-1k [14] pre-trained MAE [13], where we use the same fine-tuning settings as that of MAE-Face.",1,related,1,positive
"Among the various techniques proposed for self-supervised visual representation learning, we opt to adopt masked autoencoder (MAE) [13] as our baseline.",1,related,1,positive
"We also test the impact of patch-wise normalization (w/ norm), which has also been studied in [13].",1,related,1,positive
"CONCLUSIONS In this paper, based on MAE[16] and combined with convolution, we propose a transformer image inpainting model to solve the image inpainting problem of irregular missing areas.",1,related,1,positive
"In this paper, based on MAE, we propose a transformerbased inpainting model for irregular missing images.",1,related,1,positive
"By utilizing MAE pretraining, ProContEXT outperforms the recent SOTA method, OStrack [5], by 0.9%, 1.5%, and 2.1% for AO, SR0.",1,related,0,negative
We implement the proposed ProContEXT using PyTorch and utilize the ViT-base backbone that is pre-trained by MAE [28].,1,related,1,positive
Implementation Details We implement the proposed ProContEXT using PyTorch and utilize the ViT-base backbone that is pre-trained by MAE [28].,1,related,1,positive
"As mentioned in Section 1, we follow a similar approach to the work put forth in [7], where masked autoencoders reconstructed images with large amounts of masking.",1,related,1,positive
"Unlike [7], we don’t remove the masked patches from the input to the encoder and provide positional information to the input of the decoder.",1,related,1,positive
"We denote the former as “speech branch” and the latter as “text branch”, where the former can be viewed as a masked autoencoder [28] and the latter is TTS.",1,related,1,positive
"Specifically, we propose an SSL model, MAEEG, inspired by both BENDR and the MAE model proposed in computer vision [He et al., 2022], to compare with BENDR and to broaden our understanding of reconstruction-based SSL for EEG.",1,related,1,positive
This study was inspired by MAE [1] for an MIM and Bootstrap Your Own Latent [12] (BYOL) as a framework for directly learning la-,1,related,1,positive
"While we use the same positional encoding as MAE [1], we tested various masking ratios as discussed in Section 4.",1,related,0,negative
"In this work, we capitalize in particular on the masked autoencoding of [2] and investigate using it for object-centric learning.",1,related,1,positive
"Then, exactly as in MAE [2], we add a learnable mask token at the positions of the masked tokens and the sine-cosine position embeddings.",1,related,1,positive
"We therefore choose the random masking strategy, exactly as in MAE [2].",1,related,1,positive
"Just as in MAE [2], we add fixed sine-cosine postional encodings from [23] to the embeddings of the patches.",1,related,1,positive
Our model has a narrower bottleneck in comparison to MAE [2].,1,related,0,negative
Our model is inspired by the transformer-based masked autoencoder [2] with modifications to enable end-to-end unsupervised multi-object segmentation and representation learning.,1,related,1,positive
"More specifically, we adapt the Masked Autoencoder (MAE) [2] design and modify it to for explicit object-centric representation learning and segmentation.",1,related,1,positive
"Similarly, while ViT has been shown to outperform CNNs on large datasets [17], we observe that ViT outputs contain uneven pixelation when decoding the 16 × 16 patch embeddings, resulting in its low SSIM score.",1,related,1,positive
"For MAE He et al. (2022), a method based on a reconstruction objective, we select an attention-based ViT encoder.",1,related,1,positive
"0 0.5 1
−0.6
−0.4
−0.2
0
0 0.5 1 0 0.5 1
0 0.5 1
−0.6
−0.4
−0.2
0 0.5 1
CLIPPretrained MAEPretrained MLPMixerPretrained1k MLPMixerPretrained21k ResNet50Pretrained1k ResNet50Pretrained21k SimCLRPretrained ViTPretrained1k ViTPretrained21k iBotPretrained1k iBotPretrained21k
Generalization when Lighting varies in training
percent of diverse Lighting training samples percent of diverse Lighting training samples percent of diverse Lighting training samples
percent of diverse Lighting training samples percent of diverse Lighting training samples
t o p - 1
a c c u r a c y d
r o p
t o p - 1
a c c u r a c y d
r o p",1,related,1,positive
"In addition to the finetning results, we include here linear evaluation results for class generalization gaps A11.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Translation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 92.24% 91.37% 92.25% 94.01% 96.33% 77.78% 75.93% 68.52% 68.52% 58.89% 39.82% 57.70% 64.51% 65.45% 66.98% MAEPretrained 52.54% 55.93% 60.44% 67.91% 83.79% 20.37% 27.78% 33.33% 38.89% 27.78% 9.67% 12.91% 14.26% 15.56% 15.18% MLPMixerPretrained1k 94.66% 93.98% 94.58% 95.75% 97.25% 77.78% 74.07% 72.22% 66.67% 48.15% 36.86% 53.70% 59.35% 63.05% 63.46% MLPMixerPretrained21k 94.95% 95.09% 95.19% 96.02% 97.13% 75.93% 75.93% 74.07% 77.78% 70.37% 43.89% 67.36% 71.25% 73.69% 76.18% ResNet50Pretrained1k 95.00% 94.58% 94.83% 95.79% 97.23% 88.89% 90.74% 87.04% 83.33% 70.37% 44.35% 63.26% 68.99% 70.04% 70.01% ResNet50Pretrained21k 95.51% 95.30% 95.90% 96.27% 97.39% 77.78% 72.22% 74.07% 74.07% 70.37% 46.61% 68.04% 73.02% 75.90% 77.13% SimCLRPretrained 96.13% 95.74% 96.22% 96.82% 97.50% 81.48% 79.63% 81.48% 72.22% 70.00% 43.73% 62.96% 69.10% 70.63% 72.02% ViTPretrained1k 95.79% 96.18% 96.37% 96.80% 97.75% 88.89% 83.33% 77.78% 77.41% 75.93% 49.34% 67.92% 72.74% 75.65% 77.22% ViTPretrained21k 95.43% 95.01% 95.62% 96.39% 97.50% 83.33% 83.33% 83.33% 77.78% 72.22% 46.59% 67.21% 71.71% 74.02% 75.17% iBotPretrained1k 96.67% 96.43% 96.49% 97.01% 97.66% 81.48% 81.48% 79.63% 79.63% 72.22% 40.27% 65.23% 73.10% 76.06% 77.33% iBotPretrained21k 96.84% 96.30% 96.44% 96.92% 97.61% 90.74% 85.19% 87.04% 81.48% 72.22% 47.69% 70.55% 76.57% 78.53% 79.04%
Table A1: Position varying linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Rotation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 91.70% 91.43% 92.42% 93.98% 96.28% 81.48% 85.19% 85.19% 79.63% 55.56% 28.64% 41.16% 44.72% 46.17% 48.85% MAEPretrained 53.90% 57.24% 61.28% 68.44% 83.90% 25.93% 44.44% 38.89% 42.59% 29.63% 6.68% 7.93% 8.58% 8.63% 8.36% MLPMixerPretrained1k 94.24% 93.76% 94.74% 95.72% 97.28% 74.07% 74.07% 72.22% 70.37% 46.30% 26.84% 39.47% 43.04% 46.51% 48.73% MLPMixerPretrained21k 94.49% 94.81% 95.03% 95.88% 97.13% 79.63% 77.78% 77.78% 79.63% 72.22% 36.90% 55.46% 61.57% 63.67% 65.50% ResNet50Pretrained1k 94.72% 94.56% 94.89% 95.74% 97.18% 85.19% 87.04% 85.19% 81.48% 68.52% 34.44% 46.22% 49.90% 51.73% 53.24% ResNet50Pretrained21k 95.51% 94.96% 95.69% 96.12% 97.30% 77.78% 75.93% 75.93% 72.22% 66.67% 40.29% 57.68% 61.83% 63.91% 65.04% SimCLRPretrained 95.74% 95.63% 96.16% 96.80% 97.51% 81.48% 83.33% 83.33% 83.33% 62.96% 32.94% 47.35% 52.88% 55.97% 56.91% ViTPretrained1k 95.71% 95.76% 96.09% 96.79% 97.67% 87.04% 79.63% 81.48% 79.63% 59.26% 39.13% 55.09% 60.14% 64.14% 65.42% ViTPretrained21k 95.23% 94.89% 95.68% 96.42% 97.45% 85.19% 83.33% 83.33% 83.33% 66.67% 38.25% 54.50% 58.34% 60.94% 62.28% iBotPretrained1k 96.39% 96.22% 96.41% 96.96% 97.56% 83.33% 81.48% 81.48% 79.63% 72.22% 34.62% 51.88% 58.88% 62.19% 63.11% iBotPretrained21k 96.44% 96.09% 96.42% 96.92% 97.61% 90.74% 88.89% 90.74% 87.04% 72.22% 41.03% 57.48% 63.69% 65.49% 67.34%
Table A2: Pose linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Spot hue_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 92.49% 91.60% 92.46% 94.06% 96.32% 77.78% 75.93% 70.37% 70.37% 59.26% 39.61% 60.78% 66.72% 68.27% 68.64% MAEPretrained 52.26% 55.43% 60.36% 67.78% 83.62% 22.22% 31.48% 37.04% 37.04% 20.37% 11.51% 15.09% 16.10% 18.91% 15.58% MLPMixerPretrained1k 95.17% 94.20% 94.98% 95.86% 97.30% 79.63% 77.78% 70.37% 66.67% 57.04% 40.09% 56.90% 64.43% 67.35% 68.93% MLPMixerPretrained21k 94.89% 95.29% 95.44% 96.13% 97.20% 81.48% 79.63% 72.22% 74.07% 76.30% 44.68% 69.30% 73.32% 76.12% 77.75% ResNet50Pretrained1k 95.26% 94.81% 95.03% 95.80% 97.23% 87.04% 88.89% 87.04% 83.33% 77.78% 44.08% 62.96% 68.67% 71.81% 72.54% ResNet50Pretrained21k 95.91% 95.45% 96.00% 96.28% 97.41% 77.78% 75.93% 75.93% 72.22% 71.11% 44.22% 67.22% 70.95% 72.38% 74.39% SimCLRPretrained 96.30% 95.91% 96.27% 96.84% 97.59% 79.63% 75.93% 74.07% 74.07% 66.67% 43.36% 63.22% 71.32% 73.72% 72.26% ViTPretrained1k 95.99% 96.36% 96.54% 96.95% 97.80% 90.74% 87.04% 83.33% 81.48% 74.07% 52.53% 69.53% 71.81% 76.01% 77.28% ViTPretrained21k 95.57% 95.39% 95.84% 96.51% 97.59% 85.19% 81.48% 83.33% 77.78% 75.93% 46.01% 67.50% 71.97% 75.75% 77.06% iBotPretrained1k 96.50% 96.55% 96.74% 97.12% 97.70% 79.63% 81.48% 81.48% 77.78% 74.07% 42.32% 68.61% 72.75% 76.28% 78.04% iBotPretrained21k 97.01% 96.42% 96.65% 97.04% 97.64% 88.89% 87.04% 83.33% 83.33% 75.93% 48.83% 73.37% 78.09% 80.39% 81.55%
Table A3: Spot hue linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Scale_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 91.81% 91.51% 92.26% 93.90% 96.25% 79.63% 72.22% 74.07% 70.37% 61.11% 36.80% 51.29% 57.05% 56.99% 58.80% MAEPretrained 51.98% 56.15% 60.87% 68.07% 84.12% 25.93% 35.19% 33.33% 37.04% 35.19% 7.01% 10.74% 11.05% 11.29% 11.44% MLPMixerPretrained1k 94.27% 93.55% 94.47% 95.59% 97.17% 79.63% 77.78% 70.37% 68.52% 53.70% 32.50% 46.15% 51.66% 55.30% 56.62% MLPMixerPretrained21k 94.86% 94.98% 95.08% 95.93% 97.09% 79.63% 79.63% 75.93% 75.93% 74.07% 40.55% 60.87% 66.63% 67.52% 70.03% ResNet50Pretrained1k 94.89% 94.57% 94.84% 95.66% 97.16% 87.04% 90.74% 90.74% 83.33% 72.22% 39.22% 54.23% 59.10% 61.27% 60.89% ResNet50Pretrained21k 95.51% 95.28% 95.77% 96.08% 97.35% 81.48% 77.78% 77.78% 74.07% 74.07% 41.11% 60.06% 66.41% 69.69% 70.33% SimCLRPretrained 95.91% 95.53% 96.09% 96.66% 97.48% 83.33% 77.41% 77.78% 72.22% 62.96% 39.79% 54.93% 61.81% 62.93% 62.96% ViTPretrained1k 95.65% 96.01% 96.18% 96.69% 97.69% 87.04% 83.33% 79.63% 75.93% 72.22% 44.10% 58.13% 63.91% 67.85% 68.82% ViTPretrained21k 95.06% 94.87% 95.47% 96.30% 97.38% 83.33% 83.33% 83.33% 81.48% 72.22% 41.40% 58.53% 63.25% 65.43% 65.14% iBotPretrained1k 96.58% 96.37% 96.48% 96.98% 97.60% 84.07% 77.78% 79.63% 77.78% 66.67% 41.34% 58.93% 67.24% 68.66% 69.08% iBotPretrained21k 96.92% 96.18% 96.39% 96.86% 97.53% 85.19% 77.78% 81.48% 81.48% 75.93% 44.59% 63.11% 68.90% 71.45% 70.82%
Table A4: Scale linear eval top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Background path_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 90.37% 91.88% 92.73% 94.36% 96.70% 85.19% 77.78% 77.78% 77.78% 66.67% 41.30% 54.70% 59.96% 61.75% 63.41% MAEPretrained 51.51% 56.16% 62.31% 67.08% 84.32% 27.78% 31.48% 37.04% 37.04% 29.63% 9.47% 12.21% 11.84% 13.30% 12.67% MLPMixerPretrained1k 92.84% 93.82% 94.81% 95.41% 97.38% 75.93% 77.78% 72.22% 74.07% 61.11% 37.39% 49.19% 52.41% 55.75% 56.74% MLPMixerPretrained21k 95.42% 95.33% 95.62% 96.52% 97.55% 83.33% 81.48% 75.93% 75.93% 77.78% 48.30% 64.37% 66.79% 70.15% 70.84% ResNet50Pretrained1k 94.35% 95.12% 94.83% 95.93% 97.46% 90.74% 92.59% 88.89% 88.89% 83.33% 44.89% 59.87% 64.15% 66.70% 67.29% ResNet50Pretrained21k 95.20% 96.40% 95.89% 96.65% 97.67% 81.48% 79.63% 77.78% 77.78% 75.93% 51.21% 65.75% 69.47% 72.01% 73.90% SimCLRPretrained 95.50% 95.98% 96.14% 96.45% 97.52% 83.33% 83.33% 81.48% 77.78% 72.22% 44.33% 59.82% 65.39% 67.93% 68.93% ViTPretrained1k 95.72% 96.42% 96.27% 96.57% 97.95% 94.44% 88.89% 88.89% 87.04% 77.78% 50.62% 64.09% 67.14% 72.33% 73.19% ViTPretrained21k 94.91% 95.22% 96.09% 96.45% 97.71% 83.33% 83.33% 83.33% 83.33% 77.78% 49.36% 64.21% 67.42% 70.77% 72.13% iBotPretrained1k 96.42% 96.58% 96.60% 97.40% 97.28% 88.89% 83.33% 87.04% 81.48% 79.63% 44.58% 62.59% 68.10% 71.20% 73.36% iBotPretrained21k 96.16% 96.14% 96.38% 97.37% 97.27% 88.89% 90.74% 90.74% 83.33% 81.48% 51.20% 68.58% 71.57% 74.62% 75.94%
Table A5: Background path linear eval top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Translation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.49% 97.00% 97.25% 97.55% 96.80% 87.04% 90.74% 77.78% 81.48% 75.93% 45.33% 69.74% 74.53% 77.72% 78.19% MAEPretrained 96.75% 96.63% 97.20% 97.46% 97.91% 83.33% 74.07% 66.67% 64.81% 72.22% 29.17% 51.35% 61.05% 65.16% 70.07% MLPMixerPretrained1k 96.95% 96.73% 97.17% 97.24% 97.88% 88.89% 77.78% 77.78% 74.07% 64.81% 44.65% 64.56% 70.67% 74.34% 75.95% MLPMixerPretrained21k 97.71% 97.69% 97.90% 97.98% 98.35% 85.19% 88.89% 85.19% 81.48% 77.78% 46.53% 70.54% 77.27% 79.78% 81.68% ResNet50Pretrained1k 97.97% 97.92% 97.91% 97.86% 98.27% 87.04% 87.04% 72.22% 81.48% 81.48% 45.05% 64.83% 71.87% 76.56% 79.63% ResNet50Pretrained21k 97.54% 97.62% 97.74% 97.69% 98.20% 88.89% 83.33% 83.33% 83.33% 77.78% 52.22% 70.96% 75.78% 81.10% 82.45% SimCLRPretrained 97.40% 97.57% 97.68% 97.81% 98.12% 90.74% 77.78% 87.04% 83.33% 77.78% 45.21% 68.73% 74.59% 76.55% 79.86% ViTPretrained1k 97.80% 97.88% 98.00% 97.92% 98.28% 90.74% 90.74% 85.19% 81.48% 81.48% 49.30% 71.82% 76.95% 80.39% 82.58% ViTPretrained21k 97.80% 97.59% 97.89% 97.91% 98.25% 87.04% 88.89% 81.48% 81.48% 87.04% 45.83% 71.80% 75.51% 79.96% 81.86% iBotPretrained1k 97.77% 97.55% 97.64% 97.77% 98.06% 88.89% 85.19% 79.63% 75.93% 79.63% 45.69% 68.94% 74.76% 76.63% 79.83% iBotPretrained21k 97.97% 97.83% 97.88% 97.92% 98.13% 88.89% 87.04% 88.89% 81.48% 79.63% 49.84% 70.97% 78.13% 82.53% 84.07%
Table A6: Position finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Rotation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.60% 97.01% 97.30% 97.58% 97.97% 88.89% 88.89% 85.19% 83.33% 72.22% 34.29% 57.01% 62.67% 65.58% 68.45% MAEPretrained 96.87% 96.75% 97.25% 97.52% 97.95% 75.93% 68.52% 62.96% 68.52% 62.96% 22.64% 45.24% 50.87% 54.18% 53.82% MLPMixerPretrained1k 96.75% 96.58% 97.08% 97.25% 97.86% 83.33% 85.19% 83.33% 77.78% 64.81% 33.29% 51.43% 55.55% 58.78% 59.35% MLPMixerPretrained21k 97.68% 97.65% 97.90% 97.90% 98.35% 87.04% 87.04% 77.78% 77.78% 75.93% 38.93% 62.76% 67.97% 71.90% 73.47% ResNet50Pretrained1k 98.05% 97.81% 97.89% 97.93% 98.24% 84.81% 81.48% 81.48% 81.48% 75.93% 33.29% 53.51% 62.56% 64.45% 67.47% ResNet50Pretrained21k 97.52% 97.45% 97.65% 97.61% 98.18% 85.19% 79.63% 83.33% 87.04% 85.19% 37.45% 59.85% 65.39% 70.66% 73.13% SimCLRPretrained 97.37% 97.43% 97.53% 97.74% 98.07% 87.04% 87.04% 83.33% 87.04% 75.93% 35.50% 55.87% 64.34% 66.79% 69.34% ViTPretrained1k 97.94% 97.87% 97.98% 97.95% 98.31% 88.89% 87.04% 85.19% 77.78% 75.93% 40.13% 62.66% 68.53% 71.32% 73.36% ViTPretrained21k 97.68% 97.61% 97.90% 97.97% 98.27% 88.89% 79.63% 83.33% 74.07% 70.74% 39.75% 61.42% 68.39% 71.51% 73.76% iBotPretrained1k 97.49% 97.55% 97.56% 97.75% 98.02% 88.89% 77.78% 83.33% 75.93% 68.52% 36.30% 60.69% 65.98% 68.22% 70.00% iBotPretrained21k 98.00% 97.79% 97.96% 98.01% 98.19% 88.89% 87.04% 83.33% 85.19% 72.22% 38.86% 62.35% 69.18% 71.96% 73.84%
Table A7: Pose finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Spot hue_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.66% 97.13% 97.40% 97.62% 97.98% 90.74% 88.89% 87.04% 87.04% 81.48% 49.99% 70.06% 75.12% 82.36% 82.48% MAEPretrained 97.04% 96.67% 97.20% 97.41% 97.95% 79.63% 77.78% 72.22% 74.07% 68.52% 30.10% 54.63% 60.74% 66.78% 69.34% MLPMixerPretrained1k 97.32% 96.92% 97.20% 97.36% 97.89% 87.04% 83.33% 77.78% 79.63% 72.22% 48.80% 65.60% 70.82% 76.88% 78.59% MLPMixerPretrained21k 97.80% 97.83% 97.99% 97.99% 98.42% 88.89% 87.04% 81.48% 75.93% 79.63% 49.81% 73.15% 75.23% 79.01% 82.35% ResNet50Pretrained1k 98.02% 97.99% 98.03% 97.98% 98.28% 88.89% 87.04% 83.33% 81.48% 77.78% 48.48% 67.72% 74.65% 76.90% 75.83% ResNet50Pretrained21k 97.68% 97.60% 97.87% 97.79% 98.24% 90.74% 87.04% 83.33% 77.78% 75.93% 54.35% 71.69% 76.70% 81.03% 81.86% SimCLRPretrained 97.60% 97.61% 97.72% 97.87% 98.17% 90.00% 76.30% 85.19% 77.78% 74.07% 45.58% 70.36% 79.09% 78.06% 81.75% ViTPretrained1k 97.68% 97.93% NaN 98.00% 98.37% 94.44% 88.89% NaN 81.48% 81.48% 54.38% 70.94% NaN 82.53% 83.94% ViTPretrained21k 97.77% 97.68% 97.94% 98.00% 98.24% 92.59% 85.19% 77.78% 78.15% 81.48% 52.49% 72.55% 76.58% 79.75% 82.39% iBotPretrained1k 97.91% 97.58% 97.76% 97.88% 98.08% 88.89% 81.48% 79.63% 75.93% 74.07% 48.67% 69.24% 75.01% 79.44% 80.81% iBotPretrained21k 98.25% 97.87% 97.88% 97.96% 98.13% 90.74% 83.33% 92.59% 79.63% 83.33% 49.39% 74.42% 80.67% 83.05% 85.02%
Table A8: Spot hue finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Scale_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.68% 97.08% 97.39% 97.63% 97.95% 90.74% 90.74% 87.04% 83.33% 79.63% 45.47% 66.39% 72.46% 75.63% 78.05% MAEPretrained 96.81% 96.64% 97.19% 97.40% 97.91% 81.48% 70.37% 70.37% 70.37% 53.70% 28.70% 51.73% 60.66% 62.49% 64.47% MLPMixerPretrained1k 97.23% 96.60% 97.07% 97.24% 97.82% 87.04% 85.19% 83.33% 74.07% 70.37% 41.62% 60.01% 65.78% 70.24% 71.64% MLPMixerPretrained21k 97.80% 97.80% 97.96% 97.95% 98.37% 85.19% 87.04% 81.48% 72.22% 77.78% 46.02% 68.37% 73.08% 76.51% 77.38% ResNet50Pretrained1k 97.88% 97.94% 97.95% 97.89% 98.24% 87.04% 85.19% 81.48% 75.93% 79.63% 44.47% 62.86% 71.55% 73.81% 75.80% ResNet50Pretrained21k 97.40% 97.58% 97.84% 97.72% 98.18% 90.74% 79.63% 81.48% 79.63% 79.63% 49.37% 68.72% 70.89% 76.85% 79.18% SimCLRPretrained 97.57% 97.54% 97.65% 97.83% 98.10% 88.89% 83.33% 83.33% 83.33% 74.44% 42.25% 65.04% 71.88% 74.89% 76.25% ViTPretrained1k 97.80% 97.92% 98.05% 97.92% 98.34% 88.89% 83.33% 85.19% 79.63% 79.63% 44.17% 65.86% 71.66% 77.46% 78.46% ViTPretrained21k 97.77% 97.71% 97.85% 97.99% 98.24% 85.19% 85.19% 85.19% 79.63% 79.63% 42.63% 67.71% 70.61% 74.96% 76.14% iBotPretrained1k 97.85% 97.61% 97.74% 97.79% 98.04% 90.74% 87.04% 83.33% 83.33% 79.63% 45.14% 64.09% 71.34% 73.90% 76.99% iBotPretrained21k 97.88% 97.75% 97.86% 97.97% 98.12% 88.89% 87.04% 87.04% 81.48% 75.93% 48.70% 65.52% 72.39% 79.16% 80.04%
Table A9: Scale finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Background path_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 96.49% 97.05% 97.52% 97.81% 98.01% 94.44% 88.89% 92.59% 87.04% 81.48% 50.79% 67.24% 76.25% 79.79% 80.92% MAEPretrained 96.20% 96.61% 97.31% 97.63% 97.99% 81.85% 72.22% 77.78% 72.22% 62.96% 30.49% 52.04% 64.77% 66.67% 68.56% MLPMixerPretrained1k 96.16% 97.03% 97.13% 97.66% 97.76% 90.74% 87.04% 81.48% 75.93% 72.22% 47.41% 63.84% 71.85% 73.76% 75.96% MLPMixerPretrained21k 98.08% 98.15% 98.09% 98.33% 98.71% 88.89% 92.59% 90.74% 85.19% 79.63% 51.84% 72.00% 77.05% 80.46% 81.10% ResNet50Pretrained1k 97.71% 97.76% 97.95% 98.04% 98.38% 92.59% 92.59% 90.74% 92.59% 83.33% 46.50% 63.48% 70.90% 73.73% 77.72% ResNet50Pretrained21k 97.38% 98.09% 97.72% 98.33% 98.34% 88.89% 83.33% 87.04% 81.48% 85.19% 50.87% 71.59% 76.21% 79.13% 83.24% SimCLRPretrained 97.38% 97.31% 97.77% 97.55% 98.05% 77.78% 87.04% 88.89% 90.74% 83.33% 43.32% 61.47% 69.94% 76.99% 79.07% ViTPretrained1k 98.12% 97.76% 97.89% 97.73% 98.44% 88.89% 90.74% 87.04% 81.48% 83.33% 54.25% 71.56% 75.73% 80.58% 84.92% ViTPretrained21k 97.77% 97.49% 97.95% 98.04% 98.37% 91.67% 88.89% 90.74% 87.04% 81.48% 55.17% 73.53% 76.50% 82.28% 81.94% iBotPretrained1k 97.47% 97.44% 97.61% 98.15% 97.86% 88.89% 92.59% 90.74% 81.48% 79.63% 51.19% 71.17% 75.32% 78.37% 79.93% iBotPretrained21k 97.69% 97.91% 97.77% 98.17% 97.93% 93.52% 92.59% 90.74% 85.19% 83.33% 55.00% 73.11% 76.62% 83.56% 82.90%
Table A10: Background path finetuning top-1 accuracy across multiple percentages of varying training instances",1,related,1,positive
"0 0.5 1
−0.8
−0.6
−0.4
−0.2
0
0 0.5 1 0 0.5 1
0 0.5 1
−0.6
−0.4
−0.2
0 0.5 1
CLIPPretrained MAEPretrained MLPMixerPretrained1k MLPMixerPretrained21k ResNet50Pretrained1k ResNet50Pretrained21k SimCLRPretrained ViTPretrained1k ViTPretrained21k iBotPretrained1k iBotPretrained21k
Generalization when Lighting varies in training
percent of diverse Lighting training samples percent of diverse Lighting training samples percent of diverse Lighting training samples
percent of diverse Lighting training samples percent of diverse Lighting training samples
t o p - 1
a c c u r a c y d
r o p
t o p - 1
a c c u r a c y d
r o p",1,related,1,positive
"0 50 100 −40
−20
0 50 100 0 50 100 0 50 100 0 50 100
CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k best fit
Percent of varying images across all instances
0 50 100 −40
−20
0
0 50 100 0 50 100 0 50 100 0 50 100",1,related,1,positive
"Using the typical evaluation procedures of self-supervised models (Caron et al., 2021; Dosovitskiy et al., 2021; Chen et al., 2020b) (finetuning and linear evaluation), we examine robustness across a catalog of state-of-the-art vision architectures, such as CLIP (Radford et al., 2021) that have significantly outperformed earlier models on robustness benchmarks such as ObjectNet (Barbu et al., 2019), Masked AutoEncoders (MAE, (He et al., 2022)), or ViTs Dosovitskiy et al. (2021)) among others.",1,related,1,positive
We use pre-trained weights from the official repo of He et al. (2022).,1,related,0,negative
"In our experiment we deploy our Lie operator on top of the Variance-Invariance-Covariance Regularization model (VICReg, (Bardes et al., 2021)), and of the very recent masked autoencoders model (MAE, (He et al., 2021)).",1,related,1,positive
"We incorporate our proposed Lie operator in two recent SSL approaches: masked autoencoders (MAE, (He et al., 2021)), and Variance-Invariance-Covariance",1,related,1,positive
"We incorporate our proposed Lie operator in two recent SSL approaches: masked autoencoders (MAE, (He et al., 2021)), and Variance-Invariance-Covariance
Evaluate Robustness
Self-Supervised Lie Operator
Training Data
Regularization (VICReg (Bardes et al., 2021)) to directly model transformations in…",1,related,1,positive
"In addition to the differentiation of the network structure pre-trained on ImageNet, we include models with a variety of pre-trained strategies, including SimCLR [6], MoCov2 [8] and
1https://pytorch.org/vision/stable/index.html 2https://github.com/rwightman/pytorch-image-models 3https://github.com/open-mmlab
BYOL [21] for ResNet50, MoCov3 [9] and MAE [26] for ViT-B.",1,related,1,positive
The most popular pretraining scheme for ViTs is called Masked Autoencoders (MAE) [45].,1,related,0,negative
"Following this spirit, we take masked autoencoders (MAE) [45] as pre-training task for its simplicity, efficiency, scalability, and compelling performance.",1,related,0,negative
"where LMIM is the MIM loss such as the prediction error used in the MAE [35] when reconstructing missing matches, p[u,v ] is the relative positional embedding with a 2D index of [u, v], g(xi − x j + δx ) and g(yi − y j + δy) are the continuous indexing function for x− and y−coordinate respectively, andM is the index of masked patches.",1,related,1,positive
"Publication date: October 2022.
where L𝑀𝐼𝑀 is the MIM loss such as the prediction error used in the MAE [35] when reconstructing missing matches, p[𝑢,𝑣 ] is the relative positional embedding with a 2D index of [𝑢, 𝑣], 𝑔(𝑥𝑖 − 𝑥 𝑗 + 𝛿𝑥 ) and 𝑔(𝑦𝑖 − 𝑦 𝑗 + 𝛿𝑦) are the continuous indexing function for 𝑥− and 𝑦−coordinate respectively, andM is the index of masked patches.",1,related,1,positive
"Then one adopts a Masked Auto-Encoder (MAE) [35] reconstruction error
L𝑚𝑎𝑒 (xm, x;𝜃 ) = D(I𝜃 (xm), x) with a distance measure D between reconstructed and original images to pre-train the network 𝜃 .",1,related,1,positive
Then one adopts a Masked Auto-Encoder (MAE) [35] reconstruction error,1,related,1,positive
"…al. (2022) requires access to an unlabeled public dataset for all models to measure the distance, and requires the communication of some datasets; He et al. (2021a) experiments with multiple methods to do personalization in decentralized learning, and our clustering-based approach can be viewed…",1,related,1,positive
"For masked autoencoder (He et al., 2021b) experiment on CIFAR-10, we use Vit-Tiny (Dosovitskiy et al., 2020) with AdamW optimizer for 1000 epochs with batch size 256.",1,related,1,positive
"Hence, a high masking ratio (75% (He et al., 2022)) may not be necessary to suppress the amount of information the encoder sees, and we consequently attempt lower ratios of 50%, 60% to introduce more information about the subordinate target.",1,related,0,negative
"In this work, we focus on MAE (He et al., 2022), which proposes to use a high masking ratio and a non-arbitrary ViT decoder.",1,related,1,positive
"We notice that recent work in the literature (He et al., 2022; Bao et al., 2022) performs many experiments in masking strategies, but to the best of our knowledge, we are the first to introduce image mixtures in the pre-training of MIM.",1,related,1,positive
"For better classification performance, we use normalized pixels (He et al., 2022) and a high masking ratio (0.75); for better visual reconstructions, we use a lower masking ratio (0.5) without normalizing target pixels.",1,related,1,positive
"Linear Probing: For linear evaluation, we follow MAE (He et al., 2022) to train with no extra augmentations and use zero weight decay.",1,related,1,positive
"Instead, our patch-dim normalization stresses the relations among patches, which is compatible to the patch-prediction in the MIM scheme.",1,related,1,positive
"We therefore choose a mask-reconstruction [19] SSL scheme for the pretraining of the new model, in which the base model generates reconstruction targets from the full input images and the new model tries to predict base model targets from random masked image input.",1,related,1,positive
We then propose to utilize the self-attention maps as a type of reconstruction targets for MIM to further enhance the semantic relation modeling capability of the new model.,1,related,1,positive
"With the plain backbone pretrained as a MAE [21], our method achieves 4.",1,related,0,negative
"Instead, we simply use the readily available pretrained MAE weights from [21].",1,related,0,negative
"• We show that block-wise masking provides superior performance on Masked Siamese ConvNets to the discrete random masking, commonly used in selfsupervised representation learning frameworks [20, 24, 1].",1,related,1,positive
"Furthermore, instead of using discrete random masking that is applied in [20], we found using block-wise masking will give better performance since it can better preserve global information important for contrastive loss.",1,related,1,positive
"1) Evalutation protocols: Following previous works [14], [28], [31], we adopt three common evaluation protocols, namely fine-tuning evaluation, linear evaluation and k-NN classification [43], to assess the performance of each pretrained model.",1,related,1,positive
"We also collect 7 ViT-S models with different training regimes, including the original ViT training setup [26]6, a stronger data augmentation setup in the Deit paper [89]-36, the training setup with distillation [89]-36, an improved DeiT training setup [90]-36 , and selfsupervised training fashions by MoCo v3 [12]7, MAE [38]8 and BYOL [33]9.",1,related,0,negative
"Following MAE (He et al., 2022) and BEiT v2 (Peng et al., 2022), we test the robustness of MaskDistill on three ImageNet validation sets, i.e., ImageNet-Adversarial (Hendrycks et al., 2021b), ImageNet-Rendition (Hendrycks et al., 2021a) and ImageNet-Sketch (Wang et al., 2019).",1,related,1,positive
We compare between DINO [5] and MAE [11] as a representation for a kNN based anomaly detection algorithm.,1,related,1,positive
"For MAE, we experimented both with kNN and reconstruction error for anomaly scoring and found that the latter works badly, therefore we report just the kNN results.",1,related,0,negative
"A.1 Anomaly detection comparison of MAE and DINO
We compare between DINO [5] and MAE [11] as a representation for a kNN based anomaly detection algorithm.",1,related,1,positive
"In particular, we compare to DINO [14], a state-of-the-art self-supervised method based on instance discrimination, and to MIM methods with MAE [38] and MultiMAE [4].",1,related,1,positive
"We follow the exact same protocol as MAE [38] for that, with global average pooling for CroCo as we did not include a [CLS] token in our model.",1,related,1,positive
"Our model is trained using a simple pixel reconstruction loss over all masked patches, similar to MAE [38].",1,related,1,positive
"Therefore, we used ArcFace [9] to train a projection head composed of two layers, BatchNormalization and Linear. this architecture is based on MAE [13].",1,related,1,positive
"In order to increase robustness to such varying resolution, we utilize up to 2⇥ higher resolution images during training but randomly drop 80% of visual tokens to minimize additional compute overhead (similar to [38, 52]).",1,related,1,positive
"Table 1: Token Merging ablation experiments using ViT-L/16 from MAE (He et al., 2022) on ImageNet-1k evaluated off-the-shelf without training, using r = 8.",1,related,0,negative
"We perform several experiments on ImageNet-1k (Deng et al., 2009) using ViT models trained in four different ways: AugReg (Steiner et al., 2022), MAE (He et al., 2022), SWAG (Singh et al., 2022), and DeiT (Touvron et al., 2021).",1,related,1,positive
"Different from MAE [16], our encoder operates on the full set.",1,related,0,negative
"The experiments reveal that MAE and U-Net are the best shape denoising methods we evaluated for all six
types of noise.",1,related,1,positive
"We evaluate seven shape denoising methods (ASM [6], DBM [8], CDBM [9], EBM [10], U-Net [11], Deeplabv3+ [12], MAE [13]) for denoising shapes corrupted by the six types of noise introduced in Section 2.",1,related,1,positive
"We evaluate seven shape denoising methods (ASM [6], DBM [8], CDBM [9], EBM [10], U-Net [11], Deeplabv3+ [12], MAE [13]) for denoising shapes corrupted by the six types of noise introduced in Section 2.2.",1,related,1,positive
"For the decoder, we use a flexible one following [15].",1,related,1,positive
"We mainly follow the basic setup of MAE [15]: for the encoder, we adopt different variants of ViT [10], i.",1,related,1,positive
"Specifically, MAE adopts a simple mean square error (MSE) loss: LMAE(h) = Ex̄Ex1,x2|x̄ ‖g(f(x1))− x2‖ 2 , (2) where the decoder output x̂2 = h(x1) = g(f(x1)) is assumed to be l2-normalized following the original paper of MAE saying that normalized target x2 yields better performance [15].",1,related,1,positive
We begin by introducing a mathematical formulation of MAE [15].,1,related,1,positive
As inpainter we use a publicly available Masked AutoEncoder (MAE) [21] trained with an adversarial loss.,1,related,0,negative
The MAE [21] we use is based on a ViT architecture and has been pre-trained in an adversarial fashion (as opposed to the standard training with an MSE loss) to output more realistic-looking details,1,related,0,negative
"Following the recent trend of methods for unsupervised object segmentation [7–12, 22], we build our method on top of SSL features, and, in particular, DINO [4] or MAE [21] features.",1,related,1,positive
"This may be somewhat related to the network (transformer) used in MAE, so although we use a masked representation learning approach similar to MAE, we improve the network structure to make it more adaptable to the learning of ECG representations.",1,related,1,positive
"We also compare the performance of those models to instance-based methods such as MAE (He et al., 2021) and data2vec (Baevski et al., 2022), which do not employ volume maximization regularizers.",1,related,1,positive
"We also compare the performance of those models to instance-based methods such as MAE (He et al., 2021) and data2vec (Baevski et al.",1,related,1,positive
Our pretraining follow the standard hyperparameters defined in He et al. (2021).,1,related,1,positive
"Tables 5, 6,7, 8 and 9, and show the results of SimCLR, MSN, VICReg, data2vec and MAE on the CIFAR100, CIFAR100 1%, Place205, Clevr/Count, Clevr/Dist and KITTI downstream tasks.",1,related,1,positive
"We pretrain a ViT-B/16 with data2vec (He et al., 2021) using the AdamW optimizer with a batch size of 2048 for 800 epochs using the official code base, which is publicly available: http: //github.com/facebookresearch/data2vec_vision/tree/main/beit.",1,related,1,positive
"We pretrain a ViT-L/16 with MAE (He et al., 2021) using the AdamW optimizer with a batch size of 1024 for 800 epochs using the official code base, which is publicly available: https:// github.com/facebookresearch/mae.",1,related,0,negative
"Our long-sequence MAE is a simple and minimallychanged extension of MAE [19], which we summarize first.",1,related,1,positive
"To help the model better model the trajectory, we adopt the patch norm method to normalize the trajectory values in each patch into a standard normal distribution, as has also been done in MAE [37].",1,related,1,positive
"For computation efficiency, we follow MAE [37] to only feed the unmasked patches (and their positions) to the encoder.",1,related,1,positive
"We consider two supervised feature backbones: ResNet50 [25] and ViT-B/16 [18], and four selfsupervised backbones: SimCLR [13], MAE [23], MSN [4] and DINO [11].",1,related,1,positive
"We consider two supervised feature backbones: ResNet50 [16] and ViT-B/16 [13], and four self-supervised backbones: SimCLR [9], MAE [17], MSN [2] and DINO [7].",1,related,1,positive
The random mixing and block-wise mixing strategies are inspired by MAE [18] and BEiT [3] and we replace the masking operation with image mixing on patch-level and block-level (both of size 16×16) respectively.,1,related,1,positive
"In particular, we compare to two approaches: R3M [41], which utilizes the Ego4D dataset of human videos to obtain a representation, and MVP [46, 56], which trains a masked auto-encoder [16] on the Bridge Dataset and utilizes the learned latent space as the representation of the new image.",1,related,1,positive
"For a fair comparison, we compare our DMAE ViT-B model with the MAE ViT-B checkpoint released by He et al. (2022) in the linear probing setting on ImageNet.",1,related,1,positive
"Following He et al. (2022); Xie et al. (2022), we use ImageNet-1k as the pre-training corpus which contains 1.28 million images.",1,related,0,negative
"Like MAE (He et al., 2022), we adopt the asymmetric encoder-decoder design for DMAE.",1,related,1,positive
"To enforce the encoder (but not the decoder) to learn robust semantic features, we control the capacity of the decoder by setting a smaller value of the hidden dimension and depth following He et al. (2022).",1,related,1,positive
"For the pre-training of the two DMAE models, we set the masking ratio to 0.75 following He et al. (2022).",1,related,1,positive
"Motivated by the Masked AutoEncoder (MAE) (He et al., 2022), which learns latent representations by reconstructing missing pixels from masked images, we design a new self-supervised task called Denoising Masked AutoEncoder (DMAE).",1,related,1,positive
"In the case of only few labeled data for calibration, the proposed MV-SSTMA is evaluated with the self-supervised method MAE and the supervised methodMD-AGCN.",1,related,1,positive
"In addition, our model outperforms MAE and MD-ADCN in every scenario.",1,related,0,negative
"2 Comparisons of different types of augmentations In this section, we compare the generalization of three canonical augmentations that we analyzed in this work: 1) Gaussian noise injection [21], 2) random mask [16], and 3) random rotation (which we introduced in Section 3.",1,related,1,positive
"In our framework, we allow for cases in which augmentation leads to significant changes in distribution and provide a path to analysis for such OOD augmentations that encompass empirically popular DA [16, 17].",1,related,1,positive
"3, we simulate the biased and unbiased random mask augmentation [16] and test their performance in regression and classification tasks.",1,related,1,positive
"Note that a natural constraint r ≤ m holds, and the original MAE setting [14] can be regarded as the special case when r = m.",1,related,1,positive
"Inspired by MAE [14], we also use a shallow decoder with much fewer Transformer blocks than the encoder (Φdec(.",1,related,1,positive
"We only calculate the loss on the mask tokens, which is consistent with MAE and SimMIM.",1,related,1,positive
"Different from the MAE [8] which utilizes a multilayer converter structure as decoder, we only employ a single-layer MLP is used as decoder.",1,related,1,positive
"Given an image-text pair (𝐼 ,𝑇 ), we random sample 25% of the image patches, the other 75% image patches will not be fed into the models following MAE[11].",1,related,1,positive
"Given an image-text pair (I ,T ), we random sample 25% of the image patches, the other 75% image patches will not be fed into the models following MAE[11].",1,related,1,positive
"The encoder and decoder of PA-former adopt the same structure following [11], in which the encoder transforms the input sub-sampled signal into latent representation and the encoder maps the latent representation back to the full-sampled data.",1,related,1,positive
"For computational efficiency, we adopt an asymmetric MAE architecture (He et al., 2022) based on vanilla Vision Transformers (ViTs) (Dosovitskiy et al.",1,related,1,positive
"For computational efficiency, we adopt an asymmetric MAE architecture (He et al., 2022) based on vanilla Vision Transformers (ViTs) (Dosovitskiy et al., 2020): The encoder operates only on visible patches and the decoder on all the patches.",1,related,1,positive
"For each decoder head, specific positional embeddings are applied following MAE (He et al., 2022).",1,related,1,positive
"Our decoder is another vanilla ViT deployed on the union of the encoded patch set and a set of mask tokens (He et al., 2022).",1,related,1,positive
"Following (He et al., 2022; Feichtenhofer et al., 2022), the encoder is applied only on unmasked patches.",1,related,0,negative
"Our proposed MotionMAE adopts the asymmetric Transformer based masked autoencoder architecture (He et al., 2022), as depicted in Fig.",1,related,1,positive
"MAE uses masked autoencoders (He et al., 2022) and we use its ViT-Large variant.",1,related,1,positive
"B we evaluate the transformer-based models MAE (He et al., 2022) and MOCOv3 (Chen et al.",1,related,1,positive
"B we evaluate the transformer-based models MAE (He et al., 2022) and MOCOv3 (Chen et al., 2021) as well as adversarially fine-tuned versions of MOCOv3 using the same three attacks as for MOCOv2.",1,related,1,positive
"As for segmentation, we use the ViT-base model provided by MMSegmentatation, which is pre-trained by MAE on ImageNet and then finetuned on the ADE20k dataset.",1,related,1,positive
"Note that SimR101, SimR101 and MAEViT stand for Resnet101 pretrained by SimCLRv2, Resnet50 pretrained by SimCLRv2 and ViT-base-16 pretrained by MAE, respectively.",1,related,0,negative
"We craft pre-trained adversarial perturbations (PAPs) for three pre-trained models (i.e., Resnet50 by SimCLRv2, Resnet101 by SimCLRv2, ViT16 by MAE) and evaluate the attack success rates on ten downstream tasks.",1,related,1,positive
We report the results of SimCLR and MAE in Section 4.2.,1,related,0,negative
"Note that Resnet50 and Resnet101 [18] are pre-trained by SimCLRv2 [4], and ViT16 [56] is pre-trained by MAE [21].",1,related,1,positive
"We fine-tune a stateof-the-art vision transformer model pretrained with masked autoencoding (He et al., 2021) for 100 epochs on rotated MNIST (Weiler and Cesa, 2019) (details in Appendix C.5).",1,related,1,positive
"We fine-tune a stateof-the-art vision transformer model pretrained with masked autoencoding (He et al., 2021) for 100 epochs on rotated MNIST (Weiler and Cesa, 2019) (details in Appendix C.",1,related,1,positive
"We use the base model and default finetuning procedure from (He et al., 2021), finetuning for 100 epochs, halving the learning rate on loss plateaus.",1,related,1,positive
"We opted to use a more flexible transformerbased visual architecture, which has recently achieved stateof-the-art results in computer vision tasks [11, 14], and language tasks [10, 31].",1,related,1,positive
125 We pre-train the models via the MAE framework [15].,1,related,1,positive
We leverage the masked 29 autoencoders (MAE) [15] that learn representations by masked prediction.,1,related,1,positive
"An important property of our visual pre-training approach is that uses a self-supervised objec194 tive [15] that makes minimal assumptions about the data distribution and does not rely on human195 designed proxy tasks, like data augmentations.",1,related,1,positive
We pre-train the models via the MAE framework [16].,1,related,1,positive
We train the MAE models for 400 epochs for the combined Ego dataset; 1600 epochs for the HOI dataset; and 1600 epochs for ImageNet dataset.,1,related,1,positive
"Our experiments follow the same architecture, settings, and pre-training recipe as MAE (He et al., 2022), and we find that the simple addition of a teacher (RC-MAE) consistently outperforms MAE in all model sizes (e.",1,related,1,positive
"Method Pre-Train Data APbox APmask Superivsed (He et al., 2022) IN1K w/ labels 47.",1,related,0,negative
"To validate the effectiveness of the EMA Teacher in RC-MAE, we compare RC-MAE to an otherwise identical MAE (He et al., 2022) baseline.",1,related,0,negative
"For the reconstruction target, we use standard normalized patch pixels as done with MAE (He et al., 2022).",1,related,1,positive
", 2021) and implementation details as MAE (He et al., 2022), we fine-tune Mask R-CNN (He et al.",1,related,1,positive
We observe the ASR ranged from 66.34% to 99.18% on both MAE and CAE.,1,related,0,negative
We then take MAE as the target model’s architecture and conduct comprehensive ablation studies to understand the impacts of important backdoor attack components in each supply chain’s phase.,1,related,0,negative
"Concretely, for Type I and Type II attacks, as the adversary does not involve in the pre-training phase, we utilize the public MAE 3 and CAE 4 as our target model.",1,related,0,negative
"We compare the MAE performance of using AdamW, SGD, and LARS as the optimizer and find AdamW reaches the best clean accuracy (see Table 9).",1,related,1,positive
"We consider two MIM architectures as the target models, i.e., Masked Autoencoder (MAE) [15] for end-to-end MIM and Contextual Autoencoder (CAE) [8] for tokenizer-based MIM.",1,related,1,positive
VideoMAE: Masked Autoencoders are DataEfficient Learners for Self-Supervised Video PreTraining.,1,related,0,negative
"To promise the results are comparable, we adopt the same linear probing configurations in all three scenarios for both MAE and CAE.",1,related,1,positive
MAE-AST: Masked Autoencoding Audio Spectrogram Transformer.,1,related,0,negative
", Masked Autoencoder (MAE) [15] for end-to-end MIM and Contextual Autoencoder (CAE) [8] for tokenizer-based MIM.",1,related,1,positive
We find that threshold [3-7] produces the best results which shows that it is important to keep a balance between similar and dissimilar view-pairs.,1,related,1,positive
Our decoder must be initialized randomly because the MAE authors do not share their decoder model parameters.,1,related,1,positive
"For our third specialized model, we first pretrain on in-domain simulated data for 150 epochs, initializing our ViT encoder from MAE’s pretrained model parameters and our ViT decoder randomly.",1,related,1,positive
"We introduce steps 2 and 3 that leverage self-supervised MAE training and finetuning, respectively, using additional adult pose datasets.",1,related,1,positive
"We leverage hierarchical pretraining by continuing to pretrain our ViT encoder on in-domain (i.e., fused depth and pressure) data using the MAE SSL algorithm.",1,related,1,positive
"fied by MAE [34], which we leverage in Section V.",1,related,0,negative
"For our first two specialized models, we pretrain on in-domain—either simulated or real—data for 150 epochs, initializing our ViT encoder from MAE’s pretrained model parameters and our ViT decoder randomly.",1,related,1,positive
We also demonstrate a masked autoencoding (MAE) hierarchical pretraining strategy for ViT that significantly improves accuracy on the SMaL dataset.,1,related,1,positive
"Given that the ViT backbone is amenable to hierarchical MAE pretraining strategy, we can further improve the performance of the best-performing architecture, ViTPose.",1,related,1,positive
"In ViTPose, they initialize with MAE’s encoder; we use this as a baseline to compare with ViTPose models initialized with our three specialized encoders.",1,related,1,positive
"5(left), our method is significantly superior to MAEL, which has the best accuracy of all single deep networks.",1,related,1,positive
"Following the procedure described in previous section, we first constructed object embeddings based on ViT (MAE), CNN (DenseNet) and used kNN classifier (our first approach).",1,related,1,positive
"A. Classifier comparison
In the first round of experiments, we encode RGB and depth modalities of the object separately using ViT (MAE) and CNN (DenseNet) and assessed the performance of seven classifiers, including k-Nearest Neighbors (kNN) [50], Multi-layer Perceptron (MLP) [51], Support Vector Machine (SVM) [52], Decision Tree (DT) [53], Gaussian Process (GP) [54], Random Forest (RF) [55], and Gaussian Naive Bayes (GNB) [56], on the restaurant fine-grained object dataset.",1,related,1,positive
We used MAE (RGB) + DenseNet (RGB-D) to represent each of the objects.,1,related,1,positive
"The accuracy of our multimodal appraoch-II with DeseNet (RGB-D) and MAE (RGB) was 93.51%, which outperformed all single models, CNNs (Dense.+Mnas.)",1,related,1,positive
"For future work, we are interested to explore AttnDistill for knowledge distillation between ConvNets and ViT.",1,related,0,negative
"Observing that the previous SSKD methods focussed on ConvNet do not work well on ViT, we proposed AttnDistill to distill the knowledge from a pretrained teacher model to its student model.",1,related,0,negative
"We draw the following conclusions:
• Based on ViT-T/16 distilled from Mugs(ViT-S/16), our method AttnDistill gets state-of-theart k-NN and Linear probing performance compared with previous knowledge distillation methods based on ConvNet.",1,related,1,positive
"He et al., 2022), we avoid such learnable prefix design with random initialization and propose a parallel attention (PATT) to the original attention module (see Figure 3).",1,related,1,positive
"He et al., 2022) in the NLP domain, we aim to propose a unified model for the vision domain, especially for video-based downstream tasks.",1,related,1,positive
"He et al., 2022; Tong et al., 2022), adding a prefix for a sentence input in NLP can be structurally different from the visual domain.",1,related,1,positive
"Specifically, we use the weight from the original vision MAE model (He et al., 2022) (Weights from https://github.com/ facebookresearch/mae) with only self-supervised learning (SSL) pretraining for all audio, visual, and joint encoder and the decoder.",1,related,1,positive
"Specifically, we use the weights of the original vision MAE He et al. (2022).",1,related,1,positive
"Due to the space limitation, please refer to He et al. (2022); Huang et al. (2022a) for single-modal MAEs.",1,related,1,positive
"Implementation Detail For the image classification problem, we use Vision Transformer pretrained on unlabeled ImageNet dataset with masked auto-encoding (He et al., 2022) and fine-tune it on the downstream task with AdamW optimizer (Loshchilov & Hutter, 2019) for 10,000 steps with batch size 32.",1,related,1,positive
"Firstly, we remove masked auto-encoding objective LMAE and train the model with only distillation loss LDistill before fine-tuning.",1,related,1,positive
"Then we take the copy of the pre-trained model (fθinit , gϕinit) as a student, and match the representations of the student encoder and those of the teacher encoder while optimizing the student with the MAE on the target unlabeled data.",1,related,1,positive
"Masked Auto-Encoding We briefly describe the masked auto-encoding objective (Liu et al., 2019; He et al., 2022) for a language model such as RoBERTA (Liu et al.",1,related,1,positive
"In this work, we focus on the masked auto-encoding (Devlin et al., 2019; He et al., 2022) as a pre-training objective function since its generality compared to other self-supervised methods (Chen et al.",1,related,1,positive
"Masked Auto-Encoding We briefly describe the masked auto-encoding objective (Liu et al., 2019; He et al., 2022) for a language model such as RoBERTA (Liu et al., 2019) and Vision Transformer (ViT) (Dosovitskiy et al., 2021).",1,related,1,positive
"Specifically, we take the pre-trained model with an encoder fθinit and a decoder gϕinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fθ0 and gϕ0 .",1,related,1,positive
"Then we take the copy of the pre-trained initial network gϕinit ◦fθinit as a student and further pre-train the student with masked auto-encoding objective but enforce hidden representation of the encoder of the student fθinit to be close to that of the teacher fθ0 as follows:
(θ1, ϕ1) ∈ argmin θ,ϕ (LMAE(θ, ϕ;Du) + LDistill(θ; θ0,Du))
LDistill (θ; θ0,Du) = 1
n n∑ i=1 ∥∥∥fθ(x(i))− StopGrad(fθ0(x(i)))∥∥∥2 2
(3)
where θ and ϕ are initialized with the pre-trained parameters θinit and ϕinit, respectively and StopGrad denotes the stop-gradient operation which does not back-propagate through the input.",1,related,1,positive
"Then the final objective for masked auto-encoding is defined as follows:
LMAE(θ, ϕ;Du) = 1
n n∑ i=1 Ez(i)∼pγ,T (z)
[ −
K∑ k=1 z (i) k Z(i) · log pθ,ϕ(x(i)k |x̂(i))
] , Z(i) =
K∑ k=1 z (i) k , (2)
where pγ,K(z) denotes a Binomial distribution with its parameters γ for probability that zk = 1 and K for the number of trials.",1,related,1,positive
"In this experiment, we continue to pretrain the encoder of the pre-trained transformer (He et al., 2022) with supervised contrastive loss (Khosla et al.",1,related,1,positive
"Specifically, we take the pre-trained model with an encoder fθinit and a decoder gφinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fθ0 and gφ0 .",1,related,1,positive
"…model with an encoder fθinit and a decoder gϕinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fθ0 and gϕ0 .",1,related,1,positive
"In this work, we focus on the masked auto-encoding (Devlin et al., 2019; He et al., 2022) as a pre-training objective function since its generality compared to other self-supervised methods (Chen et al., 2020b; He et al., 2020; Grill et al., 2020; He et al., 2020; Chen & He, 2021; Caron et al.,…",1,related,1,positive
"In this experiment, we continue to pretrain the encoder of the pre-trained transformer (He et al., 2022) with supervised contrastive loss (Khosla et al.,
2020) while fine-tuning a randomly initialized linear classifier with cross-entropy loss.",1,related,1,positive
"Xiao et al. (2022) demonstrates that masked-autoencoder (He et al., 2022) trained on diverse video data (Goyal et al., 2017; Shan et al., 2020) can be an effective visual embedding for online RL.",1,related,1,positive
"We address this gap by profiling four popular self-supervised methods (BYOL [12], SimCLR [7], DINO [6], and MAE [13]) and two supervised methods (CLIP [27] and standard classification).",1,related,1,positive
"We use the MAE ViT [13] as the backbone in SimCLR, BYOL, and CLIP (using e.",1,related,1,positive
"1 Pre-training Methods and Models We run four common self-supervised methods (BYOL [12], SimCLR [7], DINO [6], and MAE [13]) and two supervised methods (CLIP [27] and standard softmax classification).",1,related,1,positive
"Similar to [13], we ran an exploratory learning rate (LR) search across {1e-5, 5e-5, 1e-4, 5e-4} and weight decay (WD) values {0.",1,related,1,positive
"4.3), we investigate several recent ones (DINO (Caron et al., 2021), MoCo-v3 (Chen et al., 2021), MSN (Assran et al., 2022), MAE (He et al., 2022)).",1,related,1,positive
"For the pre-trained weights, we use the timm library for DINO and third-party releases for MoCo-v35, MSN6, and MAE7.",1,related,0,negative
"To this end, we train DINOSAUR with a ResNet34 encoder (from scratch) on COCO, but reconstruct targets obtained from ViTs pre-trained with different methods: DINO, MoCo-v3, MSN, and MAE.",1,related,0,negative
"In the following, we use block 9 for supervised training, DINO and MSN, and block 12 for MoCo-v3 and MAE.",1,related,0,negative
"Method Ours SKD BYOL SimSiam MAE Transfer From Scratch Accuracy 82.7% 74.2% 68.3% 66.8% 62.3% 53.9% 28.4%
Figure 3: Examples of real and distilled images.",1,related,0,negative
"For each training batch, we compute each objective through a separate forward pass and use the weighted sum of them for the final loss, where λVAM = 1.0 and λMAE = 0.3.
loss = λVAMlossVAM + λMAElossMAE (1)",1,related,1,positive
"(1), we use λVAM = 1.0 and λMAE = 0.3.",1,related,1,positive
"Following MAE-AST [7], we randomly mask 75% of the spectrogram patches.",1,related,0,negative
"We calculate the mean squared error between the reconstructed and original video frames and spectrograms:
lossMAE = 1
NVM ∑ i∈masked ||xVi − x̂Vi ||22 + 1 NAM ∑ j∈masked ||xAj − x̂Aj ||22 (3)
whereNVM andN A M are the number of masked patches for vision and audio, respectively.",1,related,1,positive
"4.2, we use separate decoders (with shared weights) for the vision and audio MAE pretraining objectives.",1,related,0,negative
"Following MAE [26], we randomly mask 75% of the visual patches, and the masking is applied for each video frame independently.",1,related,1,positive
"In addition to the VAM objective to learn cross-modal representation, we also use the masked autoencoding (MAE) objective to improve unimodal representations in the vision-and-language settings, by masking random patches of visual frames and the audio spectrogram, and reconstruct missing inputs as shown in Fig.",1,related,1,positive
"In Figure 3 and Figure 4, we show the reconstruction results with the MAE head, described in the main paper Sec.",1,related,1,positive
"The combination of VAM and MAE further improves the finetuning performance, and we use this configuration as default for TVLT pretraining.",1,related,0,negative
"(a) Using the state-ofthe art reconstruction-based SSL strategy, MAE [8] architecture for pre-training an representation extractor (encoder).",1,related,1,positive
"As indicated in [1, 7, 8] that SSL training requires a large amount of data, we begin with training a MAE on SSL Pre-Training Set.",1,related,0,negative
"1), we utilize a state-of-the-art reconstructed-based strategy, Masked Auto-Encoder (MAE) [8] to learn lowerdimensional semantic feature embedding.",1,related,1,positive
"Motivated by theoretical implication of SSL embedding space, we leverage a Masked Autoencoder [8] for feature extraction.",1,related,1,positive
"In this section, we verify that GPaCo can enhance model generalization ability and robustness on full ImageNet and its variants when compared with MAE [14] models.",1,related,1,positive
"To demonstrate the advantages of GPaCo on out-ofdistribution data, we load MAE [14] pre-trained weights and then fine-tune on full ImageNet with same training strategy as in [14].",1,related,0,negative
"We also experiment with the recently proposed masked auto-encoder framework [60], which is based on the ViT",1,related,1,positive
"Another exception is the masked auto-enconder [60] based on the ViT backbone, where we place SSPCAB and SSMCTB before the first transformer block.",1,related,1,positive
"Mixed Feature Prediction Task Motivated by MAE [12] and Point-BERT [11], we wanted to extend this self-supervised method to point cloud segmentation.",1,related,1,positive
"Mixed Feature Prediction Task
Motivated by MAE [12] and Point-BERT [11], we wanted to extend this self-supervised method to point cloud segmentation.",1,related,1,positive
"We pretrained five encoders, one using our proposed method TOV-VICReg and four using state-of-the-art self-supervised methods: MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021), VICReg (Bardes et al., 2022), and MAE (He et al., 2021).",1,related,1,positive
"For this study we selected DINO (Caron et al., 2021), MoCo (Chen et al., 2021), MAE (He et al., 2021), and VICReg (Bardes et al., 2022) since they are currently considered state-of-the-art, their official implementations are available in
PyTorch, and each represents a different type of approach.",1,related,0,negative
"Lastly, we also consider MAE (He et al., 2021), a masked reconstruction method, which consists in training an auto-encoder based on ViT to reconstruct an image with a set of patches masked.",1,related,1,positive
"The procedures here can be summarized as follow:
y = ClassHead(FF (CLSL))
ViT and other models based on it have found much success in producing state-of-the-art results in image processing domains [23, 24] and have been the building block for many pre-trained models such as, to mention a few, DINO [25], MAE [26], and MOCOv3 [27].",1,related,1,positive
"We take Vit [38], MAE [185], and MoCo [186] in our experiments.",1,related,1,positive
"To further reduce the encoder training time and improve robustness, a pre-trained encoder [27], [28] could be used and fine-tuned on our data.",1,related,0,negative
"Thus, we treat STMM as images and borrow the training approach of MAE [14].",1,related,1,positive
"To this end, we treat STMM as images and borrow the key principle of MAE [14] to process",1,related,1,positive
‘w/ SSL’ denotes training the segmentation network with the model weights pre-trained by a 3D Masked Autoencoder (MAE) SSL method [8].,1,related,1,positive
"Implementation Details To pre-train the ASA model, we use center-cropping augmentation, Xavier uniform initializer [5] for SW-ViT blocks and set the hyper-parameters following [8] (see Table 1(a)).",1,related,1,positive
"To address this problem, the Masked Autoencoder (MAE) self-supervised training method [41] is used to pre-train the proposed model on unlabeled data.",1,related,1,positive
"Further, the Masked Autoencoder pre-training method [41] is introduced.",1,related,0,negative
"By combining MAE [19] pre-training and DAT fine-tuning, our ViT-Huge [20] achieves 31.40 mCE on ImageNet-C [2] and 32.77% top-1 accuracy on Stylized-ImageNet [3].",1,related,1,positive
"Besides, we use DAT to conduct supervised finetuning on downstream ImageNet classification task based on a self-supervised ViTHuge pretrained by MAE [19].",1,related,1,positive
"We perform the pre-training on three largescale medical image-text datasets, i.e., ROCO [40], MedICaT [44], and MIMIC-CXR [21].",1,related,1,positive
"For ROCO and MedICaT, we filter nonradiology samples, and for MIMIC-CXR, we only keep images in the frontal view.",1,related,0,negative
"As for the dataset split, we adopt the official splits of ROCO and MIMIC-CXR.",1,related,1,positive
"For the pretext tasks, we adopt (knowledge-enhanced) MLM, MIM [17], and ITM, where the masking ratios of MLM and MIM are set to 15% and 75%, respectively.",1,related,1,positive
"We first fine-tune a ViT-Large model [9, 14] on Places365 [54], which is dubbed PlacesViT.",1,related,0,negative
The architectural settings strictly follow [19].,1,related,0,negative
"As a special case of MIM, we formulate MKD upon which an empirical investigation is conducted about the influence of different target representations on self-supervised masked autoencoders.",1,related,1,positive
"To answer these questions, we employ the standard masked autoencoder framework [19] to give a system-level study, introduced next.",1,related,1,positive
"To be specific, MIM randomly masks a portion of the input and then reconstructs the masked portion according to the transformed target, formulated as
min θ E x∼D M(T (x (1−M)), fθ(x M)), (1)
where “ ” means element-wise product; M is the patch mask; “x M” represents “unmasked patches” and vice ∗Equal contribution.",1,related,1,positive
"In this work, we paraphrase a term Masked Knowledge Distillation (MKD) to focus our discussion on a special case of MIM where the target is generated by a parameterized network (teacher network), i.e., T (·) = hφ(·).",1,related,1,positive
method data2vec [2] BEiT [3] MAE [19] dBOT,1,related,1,positive
"Similar to other MIM-based SSL work [19, 45], we reconstruct knowledge for those masked patches of input sample x .",1,related,1,positive
"MIM [45], MAE [19], and our MimCo on ImageNet-1K dataset.",1,related,1,positive
"What Semantic PatternsDoesMimCoLearn? To further help reveal what patterns does MIM learn, we follow the visualization of iBOT [47] to explore the learned patterns of the pre-trained models of SimMIM [45], MAE [19], and our MimCo via visualization, respectively.",1,related,1,positive
"We randomly choose 10 classes of ImageNet-1K dataset to visualize for simplicity, the visualization of learned representation shows that our MimCo significantly improves the linear separability of representations compared to SimMIM [45] and MAE [19].",1,related,1,positive
"Furthermore, we choose different generative blocks, including cross-attention block (Chen et al., 2022), self-attention block (He et al., 2022), and convolutional projector (Yang et al., 2022e).",1,related,1,positive
"Different from existing works that focus on MVM for pure vision problems [4, 22, 86], we study MVM as a VidL pre-training task.",1,related,0,negative
"Inspired by the MAE [15], we propose a spatial-temporal masked autoencoder framework for self-supervised 3D skeleton-based action recognition (SkeletonMAE).",1,related,1,positive
The generative SSL trains a generator consisting of an encoder and decoder to reconstruct the input data.,1,related,1,positive
"Like what ImageMAE does in [9], we directly discard a subset (e.g., 50",1,related,1,positive
"Like what ImageMAE does in [9], we directly discard a subset (e.",1,related,1,positive
"We propose the MAE-VQGAN model, which combines ideas from MAE [20] and VQGAN [15].",1,related,1,positive
"…our lexiconbottlenecked masked autoencoder (LexMAE) contains one encoder and one decoder with masked inputs in line with the masked autoencoder (MAE) family (He et al., 2021a; Liu & Shao, 2022), while is equipped with a lexicon-bottlenecked module for document-specific lexicon-importance learning.",1,related,1,positive
"We also compared our model with the few-shot counting sota method Fam-
Net [24], and our model outperforms it with a large advance (15.16 MAE and 24.29 RMSE on Val-COCO and 11.87 MAE and 14.81 RMSE on Test-COCO), which demonstrates the superiority of our model.",1,related,1,positive
"We use two standard metrics to measure the performance of our model, namely, Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).",1,related,1,positive
"MAE = 1 NI
NI ∑ i=1 |Ci−CGTi |, RMSE = √√√√ 1 NI NI ∑ i=1 (Ci−CGTi )2 (6)
Here, NI is the total number of testing images, and Ci and CGTi are the predicted number and ground truth of the ith image.",1,related,1,positive
"In this section, we aim to give the detail of our proposed two-stage training procedure, that is, first pre-train the ViT encoder with MAE [11], and then fine-tune the whole model on supervised object counting.",1,related,1,positive
"We propose a two-stage training scheme, with the transformer-based image encoder being firstly pre-trained with self-supervision via masked image modeling [11], followed by supervised fine-tuning for the task at hand.",1,related,1,positive
"# Shots Val Test
MAE RMSE MAE RMSE
A0 % % % % 0 24.84 86.",1,related,0,negative
"As input for pre-training ViT with MAE, we randomly drop 50% of the visual tokens, and task the model to reconstruct the masked patches with pixel-wise mean square error.",1,related,1,positive
"1 Training Details In this section, we aim to give the detail of our proposed two-stage training procedure, that is, first pre-train the ViT encoder with MAE [11], and then fine-tune the whole model on supervised object counting.",1,related,1,positive
"Specifically, we adopt the recent idea from Masked Autoencoders (MAE), to train the model by image reconstruction with only partial observations.",1,related,1,positive
"In Section 3.2, we further introduce a two-stage training regime, in which the model is firstly pre-trained with self-supervision via masked image reconstruction (MAE), followed by fine-tuning on the downstream counting task.",1,related,1,positive
"The two methods are based on the recent Masked Auto Encoder (MAE) approach [26], where we randomly mask the image ( e.g. , 75",1,related,1,positive
"The two methods are based on the recent Masked Auto Encoder (MAE) approach [29], where we randomly mask the image (e.",1,related,1,positive
"We also exploit more efficiently the input data with Masked Image Modeling, based on the recently proposed Masked Auto Encoder (MAE) [26].",1,related,1,positive
"We also exploit more efficiently the input data with Masked Image Modeling, based on the recently proposed Masked Auto Encoder (MAE) [29].",1,related,1,positive
"We propose a new efficient VLP approach centered on 3 main components; stronger Vision-Language pre-alignment through hierarchical contrastive objective, self supervision via masked image modeling based on MAE, and a new Visual Concepts injection and extraction technique.",1,related,1,positive
"We favor the MAE based, unimodal MIM loss which improves the results by 2.4% RSUM.",1,related,0,negative
"With the most recent representative work in this direction, masked autoencoder (MAE) [11], we are now able to efficiently and effectively pre-train high-capacity Vision Transformers (ViTs) with strong feature representations, leading to state-of-the-art solutions for a wide range of downstream visual tasks.",1,related,1,positive
"Following MAE [11], we first perform self-supervised pre-training on ImageNet-1k [7].",1,related,0,negative
We choose a decoder depth of 8 as the default setting as in [11].,1,related,1,positive
"Following the setting in MAE [7], we add a batch-norm layer without learnable affine parameters before the classifier to avoid adjusting the learning rate for each model.",1,related,1,positive
"CLIP
+
SimCLR
CLIP
+ MAE
Sandwich bread
MaskCLIP
(Ours)
CLIP
Bird
MaskCLIP
(Ours)
CLIP
CLIP
+
SimCLR
CLIP
+ MAE
SnowMountain goats Santa hatBearded Man
BandanaDog",1,related,1,positive
We start from CLIP+MAE and add three components of the distillation loss one by one.,1,related,1,positive
We adopted the MAE structure proposed in [14].,1,related,0,negative
"Following the previous work [9], we take ViTB [57] as the backbone network, which consists of 12 transformer layers and was pre-trained on ImageNet-21K with the self-supervised method MAE [21].",1,related,1,positive
"Inspired by [8], we design a decoder using a lightweight transformer structure [8] and is only used during the pre-training phase.",1,related,0,negative
"For image encoder, we employ a standard ViT-B/16 (Dosovitskiy et al. 2020) with random masking strategy (He et al. 2022).",1,related,1,positive
"Following MAE (He et al. 2022), we randomly divide the patches",1,related,1,positive
"Following (He et al. 2022), we take a 4-layer transformer with 512 dim as our image decoder.",1,related,1,positive
"Unlike MAE (He et al. 2022), we aim to reconstruct the invisible patches with both image and text features to facilitate multi-modal information fusion.",1,related,1,positive
"Motivated by recent generative methods in visual self-supervised learning (He et al. 2022; Chen et al. 2022), we propose to introduce pixel-level reconstruction task to VLP.",1,related,1,positive
"Following MAE (He et al. 2022), we adopt a lightweight vision transformer as our image decoder.",1,related,1,positive
"Following MAE (He et al. 2022), we randomly divide the patches into visible patches { xvisi }N−M i=1
and invisible patches{ xmski }M i=1
according to mask ratio α, where M = αN .",1,related,1,positive
"We compared the performance with other baselines involving Vision Transformers such as ConViT [16], Masked Auto Encoders (MAE) [53], Convolution-enhanced image Transformer (CeiT) [33], LeViT [34].",1,related,1,positive
"Inspired by (He et al. 2022; Wettig et al. 2022) that a large mask rate may be beneficial for pre-training, we employ a mask rate greater than the BERT’s 15% setting.",1,related,0,negative
"Inspired by (He et al. 2022; Wettig et al. 2022) that a large mask rate may benefit pre-training, we adopt up to 30% and 45% mask rate in CoT-MAE’s encoder and decoder, respectively.",1,related,0,negative
"Thus, borrowing insights from masked autoencoders [33], we believe most of the points can be redundant and we randomly masked the points with a probability of 75% per visit.",1,related,1,positive
"In the long series prediction, we employ Mean Squared Error (MSE) and Mean Absolute Error (MAE) as indicators and follow the setting (Zhou et al. 2021).",1,related,1,positive
"The formula is as follows: • Mean Squared Error (MSE):
MSE = 1
n n∑ i=1 (Yi − Ŷi)2
• Mean Absolute Error (MAE):
MAE = 1
n n∑ i=1 |Yi − Ŷi|
In the short series prediction, we employ Root Relative Squared Error (RSE) and Empirical Correlation Coefficient (CORR) as indicators and follow the setting (Lai et al. 2018).",1,related,1,positive
"In the absence of prior work applying masked autoencoding to abstract/synthetic imagery, we explore a large masked pixel percentage (75% of the image), thus forcing the model to attempt to recover the image based only on the unmasked 25% of the image, such high percentages have been shown to work well for natural imagery [30].",1,related,1,positive
%) of ViT-B and ViT-L trained by selfsupervised MAE on ImageNet.,1,related,0,negative
"2) self-supervised settings: we follow the MAE training framework to pretrain and fine-tune ViT-B and ViT-L, and report results in Table 3.",1,related,0,negative
"To obtain the benefits of self-supervised learning in video classification, we utilize VideoMAE pre-trained in Something-Something [8] to extract video features inherent in make-up scenes.",1,related,0,negative
"When self-pretrained, MAE [24] is mainly used, and we directly use their pretrained models.",1,related,0,negative
"When the model is self-pretrained by MAE [24], we first evaluate the fine-tuning performances of MAE on the labeled data only, as the common practice in self/un-supervised learning literature [25, 14, 22], with results shown Table 1.",1,related,1,positive
"Following the training recipe provided by MAE [25], the ViT models pretrained on ImageNet1K dataset serve as the backbone of UperNet [59], and are finetuned together with the segmentation layers.",1,related,0,negative
We use a masked autoencoder architecture similar to MAE [25].,1,related,1,positive
"Using the default 75% masking ratio, our prompting decoder reduces the decoding computation cost by 20% compared to MAE [25].",1,related,1,positive
"Our work takes some inspiration from MAE, however, PatchDropout can be applied to target tasks directly using standard ViTs (unlike MAE).",1,related,0,negative
"To ensure the effectiveness of training, we compute the loss on the common parts of the masked patches of T1 and unmasked patches of T2 (following [29]).",1,related,1,positive
"For simplicity, we take MAE [29] as an representative example of MIM, in which the similarity measurement is simply l2−distance on the masked patches.",1,related,1,positive
"3In original MAE [29], the encoder network only generates tokens of unmasked patches and the decoder only predict the masked patches during training.",1,related,0,negative
"Furthermore, taking MAE [29] as a representative example of MIM, we empirically find the success of MIM models relates a little to the choice of similarity functions, but the learned occlusion invariant feature introduced by masked image – it turns out to be a favored initialization for vision transformers, even though the learned feature could be less semantic.",1,related,1,positive
"Inspired by the above pilot studies, we employ the same MIM pretraining and finetuning routine to investigate the influences of plain ViTs on RS tasks.",1,related,0,negative
"For more details about MAE, please refer to [12].",1,related,0,negative
"Different from these works, we use the representative MAE method [12] for MIM pretraining and specifically focus on advancing plain ViTs for remote sensing tasks.",1,related,0,negative
"Different from these works, we use the representative MAE method [12] for",1,related,1,positive
"To explore the effectiveness of MIM pretraining in remote sensing tasks, we pretrain our model using MAE [12] with the large-scale RSI dataset, i.e., MillionAID [11].",1,related,1,positive
"To explore the effectiveness of MIM pretraining in remote sensing tasks, we pretrain our model using MAE [12] with the large-scale RSI dataset, i.",1,related,1,positive
