text,label_score,label
"Instead of mixing the representations, G-Mixup [49] augments graphs by interpolating the graphon-based generator of graphs belonging to different classes.",,
Graph Data Augmentation DropEdge [131] Randomly remove edges Perturbation-based GRAND [39] Stochastically discard nodes Perturbation-based NASA [9] Random neighbor replacement strategy Perturbation-based NodeAug [171] Graph properties based probability Perturbation-based GAUG [209] Learnable edge sampling probability Perturbation-based GraphMix [160] Hidden states and labels interpolation Synthetic sample-based GraphMixup [170] Node/Edge embedding mixup Synthetic sample-based GraphSMOTE [211] Node embedding based synthesis Synthetic sample-based FLAG [79] Gradient-based adversarial perturbations Perturbation-based G-Mixup [49] Graphon-based generator interpolation Synthetic sample-based LAGNN [93] Conditional distribution based synthesis Synthetic sample-based,,
"DropEdge [131] Randomly remove edges Perturbation-based GRAND [39] Stochastically discard nodes Perturbation-based NASA [9] Random neighbor replacement strategy Perturbation-based NodeAug [171] Graph properties based probability Perturbation-based GAUG [209] Learnable edge sampling probability Perturbation-based GraphMix [160] Hidden states and labels interpolation Synthetic sample-based GraphMixup [170] Node/Edge embedding mixup Synthetic sample-based GraphSMOTE [211] Node embedding based synthesis Synthetic sample-based FLAG [79] Gradient-based adversarial perturbations Perturbation-based G-Mixup [49] Graphon-based generator interpolation Synthetic sample-based LAGNN [93] Conditional distribution based synthesis Synthetic sample-based class-balanced graph-structured data in a graph data-centric way, thereby benefiting the training of graph machine learning models.",,
"For G-Mixup, we use the same hyper-parameters reported in [7].",,
"Few works have proposed methods to adapt mixup to graph data, including G-Mixup [7] and M-Mixup [21].",,
"Only in the case of the Reddit-5K dataset with label rates of larger than 25 labeled graphs per class, G-Mixup outperforms our proposed method.",,
"G-Mixup performs mixup to the graphons of different classes which are learned from the graph samples, and generates augmented graphs by sampling from the mixed graphons [7].",,
"We apply our proposed Graph Dual Mixup on the Graph Convolution Network (GCN) baseline [11] and compare our proposed method against 5 other graph augmentation methods from the literature: DropNode [25], DropEdge [14], M-Mixup [21], SoftEdge [5] and G-Mixup [7].",,
Our result generalizes that of [16] by allowing arbitrary convex combinations and any complexon dimension.,,
"Note that for complexons of dimension 1, when Œ≥i = Œª, Œ≥j = 1 ‚àí Œª, and Œ≥k = 0 for every k Ã∏= i, j, Theorem 1 reduces to the result for pairwise graphon mixup in [16].",,
"Step (1) of SC-MAD is common for mixup methods, where samples are interpolated in an embedding space [16, 17, 28].",,
"Graphons allow us to perform tasks on graph data typically restricted to continuous objects, such as barycenter obtention and interpolation for mixup [16, 17, 22].",,
"Variants have been proposed for several domains and applications, including graph mixup [16, 17], interpolation in an embedding space [28], and nonlinear implementations [17].",,
"In particular, we assume that for each class y, there is a finite set of discriminative simplicial complexes Fy such that for every labeled simplicial complex (K, y), there exists at least one F ‚àà Fy that is a subcomplex of K [16], that is, there is a homomorphism from F to K.",,
"We present the following result on the structural similarities between a complexon mixture and one of the complexons, inspired by a similar result for graphon mixup [16].",,
"In the past few years, graph neural networks (GNNs) have achieved superior performance in graph-level representation learning [26, 87].",,
"As Mixup assumes that all the classes are uniformly distributed for image classification, it does not apply when the class distribution is skewed.",,
"The core idea of Mixup is to linearly combine two samples as follows:
ùë•syn = ùõº ‚àó ùë•0 + (1 ‚àí ùõº) ‚àó ùë•1, (1)
where ùë•0 and ùë•1 are the two selected source samples and ùõº ‚àà [0.0, 1.0] controls the composition ofùë•syn.",,
"To generalize label information from two different classes,Mixup [44] performs synthetic data generation over two samples from different classes, which has been extensively studied to augment image and textual data.",,
"Recently, instead of conducting synthetic data sampling on a single class, Mixup [7, 13, 23, 44] achieves a significant improvement in the image domain by synthesizing data points through linearly combining two random samples from different classes with a given combination ratio and creating soft labels for training the neural networks.",,
"In addition, the graphs we considered in the experiment all have node features, while G-Mixup [49] only applies to undirected graphs without node features, and therefore is not within the scope of our baselines.",,
"Several recent studies have implemented mixup techniques on graphs, including GraphTransplant [9], G-Mixup [5], IfMixup [3], and Manifold Mixup [13].",,
"‚Ä¶pairs of training examples to extend the training distribution and prevent the deep neural network from overfitting the training data (Zhang et al. 2018; Yun et al. 2019; Kim, Choo, and Song 2020; Beckham et al. 2019; Verma et al. 2019; Wang et al. 2021; Han et al. 2022a; Yao et al. 2022).",,
"To this end, mixup-based methods create mixed samples by combining pairs of training examples to extend the training distribution and prevent the deep neural network from overfitting the training data (Zhang et al. 2018; Yun et al. 2019; Kim, Choo, and Song 2020; Beckham et al. 2019; Verma et al. 2019; Wang et al. 2021; Han et al. 2022a; Yao et al. 2022).",,
"Most of the previous mixup variants focus on designing how to mix different samples so that the mixed samples are helpful for neural network training (Yun et al. 2019; Kim, Choo, and Song 2020; Beckham et al. 2019; Verma et al. 2019; Wang et al. 2021; Han et al. 2022a; Yao et al. 2022).",,
"In addition, mixup variants have been shown to be effective on a variety of tasks, including fairness machine learning (Han et al. 2022b, 2023; Mroueh et al. 2021), domain generalization (Zhou et al. 2020; Yao et al. 2022), confidence calibration (Zhang et al. 2022; Thulasidasan et al. 2019).",,
"Due to the simplicity and effectiveness, mixup-based methods have gained popularity in various data types and tasks (Yun et al. 2019; Kim, Choo, and Song 2020; Kim et al. 2023; Sahoo et al. 2021; Wang et al. 2021; Han et al. 2022a; Verma et al. 2019; Han et al. 2022b, 2023; Zhou et al. 2020; Mroueh et al. 2021; Zhang et al. 2022; Thulasidasan et al. 2019).",,
"Overall, previous mixup variants mainly focus on improving the mixing process to extend the training distribution (Yun et al. 2019; Kim, Choo, and Song 2020; Kim et al. 2023; Sahoo et al. 2021; Verma et al. 2019; Wang et al. 2021; Han et al. 2022a).",,
"‚Ä¶have gained popularity in various data types and tasks (Yun et al. 2019; Kim, Choo, and Song 2020; Kim et al. 2023; Sahoo et al. 2021; Wang et al. 2021; Han et al. 2022a; Verma et al. 2019; Han et al. 2022b, 2023; Zhou et al. 2020; Mroueh et al. 2021; Zhang et al. 2022; Thulasidasan et al. 2019).",,
"graph classification [34], [35], link prediction [36], [37], [38], [39], node clustering [40], [41], and anomaly detection [42], [43], [44], [45].",,
"Note that our proposed mixup approach is different from traditional mixup approaches [15, 49, 54] in data augmentation, where they usually follow a form similar to M (mix) = ŒªMa + (1 ‚àí Œª)Mb .",,
"[15] and [45] generate interpolated graphs with the estimation of the properties in the graph data, like the graphon of each class or nearest neighbors of target nodes.",,
"All the previous methods [13, 15, 39, 40, 43] aim to generalize the mixup approach to improve the performance of classification models like GNNs.",,
"There are also many mix-up related technologies including GraphMix [40], MixupGraph [42], GMixup [15], and ifMixup [14].",,
", natural image [27]‚Äì[29], graph [30], 3D point cloud, [31], visual-language [32], etc.",,
Mixup (2018) [26] data-agnostic 3 aligned by default interpolate 3 input interpolation scale TransMix (2022) [29] natural image 3 scaling or cropping mask & mix 3 target attention weights G-Mixup (2022) [30] graph 3 graphon estimation interpolate 3 graphon interpolation scale PointPatchMix (2023) [31] point cloud 3 point patch mask & mix 3 patch attention scores,,
Another work [12] proposes to learn a graph generator to align the pair of graphs and interpolate the generated counterparts.,,
"Graph neural network (GNN) has been widely used in multiple tasks, such as node classification [7], [8], graph classification [9], [10], and link prediction [11], [12], and has achieved better task performance than traditional methods, which proves that graph neural networks can learn the representation of nodes very well.",,
"In recent years, Graph data augmentation techniques based on graphs and subgraphs have been extensively studied [14], [37].",,
"Note that G-mixup relies on a strong assumption that graphs from the same class can be generated by the same graph generator (i.e., graphon).",,
Han et al. (2022) proposes to learn a Graphon for each class and performs mixup in Graphon space.,,
", 2022), which mixes random subgraphs of input graph pairs; (6) G-Mixup (Han et al., 2022), which is a class-level graph mixup method by interpolating graphons of different classes.",,
"Meanwhile, G-Mixup generates the same node features for all augmented graphs, thus leading to performance degradation on PROTEINS and NCI1 datasets which have node features.",,
"‚Ä¶al., 2019; Wang et al., 2021b)4, which linearly interpolates the graph-level representations; (5) SubMix (Yoo et al., 2022), which mixes random subgraphs of input graph pairs; (6) G-Mixup (Han et al., 2022), which is a class-level graph mixup method by interpolating graphons of different classes.",,
"Several existing graph mixup methods (Han et al., 2022; Park et al., 2022; Yoo et al., 2022; Guo & Mao, 2021) use various tricks to sidestep this problem.",,
"Instead of directly mixing instances, G-mixup (Han et al., 2022) proposes a class-level graph mixup method that interpolates the graph generators of different classes.",,
"Comparison between ours and other graph mixup methods Preserving Mixing node Perserving Methods Instance-level motif feature space Input-level graph size G-mixup (Han et al., 2022) ‚úì ‚úì",,
"We compare our methods with the following baseline methods, including (1) DropEdge (Rong et al., 2020), which uniformly removes a certain ratio of edges from the input graphs; (2) DropNode (Feng et al., 2020; You et al., 2020), which uniformly drops a certain portion of nodes from the input graphs; (3) Subgraph (You et al., 2020), which extract subgraphs from the input graphs via a random walk sampler; (4) M-Mixup (Verma et al., 2019; Wang et al., 2021b)4, which linearly interpolates the graph-level representations; (5) SubMix (Yoo et al., 2022), which mixes random subgraphs of input graph pairs; (6) G-Mixup (Han et al., 2022), which is a class-level graph mixup method by interpolating graphons of different classes.",,
"(19)
For the setting of classification, ùëÉ (ùëÜùëù,ùëû | ùë¶ = ùëê) ‚àº N ( ùúá ùëù,ùëû ùëê , (ùúé ùëù,ùëû ùëê )2 ) , (20)
To extend G-Mixup for regression, we slightly modify the augmentation process to adapt it for regression tasks as
ùëÉ (ùëÜùëù,ùëû | ùë¶) ‚àº N ( ùúáùëù,ùëû + ùúéùëù,ùëû
ùúéùë¶ ùúåùëù,ùëû
( ùë¶ ‚àí ùúáùë¶ ) , ( 1 ‚àí (ùúåùëù,ùëû)2 ) (ùúéùëù,ùëû)2 ) ,
(21)
where ùúá and ùúé are the mean and standard deviation of the weight for each edge, ùúå is the correlation coefficient between ùëÜùëù,ùëû and ùë¶.
C-Mixup [93] shares the same process with the V-Mixup.",,
"There are several attempts to study Mixup on non-Euclidean data, graphs, like NodeMixup [84], GraphMixup [85] and G-Mixup [39].",,
"(18)
G-Mixup [39] is originally proposed for classification tasks, which augments graphs by interpolating the generator of different classes of graphs.",,
"G-Mixup [39] is originally proposed for classification tasks, which augments graphs by interpolating the generator of different classes of graphs.",,
"Other works study OOD graph classification tasks and can be categorized similarly as above (Zhu et al., 2021b; Miao et al., 2022; Chen et al.; Li et al., 2022a; Han et al., 2022; Yang et al., 2022; Suresh et al., 2021).",,
"Baselines For baseline augmentation models, we employ two classical graph augmentation methods: DropEdge [4] and DropNode [5], and three Mixup-based augmentations: SubMix [10], ManifoldMixup (M-Mixup) [8], and G-Mixup [11].",,
G-mixup [11] mixes graphons [32] of different classes and augments training set by generating the graphs from the mixed graphon.,,
"The detailed statistics of each dataset are shown in Appendix A.
Baselines For baseline augmentation models, we employ two classical graph augmentation methods: DropEdge [4] and DropNode [5], and three Mixup-based augmentations: SubMix [10], ManifoldMixup (M-Mixup) [8], and G-Mixup [11].",,
", node, link [10, 16], and graph predictions [8, 18].",,
"For example, the critical edges connecting different clusters within a graph play a crucial role in tasks like graph partition [16, 28], graph classification [18, 12], and link prediction [55, 50].",,
"However, we cannot use Mixup directly because it is suitable for regular, Euclidean data [54], while the user‚Äôs rating is discrete and non-interpolative, and there is no label for supervised learning.",,
G-Mixup[11] employs graphon to augment graphs and improve graph classifcation task.,,
"Data enhancement is derived on existing annotated data pictures, mainly using data augmentation methods and advanced data augmentation methods, commonly used data augmentation methods mainly include geometric transformation and color transformation, and advanced data augmentation methods mainly include Mixup [10], Cutout [11], Cutmix [12] and Mosaic [13].",,
"Graph augmentation modifies the overall structure of the graph, and can be seen as a combination of the previous methods [Han et al., 2022].",,
"[17,2,4] used mixup based techniques to augment the graph data so as to improve the training performance.",,
"Techniques like random graph data augmentations (e.g., edge and node dropping) (Han et al., 2022; Liu et al., 2022) and large-scale pre-training on diverse graphs (You et al., 2020a;b; 2021; Hou et al., 2022) have been widely adopted to augment the diversity of training graph structures.",,
", edge and node dropping) (Han et al., 2022; Liu et al., 2022) and large-scale pre-training on diverse graphs (You et al.",,
"Literaturelly, graphon has been studied from two perspectives: as limit of graph sequence, and as graph generators[1, 11, 24].",,
"[3] mentioned the same motifs were also found from bacteria [11] to yeast [29], animal [32] to plants.",,
"‚Ä¶learning methods including self-training with selected unlabeled graphs (ST-REAL) and generated graphs (ST-GEN) and INFOGRAPH (Sun et al., 2020), and (3) graph data augmentation (GDA) methods including FLAG (Kong et al., 2022), GREA (Liu et al., 2022), and G-MIXUP (Han et al., 2022).",,
"Generation models (Antoniou et al., 2017; Bowles et al., 2018; Han et al., 2022) create in-class examples.",,
"GDA (GREA and G-MIXUP) methods outperform self-training in most classification tasks except ogbg-SIDER, because they are often designed to exploit categorical labeled data and remain underexplored for regression.",,
"They learn to create new examples that preserve the properties of original graphs (Kong et al., 2022; Han et al., 2022; Luo et al., 2022).",,
"Baselines and implementation: Besides GIN, there are three lines of baseline methods: (1) selfsupervised learing methods including EDGEPRED, ATTRMASK, CONTEXTPRED in (Hu et al., 2019), INFOMAX (Velickovic et al., 2019), JOAO (You et al., 2021), GRAPHLOG (Xu et al., 2021), and D-SLA (Kim et al., 2022), (2) semi-supervised learning methods including self-training with selected unlabeled graphs (ST-REAL) and generated graphs (ST-GEN) and INFOGRAPH (Sun et al., 2020), and (3) graph data augmentation (GDA) methods including FLAG (Kong et al., 2022), GREA (Liu et al., 2022), and G-MIXUP (Han et al., 2022).",,
"‚Üë ogbg-HIV ogbg-ToxCast ogbg-Tox21 ogbg-BBBP ogbg-BACE ogbg-ClinTox ogbg-SIDER
# Training Graphs 32,901 6,860 6,264 1,631 1,210 1,181 1,141
GIN 77.4(1.2) 66.9(0.2) 76.0(0.6) 67.5(2.7) 77.5(2.8) 88.8(3.8) 58.1(0.9)
Se lf-
Su pe
rv is
ed EDGEPRED 78.1(1.3) 63.9(0.4) 75.5(0.4) 69.9(0.5) 79.5(1.0) 62.9(2.3) 59.7(0.8) ATTRMASK 77.1(1.7) 64.2(0.5) 76.6(0.4) 63.9(1.2) 79.3(0.7) 70.4(1.1) 60.7(0.4) CONTEXTPRED 78.4(0.1) 63.7(0.3) 75.0(0.1) 68.8(1.6) 75.7(1.0) 63.2(6.5) 60.7(0.8) INFOMAX 75.4(1.8) 61.7(1.0) 75.5(0.4) 69.2(0.5) 76.8(0.2) 73.0(0.2) 58.6(0.5) JOAO 76.2(0.2) 64.8(0.3) 74.8(0.5) 69.3(2.5) 75.9(3.9) 69.4(4.5) 60.8(0.6) GRAPHLOG 74.8(1.1) 63.2(0.8) 75.4(0.8) 67.5(2.3) 80.4(3.6) 69.0(6.6) 57.0(0.9) D-SLA 76.9(0.9) 60.8(1.2) 76.1(0.1) 62.6(1.0) 80.3(0.6) 78.3(2.4) 55.1(1.0)
Se m
iSL INFOGRAPH 73.3(0.7) 61.5(1.1) 67.6(0.9) 61.6(4.4) 75.9(1.8) 62.2(5.5) 56.3(2.3)
ST-REAL 78.3(0.6) 64.5(1.0) 76.2(0.5) 66.7(1.9) 77.4(1.8) 82.2(2.4) 60.8(1.2) ST-GEN 77.9(1.6) 65.1(1.0) 75.8(0.9) 66.3(1.5) 78.4(3.0) 87.3(1.3) 59.3(1.3)
G D A FLAG 74.6(1.7) 59.9(1.6) 76.9(0.7) 66.6(1.0) 79.1(1.2) 85.1(3.4) 57.6(2.3) GREA 79.3(0.9) 67.5(0.7) 77.2(1.2) 69.7(1.3) 82.4(2.4) 87.9(3.7) 60.1(2.0) G-MIXUP 77.1(1.1) 55.6(1.1) 64.6(0.4) 70.2(1.0) 77.8(3.3) 60.2(7.5) 56.8(3.5)
DCT (Ours) 79.5(1.0) 68.1(0.2) 78.2(0.2) 70.8(0.5) 85.6(0.6) 92.1(0.8) 63.9(0.3) Molecule Regression: MAE ‚Üì Polymer Regression: MAE ‚Üì Bio: AUC (%)‚Üë
ogbg-Lipo ogbg-ESOL ogbg-FreeSolv GlassTemp MeltingTemp ThermCond O2Perm PPI # Training Graphs 3,360 902 513 4,303 2,189 455 356 60,715
GIN 0.545(0.019) 0.766(0.016) 1.639(0.146) 26.4(0.2) 40.9(2.2) 3.25(0.19) 201.3(45.0) 69.1(0.0)
Se lf-
Su pe
rv is
ed EDGEPRED 0.585(0.008) 1.062(0.066) 2.249(0.150) 27.6(1.4) 47.4(2.8) 3.69(0.50) 207.3(41.7) 63.7(1.1) ATTRMASK 0.573(0.009) 1.041(0.041) 1.952(0.088) 27.7(0.8) 45.8(2.6) 3.17(0.32) 179.9(30.8) 64.1(1.8) CONTEXTPRED 0.592(0.007) 0.971(0.027) 2.193(0.151) 27.6(0.3) 46.7(1.9) 3.15(0.24) 191.2(35.2) 62.0(1.2) INFOMAX 0.581(0.009) 0.935(0.018) 2.197(0.129) 27.5(0.8) 46.5(2.8) 3.31(0.25) 231.0(52.6) 63.3(1.2) JOAO 0.596(0.016) 1.098(0.037) 2.465(0.095) 27.5(0.2) 46.0(0.2) 3.55(0.26) 207.7(43.7) 61.5(1.2) GRAPHLOG 0.577(0.010) 1.109(0.059) 2.373(0.283) 29.5(1.3) 50.3(3.3) 3.01(0.17) 229.7(48.3) 62.1(0.6) D-SLA 0.563(0.004) 1.064(0.030) 2.190(0.149) 27.5(1.0) 51.7(2.5) 2.71(0.08) 257.8(30.2) 65.0(1.2)
Se m
iSL INFOGRAPH 0.793(0.094) 1.285(0.093) 3.710(0.418) 30.8(1.2) 51.2(5.1) 2.75(0.15) 207.2(21.8) 67.7(0.4)
ST-REAL 0.526(0.009) 0.788(0.070) 1.770(0.251) 26.6(0.3) 42.3(1.2) 2.64(0.07) 256.0(17.5) 68.9(0.1) ST-GEN 0.531(0.031) 0.724(0.082) 1.547(0.082) 26.8(0.3) 42.0(0.9) 2.70(0.03) 262.2(10.1) 68.6(0.6)
G D A FLAG 0.528(0.012) 0.755(0.039) 1.565(0.098) 26.6(1.3) 44.2(2.0) 3.05(0.10) 177.7(60.7) 69.2(0.2) GREA 0.586(0.036) 0.805(0.135) 1.829(0.368) 26.7(1.0) 41.1(0.8) 3.23(0.18) 194.0(45.5) 68.8(0.2)
DCT (Ours) 0.516(0.071) 0.717(0.020) 1.339(0.075) 23.7(0.2) 38.0(0.8) 2.59(0.11) 165.6(24.3) 69.5(0.2)
(ogbg-HIV, ogbg-ToxCast, ogbg-Tox21, ogbg-BBBP, ogbg-BACE, ogbg-ClinTox, ogbg-SIDER), three molecule regression tasks (ogbg-Lipo, ogbg-ESOL, ogbg-FreeSolv) from open graph benchmarks (Hu et al., 2020), four polymer regression tasks (GlassTemp, MeltingTemp, O2Perm, and thermal conductivity prediction ThermCond), and also protein function prediction (PPI) (Hu et al., 2019).",,
"The learning to augment approaches learn from labeled graphs to perturb graph structures (Luo et al., 2022), to estimate graphons for different classes (Han et al., 2022), or to split the latent space for augmentation (Liu et al., 2022).",,
"Augmentation Basic manipulation Automation Programmatic [42, 93, 250, 282, 282, 288].",,
"For example, compared to image data, graph data is irregular and not well-aligned, and thus the vanilla Mixup strategy can not be directly applied [93].",,
"Beyond image data, basic manipulation often needs to be tailored for the other data types, such as permutation and jittering in time-series data [250], mixing data in the hidden space for text data to retain semantic meanings [42], and mixing graphon for graph data [93].",,
"Mixup Improves Generalization After the initial work of Zhang et al. (2018), a series of the Mixup‚Äôs variants have been proposed (Guo et al., 2019a; Verma et al., 2019; Yun et al., 2019; Kim et al., 2020; Greenewald et al., 2021; Han et al., 2022; Sohn et al., 2022).",,
"(2018), a series of the Mixup‚Äôs variants have been proposed (Guo et al., 2019a; Verma et al., 2019; Yun et al., 2019; Kim et al., 2020; Greenewald et al., 2021; Han et al., 2022; Sohn et al., 2022).",,
"More specifically, inspired by theMixup technology employed in the field of computer vision [35] [5], we first generate new negative samples‚Äô representation by mixing positive sample embeddings into the negative sample embedding.",,
"Some strong baselines such as G-MIXUP (Han et al., 2022) were excluded because they require labels during the pre-training phase.",,
"It is worth noting that the drop edge technique we use here is different to the standard data augmentation techniques such as DropEdge (Rong et al., 2019), and G-Mixup (Han et al., 2022b), which either add slightly modified copies of existing data or generate synthetic based on existing data.",,
"‚Ä¶et al., 2015; Gilmer et al., 2017), physics (Cranmer et al., 2019; Bapst et al., 2020), transportation (Derrow-Pinion et al., 2021), vision (Han et al., 2022a), natural language processing (NLP) (Wu et al., 2021a), knowledge graphs (Schlichtkrull et al., 2018), drug design (Stokes et al.,‚Ä¶",,
"It is worth noting that the drop edge technique we use here is different to the standard data augmentation techniques such as DropEdge [77], and G-Mixup [78], which either add slightly modified copies of existing data or generate synthetic based on existing data.",,
"Graph [218] Cora/Citeseer/Pubmed [219] [220], [221], [222] [223], [224], [225] Cora-Full [226] [220], [221] Coauthor [227], [227] [220], [221], [222] IMDB/Reddit [228] [222], [229] BlogCatalog [230] [223], [225]",,
"G-Mixup [229] generates synthetic graphs by interpolating sampled graphons in the Euclidean space, which is a generator estimated for each class.",,
com/gasteigerjo/gdc [13] G-mixup ICML 2022 GI https://github.,,
"It is important to note that variations of mixup for Graph Neural Networks have been applied in previous papers [6], but they deal with graph datasets that have fluctuating numbers of nodes that come in different order that require far more complex implementations.",,
G-Mixup [6] applies the principles of mixup to probability matrices with individual points representing the likelihood of an edge existing between two nodes.,,
"Another study (Han et al., 2022) have adapted the mixup technique and introduced GMixup to improve graph classification robustness on GNNs.",,
"Another study (Han et al., 2022) have adapted the mixup technique and introduced GMixup to improve graph classification robustness on GNNs. G-Mixup makes use of graphons of a specific class as generators.",,
"The sub-graph perturbation could be categorized as the graph-level data augmentation strategy and is frequently used for graph-level tasks such as graph classiication[11, 16, 43, 49], while the node perturbation and edge perturbation are frequently adopted for node-level or edge-level tasks.",,
"‚Ä¢ Graph Data Augmentation: DropEdge (Rong et al., 2020), GREA (Liu et al., 2022), FLAG (Kong et al., 2022), M-Mixup (Wang et al., 2021), G-Mixup (Han et al., 2022).",,
"G-Mixup significantly increases Aug-Train by generating OOD samples, while it cannot guarantee a decrease in Aug-Test.",,
"We choose three data augmentation baselines, DropEdge, FLAG and G-Mixup, which augment graphs from different views.",,
", 2020), and graph-level (Wang et al., 2021; Han et al., 2022) with random (You et al.",,
"It can be roughly divided into node-level (Kong et al., 2022), edge-level (Rong et al., 2020), and graph-level (Wang et al., 2021; Han et al., 2022) with random (You et al., 2020) or adversarial strategies (Suresh et al., 2021).",,
"Note that G-Mixup [15] has gfeat as (10) and glabel as (1b)
cp (2‚àí Œ±) for Œ± ‚àà [1, 2].",,
"For mixup of graph data gfeat, we compare GraphMAD‚Äôs clusterpath data mixup (7) with linear graphon mixup [15].",,
"Similarly to [15], our graph descriptors are SBM graphon approximations, where Wi ‚àà [0, 1]D√óD is obtained for each graph Gi by sorting and smoothing (SAS) [22] with D denoting the fineness of the graphon estimate.",,
"An attractive alternative is mixing graph data in a latent space, which includes mixup of learned graph representations [13, 14] or graph models [15].",,
Note that G-Mixup [15] has gfeat as (10) and glabel as (1b) Fig.,,
"In this work, similar to [15], we adopt the graphon, a bounded symmetric function W : [0, 1](2) ‚Üí [0, 1] that can be interpreted as a random graph model associated with a family of graphs with similar structural characteristics [18‚Äì20].",,
"[15] X. Han, Z. Jiang, N. Liu, and X. Hu, ‚ÄúG-Mixup: Graph data augmentation for graph classification,‚Äù arXiv preprint arXiv:2202.07179, 2022.",,
Authors in [15] present the closest work to our own.,,
"Data augmentation [20, 27] is wildly adopted in the graph learning for improving model generalization, including dropping nodes [14], dropping edges [32], and graph mixup [20].",,
"The mixup on graphs is regarded as challenging due to the irregularity and connectivity, and existing mixup methods for GNNs aim to mix hidden embedding [11, 36].",,
"These generate new instances by feature or graph structure interpolation (Verma et al., 2021; Wang et al., 2021; 2020; Han et al., 2022).",,
"94 3 [3], [9], [11], [51], [52] Proteins Interaction OGB-ppa [12] 158,100 243.",,
"Social Node/Edge Facebook [23] 4,039 88,234 2 [37], [65] Node BlogCatalog [27] 5,196 1,71,743 6 [42], [66] Node Flickr [27] 7,575 2,39,738 9 [42], [52], [66] Node/Edge Reddit [2] 232,965 11,606,919 41 [32], [44], [50], [68] Node Yelp [25] 716,847 6,977,410 100 [52] Node Polblogs [1] 1,222 16,714 2 [13] Drugs Interaction Edge OGB-DDI [12] 4,267 1,334,889 - [11], [19], [65] Proteins Interaction Node OGB-Proteins [12] 132,534 39,561,252 [19] Node PPI [10] 10,076 157,213 121 [44], [66], [68] Collaboration Edge OGB-Collab [12] 235,868 1,285,465 - [19]",,
"53 2 [9], [11], [51], [60] IMDB-M [2] 1500 13.",,
"0 2 [11], [40], [60] sider [20] 1,427 33.",,
"5 2 [11], [19], [60] bace [38] 1,513 34.",,
"9 2 [11], [40], [60] pcba [12] 437,929 26.",,
"75 2 [3], [11], [51], [60], [61] Reddit-5K [2] 4999 508.",,
"87 5 [11], [50], [51], [60] Reddit-12K [2] 11929 391.",,
"82 2 [3], [9], [11], [51], [52], [60], [61] Collaboration Collab [59] 5000 74.",,
"Graph Classification Edge Manipulation [70] Node Manipulation [52] Feature Manipulation [19] Sub-Graph Manipulation [3], [9], [11], [30], [40], [51] Hybrid Manipulation [60], [61]",,
"In GNNs, many GCL methods are arisen for graph representation learning, such as GraphCL (You et al., 2020), GRACE (Zhu et al., 2020), AD-GCL (Suresh et al., 2021) and G-Mixup (Han et al., 2022).",,
"To this end, [43; 44; 45; 46] propose to augment graph data directly in the data space, while [47] interpolates latent representations to create novel ones.",,
"Additionally, graph mixup methods (Wang et al., 2021; Han et al., 2022; Guo & Mao, 2021; Park et al., 2022) synthesize a new graph or graph representation from two input graphs.",,
"Some studies (Wang et al., 2021; Han et al., 2022; Guo & Mao, 2021; Park et al., 2022) propose interpolation-based mixup methods for graph augmentations, and Kong et al. (2022) propose to augment node features through adversarial learning.",,
", 2021), G-Mixup (Han et al., 2022), and FLAG (Kong et al.",,
"In addition to the baselines in Section 4.1, we also compare with previous graph augmentation methods, including DropEdge (Rong et al., 2020), M-Mixup (Wang et al., 2021), G-Mixup (Han et al., 2022), and FLAG (Kong et al., 2022).",,
"Some studies (Wang et al., 2021; Han et al., 2022; Guo & Mao, 2021; Park et al., 2022) propose interpolation-based mixup methods for graph augmentations, and Kong et al.",,
"The similar strategies are also applied in graphs [78, 79, 80, 81, 82, 83].",,
"G-Mixup [83] tackles the key challenges when mixing up directly on the graph data, as graph data is irregular and not well-aligned, and graph topology between classes is divergent.",,
"Besides, an advanced data augmentation strategy namely Mixup is recently applied to DGL and is proved to be effective for several graph-related tasks [Han et al., 2022].",,
"‚Ä¶a graph pair, through mixing the graph representation resulting from passing the graph through the GNNs. Similarly, a concurrent work G-Mixup (Han et al. 2022) first interpolates represented graph generators (i.e., graphons) of different classes, and then leverages the mixed graphons for‚Ä¶",,
"Similarly, a concurrent work G-Mixup (Han et al. 2022) first interpolates represented graph generators (i.",,
"Manifold Intrusion The manifold intrusion degrades the performance of Mixup-like methods (Guo, Mao, and Zhang 2019b; Han et al. 2022).",,
"MixupGraph (Wang et al. 2021b) also leverages a simple way to avoid dealing with the arbitrary structure in the input space for mixing a graph pair, through mixing the graph representation resulting from passing the graph through the GNNs. Similarly, a concurrent work G-Mixup (Han et al. 2022) first interpolates represented graph generators (i.e., graphons) of different classes, and then leverages the mixed graphons for sampling to generate synthetic graphs.",,
"G-Mixup (Han et al., 2022) uses graphons as a surrogate to apply mixup techniques to graph data.",,
"Literaturelly, graphon has been studied from two perspectives: as limit of graph sequence, and as graph generators[1, 11, 24].",,
"[3] mentioned the same motifs were also found from bacteria [11] to yeast [29], animal [32] to plants.",,
"The graph augmentation methods combat the distributional shifts by increasing the data diversity (Zhao et al., 2021; Wang et al., 2021; Han et al., 2022).",,
"Despite recent advances in graph representation learning (Grover & Leskovec, 2016; Kipf & Welling, 2017; 2016; Gilmer et al., 2017; Han et al., 2022b), these GNN models may inherit or even amplify bias from training data (Dai & Wang, 2021), thereby introducing prediction discrimination against‚Ä¶",,
"‚Ä¶their remarkable performance (Gao et al., 2021; Gao & Ji, 2019; Liu et al., 2021a;b; Yuan et al., 2021) in many applications, such as knowledge graphs (Hamaguchi et al., 2017), molecular property prediction (Liu et al., 2022; 2020; Han et al., 2022a) and social media mining (Hamilton et al., 2017).",,
"Other approaches focus on augmenting specific graph model-based parameters, such as graphons [18, 41] or contextual stochastic block models (CSBMs) [55], based on the explicit downstream data generation assumptions.",,
com/gasteigerjo/gdc [13] G-mixup ICML 2022 GI https://github.,,
"G-Mixup [13] first estimates a graphon for each class of graphs, then performs interpolation in Euclidean space to generate mixed graphons, and finally augments synthetic graphs by sampling from the mixed graphons.",,
