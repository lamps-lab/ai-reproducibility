text,label_score,label
"Among them, network quantization [8, 44, 32, 21, 1] is a kind of excellent method with high compression ratio and little performance degradation.",,
Continuous weights in our framework are updated with Adam optimisation while the binary weights in R are updated using the Bop algorithm proposed by [14].,,
"This is achieved by first learning the edges or equivalently the relations/partOf predicate with a binary layer [14] between symbols in consecutive codebooks with weights: wl ∈ RMi×Mi+1 ∈ {0, 1}.",,
This is achieved in our works using a binary neural network [14].,,
Bop [28] and UniQ [29] each propose a new optimizer for training BNN.,,
"Specifically, these networks have significantly lower memory footprint, less computational complexity, and consume less energy (Helwegen et al., 2019).",,
We then convert to the latent-weight free setting of Helwegen et al. (2019) where latent weights are interpreted as accumulating negative gradients.,,
"We draw inspiration from the seminal work of Helwegen et al. (2019), who reinterpret latent weights from an inertia perspective and state that latent weights do not exist.",,
"We start with a latent weights BNN and convert it to an equivalent latent-weight free setting, as in Helwegen et al. (2019).",,
"In binary network optimization, Bop (Helwegen et al., 2019) and its extension (Suarez-Ramirez et al., 2021) introduce a threshold to compare with the smoothed gradient by EMA to determine whether to flip a binary weight.",,
"In binary network optimization, Bop (Helwegen et al., 2019) and its extension (Suarez-Ramirez et al.",,
"The intuition of CGA also aligns with the weight confidence proposition in (Helwegen et al., 2019; Liu et al., 2021a): the confidence level of a quantized weight is proportional to the distance of its real-valued latent weights to the closest threshold.",,
"As a compression approach that reduces the bit-width to 1-bit, network binarization is regarded as the most aggressive quantization technology (Rusci et al., 2020; Choukroun et al., 2019; Qin et al., 2022; Shang et al., 2022b; Zhang et al., 2022b; Bethge et al., 2020; 2019; Martinez et al., 2019; Helwegen et al., 2019).",,
"…that reduces the bit-width to 1-bit, network binarization is regarded as the most aggressive quantization technology (Rusci et al., 2020; Choukroun et al., 2019; Qin et al., 2022a; Shang et al., 2022b; Zhang et al., 2022b; Bethge et al., 2020; 2019; Martinez et al., 2019; Helwegen et al., 2019).",,
"Also called the latent weight [9], it is used to calculate the pseudogradient during backpropagation.",,
Latent weights are real-valued weights that are used to obtain the pseudogradient vector during backpropagation as the real gradient vector cannot be obtained from binary weights [9].,,
The sign and magnitude can be thought of separately as follows [9]:,,
[15] revisited the functional role of latent weights in BNNs and proposed a specialized optimizer BOP to flip the binary states.,,
"QUANTITATIVE RESULTS ON TOP-1 AND TOP-5 ACCURACY (%) FOR DIFFERENT BINARIZATION METHODS, INCLUDING REAL-TO-BINARY NET [17], REACTNET [19], IR-NET [22], BOP [35], CI-NET [51], BONN [39], BI-REAL NET [13], AND XNOR-NET [12].",,
"We compare our method with several relevant state-ofthe-art methods, including Real-to-Binary Net [17], ReActNet [19], IR-Net [22], Bop [35], CI-Net [51], BONN [39], Bi-Real Net [13], and XNOR-Net [12] in Table VI.",,
A binary optimizer is introduced in [35] to determine whether to flip a weight or not based on a sequence of gradients while treating,,
"To encode the momentum values in Bop for BNN training, a standard FP format [2] and the brain FP format [10] have been used.",,
"For the batch normalization parameters, we use Adam optimization as proposed by [2], with an initial learning rate of 10−2 for FC and VGG3, and 10−3 for VGG7.",,
"The activation function is the binarization function with a learned threshold Algorithm 1: Training BNNs with Binary Optimizer (Bop) [2], recapped for completeness.",,
"However, if γ is chosen to be large, then the training is highly unstable, as reported in [2].",,
"Other variants of BNNs, such as XNOR-Net and BiReal-Net (presented in [2]), are easier to train and lead to higher accuracy.",,
The algorithm for training BNNs with Bop [2] is recapped in Alg.,,
"When one of the most memory-efficient training procedures for BNNs, the Binary optimizer (Bop) [2], is used, one momentum value encoded as FP per binary weight is stored.",,
"To update the binary weights, the study in [2] proposes the binary optimizer (Bop) to directly flip the signs based on momentum signals, which are exponential moving averages of the partial derivatives.",,
"This is shown in Equation 11 below, where the first line corresponds to gradient strength where γ is the adaptivity rate [49] and the second line defines the update rule [49].",,
"This is achieved by first learning the edges or equivalently the property of conjunction over symbols with a binary attention layer [49] in R, with weights w l ∈ Rj×k ∈ {0, 1}, j = 0, 1 .",,
"In our case, we map our weights to {0, 1} rather than {-1, 1}, by initializing weights randomly ∈ {0, 1} and modify the update rule proposed in [49].",,
All continuous weights in our model framework are updated with Adam optimisation while the binary weights inR are updated using the Bop algorithm proposed by [49].,,
The quantization noise hinders the training step to find its optimized weights (Helwegen et al. (2019); Xu et al. (2021a)).,,
"There have been several works to overcome the degraded performance in low-bit quantized models (Helwegen et al. (2019); Kim et al. (2020); Martinez et al. (2020); Xu et al. (2021a,b)).",,
"Next to the default optimizers, [18, 41] have developed new optimizers that are dedicated to BNNs, which are respectively called Bop and Bop2ndOrder.",,
1 n/a BN n/a N ADAM LF n/a N N Latent Weights [18] 2019 n/a Custom BENN (6x) [55] 2019 61 LC_1 LC_1 N ADAM EN n/a N N Circulant BNN [28] 2019 61.,,
We use the Larq [11] framework for implementation of BNNs and utilize Binary Optimizer (Bop) [22] and Adam [23] for training models with a learning rate of 0.,,
"Therefore, in the future, we will explore the efficacy brought by employing a binary optimizer [Helwegen et al., 2019] that only modifies signs of weights without the need for latent parameters like the sign supermasks.",,
"These include adding learnable scaling factors (also known as gain term) [33]–[36], adopting multiple basis [33]–[35], [37], designing BNN-oriented network structure [16], [18], [23], [38], and reforming BNN training methodologies [22], [23], [36], [39]–[41].",,
Alizadeh et al. [46] empirically studied various optimizer and found that adapting the learning rate using secondmoment methods was crucial for the successful use of STE. Binary Optimizer [47] introduced a BNN-specific optimizer by viewing the latent weights as inertia.,,
Binary Optimizer [47] introduced a BNN-specific optimizer by viewing the latent weights as inertia.,,
The concept of latent weights is introduced aside quantized weights to be used by the photonic hardware [54].,,
"As described in (Helwegen et al., 2019) the underlying weights of a neural network when trained with STE are not representative for the network’s performance, but rather can be interpreted as a reservoir that slowly accumulates small gradient updates.",,
"Similar to early research, [18] does not introduce latent real-valued weights, but rather updates the binary weights directly using a momentum based optimizer designed specifically for BiNNs.",,
"Commonly, binarization methods define each binary weight update by considering only its associated value in the real-valued latent weights (Bulat and Tzimiropoulos 2019; Li et al. 2017; Rastegari et al. 2016; Liu et al. 2018) or in the gradient vector (Helwegen et al. 2019).",,
"From a different perspective, recent work tries to sidestep having to approximate the gradient of the sign function, and uses bit flips to train binary networks (Helwegen et al. 2019).",,
"Here, we also use filterweight statistics to update the binary weights, however similar to (Helwegen et al. 2019) Table 1(d)) we do not rely on the sign function for binarization, but instead use binary weight flips.",,
"An equivalent number of weight updates for both offline and online training processes is assumed, in agreement with [30].",,
"Again, the 1S1R-based BNN synaptic weight storage for online training feasibility is extrapolated, assuming the equivalent number of binary weight updates for both offline and online training processes [30].",,
Training is performed over 30 epochs using the Bop approach [30].,,
Training is performed over 30 epochs using the binary optimizer (Bop) approach [30].,,
"…SGD, including BNN (Courbariaux et al., 2016) and XNOR-Net (Rastegari et al., 2016), Real-to-Binary Network (Brais Martinez, 2020), Structured BNN (Zhuang et al., 2019), ReActNet (Liu et al., 2020), etc. Helwegen et al. proposed a new binary optimizer design based on Adam (Helwegen et al., 2019).",,
"Thus, it is not far-fetched that real-valued weights can be regarded as the confidence of a binary value to be -1 or +1, as also being mentioned in (Helwegen et al., 2019).",,
"The magnitude of these real-valued weights are regarded as ‘inertial’ (Helwegen et al., 2019), indicating how likely the corresponding binary weights are going to change their signs.",,
"proposed a new binary optimizer design based on Adam (Helwegen et al., 2019).",,
"We relied on recommendations from [6] to define our custom BNN architectures, and took notions from [4] and used the Bop optimizer[23] to speed up training.",,
"accuracy gap, including specific architecture design [30, 4, 3, 29], real-valued weight and activation approximation [27, 48], specific training recipes [34], a dedicated optimizer [20], leveraging neural architecture search [6, 46] and dynamic networks [7].",,
"In this section, we first discuss the difference of SA-BNN with related methods in (Helwegen et al. 2019; Bai, Wang, and Liberty 2018), and then further analyze the effectiveness of the proposed SA-BNN.",,
"In particular, Helwegen et al. (Helwegen et al. 2019) argue that latent weights are not necessary for gradient-based
optimization of BNNs, and they directly update the state of binarized weights with:
wt = { −wt−1 if |gt| ≥ β and sign(gt) = sign(wt−1) wt−1 otherwise ,
(11) where gt is exponential moving average of gradient (gt = (1− γ)gt−1 + γ ∂L∂wt , where γ is the adaptive rate) and β is a manually defined threshold to control the weight flipping.",,
"To solve this problem, Helwegen et al. (Helwegen et al. 2019) introduce an optimizer specifically designed for BNNs by directly updating the state of binarized weights.",,
"To better understand our SA-BNNs, we follow (Helwegen et al. 2019; Bai, Wang, and Liberty 2018) to calculate the flipping state at each epoch and set the ratio of sequential weight flip as
πt =
∑ w At ∧ · · · ∧ At+m−1
||sign(W )||1 , (12)
where At represents sign(wt) 6= sign(wt+1) and m is the examined epoch interval.",,
"To better understand our SA-BNNs, we follow (Helwegen et al. 2019; Bai, Wang, and Liberty 2018) to calculate the flipping state at each epoch and set the ratio of sequential weight flip as
πt =
∑ w At ∧ · · · ∧ At+m−1
||sign(W )||1 , (12)
where At represents sign(wt) 6= sign(wt+1) and m is the…",,
"We carry out a comparative study with six methods: IR-Net (Qin et al. 2019), Bop (Helwegen et al. 2019), CI-Net (Wang et al. 2019), BONN (Gu et al. 2019b), Bi-Real Net (Liu et al. 2018), and XNOR-Net (Rastegari et al. 2016) on ResNet18, ResNet-34 and ResNet-50 in Table 3.",,
(Helwegen et al. 2019) argue that latent weights are not necessary for gradient-based,,
"To better understand our SA-BNNs, we follow (Helwegen et al. 2019; Bai, Wang, and Liberty 2018) to calculate the flipping state at each epoch and set the ratio of sequential weight flip as",,
"However, the method in (Helwegen et al. 2019) suppresses the weight flip equally for different states, while SA-BNN treats different binarization states distinctively by employing an independent coefficient for each state.",,
(Helwegen et al. 2019) introduce an optimizer specifically designed for BNNs by directly updating the state of binarized weights.,,
"From this perspective, threshold β in (Helwegen et al. 2019) is consistent with the coefficients τ in our method.",,
"In particular, Helwegen et al. (Helwegen et al. 2019) argue that latent weights are not necessary for gradient-based
optimization of BNNs, and they directly update the state of binarized weights with:
wt = { −wt−1 if |gt| ≥ β and sign(gt) = sign(wt−1) wt−1 otherwise ,
(11) where gt is exponential…",,
"A Binary Neural Network (BNN) [13, 14, 34, 73, 51, 14, 36, 48, 44, 29, 72, 39, 25, 37, 10, 57, 20, 66] represents the most extreme form of model quantization as it quantizes weights in convolution layers to only 1 bit, enjoying great speed-up compared with its full-precision counterpart.",,
"(ii) optimizationbased BNNs techniques, including minimizing the quantization error [73, 51, 14, 36], improving the network loss function [48, 44, 29, 72], and reducing the gradient error [39, 25, 37, 10].",,
"viewed the latent weights as inertia, and introduced a BNN-specific optimizer called Binary Optimizer (Bop) [43] for the training.",,
"Besides, new training methods and optimizing tricks for BNNs have been researched for obtaining better classification accuracy (Alizadeh et al., 2018; Bulat & Tzimiropoulos, 2019; Zhu, Dong & Su, 2019; Wang et al., 2019; Hubara et al., 2017; Ghasemzadeh, Samragh & Koushanfar, 2018; Gu et al., 2019; Helwegen et al., 2019; Ding et al., 2019; Martinez et al., 2020).",,
"Inspired by [23], the latent weights, which refer to the realvalued weights used during backpropagation, play an important role in binarizing DNNs.",,
"[13] proposed a Binary Optimizer (BOP) which flips the binary weights solely based on the value of their associated momentum (without latent weights per se): if the momentum is large enough and crosses a threshold from below, the binary weight is switched.",,
[13] suggested that they were not weights in the strictest sense (they are not used at run time) but were only meant to convey inertia for the optimization of the binary weights.,,
"Similarly to [13], we monitor the number of weight flips per epoch and layer-wise in order to tune the hyperparameters of BOP, using the metric:",,
"We first leverage the recent progress made in Binary Neural Networks (BNNs) optimization [13], to binarize the synapses in energy-based models trained by EP.",,
"There are also abundant works that explore the optimization of BNNs [33], [45], [46], [47], [48] and explain their effectiveness [49].",,
useful in another latest work on optimizing binary neural networks (BNNs) [47].,,
"[52] took the view that the real-valued latent weights cannot be treated analogously to weights in realvalued networks, while their main role is to provide inertia during training.",,
"Shrinking the cost-dominate multiplications has been widely considered in many DNN designs for reducing the computational complexity [10, 11]: [10] decomposes the convolutions into separate depthwise and pointwise modules which require fewer multiplications; and [12, 13, 14] binarize the weights or activations to construct DNNs consisting of sign changes paired with much fewer multiplications.",,
", 2019), Bop (Helwegen et al., 2019), GBCN (Liu et al.",,
"The compared binary neural network methods include BinaryNet (Hubara et al., 2016), Dorefa-Net (Zhou et al., 2016), XNOR-Net (Rastegari et al., 2016), Bireal-Net (Liu et al., 2018), PCNN (Gu et al., 2019), Bop (Helwegen et al., 2019), GBCN (Liu et al., 2019), etc.",,
"3% Bop (Helwegen et al., 2019) 1 1 34 Mbit 163 M 54.",,
BNN [22] 56.,,
"To compensate, [21, 37] strengthen the learning ability of BNNs during network training via increasing the probability of weight flips.",,
"To solve this problem, many compression techniques have been proposed including network pruning [30, 14, 29], low-rank decomposition [12, 43, 18], efficient architecture design [24, 42, 7] and network quantization [28, 2, 21], etc.",,
"However, the scaling factor results in a small ratio of flipping weights thus leading to little information gain in the training process [21, 37]2.",,
"A common problem in existing quantization methods is the estimated gradients for quantization functions, using either STE [6, 39] or self-designed gradient computation manner [34].",,
"A common problem in existing quantization methods is the estimated gradients for quantization functions, using either STE [8, 52, 29, 47, 59, 4] or self-designed gradient computation manner [46].",,
"The Straight Through Estimator (STE) [6] is a widely used strategy to estimate the gradient of W latq , i.e.,
∇W latq = ∇Wq (2)
where the approximate gradients∇W latq are use to update weights W lat q ,
ˆW latq =W lat q − η · ∇W latq · σ(∇W latq ), (3)
where η is the learning rate and σ(·) is the gradient clip function.",,
" of network lters pruning [31,14,30] and compact architecture design [26,6], how to alleviate the performance degradation in quantized model [19] is still unsolved, especially for the binarized model [39,1,15]. Deterministic Weight Quantization Through introducing a deterministic function, traditional methods quantize network weights (or activations) by minimizing quantization errors. For examples, BinaryC",,
"g the vanishing gradients. The Straight-Through Estimator (STE) [3] is commonly used to estimate the vanishing gradients during the back-propagation, while the well-known gradient mismatching problem [39,19,15] is introduced. As the number of quantized bits decrease, the gradients estimated by STE depart further from the real gradients. Thus, the gradient mismatching is considered as the main bottleneck of ",,
"Following this insight, the authors of [19] proposed the first optimizer that is specifically designed for BNN known as Binary Optimizer (Bop).",,
"In [19], it is shown that this is not necessarily true and instead the real values parameters should be viewed as inertia parameters for each binary weights.",,
"Following initialization, these values should be seen as the momentum of inertia [19] in rest and it may take a few epochs to break out of this, and then, the binary weights start flipping the sign.",,
Responding to the work “Latent weights do not exist: Rethinking binarized neural network optimization” [24] and the lack of formal basis to introduce latent weights in the literature (e.,,
[24] argues that “latent weights do not exist” meaning that discrete optimization over binary weights needs to be considered.,,
"Moreover, the trained model can achieve comparable performance than the specific discrete optimizers [39, 18].",,
"1In this paper, we follow the existing works [18] that focus on the most extreme case of defining the deep quantized model as the BNNs.",,
"Despite the achieved success from gradient approximation, the optimization method [18] argues that the latent variable is not necessary for training BNNs and raises a question if a general optimization method exists to well optimize both full-precision and quantized models with the theoretical guarantee.",,
"Next, we generate the empirical results through comparing our algorithm with existing optimization methods including SGD(M) [40], Adam [10], AMSGrad [42], AdaBound [36], and specific binary optimization method [18], and it mainly evaluates these optimization methods in the setting with different datasets and network architectures.",,
"In brief, for a 3-Layer LSTM models, our method improves the performance than the traditionally adaptive methods [42, 36] and specific binary optimizer [18] in terms of perplexity.",,
"Specially, the binary weights are not learned directly, but are learned with the scaling factor [18] during the training.",,
The resulting algorithm has a very similar convergence speed of training the full-precision models with the adaptive method [36] even better than the specific binary optimizer [18].,,
"For the generalization ability shown in the test accuracy, we find that our method always obtains the best accuracy by comparing to the traditional adaptive methods [42, 36] and specific binary optimizer [18].",,
"9) mAP than the latest optimization methods [42, 36, 18].",,
"Furthermore, the Bop [18] claims that the momentum estimated by past gradients history [48] is the key issue, as it can avoid a rapid sign change of binary weights during the training.",,
"Intuitively, low resolution arithmetic might not match the setting of gradient-based training because the gradient only gives reliable information in a small neighborhood around the current model parameters’circumstantial evidence for this is the significant amount of work on the improvement of training methods in the context of low-resolution weights (e.g., Müller et al., 2017; Alizadeh et al., 2019; Helwegen et al., 2019).",,
"Binary Neural Networks (BNNs) recently have attracted many interests and achieved significant improvements [39, 15, 32, 3, 45].",,
Our proposed method using evolutionary search based on recent ideas of one-shot neural architecture search aims at exploring the group architecture design for BNNs improvement.,,
"In this section, to evaluate the proposed method we compare our BNNs with several recent works: Binary Connect [8], BNNs [19], ABC-Net [26], DoReFa-Net [53], XNORNet [39], etc.",,
"First, we describe experi-
Algorithm 2 Overall Training BNNs
Input: Full binary neural model and inputs for evolutionary search Output: New optimal binary neural model with new group structure.",,
"Then we compare with the state-of-the-arts to see improvement impacts of our proposed BNNs. Finally, the computation analysis are presented.",,
The main objective in our work is to explore efficient designs of BNNs with the hope that techniques in neural architecture search (NAS) can leverage the exploration for compact structures.,,
These binary models described a deployment of compact modules with skip connection and group convolution to enhance the capability of BNNs in terms of feature representation.,,
"A recent work on BNNs [15] introduced Binary Optimizer to remove the dependency of binary weights from the real values, opening a new way to improve the BNNs.",,
Efficient group design for BNNs can yield good outcomes.,,
"Our work sheds the light on a new direction for enhancing the capability of BNNs.
• We propose an adaptive weight-sharing training mechanism that automatically searches in the group space
to build efficient BNNs.",,
"Binary Neural Networks (BNNs), known to be one among the effectively compact network architectures, have achieved great outcomes in the visual tasks.",,
"In addition to architectural design [2,23,28], studies on 1-bit CNNs expand from training algorithms [36,1,46,3], binary optimizer design [13], regularization loss design [8,29], to better approximation of binary weights and activations [30,11,37].",,
] 66.3 86.6 (1/1)×4 CBCN [26] 61.4 82.8 (1/1)×4 Ensamble [50] 61.0 - (1/1)×6 BNN [10] 42.2 69.2 1/1 XNOR-Net [33] 51.2 73.2 1/1 CCNN [43] 54.2 77.9 1/1 Bi-Real Net** [28] 56.4 79.5 1/1 Rethink. BNN** [17] 56.6 79.4 1/1 XNOR-Net++ [5] 57.1 79.9 1/1 CI-Net** [40] 59.9 84.2 1/1 BATS(Ours) 60.4 83.0 1/1 BATS[2x-wider] (Ours) 66.1 87.0 1/1 7 Conclusion In this work we introduce a novel Binary Architecture ,,
", 2019); latent-free Bop (Helwegen et al., 2019); and the proximal mean-field (PMF) (Ajanthan et al.",,
", 2016; Zhu, Dong, & Su, 2019; Zhuang, Shen, Tan, Liu, & Reid, 2019) and optimization strategies (Alizadeh, FernándezMarqués, Lane, & Gal, 2019; Helwegen et al., 2019), and the accuracy gap between efficient BNNs and regular DNNs is rapidly closing.",,
"Binarized Neural Networks (BNNs) represent an extreme case of quantized networks, that cannot be viewed as approximations to real-valued networks and therefore requires special tools and optimization strategies (Helwegen et al., 2019).",,
"…architectures (Liu et al., 2018; Rastegari et al., 2016; Zhu, Dong, & Su, 2019; Zhuang, Shen, Tan, Liu, & Reid, 2019) and optimization strategies (Alizadeh, FernándezMarqués, Lane, & Gal, 2019; Helwegen et al., 2019), and the accuracy gap between efficient BNNs and regular DNNs is rapidly closing.",,
"For example, using learned clipping in PACT [9], double skip-connection in BiRealNet [37], parametric ReLU in [6], multi-stage knowledge distillation in [2, 6], and tailored binary optimization in [24].",,
"In this report, we present a detailed study on the paper titled ""Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization"" by Helwegen et al. [2019] which proposes a new optimization method for training BNN called BOP.",,
Authors of this paper (Helwegen et al. [2019]) claimed that using latent weights in BNN may not always result into higher accuracy.,,
"To use the dataset for training a binary network, in our work similar modifications are done to the dataset as Helwegen et al. [2019]. We use batch normalization to normalize the activations with a minibatch size of 64.",,
The paper Helwegen et al. [2019] considers a binary convolutional architecture called BVGG net as,,
" been implemented in neuromorphic devices like the DynapSEL [44]. Although it may not suce to quantize previously trained high-precision weights, there are learning algorithms such as [45], [46] and [47] that are designed to nd suitable low-precision weights. Another potential solution is the application of transfer methods as suggested in [34]. Assuming similar power gures as for DYNAP-SE, we estima",,
"In contrast to these, recently Helwegen et al. (2019) proposed Binary Optimizer (bop) to avoid using “latent” real-valued weights during training.",,
"In the future, we will employ the latent-free optimizer Helwegen et al. (2019) for BNNs that directly update the binary weights, to reduce the memory consumption during training.",,
"Specifically, these networks have significantly lower memory footprint, less computational complexity, and consume less energy (Helwegen et al., 2019).",,
"Moreover, for its second stage of unmasking, we optimize weight signs without extra latent weights or scores (Helwegen et al., 2019).",,
"To approximately solve (4) with sign flipping transformation Us, we leverage a prior art called Bop from BNN optimization (Helwegen et al., 2019).",,
"For example, Bop (Helwegen et al., 2019) keeps a running average of historical gradients, and flips a weight’s sign when its accumulated gradients surpasses a pre-defined threshold.",,
"…weights using the flipping with latent weights method in (Ivan & Florian, 2020) 1; (2) PaB-Latent-PSG, the variant of PaB-Latent combined with PSG; (3) PaB-Bop, which trains the unpruned weights using Bop (Helwegen et al., 2019); and (4) PaB-BopPSG, the variant of PaB-Bop combined with PSG.",,
"For the proposed PaB methods, we compare: (1) PaB-Latent, which first prunes the network and trains the unpruned weights using the flipping with latent weights method in (Ivan & Florian, 2020) 1; (2) PaB-Latent-PSG, the variant of PaB-Latent combined with PSG; (3) PaB-Bop, which trains the unpruned weights using Bop (Helwegen et al., 2019); and (4) PaB-BopPSG, the variant of PaB-Bop combined with PSG.",,
"Bop (Helwegen et al., 2019) selects what weights and when to flip their signs by taking into account past gradient information.",,
"In this paper ([1]), the authors have developed a novel optimization method, Binary Optimizer (BOP), for training BNN.",,
"In this report, we present a detailed study on the paper titled ""Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization"" by [1] which proposes a new optimization method for training BNN called BOP.",,
"To use the dataset for training a binary network, in our work similar modifications are done to the dataset as [1].",,
The paper [1] considers a binary convolutional architecture called BVGG net as given in [8] inspired from the architecture of ConvNet in [3] we discussed before.,,
Authors of this paper ([1]) claimed that using latent weights in BNN may not always result into higher accuracy.,,
"Since BNN training is not well-founded, there are still tremendous efforts on the study of BNNs’ optimizations [1,5,17,29] and how to explain the effectiveness of BNNs [2].",,
"…of multiple binary models (Zhu et al., 2019), searching high-performance binary network architectures (Kim et al., 2020), and designing improved regularization objectives, optimizers and activation functions (Cai et al., 2017; Liu et al., 2018; Helwegen et al., 2019; Martinez et al., 2020).",,
", 2020), and designing improved regularization objectives, optimizers and activation functions (Cai et al., 2017; Liu et al., 2018; Helwegen et al., 2019; Martinez et al., 2020).",,
"Modified Bop: BOAT uses a modified version of the Bop optimizer (Helwegen et al., 2019) to optimize the binary-valued parameter {W ,S} given gradient as learning signals.",,
"…Neural DNF. BOAT utilizes two optimizers: a standard deep learning optimizer Adam (Kingma and Ba, 2014) that optimizes the continuous parameters θ of the neural network φ and a binary-parameter optimizer adopted from [Helwegen et al., 2019] that optimizes the binary parameters {W ,S} of the DNF g.",,
"1): we use standard continuous-parameter optimizer Adam (Kingma and Ba, 2014) to optimize the first-stage neural network φ and modify a binary-parameter optimizer from Helwegen et al. (2019) to optimize the parameters of the second-stage rule-based g.",,
