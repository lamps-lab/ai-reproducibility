text,label_score,label
"Noise-Robust Loss Functions Prior work examines how regularization techniques can be adapted to the noisy labels setting, addressing issues related to overfitting on noisy data (Menon et al., 2019; Lukasik et al., 2020; Englesson and Azizpour, 2021).",,
"When done separately and at the level of a minibatch, clipping [16, 29] or noising [9] effectively regularize learning because they respectively control the dynamics of iterates and smoothen the loss landscape.",,
"Many approaches [23, 25, 36, 37, 42, 45, 72] have been proposed to address this problem, but most of them focus on designing robust loss functions for classiiers that can handle noisy labels.",,
"GCE [37], DAC [31], PHuber loss [23], Curriculum loss [21], and other robust losses are designed to reduce the effect of noise labels on the model.",,
"Gradient clipping techniques [11, 57, 92] were proposed to boost test accuracy or reduce training time, without any resilience considerations.",,
", partial labels [76, 68, 80, 15] and noisy labels [43, 47, 74, 29, 70, 71].",,
"Besides, methodologies accompanying classical losses for robustness have been proposed, such as gradient-clipping [Menon et al., 2020] or sub-gradient optimization methods [Ma and Fattahi, 2022].",,
"For instance, multiple works iteratively modified the labels to better align with the model’s predictions [25, 27], estimated the noise transition matrix [8], or applied a regularization [21, 37].",,
"The scaling is done to ensure that the gradient norm strictly stays below a predetermined “clipping constant” [18, 30].",,
"To address this challenge, numerous studies have explored supervised learning under the label noise setting, leading to the development of various techniques, such as robust loss functions [45], sample selection [22, 30, 32, 39], robust regularization [28, 48, 65], and robust architecture [11,21,34,69].",,
Menon et al. (2019) showed that gradient clipping can help mitigate label noise.,,
"The lower norm implies that the magnitude of gradient is lower, and it won’t cause any significant changes to the current model state [38].",,
"1, 3 [23] Aditya Krishna Menon, Ankit Singh Rawat, Sashank J.",,
[23] suggests a composite loss-based gradient clipping for label noise robustness.,,
"More specifically, the common label noise has two types: class-conditional noise (CCN) [44, 28, 27] and instancedependent noise (IDN) [3, 6, 7].",,
"Methods modeling label noise implicitly include noise-tolerant loss function [22, 31–36] and regularization [37, 38].",,
", 2017), PHuber-CE (Menon et al., 2020), SCE (Wang et al.",,
"Partial Huberised Cross Entropy (PHuber-CE) (Menon et al., 2020) enhances the noise robustness of CE with a loss variant of gradient clipping.",,
"(4) PHuber-CE (Menon et al., 2020), a loss variant of gradient clipping for learning with noisy labels.",,
"…we show that LogitClip can boost the performance of a wide range of popular robust loss functions, including MAE (Ghosh et al., 2017), PHuber-CE (Menon et al., 2020), SCE (Wang et al., 2019), GCE (Zhang & Sabuncu, 2018), Taylor-CE (Feng et al., 2020), NCE (Ma et al., 2020), AEL, AUL (Zhou et…",,
"Indeed, recent work (Menon et al., 2020) has shown that gradient clipping alone does not endow label noise robustness to neural networks.",,
"Several noise-robust losses have been proposed for training models with noisy labels (Reed et al., 2015; Zhang and Sabuncu, 2018; Wang et al., 2019; Ma et al., 2020; Menon et al., 2020; Jin et al., 2021; Zhou and Chen, 2021), which were shown to be more robust than CE.",,
"One direction is to develop noise-robust losses that can mitigate the effect of noisy labels (Ghosh et al., 2017; Zhang and Sabuncu, 2018; Charoenphakdee et al., 2019; Kim et al., 2019; Lyu and Tsang, 2019; Menon et al., 2020; Thulasidasan et al., 2019).",,
"Domain adaptation (DA) [15, 18, 22, 31] relieves the burden of manual annotation by leveraging the knowledge from related source domains with rich labeling.",,
"Among them, robust regularization is the most direct approach, which is mainly composed of two commonly used types: explicit regularization [7, 8, 15, 29, 31] and implicit regularization [18, 32].",,
"In recent years, an increasing number of studies are using deep learning techniques to overcome the issue of noisy labels [19, 27, 35].",,
"[21] went further and proposed a partially Huberised Cross Entropy loss, which utilized gradient clipping to arrive at a more robust training solution.",,
• GCE: Generalised Cross-Entropy [20] • PHuber: partially Huberised Cross-Entropy [21],,
"Several studies have, therefore, been conducted to investigate supervised learning under the label noise setting, including robust loss function [41, 58], sample selection [53, 55, 59], robust regularisation [14, 23, 43, 60] and robust architecture [11,16,29,64].",,
"This approach has been pursued in a large body of work (Long & Servedio, 2008; Wang et al., 2019a; Liu & Guo, 2020; Lyu & Tsang, 2020; Menon et al., 2020; Feng et al., 2020) that embraces new loss functions, especially symmetric losses and their variants (van Rooyen et al., 2015; Ghosh et al.,…",,
"This approach has been pursued in a large body of work (Long & Servedio, 2008; Wang et al., 2019a; Liu & Guo, 2020; Lyu & Tsang, 2020; Menon et al., 2020; Feng et al., 2020) that embraces new loss functions, especially symmetric losses and their variants (van Rooyen et al.",,
", 2019), gradient clipping (Menon et al., 2020), label smoothing (Lukasik et al.",,
"One representative class of methods aims to reduce noisy samples’ impact with carefully designed losses [12], [22], [23], regularization terms [24], [25] or adaptive sample re-weighting [13], [26]–[28].",,
"Robust loss to label noise Another direction of research is to design loss functions that are robust to label noise (Ghosh et al., 2015; 2017; Zhang & Sabuncu, 2018; Wang et al., 2019; Oksuz et al., 2020; Menon et al., 2020).",,
"Methods following this direction includes constructing robust network [6, 12, 13, 59], robust loss function [11, 27, 53, 65, 68, 69], robust regularization [31, 47, 58] against noisy labels.",,
", ptar that is closer to p ∗, leads to better generalization performance; this is supported by results of Menon et al. (2021) and further empirical suggestions given here.",,
"This hypothesis is suggested by Proposition 3 of Menon et al. (2021), which shows (tracking constants omitted in their proof) that for any predictor f and loss bounded as L(y, ŷ) ≤ `,",,
"There are various ways to cope with it; for instance, Menon et al. (2019) use gradient clipping, Patrini et al. (2017) use loss correction, Huang et al. (2020) change the supervision during training, and Zhang et al. (2020) employ extra information.",,
"Gradient clipping [9, 32] is a functionality of an optimizer to prevent gradient explosion [56].",,
"A similar idea is also explored in gradient clipping (Menon et al., 2019) and loss reweighting (Liu & Tao, 2015; Wang et al.",,
"A similar idea is also explored in gradient clipping (Menon et al., 2019) and loss reweighting (Liu & Tao, 2015; Wang et al., 2017; Chang et al., 2017; Zhang et al., 2021b; Zetterqvist et al., 2021) methods.",,
leveraged gradient clipping for designing new loss functions [21].,,
"Compare to standard lossl [12][13][14], the output of l is smaller when confronting noisy labels which will avoid models overfitting noisy labels during backward propagation.",,
", 2019; 2020), or outlier/noisy sample detection (Huber, 1992; Bhatia et al., 2015; Menon et al., 2019; Li et al., 2020).",,
"…of distributional robustness (Duchi et al., 2019; Wang et al., 2020; Zhang et al., 2021; Ben-Tal et al., 2013), fairness (Hardt et al., 2016; Agarwal et al., 2018; Li et al., 2019; 2020), or outlier/noisy sample detection (Huber, 1992; Bhatia et al., 2015; Menon et al., 2019; Li et al., 2020).",,
"They can be further categorized as follows: various loss functions [21, 54, 58–60, 68, 72, 90, 90, 96, 112], regularizations [2, 10, 34, 35, 37, 38, 41, 44, 50, 52, 54, 56, 59, 61, 62, 70, 71, 83, 110], re-weighting training samples [14, 42, 43, 57, 64, 74, 76, 89, 95, 98, 99, 99], and correcting noisy labels [29, 33, 82, 86, 107, 114].",,
"However, it has been proven that in classification problems standard gradient clipping does not in general provide robustness to label noise [20].",,
"…2016, Han et al., 2018a], (2) loss regularization techniques [Goodfellow et al., 2015, Pereyra et al., 2017, Tanno et al., 2019, Hendrycks et al., 2019, Menon et al., 2020], and (3) loss correction techniques [Patrini et al., 2017, Chang et al., 2017, Ma et al., 2018, Arazo et al., 2019].",,
", 2012; Zhang and Sabuncu, 2018) or gradient clipping (Menon et al., 2020).",,
"Losses can also be modified to address outliers by favoring small losses (Yu et al., 2012; Zhang and Sabuncu, 2018) or gradient clipping (Menon et al., 2020).",,
"We also compare with Co-teaching[9], which is the representative work of sample selection, and PHuber-CE [23], which is a simple variant of gradient clip-
ping.",,
"We also compare with Co-teaching[9], which is the representative work of sample selection, and PHuber-CE [23], which is a simple variant of gradient clip-",,
"As shown in Table 4, our method works better than Co-teaching and PHuber-CE.",,
Recent studies demonstrate that gradient clipping can be applied for robustness to model update poisoning attacks [38] and label noise [39].,,
"3) PHuber-CE [41], which uses a composite loss-based gradient clipping, a variation of standard gradient clipping for label noise robustness.",,
"As the setting in F-correction, we first train a standard network to estimate the transition matrix Q. 3) PHuber-CE [41], which uses a composite loss-based gradient clipping, a variation of standard gradient clipping for label noise robustness.",,
"5) Some method apply regularization techniques to improve generalization under the settings of label noise [10, 23, 66], like gradient clipping [41], label smoothing [36, 57], temporal ensembling [28] and virtual adversarial training [42].",,
"Successful recent approaches include correcting the loss for class or label noise, such as [32, 39, 56] (and references therein).",,
"The approaches in [2, 4, 6, 13, 31, 32, 36, 39, 53, 60, 61, 62] share a higher-level technical commonality: they alter the loss via its surrogate.",,
"the APL losses is approximated to that of the Bayes classifier (learned using supervised data) [43, 44, 45, 46].",,
"Table 1: Bounds of multi-class losses, including the mean absolute error (MAE) loss, the mean square error (MSE) loss, the reverse cross entropy (RCE) loss [67], the generalized cross entropy (GCE) loss, the partially Huberised cross entropy (PCE) loss [46], the categorical cross entropy (CCE) loss, and the focal loss (FL) [68].",,
"Constructing robust losses from the perspective of the objective function is a powerful means in weakly supervised learning [43, 44, 46].",,
"To achieve this we focus on gradient clipping methods (Pascanu et al., 2013; Gehring et al., 2017; Menon et al., 2020; Mai and Johansson, 2021; Zhang et al., 2020a,b).",,
"To achieve this we focus on gradient clipping methods [31, 11, 24, 23, 41, 42].",,
"[24] Aditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar.",,
"This approach has been pursued in a large body of work (Long & Servedio, 2008; Wang et al., 2019a; Liu & Guo, 2020; Lyu & Tsang, 2020; Menon et al., 2020; Feng et al., 2020) that embraces new losses, especially symmetric loss functions and their variants (Manwani & Sastry, 2013; van Rooyen et al.,…",,
"This approach has been pursued in a large body of work (Long & Servedio, 2008; Wang et al., 2019a; Liu & Guo, 2020; Lyu & Tsang, 2020; Menon et al., 2020; Feng et al., 2020) that embraces new losses, especially symmetric loss functions and their variants (Manwani & Sastry, 2013; van Rooyen et al.",,
"Therefore, it is of great importance to achieve robust training against noisy labels [10, 9, 18, 39, 26].",,
"Class-conditional label noise Label noise is commonly treated as a class-conditional phenomenon, where the noisy labels are treated strictly as a function of the true label [54, 57, 72, 98, 33, 102, 51, 67, 58, 5, 11, 4, 3, 94, 100, 90, 104, 93].",,
"…Mirzasoleiman et al., 2020, Wu et al., 2020, Chen et al., 2021] that selects training examples dynamically during training; training techniques [Menon et al., 2020, Liu et al., 2020] that are designed to increase robustness and avoid memorization of noisy labels; learning with rejection or…",,
"Furthermore, many existing approaches are exclusively designed for classification problems (e.g., Malach & ShalevShwartz (2017); Reed et al. (2014); Menon et al. (2019); Zheng et al. (2020)); extending them to solving regression problems is not straightforward.",,
"Robust loss function: The literature has also observed the proposal of robust loss functions that perform well with dealing outlier noisy examples (Zhang & Sabuncu, 2018; Menon et al., 2019; Charoenphakdee et al., 2019; Wang et al., 2019).",,
"Proposed methods range from robust loss functions [18], [19], [20], [21], or noise modeling [22], [23], [24], to sample selection [25], [26], [27], or re-weighting samples [28], [29].",,
"Regularization techniques [30], [31], [21] have also been shown to reduce the effect of label noise.",,
"We experiment with the partially Huberized cross-entropy loss [21], a more robust loss function, and the mixup regularization [35].",,
"In our work, we experiment with the partially Huberized cross-entropy loss [21] and the mixup regularization [35] to deal with the inherent noise of pseudo-labels.",,
"Most existing works, for their theoretical analysis or noise synthesizing in experiments, follow the class-conditional noise (CCN) assumption (Scott, Blanchard, and Handy 2013; Zhang and Sabuncu 2018; Menon et al. 2020; Ma et al. 2020), where the label noise is independent of its input features conditional on the latent true label.",,
"…analysis or noise synthesizing in experiments, follow the class-conditional noise (CCN) assumption (Scott, Blanchard, and Handy 2013; Zhang and Sabuncu 2018; Menon et al. 2020; Ma et al. 2020), where the label noise is independent of its input features conditional on the latent true label.",,
…as clearly stated in theoretical analysis (Blum and Mitchell 1998; Yan et al. 2017; Patrini et al. 2016; Zhang and Sabuncu 2018; Xu et al. 2019; Menon et al. 2020; Ma et al. 2020) or inexplicitly used in experiments for synthetizing noisy labels (Han et al. 2018b; Yu et al. 2019; Arazo et al.…,,
"The CCN assumption is commonly used in previous works, as clearly stated in theoretical analysis (Blum and Mitchell 1998; Yan et al. 2017; Patrini et al. 2016; Zhang and Sabuncu 2018; Xu et al. 2019; Menon et al. 2020; Ma et al. 2020) or inexplicitly used in experiments for synthetizing noisy labels (Han et al.",,
"One typical idea is to reduce the influence of noisy samples with carefully designed losses [35, 1, 52, 40, 44, 25, 14, 6] or regularization terms [20, 28, 23].",,
"This is because the commonly used cross-entropy loss is known to be highly overconfident [27, 40].",,
"As the commonly used cross-entropy loss is known to be highly overconfident [27, 40], LossNet tends to produce polarized results, and high weights could be assigned to some noisy data.",,
", sample selection [21, 29, 35, 51, 60, 67], label correction [22, 54, 56, 62, 66], and robustifying loss functions [8, 14, 32, 39, 42, 55, 71].",,
"Many approaches are proposed to modify or redesign them [18, 40, 42, 45, 69, 71].",,
"[35] mitigated the effects of label noise from an optimization lens, which naturally introduces the partially Huberised loss.",,
[35] leverage gradient clipping to design a new loss.,,
"Their designs are based on different principles, such as gradient clipping [35] and curriculum learning [89].",,
"From the perspective of objective function, the focus is to derive the statistical consistency guarantees for robust  ̃̀ [5], [35], [64].",,
[28] propose the partially Huberised cross entropy loss,,
"Though clipping the training gradient does not tighten the PAC-Bayes bound, Menon et al. (2020) prove that gradient clipping can accelerate training convergence when optimizing deep neural networks.",,
"Examples include semi-supervised learning [7, 4, 59, 64, 63, 47, 54, 5, 40], multi-instance learning [1, 79], positive-unlabeled learning [14, 15, 58, 30, 22, 9], complementary-label learning [33, 73, 34, 69, 11], noisy-label learning [56, 52, 62, 28, 72, 53, 45, 65, 70, 26, 66], positive-confidence learning [35], similar-unlabeled learning [2], and unlabeled-unlabeled learning [43, 44].",,
[53] A.,,
(§ II IB ) Explicit Regularization Bilevel Learning [87] © © © 5 4 4 Official (TensorFlow)9 Annotator Confusion [86] © 5 © © 4 4 Official (TensorFlow)10 Pre-training [88] © 5 © © 4 4 Official (PyTorch)11 PHuber [89] © © © © 4 4 Unofficial (PyTorch)12 Robust Early-learning [90] © © © © 4 4 Official (PyTorch)13 ODLN [91] © © © © 4 4 Official (PyTorch)14,,
"PHuber [89] proposes a composite loss-based gradient clipping, which is a variation of standard gradient clipping for label noise robustness.",,
", 2012; Zhang & Sabuncu, 2018) or gradient clipping (Menon et al., 2020).",,
"Losses can also be modified to address outliers by favoring small losses (Yu et al., 2012; Zhang & Sabuncu, 2018) or gradient clipping (Menon et al., 2020).",,
", 2020), which employs a joint loss function to select small-loss samples; PHuber-CE (Menon et al., 2020), which introduces gra-",,
"…conduct experiments on Co-teaching (Han et al., 2018b), which is a representative algorithm of selecting reliable samples for training; JoCoR (Wei et al., 2020), which employs a joint loss function to select small-loss samples; PHuber-CE (Menon et al., 2020), which introduces gra-
Table 2.",,
"Besides, we ex-
ternally conduct experiments on Co-teaching (Han et al., 2018b), which is a representative algorithm of selecting reliable samples for training; JoCoR (Wei et al., 2020), which employs a joint loss function to select small-loss samples; PHuber-CE (Menon et al., 2020), which introduces gra-
Table 2.",,
"6
Co-teaching 55.32±0.28 51.09±1.06 47.07±0.83 55.29±0.41 53.08±0.26 45.63±0.75 JoCor 52.21±0.70 49.84±0.92 48.83±0.43 55.58±0.27 49.35±0.62 46.21±0.73
PHuber-CE 55.73±0.38 54.33±0.92 45.05±0.49 56.76±0.26 51.15±0.65 41.59±1.05 APL 56.91±0.21 53.12±1.21 43.60±1.28 56.11±0.23 50.93±1.05 43.60±1.28 S2E 57.93±0.37 47.16±1.32 28.53±5.04 54.89±1.92 50.42±1.71 30.67±3.12
Revision 58.06±0.19 52.30±1.73 46.84±1.09 56.41±0.77 53.44±0.83 43.77±1.08
Reweight 53.34±1.08 50.15±1.33 44.73±0.79 53.37±0.66 49.82±0.44 39.46±1.27 Forward 57.30±0.32 53.94±0.42 46.91±1.48 53.58±0.54 49.90±1.44 42.55±3.81
R-Class2Simi 58.67±0.38 56.59±0.74 50.48±0.97 58.44±0.66 55.03±1.55 47.75±2.17 F-Class2Simi 58.27±0.47 56.70±1.13 50.18±0.89 58.46±0.68 54.92±1.66 46.07±3.54
dient clipping to mitigate the effects of noise; APL (Ma et al., 2020), which applies simple normalization on loss functions and makes them robust to noisy labels; S2E (Yao et al., 2020a), which properly controls the sample selection process so that deep networks can benefit from the memorization effect.",,
"The obtained method is known in literature as clipped-SGD (see [17, 21, 43, 44, 57, 70, 71] and references therein).",,
"Indeed, label smoothing is conspicuously absent in most treatments of the noisy label problem (Patrini et al., 2016; Han et al., 2018b; Charoenphakdee et al., 2019; Thulasidasan et al., 2019; Amid et al., 2019; Menon et al., 2020).",,
"Further expanding such study, as done for gradient clipping (Menon et al., 2020), is also of interest.",,
"Another approach restricts information in gradients by clipping them (Menon et al., 2020) .",,
"Another approach
restricts information in gradients by clipping them (Menon et al., 2020) .",,
"(4) PHuber-CE [36], a loss variant of gradient clipping for learning with noisy labels.",,
"For PHuber-CE,
we set τ = 10 for CIFAR-10 and τ = 30 for CIFAR-100 and WebVision.",,
"They instead proposed a noise-robust variant, composite loss-based gradient clipping and the resulting partially Huberised loss (PHuber-CE).",,
The results in Section 4 have shown that LogitClip not only outperforms but also enhances the performance of PHuber-CE loss.,,
"Indeed, recent work [36] has shown that gradient clipping alone does not endow label noise robustness to neural networks.",,
Partial Huberised Cross Entropy (PHuber-CE) [36] enhances the noise robustness of CE with a loss variant of gradient clipping.,,
"More importantly, we show that LogitClip can boost the performance of a wide range of popular robust loss functions, including MAE [12], PHuber-CE [36], SCE [49], GCE [62], Taylor-CE [11], NCE [34], AEL, AUL [64], Cores [8], and Active Passive losses [34].",,
"(7)
Hence,
E [ e2i ] = (σtf ) 2
12 ∥∥∥∥ ∇θif S ∥∥∥∥ 2
2
,E [eiej ]= (σtf ) 2
12 (∇θif S )T ∇θjf S (8)
Perspective 1: Menon et al. (2019) propose not overly trusting any single sample to help mitigate the label noise effect.",,
", 2014), temporal ensembling (Laine & Aila, 2017), gradient clipping (Pascanu et al., 2012; Zhang et al., 2019; Menon et al., 2020) and label smoothing (Szegedy et al.",,
"Moreover, preventing overconfidence can mitigate overfitting on noisy labels (Menon et al., 2020; Lukasik et al., 2020).",,
"On CIFAR-10 and CIFAR-100, we compare with the following baselines: 1) standard crossentropy (CE) loss, 2) Generalized Cross-Entropy (GCE) (Zhang & Sabuncu, 2018) loss, 3) CoTeaching (Han et al., 2018b) that uses co-training and sample selection, 4) PHuber-CE (Menon et al., 2020) that uses gradient clipping and 5) label-smoothing (LS) Lukasik et al. (2020) that clips the label to be less confident before training.",,
"6) Chen et al. (2019b); Menon et al. (2020); Hu et al. (2020); Harutyunyan et al. (2020); Lukasik et al. (2020) apply regularization techniques to improve generalization under label noise, including explicit regularizations such as manifold regularization (Belkin et al., 2006) and virtual…",,
"…and virtual adversarial training (Miyato et al., 2018), and implicit regularizations such as dropout (Srivastava et al., 2014), temporal ensembling (Laine & Aila, 2017), gradient clipping (Pascanu et al., 2012; Zhang et al., 2019; Menon et al., 2020) and label smoothing (Szegedy et al., 2016).",,
", 2018b) that uses co-training and sample selection, 4) PHuber-CE (Menon et al., 2020) that uses gradient clipping and 5) label-smoothing (LS) Lukasik et al.",,
"…Cross-Entropy (GCE) (Zhang & Sabuncu, 2018) loss, 3) CoTeaching (Han et al., 2018b) that uses co-training and sample selection, 4) PHuber-CE (Menon et al., 2020) that uses gradient clipping and 5) label-smoothing (LS) Lukasik et al. (2020) that clips the label to be less confident before…",,
"• PHuber-CE (Menon et al., 2020).",,
"The original paper (Menon et al., 2020) uses τ = 2 on CIFAR-10 and τ = 10 on CIFAR-100, but the default setting does not work well in our experiments.",,
"As an optimization method, we used Adam [49] with an initial learning rate of 0.001, linearly decreasing to zero from 80 epochs to 200 epochs, a momentum of 0.9, and a batch size of 128.",,
"As an optimization method, we used Adam [49] with an initial learning rate of 0.",,
Label smoothing has demonstrated its benefits in improving learning representation Müller et al. (2019). A recent paper Lukasik et al.,,
Robust loss function: The literature has also observed the proposal of robust loss functions that perform well with dealing outlier noisy examples Zhang & Sabuncu (2018); Amid et al. (2019); Menon et al. (2019); Charoenphakdee et al. (2019); Wang et al. (2019).,,
"For more information on Huberised losses, we kindly refer to the original paper (Menon et al., 2020).",,
"Successful recent approaches include correcting the loss for class or label noise, such as [32, 39, 56] (and references therein).",,
"The approaches in [2, 4, 6, 13, 31, 32, 36, 39, 53, 60, 61, 62] share a higher-level technical commonality: they alter the loss via its surrogate.",,
"• Partially Huberised Cross Entropy (PHuber-CE) (Menon et al., 2020):
LPHuber-CEpfpxq, yq “ "" ´ log pθpy|xq, if pθpy|xq ě 1τ , ´τpθpy|xq ` log τ ` 1, else,
where τ ą 0 is a user-defined hyper-parameter.",,
"We set τ “ 10, because it works well in Menon et al. (2020).",,
"We also use an unbounded loss CCE and four bounded losses MAE, MSE, GCE (Zhang & Sabuncu, 2018), and PHuberCE (Menon et al., 2020) in our empirical estimator Eq.",,
"• Partially Huberised Cross Entropy (PHuber-CE) (Menon et al., 2020):",,
"Common strategies include loss correction and reweighting (Patrini et al., 2016; Zhang & Sabuncu, 2018; Menon et al., 2020), label refurbishment (Reed et al., 2014; Song et al., 2019), abstention (Thulasidasan et al., 2019), and relying on carefully constructed trusted subsets of human-verified…",,
"Common strategies include loss correction and reweighting (Patrini et al., 2016; Zhang & Sabuncu, 2018; Menon et al., 2020), label refurbishment (Reed et al.",,
