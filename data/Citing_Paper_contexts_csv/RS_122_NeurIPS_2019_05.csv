text,label_score,label
"This quantity is very similar to that given in [Aitchison, 2019], except that it allows for a slightly more general proposal, QMP, which allows for dependencies between the K samples for a single latent variable, z(1) i , .",,
"Perhaps the most obvious related work is TMC [Aitchison, 2019], which also draws K samples for each of the n latent variables, and considers all K combinations.",,
"We can see that the TMC (orange) [Aitchison, 2019] performs considerably worse than massively parallel VI (red) and IWAE (blue) [Burda et al.",,
"This problem has been addressed in the IWAE context using TMC [Aitchison, 2019], which draws K samples for each of the n latent variables, and individually reasons about each of the K combinations of samples.",,
"To sample all K copies of the full joint latent space, TMC [Aitchison, 2019] uses an IID distribution over the K samples, z(1) i , .",,
", z K i , while TMC [Aitchison, 2019] forces these K samples to be IID.",,
"…log 1
L L∑ ℓ=1 pθ(zℓ, x) qϕ(zℓ|x)
] , (2)
which has been extensively used as an objective function (Burda et al., 2015; Sønderby et al., 2016; Aitchison, 2019; Lopez et al., 2020) and, importantly, as a metric for estimating the marginal log-likelihood, log pθ(x), in VAEs (e.g.,…",,
", 2016; Aitchison, 2019; Lopez et al., 2020) and, importantly, as a metric for estimating the marginal log-likelihood, log pθ(x), in VAEs (e.g., Tomczak and Welling (2018); Bauer and Mnih (2019); Vahdat and Kautz (2020)) and in VI in general (Domke and Sheldon, 2018; Zhang, 2020).",,
"which has been extensively used as an objective function (Burda et al., 2015; Sønderby et al., 2016; Aitchison, 2019; Lopez et al., 2020) and, importantly, as a metric for estimating the marginal log-likelihood, log pθ(x), in VAEs (e.",,
"In the context of variational inference (see, e.g. Blei et al., 2017), it was also noted in Aitchison (2019) that operations akin to sequential importance sampling could be easily written as chaining matrix multiplications, allowing to parallelize these on a GPU, both in the time and particle…",,
", 2017), it was also noted in Aitchison (2019) that operations akin to sequential importance sampling could be easily written as chaining matrix multiplications, allowing to parallelize these on a GPU, both in the time and particle dimensions. The work of Singh et al. (2017) considers blocking strategies for particle Gibbs algorithm, using the Markov property to allow the treatment",,
", 2017), it was also noted in Aitchison (2019) that operations akin to sequential importance sampling could be easily written as chaining matrix multiplications, allowing to parallelize these on a GPU, both in the time and particle dimensions.",,
"Applications of the product-form estimators it builds on can be found peppered throughout the Monte Carlo literature [64, 49, 1, 62, 45], almost always unnamed and specialized to particular contexts.",,
"With the setting of complete matching, IPF becomes tensor Monte Carlo (TMC) (Aitchison, 2019) for SSMs.",,
", h ≡ 1 in Equation (11)) can be derived from tensor Monte Carlo (TMC) (Aitchison, 2019) (See Supplement 3) or a recent result on auxiliary particle filters (Branchini and Elvira, 2021).",,
"(TMC) (Aitchison, 2019) (factorized), differentiable particle filter (DPF) (Corenflos et al.",,
"2021) obtained by combining product-form estimators with SMC and Tensor Monte Carlo (Aitchison 2019) obtained by merging the estimators with variational autoencoders. When choosing among the resulting (and at times bewildering) constellation of estimators, we recommend following one simple principle: pick estimators that somehow ‘resemble’ or ‘mirror’ the target. Good examples of this are well parametrized Gibbs samplers which generate new samples using the target’s exact conditional distributions and, consequently, often outperform other Monte Carlo algorithms (e.g., Sect. 3.4).While formany targets these conditional distributions cannot be obtained (nor are good parametrizations known), their (conditional) independence structure is usually obvious [e.g., see Gelman and Hill (2006), Gelman (2006), Koller and Friedman (2009), Hoffman et al. (2013), Blei et al. (2003), and themany references therein] and can bemirrored using product-form estimators within one’s methodology of choice.",,
"2021) obtained by combining product-form estimators with SMC and Tensor Monte Carlo (Aitchison 2019) obtained by merging the estimators with variational autoencoders. When choosing among the resulting (and at times bewildering) constellation of estimators, we recommend following one simple principle: pick estimators that somehow ‘resemble’ or ‘mirror’ the target. Good examples of this are well parametrized Gibbs samplers which generate new samples using the target’s exact conditional distributions and, consequently, often outperform other Monte Carlo algorithms (e.g., Sect. 3.4).While formany targets these conditional distributions cannot be obtained (nor are good parametrizations known), their (conditional) independence structure is usually obvious [e.g., see Gelman and Hill (2006), Gelman (2006), Koller and Friedman (2009), Hoffman et al. (2013), Blei et al.",,
"2021) obtained by combining product-form estimators with SMC and Tensor Monte Carlo (Aitchison 2019) obtained by merging the estimators with variational autoencoders. When choosing among the resulting (and at times bewildering) constellation of estimators, we recommend following one simple principle: pick estimators that somehow ‘resemble’ or ‘mirror’ the target. Good examples of this are well parametrized Gibbs samplers which generate new samples using the target’s exact conditional distributions and, consequently, often outperform other Monte Carlo algorithms (e.g., Sect. 3.4).While formany targets these conditional distributions cannot be obtained (nor are good parametrizations known), their (conditional) independence structure is usually obvious [e.g., see Gelman and Hill (2006), Gelman (2006), Koller and Friedman (2009), Hoffman et al.",,
"(2021) study their usewithin sequential Monte Carlo (SMC), and Aitchison (2019) builds on them to obtain tensor Monte Carlo (TMC), an extension of importance weighted variational autoencoders. The latter article is the aforementioned exception: its author defines the estimators in general and refers to them as ‘TMC estimators,’ but does not study them theoretically. To the best of our knowledge, there has been no previous systematic exploration of the estimators (2), their theoretical properties, and uses, a gap we intend to fill here. Furthermore, while in simple situations with fully, or almost-fully, factorized test functions [e.g., those in Tran et al. (2013) or Schmon et al. (2020)] it might be clear to most practitioners that employing a product-form estimator is the right thing to do, it may not be quite so immediately obvious how much of a difference this can make and that, in rather precise ways (cf. Theorems 2 and 4), judiciously using product-form estimators is the best thing one can do within Monte Carlo when tacklingmodelswith known independence structure but unknown conditional distributions (a common situation in practice). We aim to underscore these points through our analysis and examples. Lastly, we remark that product-form estimators are reminiscent of classical product cubature rules (Stroud 1971). These are obtained by taking products of quadrature rules and, consequently, require computing sums over NK points much like for product-form estimators [except for fully, or partially, factorized test functions φ where the cost can be similarly lowered, e.g., p. 24 in Stroud (1971)].",,
"…to estimate intractable acceptance probabilities for similarmodels, Lindsten et al. (2017) andKuntz et al. (2021) study their usewithin sequential Monte Carlo (SMC), and Aitchison (2019) builds on them to obtain tensor Monte Carlo (TMC), an extension of importance weighted variational autoencoders.",,
"(2021) study their usewithin sequential Monte Carlo (SMC), and Aitchison (2019) builds on them to obtain tensor Monte Carlo (TMC), an extension of importance weighted variational autoencoders.",,
"(2021) study their usewithin sequential Monte Carlo (SMC), and Aitchison (2019) builds on them to obtain tensor Monte Carlo (TMC), an extension of importance weighted variational autoencoders. The latter article is the aforementioned exception: its author defines the estimators in general and refers to them as ‘TMC estimators,’ but does not study them theoretically. To the best of our knowledge, there has been no previous systematic exploration of the estimators (2), their theoretical properties, and uses, a gap we intend to fill here. Furthermore, while in simple situations with fully, or almost-fully, factorized test functions [e.g., those in Tran et al. (2013) or Schmon et al. (2020)] it might be clear to most practitioners that employing a product-form estimator is the right thing to do, it may not be quite so immediately obvious how much of a difference this can make and that, in rather precise ways (cf.",,
"(2021) study their usewithin sequential Monte Carlo (SMC), and Aitchison (2019) builds on them to obtain tensor Monte Carlo (TMC), an extension of importance weighted variational autoencoders. The latter article is the aforementioned exception: its author defines the estimators in general and refers to them as ‘TMC estimators,’ but does not study them theoretically. To the best of our knowledge, there has been no previous systematic exploration of the estimators (2), their theoretical properties, and uses, a gap we intend to fill here. Furthermore, while in simple situations with fully, or almost-fully, factorized test functions [e.g., those in Tran et al. (2013) or Schmon et al.",,
2021) obtained by combining product-form estimators with SMC and Tensor Monte Carlo (Aitchison 2019) obtained by merging the estimators with variational autoencoders.,,
"For further examples, see the divide-and-conquer SMC algorithm (Lindsten et al. 2017; Kuntz et al. 2021) obtained by combining product-form estimators with SMC and Tensor Monte Carlo (Aitchison 2019) obtained by merging the estimators with variational autoencoders.",,
"In light of IWAE, tensor Monte-Carlo ([6]; TMC) was recently proposed as an attempt to improve upon IWAE by sampling exponentially many importance samples.",,
"The VAE is comprised of two parts, where the first part, qφ(z|x), is often referred to as the representation (recognition or encoder) model ([1, 6, 2]), and it learns a mapping from the input space X to the parameter set φ = {μ, σ(2)}.",,
"This is unsurprising as the TMC, for K = 20 and five layers, considers a factor millionmore importance samples ([6]).",,
"We note in our work, that the intentions of [6] were not to achieve state-of-the-art performances, but to compare the proposed the model to the baseline used in the TMC paper.",,
"Mathematically, as is shown in [6], the number of evaluated importance samples grows exponentially with the layers, so if we reduce the number of layers to one, we effectively evaluateK(1) = K samples in the TMC, the same as for the IWAE.",,
"To compute the tensor inner-product in a numerically stable way, the author provides a method referred to as logmmexp ([6]; see Appendix A).",,
"1, the TMC evaluates exponentially many importance samples as the IWAE ([6, 2]).",,
"In order to average over all different combinations ofmarginal log-likelihoods, Aitchison ([6]), defines the new marginal likelihood estimator as",,
"In this work, we reproduce what we believe are the most important results presented in the Tensor Monte Carlo paper ([6]), where we also provide our reimplementation code.",,
"Since the effects of TMC becomes apparent only when we have intermediate layers ([6]), we expect the IWAE and TMC to produce approximately the same results.",,
(2018) and to the tensor Monte Carlo approach from Aitchison (2018).,,
"That said, high variance and hence biased estimators are a problem in VI for the same reason, which is commonly mitigated by using K > 1 (Burda et al., 2015; Aitchison, 2018).",,
