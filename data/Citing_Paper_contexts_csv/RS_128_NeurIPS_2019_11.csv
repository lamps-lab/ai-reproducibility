text,label_score,label
", 2022), calibration (Thulasidasan et al., 2019; Zhang et al., 2022a), and adversarial robustness (Pang et al.",,
The mixup regularization has been verified that optimizing f on mixed-up data results in better transformation capability of the network and improves model calibration [37].,,
"MixUp, when benchmarked using calibration metrics, presented CNNs with significant calibration benefits according to various studies [56, 63, 8].",,
"Various studies have shown the advantages of these augmentations for calibration of CNNs using standard calibration-based metrics on general computer vision benchmarks [56, 63, 8, 12, 14].",,
Previous research [43] suggests that a well-calibrated model should perform well on OoD datasets.,,
"[43] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.",,
The evaluation metrics were accuracy and Overconfidence Error (OE) (Thulasidasan et al. 2019).,,
"Follow-up research (Thulasidasan et al. 2019) demonstrated that this simple technique is helpful for calibrating neural networks, particularly because interpolated soft labels reduce overconfidence.",,
"2018) tried to calibrate the generalization ability of the model by mixing a pair of images and their labels, and subsequent research (Thulasidasan et al. 2019) has shown that the mixup can help alleviate overconfidence.",,
"In addition, mixup variants have been shown to be effective on a variety of tasks, including fairness machine learning (Han et al. 2022b, 2023; Mroueh et al. 2021), domain generalization (Zhou et al. 2020; Yao et al. 2022), confidence calibration (Zhang et al. 2022; Thulasidasan et al. 2019).",,
"Due to the simplicity and effectiveness, mixup-based methods have gained popularity in various data types and tasks (Yun et al. 2019; Kim, Choo, and Song 2020; Kim et al. 2023; Sahoo et al. 2021; Wang et al. 2021; Han et al. 2022a; Verma et al. 2019; Han et al. 2022b, 2023; Zhou et al. 2020; Mroueh et al. 2021; Zhang et al. 2022; Thulasidasan et al. 2019).",,
"…have gained popularity in various data types and tasks (Yun et al. 2019; Kim, Choo, and Song 2020; Kim et al. 2023; Sahoo et al. 2021; Wang et al. 2021; Han et al. 2022a; Verma et al. 2019; Han et al. 2022b, 2023; Zhou et al. 2020; Mroueh et al. 2021; Zhang et al. 2022; Thulasidasan et al. 2019).",,
"Label smoothing (Müller, Kornblith, and Hinton 2019) and mixup (Thulasidasan et al. 2019) are popular approaches in this line.",,
Thulasidasan et al. (Thulasidasan et al. 2019) observed that the combination of label smoothing and mixup training significantly improved calibration.,,
"(21)
Mixup training (Thulasidasan et al. 2019) is another work in this line of exploration.",,
"Data augmentation is an effective way to alleviate this phenomenon and brings implicit calibration effects (Thulasidasan et al. 2019; Müller, Kornblith, and Hinton 2019).",,
"Label smoothing (Müller, Kornblith, and Hinton 2019) and mixup (Thulasidasan et al. 2019) are popular approaches in this line.",,
"Data augmentation is an effective way to alleviate this phenomenon and brings implicit calibration effects (Thulasidasan et al. 2019; Müller, Kornblith, and Hinton 2019).",,
Mixup training (Thulasidasan et al. 2019) is another work in this line of exploration.,,
(Thulasidasan et al. 2019) observed that the combination of label smoothing and mixup training significantly improved calibration.,,
"Mixup [85, 62] explores the neighbourhood of training data through random interpolation of input images and associated labels to improve model calibration degree.",,
"OE = M∑ i=1 |Bi| |D| · 1(Ci > Ai) · |Ci −Ai|, (29)
We adapt OE to different binning schemes of ECEEW, ECEEM, ECESWEEP to produce OEEW, OEEM, OESWEEP respectively.",,
"2 [62] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.",,
"Evaluation Metrics: We use Equal-Width Expected Calibration Error (ECEEW) [18] and Equal-Width Overconfidence Error (OEEW) [62] with 10 bins (B = 10) to
evaluate the model calibration degrees.",,
"8 presents model calibration degrees, evaluated in terms of Equal-Width Expected Calibration Error (ECEEW) and Equal-Width Over-confidence Error (OEEW) with 100 bins (B = 100), of various static stochastic label perturbation techniques, in which a unique label perturbation probability α is set for all samples throughout the training.",,
"We also compare with model calibration methods include: Temperature Scaling (TS) [18], Brier Loss [4], MMCE [30], Label Smoothing [46], Mixup [62], Focal Loss [45] and AdaFocal [17] implemented on our baseline model.",,
Evaluation Metrics: We use Equal-Width Expected Calibration Error (ECEEW) [18] and Equal-Width Overconfidence Error (OEEW) [62] with 10 bins (B = 10) to evaluate the model calibration degrees.,,
We report model calibration results evaluated in terms of Equal-Width Expected Calibration Error (ECEEW) and Equal-Width Over-confidence Error (OEEW) with 10 bins in Tab.,,
"They either modify loss term during network training [21], use soft labels [47, 36], or scale down the logits after training [10].",,
"Mixup is similar to label smoothing, where different data-label pairs are mixed to form new data points [36, 47].",,
"Model-free methods (M. Xu et al., 2023) modify inputs through transformations (Cubuk et al., 2020; Hendrycks et al., 2020), combining datapoints (Mintun et al., 2021; Thulasidasan et al., 2019), or using image-to-image networks (Hendrycks et al., 2021).",,
"To convert ECE to a calibration score, we normalize it by a maximum error ECEmax and subtract the result from 1.",,
"These techniques have demonstrated performance improvements across various metrics, such as distribution-shift robustness (Hendrycks & Dietterich, 2019; Sagawa et al., 2022; Taori et al., 2020), expected calibration error (ECE) (Thulasidasan et al., 2019), and out-of-distribution detection (Thulasidasan et al., 2019).",,
We use ECEmax = 0.5 as this corresponds with the pathological case of a binary classifier that is correct 50% of the time but always predicts with 100% confidence.,,
"…demonstrated performance improvements across various metrics, such as distribution-shift robustness (Hendrycks & Dietterich, 2019; Sagawa et al., 2022; Taori et al., 2020), expected calibration error (ECE) (Thulasidasan et al., 2019), and out-of-distribution detection (Thulasidasan et al., 2019).",,
"We therefore choose to compute the calibration score by averaging the calibration of the model across the in-distribution and distribution-shifted
datasets, giving
sCAL = 1− 1
(N + 1)ECEmax
[ ECE ID +
N∑ i=1 ECE i
] , (6)
where ECE ID is the expected calibration error on the in-distribution data and ECE i is measured on the ith distribution-shifted dataset.",,
"In this work, since we focus on classification tasks we use ECE .",,
"For example, Guo et al. (2017) explore metrics such as the expected calibration error (ECE ), which is computed by binning the model outputs and averaging the absoluted difference in the accuracy of each bin to the accuracy predicted by an identity function.",,
"They also discuss the negative log-likelihood, a metric also used by Lakshminarayanan et al. (2017) and Gal and Ghahramani (2016), as it is applicable to regression tasks in addition to classification tasks, though ECE has also been extended to the regression setting (Levi et al., 2022).",,
"Several studies (Thulasidasan et al., 2019; Zhang et al., 2021b; Wen et al., 2021; Zhang et al., 2021a; Li et al., 2021b; Chidambaram et al., 2021; Park et al., 2022) have attempted to understand the principles of MSDA and analyze its effects.",,
"Two works (Thulasidasan et al., 2019; Zhang et al., 2021b) have found that Mixup helps to calibrate convolutional neural networks (CNNs) and makes them less over-confident.",,
"Additionally, there are trainingstage calibration methods that employ label smoothing [57, 58] or optimize accuracy-uncertainty differentiably [59].",,
Most related to our approach are techniques that use data augmentations that blend different inputs together [54] or use ensembles to combine representations [32].,,
"[54] Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.",,
"On the one hand, several approaches proposed to modify the training process in order to obtain strongly calibrated predictions (Lakshminarayanan, Pritzel, and Blundell 2017; Thulasidasan et al. 2019; Mukhoti et al. 2020; Tomani and Buettner 2021).",,
"This may include using specialized loss functions tailored for calibration[45], employing ensemble methods[26, 46], or data augmentation[47], etc.",,
"MixUp (Thulasidasan et al., 2019b).",,
"MixUp (Thulasidasan et al., 2019b), and Convex (Zhan et al., 2021) use synthesized negative instances to regularize the model.",,
"Zhan et al. (2021) use MixUp technique (Thulasidasan et al., 2019a) to synthesize known data into unknown data.",,
"However, due to the exponential function employed in the softmax layer, the trained deep neural network often produces high confidence scores even for misclassified samples, as studied extensively in [1, 4, 24].",,
This method has demonstrated its effectiveness in enhancing the generalization and robustness of deep learning models for image classification tasks [16].,,
"As we mentioned in above sections, there are some contradictory results on mixup’s calibration performance in previous studies [16,27].",,
[27] empirically found that mixup improves calibration across various model architectures and datasets.,,
"Most of these existing studies try to improve calibration performance with the regularization effect induced by mixup [8,16,27,38].",,
"We notice there are some contradictory results on mixup’s calibration performance in previous studies [16, 27].",,
"Follow-up works (Thulasidasan et al., 2019; Rahaman & Thiery, 2021; Wen et al., 2021) have supported these findings, although the recent work of Minderer et al.",,
"Obtaining such models with good predictive uncertainty is the problem of model calibration, and has seen a flurry of recent work in the context of training deep learning models (Guo et al., 2017; Thulasidasan et al., 2019; Ovadia et al., 2019; Wen et al., 2020; Minderer et al., 2021).",,
", 2017), data augmentation (Thulasidasan et al., 2019; Müller et al., 2020), ensembling (Lakshminarayanan et al.",,
"However, several empirical works have shown that temperature scaling alone can be outperformed by training-time modifications such as data augmentation (Thulasidasan et al., 2019; Müller et al., 2020) and regularized loss functions (Kumar et al.",,
"…limited to improved generalization performance in supervised learning; they are known to be helpful in other aspects including model calibration (Thulasidasan et al., 2019), semi-supervised learning (Berthelot et al., 2019; Sohn et al., 2020), contrastive learning (Kalantidis et al., 2020;…",,
"The success of Mixup-style training schemes is not only limited to improved generalization performance in supervised learning; they are known to be helpful in other aspects including model calibration (Thulasidasan et al., 2019), semi-supervised learning (Berthelot et al.",,
"The major branch of OOD detection is classification-based methods [41, 42] including confidence enhancement methods [1,31], outlier exposure [2,7,26,44] and post-hoc detection [10–12,14,21,23,29,36].",,
"Regularization techniques are generally found to be effective for calibrating DNNs. Data augmentation methods, such as Mixup (Thulasidasan et al., 2019) and AugMix (Hendrycks et al., 2019), train DNNs on mixed
samples and have been found to reduce the tendency to make over-confident predictions.",,
"Data augmentation methods, such as Mixup (Thulasidasan et al., 2019) and AugMix (Hendrycks et al.",,
Several works have observed that the improved performance of Mixup can be attributed to better network calibration [48] and out-of-manifold regularisation [17].,,
"In standard training (ST), mixup has been widely used to improve the generalization of models [33], [34], [46], [47], [48], [49].",,
"In this research, we evaluate the calibration using the Expected Calibration Error (ECE) metric which measures the deviation be-
tween predicted confidence and accuracy.",,
"The third one is Calibration (Guo et al., 2017; Kumar et al., 2019; Thulasidasan et al., 2019; Minderer et al., 2021), which measures the predictive uncertainty of a model, and we use this metric to assess if ChatGPT is overconfidence on its prediction.",,
"The third one is Calibration (Guo et al., 2017; Kumar et al., 2019; Thulasidasan et al., 2019; Minderer et al., 2021), which measures the predictive uncertainty of a model, and we use this metric to assess if Chat-",,
"A properly calibrated classifier should have predictive scores that accurately reflect the probability of correctness (Thulasidasan et al., 2019; Minderer et al., 2021).",,
Calibration.,,
"learned with the supervised learning manner becomes overconfident [20], it may overlook other correlation information.",,
"In addition, a regularization termwas also introduced into the confidence calibration, such asMixup (Thulasidasan et al 2019) and label smoothing (Müller et al 2019).",,
"For example, deep learning models trained with mixup are less prone to making overconfident predictions on OOD test data in image classification [35].",,
[35] show that mixup also improves the model’s confidence calibration in image classification.,,
Another study showed that mixup training [34] can improve the model calibration in image classification [35].,,
"A recent work [64] demonstrates that calibration methods [20, 44, 45, 52] are harmful for MisD, and then reveals a surprising and intriguing phenomenon termed as reliable overfitting: the model starts to irreversibly lose confidence reliability after training for a period, even the test accuracy continually increases.",,
Bonus Effect of Data Augmentation Calibration [42] studied the overlooked effect of Mixup: their effect on calibration.,,
"1, 2, 3 [42] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.",,
"For instance, accurately predicting traffics at crossroads with high traffic volumes during rush hours is crucial for urban travel but can be challenging due to complex traffic patterns [9].",,
"For instance, ensemble methods [35] can leverage multiple prediction models, and data augmentation techniques [9] involve applying various data augmentation methods to the test input to obtain multiple predictions to form the PI.",,
"(2015), while the Overconfidence Error (OE)—its counterpart that only considers the miscalibration caused by overconfident predictions—was proposed by (Thulasidasan et  al., 2019).",,
[19] empirically found that mixup can significantly improve,,
"[15,19] showed the favorable calibration effect of LS.",,
"In our experiments, for mixup, we follow the setting in [19] to use α = 0.",,
"Mixup has empirically shown its effectiveness in improving the generalization and robustness of deep classification models (Zhang et al., 2018; Guo et al., 2019a;b; Thulasidasan et al., 2019; Zhang et al., 2022b).",,
Thulasidasan et al. (2019) show that Mixup helps to improve the calibration of the trained networks.,,
"1 INTRODUCTION Mixup has empirically shown its effectiveness in improving the generalization and robustness of deep classification models (Zhang et al., 2018; Guo et al., 2019a;b; Thulasidasan et al., 2019; Zhang et al., 2022b).",,
"…models, there has been a recent surge of interest attempting to better understand Mixup’s working mechanism, training characteristics, regularization potential, and possible limitations (see, e.g.,
Thulasidasan et al. (2019), Guo et al. (2019a), Zhang et al. (2021), Zhang et al. (2022b)).",,
"Training-Time Calibration Popular training-time approaches consist of reducing the predictive entropy by means of regularization [11], e.g. Label Smoothing [27] or MixUp [30], or loss functions that smooth predictions [25].",,
"For comparison, we include a standard singleloss one-head classifier (SL1H), plus models trained with Label Smoothing (LS [27]), Margin-based Label Smoothing (MbLS [22]), MixUp [30], and using the DCA loss [20].",,
"Label Smoothing [27] or MixUp [30], or loss functions that smooth predictions [25].",,
"The calibrated model can be obtained via data augmentation [22, 57, 62], adversarial training [7, 10, 20], and uncertainty modelling [5, 40].",,
"However, the point estimation of DNN parameters tends to be overfitting and overconfident [131].",,
"Despite its simplicity, MixUp has been proven to improve model calibration and better generalization [35].",,
[26] acknowledged the calibration and generalization property of mixup.,,
"Various techniques like vector scaling [7, 20], matrix scaling [7], temperature scaling [6, 12, 20], label smoothing [4, 19], mixup [26, 38], CutMix [34] etc.",,
"Mixup [26] targets the overconfidence of the model, therefore KD + mixup drastically reduces the OE of the model which makes it suitable for high risk domains.",,
DNNs trained with mixup are better-calibrated [21].,,
"A well-calibrated model helps improve the model robustness on OoD datasets [Thulasidasan et al., 2019].",,
", 2019], and data augmentation [Thulasidasan et al., 2019; Hendrycks et al., 2019] are introduced to improve model calibration.",,
"Mixup [Thulasidasan et al., 2019] and AugMix [Hendrycks et al.",,
", 2020) or mixup techniques (Thulasidasan et al., 2019; Zhang et al., 2017) as well as other intrinsically calibrated approaches (Sensoy et al.",,
[33] has shown that Mixup is efective at calibrating deep models for the tasks of image and text classifcation.,,
", 2014) and data augmentation methods such as mixup (Thulasidasan et al., 2019) and Augmix (Hendrycks et al.",,
"…smoothing (Müller et al., 2019), focal loss (Mukhoti et al., 2020), dropout (Srivastava et al., 2014) and data augmentation methods such as mixup (Thulasidasan et al., 2019) and Augmix (Hendrycks et al., 2020).
λ ∼ Beta(α, α)
More advanced methods for improving performance are also demonstrated…",,
"[28] demonstrate that neural networks trained with mixup are significantly better calibrated under dataset shift, and are less prone to over-confident predictions on out-of-distribution data.",,
"It has been shown by [28], [30] that label smoothing can also effectively improve the quality of a model’s uncertainty estimates.",,
"Another approach of mixup training [277] generates additional samples during training by convexly combining random pairs of images and their associated labels, which is found to improve not only the classification performance but also the calibration and predictive uncertainty of the model.",,
"Similar to the results of [5], with the mixup augmentation, the improvement in accuracy was not significant, but a model calibration effect could be seen in some cases.",,
"However, it did not achieve much in improving the generalization performance of the model in preparation for the correction effect [5, 6].",,
"Recently, several studies have introduced that data augmentation is effective for model generalization as well as calibration [5, 6], but the results are not significant in terms of generalization performance.",,
"However, predictions from a single model can be misleading, such as they are prone to overconfidence (Thulasidasan et al. 2019).",,
"Unfortunately, the severity of these mistakes is compounded by the fact that the predictions computed by neural networks are often overconfident (Guo et al., 2017), partly due to overfitting (Thulasidasan et al., 2019; Ovadia et al., 2019).",,
", 2017), partly due to overfitting (Thulasidasan et al., 2019; Ovadia et al., 2019).",,
"…a transformation that maps the raw outputs of classifiers to their expected probabilities Kull et al. (2019); Guo et al. (2017a); Gupta and Ramdas (2021), and ad-hoc methods that adapt the training process to produce better calibrated models Thulasidasan et al. (2019); Hendrycks et al. (2019a).",,
"Important techniques in this category include mixup training Thulasidasan et al. (2019), pre-training Hendrycks et al. (2019a), label-smoothing Müller et al. (2019), data augmentation Ashukha et al. (2020), self-supervised learning Hendrycks et al. (2019b), Bayesian approximation (MC-dropout) Gal…",,
"Mathematically, xλ = λx1 + (1 − λ)x2 (4) yλ ∼ Pyλ = λPy1 + (1 − λ)Py2 (5)",,
"In this direction, diverse methods have been proposed to calibrate the model (during training) using the regularization-like techniques, thereby constraining the prediction’s overconfidence (Thulasidasan et al., 2019; Mukhoti et al., 2020; Pereyra et al., 2017; Müller et al., 2019).",,
"Label smoothing and Mixup tend to regularize the DNN to prevent overconfidence (Müller et al., 2019; Thulasidasan et al., 2019).",,
"In this direction, diverse methods have been proposed to calibrate the model (during training) using the regularization-like techniques, thereby constraining the prediction’s overconfidence (Thulasidasan et al., 2019; Mukhoti et al., 2020; Pereyra et al., 2017; Müller et al., 2019).",,
"Label smoothing and Mixup tend to regularize the DNN to prevent overconfidence (Müller et al., 2019; Thulasidasan et al., 2019).",,
"Although data augmentation enlarges the size of training data and therefore helps meet the requirement for large training data, it can also result in poorly calibrated ensembles, especially when using modern data augmentation techniques such as mixup [61].",,
"Moreover, modern data augmentation techniques such as mixup [61] or exposure to out-of-domain examples [16, 44] have been proposed.",,
"Regularized training focuses on calibrating DNN over training, such as [29, 30, 40, 45].",,
"To calibrate DNNs’ confidence in prediction, researchers have developed a rich line of works on image classification using regularized training [29,30,40,45], post-hoc processing [11,19,24,35], and Bayesian modeling [15, 18, 21, 46], to name a few.",,
"TREC [138] Sentence Classification [135], [136], [137], [139], [140], [141], [142] Uncertainty & Calibration [106]",,
"ImageNet [101] Image Classification [14], [15], [26], [32], [35], [37], [38], [39], [40], [41], [43], [44], [46] [47], [48], [49], [50], [51], [52], [53], [54], [55], [57], [58], [59], [62] [63], [64], [65], [66], [67], [68], [70], [71], [72], [74], [75], [77], [79] [81], [82], [85], [102], [103], [104], [105] Robustness [14], [46], [49], [52], [57], [71], [74], [89], [91], [105], [106] Uncertainty & Calibration [52], [74], [86], [105], [106] Object Localization [15], [49], [52], [95] Pascal VOC [107] Object Detection [15], [46], [57], [58], [64], [66], [68], [70], [72], [102], [105], [108] [109], [110], [111], [112] Cityscapes [113] Object Detection [108], [112] MS-COCO [114] Image Captioning [15], [50], [54], [57], [66], [70], [102], [103], [105] ADE [115] Semantic Segmentation [50], [103], [104]",,
"STL [123] Image Classification [66] Semi-Supervised [93], [97] Uncertainty & Calibration [106] Robustness [106]",,
Experiments conducted on several image classification benchmarks and models [106] demonstrate that mix-trained deep neural networks can significantly improve calibration.,,
"MR [134] Sentence Classification [135], [136], [137] Uncertainty & Calibration [106]",,
"Besides classification with rejection accuracy, many other metrics have been proposed, such as expected calibration error [3], thresholded adaptive calibration error [25], overconfidence error [26], and calibrated log-likelihood [24].",,
"Calibration methods with training procedure use data augmentation [44, 54] or modify training loss [22, 26, 33].",,
"and overconfidence error which gives high weight to confident but wrong predictions, a situation that is of particular concern in medical applications [25],",,
"They include postprocessing methods such as isotonic regression [23], conformal prediction [24] and Platt scaling [17], as well as methods that modify the training process such as mixup [25, 26], modelling probability distributions of class probabilities with Dirichlet distributions [27] and the use of dropout during training and prediction [28].",,
Dropout may also be combined with mixup training [25] and post-processing schemes such as Platt scaling [17] in the future.,,
"Thulasidasan et al. found that mixup training can improve the calibration of deep neural networks, which means that the predicted probabilities of the model are more closely aligned with the true underlying probabilities of the data (Thulasidasan et al., 2019).",,
"found that mixup training can improve the calibration of deep neural networks, which means that the predicted probabilities of the model are more closely aligned with the true underlying probabilities of the data (Thulasidasan et al., 2019).",,
"For example, [42] and [31] found that Mixup can be used to improve the loss calibration and the robustness of the network.",,
"The introduction of λs requires the network prediction to be reliable, however, some recent papers [31, 41] found that deep networks tend to be under-confident when the λ is sampled from a Beta distribution Beta(α, α) with α > 0.",,
"We believe this behavior to be generally desirable, especially for classifiers that provide well-calibrated predictions during training (Thulasidasan et al. 2019).",,
"Confidence reliability is a very important issue for DNNs which can be used for various tasks like: misclassification detection [6], [23], confidence calibration [7], [24] and out-of-distribution detection [8], [30].",,
"1, the reliability of confidence can be evaluated by three tasks: misclassification detection [6], [23], confidence calibration [7], [24] and outof-distribution detection [8], [25], which have attracted growing attention in the machine learning community recently.",,
[24] found that the predicted scores of DNNs trained with mixup [11] are better indicators of the actual likelihood of a prediction.,,
"One class of approaches [24], [38] aims to learn well-calibratedmodels during training.",,
"To demonstrate the superiority of classAug over dataAug, we have made comparison between classAug and state-of-the-art data augmentation techniques, such as mixup [24], Cutmix [12], and RandAug [74].",,
"tion (Zhong et al. 2021b; Thulasidasan et al. 2019), it has shown a limitation in balancing distributions thereby losing overall accuracy.",,
"Although Mixup achieves performance gains in terms of AP by its positive impacts on confidence calibra-
tion (Zhong et al. 2021b; Thulasidasan et al. 2019), it has shown a limitation in balancing distributions thereby losing overall accuracy.",,
"To remedy this problem, various approaches have been proposed including Monte Carlo dropout [15], latent Gaussian processes [51], deep ensembles [32], training multiple independent subnetworks [20], and augmentation [49].",,
"Unfortunately, several lines of work observe that modern neural networks lack such calibration [18, 28, 30, 42, 49], particularly under distribution shift [42].",,
"This method was originally proposed as data agnostic approach which also shows good results if applied to image data [4,2,16].",,
"For Mixup and Cutmix, we perform experiments using all combinations of α values in the set {0.1, 0.4, 0.8, 1.0, 1.2}.",,
"…models, including post-hoc softmax temperature scaling (Guo et al., 2017), ensemble-based techniques (Lakshminarayanan et al., 2017), Mixup (Zhang et al., 2017; Thulasidasan et al., 2019), Monte-Carlo dropout, and uncertainty estimates in Bayesian networks (Neal, 2012; Gal & Ghahramani, 2016).",,
"Such simplicity makes CSSL
5Mixup and Cutmix are not performed simultaneously.",,
"For AutoAugment, we utilize the ImageNet augmentation policy, and α values of 0.1 and 0.8 are again adopted for Mixup and CutMix, respectively.",,
"Numerous methodologies have been proposed for producing calibrated models, including post-hoc softmax temperature scaling (Guo et al., 2017), ensemble-based techniques (Lakshminarayanan et al., 2017), Mixup (Zhang et al., 2017; Thulasidasan et al., 2019), Monte-Carlo dropout, and uncertainty estimates in Bayesian networks (Neal, 2012; Gal & Ghahramani, 2016).",,
"REMIND also leverages random resized crops and manifold Mixup (Verma et al., 2019) on the feature representations stored within the replay buffer throughout streaming.",,
"The results of these experiments, reported in terms of Top-1 Ωall, are provided in Table 10, where it can be seen that the best results are achieved by combining Mixup, Cutmix, and AutoAugment techniques into a single augmentation policy.",,
", 2017), Mixup (Zhang et al., 2017; Thulasidasan et al., 2019), Monte-Carlo dropout, and uncertainty estimates in Bayesian networks (Neal, 2012; Gal & Ghahramani, 2016).",,
"Interpolation methods (e.g., Mixup (Zhang et al., 2017; Wolfe & Lundgaard, 2019; Inoue, 2018) and CutMix (Yun et al., 2019)) take stochasticallyweighted combinations of images and label pairs during training, which provides regularization benefits.",,
"In particular, we perform experiments with Mixup, CutMix, and SamplePairing interpolation strategies13 for class-incremental learning on the CIFAR100 dataset, adopting the same experimental setting as described in Section 6.1.",,
"In particular, CSSL combines random crops and flips, Mixup, Cutmix, and
Autoaugment into a single, sequential augmentation policy5; see Appendix B.1 for further details.",,
"We test all combinations of the Mixup, Cutmix, AutoAugment, and RandAugment data augmentation strategies to arrive at the final, optimal augmentation policy used within the proposed methodology for CSSL.",,
"13The α values used for Mixup and Cutmix are tuned by searching over the set {0.1, 0.4, 0.8, 1.0, 1.2} using a hold-out validation set, and we present the results of the best-performing α for each method.",,
"Previous work indicates that using Mixup encourage good calibration properties (Thulasidasan et al., 2019), indicating that models obtained from CSSL and REMIND—both of which use some form of Mixup—should be highly-calibrated.",,
"Mixup and Cutmix use α values of 0.1 and 0.8, respectively, and AutoAugment adopts the Imagenet learned augmentation policy.",,
Our policy randomly chooses between Mixup or Cutmix for each data example with equal probability.,,
"For Mixup and Cutmix, we utilize α values of 0.1 and 0.8, respectively.",,
"Additionally, mixup training has been shown to improve network calibration for both in- and out-of-distribution data [14].",,
"[14] Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.",,
"compared to standard training over different model architectures, tasks, and domains (Liang et al., 2018; He et al., 2019; Thulasidasan et al., 2019; Lamb et al., 2019; Arazo et al., 2019; Guo, 2020; Verma et al., 2021b; Wang et al., 2021).",,
"Also, regularization during training such as label smoothing [298] and mixup [138] have been shown to improve calibration [214, 228, 306].",,
"…been several studies testing this property in modern neural networks (Guo et al., 2017; Nixon et al., 2019; Minderer et al., 2021; Wang et al., 2021b) and proposing ways to improve it (Thulasidasan et al., 2019; Mukhoti et al., 2020; Karandikar et al., 2021; Zhao et al., 2021a; Tian et al., 2021).",,
"Therefore, careful tuning of the noise parameters is necessary to strike a balance between robustness and performance [18].",,
"Similarly, other simple and effective methods to enhance model uncertainty estimation such as Ensemble [79] and Mixup [74] also demonstrate excellent performance.",,
"Representative methods include Mixup [74], CutMix [75], and PixMix [76].",,
"Fortunately, training good classifiers is easy with modern AutoML (Erickson et al., 2020) and techniques for calibration, data augmentation, and transfer learning (Thulasidasan et al., 2019).",,
", 2020) and techniques for calibration, data augmentation, and transfer learning (Thulasidasan et al., 2019).",,
"Hence, confidence calibration is a vivid field of research and proposed methods are based on additional loss functions [32, 35, 45, 48, 52], on adaptions of the training input by label smoothing [54, 60, 63, 75] or on data augmentation [20, 45, 76, 88].",,
"Mixup is a data augmentation method [14] which is shown to output well-calibrated predictive scores [13], and is again performed during training.",,
"In addition, Mixup and Focal Loss are known to outperform Label Smoothing as shown in [13, 8].",,
"[6] conclude that Transformers are calibrated better than CNNs yielding overconfident predictions [21, 22, 23].",,
"[9] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.",,
"Proper data augmentation also contributes to OOD uncertainty estimation [16, 17, 9].",,
"Additionally, it has been noted that calibration behaves di erently in deep ensembles [Lakshminarayanan et al., 2017, Wen et al., 2021], and when using data augmentations [Thulasidasan et al., 2019, Wen et al., 2021], or label smoothing [Szegedy et al., 2016], for example.",,
"It thus reduces the effect of outliers and, consequently, lessens the likelihood of overfitting (Thulasidasan et al., 2019; Zhang et al., 2020).",,
"Data augmentation strategies based on mixing [24,25] have been shown to improve calibration [26,27] along with crafted loss functions [28].",,
"Among these we can mention attempts to link: robustness and calibration [34,32,43] showing that models which are robust to adversarial attacks are more interpretable; data augmentation, calibration and interpretation showing that MixUp data augmentation greatly impacts the calibration of learnt models [42] and existing saliency methods being used to improve the MixUp procedure itself [15]; or calibration and fairness [29], showing the incompatibility between most of fairness definitions and calibration.",,
"[Thulasidasan et al., 2019] have empirically shown that the network trained with Mixup provides better-calibrated results.",,
"For the quantitative analysis of the confidence calibration, we used two popular metrics, the expected calibration error (ECE) [Naeini et al., 2015] and the overconfidence error (OE) [Thulasidasan et al., 2019].",,
"De-
spite its simplicity, Mixup training achieves robustness toward adversarial examples [Zhang et al., 2018] and improves calibration [Thulasidasan et al., 2019].",,
"One possible reason is under-fitting since its complicated training prevents sufficient convergence for the conventional learning procedure [Thulasidasan et al., 2019].",,
"In particular, using a large α value degrades the accuracy largely in Mixup [Thulasidasan et al., 2019; Zhang et al., 2018].",,
"Furthermore, recent work [Thulasidasan et al., 2019] reports that Mixup training encourages that the output of DNNs, i.e., the estimated label distribution, can serve as a better indicator of the actual likelihood of a correct prediction.",,
"Furthermore, recent work [Thulasidasan et al., 2019] reports that Mixup training encourages that the output of DNNs, i.",,
"Lately, [Thulasidasan et al., 2019] show that the label smoothing effect is a crucial factor for achieving accurate predictive uncertainty.",,
", 2015] and the overconfidence error (OE) [Thulasidasan et al., 2019].",,
"[Thulasidasan et al., 2019] also show that data augmentation alone without mixed labels can improve the prediction accuracy.",,
"has been shown to increase robustness towards adversarial samples, a better estimate of uncertainty [27], and better generalization of the trained model [33].",,
"We would like to highlight that our observation is in contrast to the prior work [Thulasidasan et al., 2019] which suggests that Mixup provides reliable uncertainty estimates for OOD data as well.",,
"It has been observed that temperature scaling [Guo et al., 2017] or replacing the typical cross-entropy loss [Chung et al., 2021, Thulasidasan et al., 2019, Mukhoti et al., 2020] can be highly effective to reduce this mismatch.",,
"Despite their extraordinary performance, DNNs are oftencriticized as being poorly calibrated and prone to be overconfident, thus leading to unsatisfied uncertainty estimation [10, 11, 12].",,
"Important techniques in this category
include mixup training [Thulasidasan et al., 2019], pretraining [Hendrycks et al., 2019a], label-smoothing [Müller et al., 2019], data augmentation [Ashukha et al., 2020], selfsupervised learning [Hendrycks et al., 2019b], Bayesian approximation (MC-dropout)…",,
"…a transformation that maps from classifiers raw outputs to their expected probabilities [Kull et al., 2019, Guo et al., 2017a, Gupta and Ramdas, 2021b], and ad-hoc methods that adapt the training process to generate better calibrated design [Thulasidasan et al., 2019, Hendrycks et al., 2019a].",,
"One is to design OOD detection oriented neural networks and objective [13] [15] [1] [11] [4] [23], which can achieve high performance OOD detection but they need overhead computation cost to retrain the models.",,
"Thus, we reasonably guess that VQAMix may contribute to the model’s interpretability by taking the soft label as the supervision during the training process according to [56].",,
"in NeurIPS 2019 (top-tier, H5 Index: 192) [42], and theoretically explained in ICLR 2021 (top-tier, H5",,
"Researchers [42, 20] showed that training with mixup can significantly improve the model calibration.",,
"• In agreement with previous work [13, 47, 32, 42], temperature scaling maintained the discrimination and improved calibration in all scenarios, including balanced- vs.",,
"However, naive pseudo labeling can result in accumulation of errors (Guo et al., 2017; Wei et al., 2022; Meinke & Hein, 2020; Thulasidasan et al., 2019; Kristiadi et al., 2020).",,
Previous works have demonstrated that mixup-like data augmentation techniques can greatly improve the uncertainty estimation on unseen data Thulasidasan et al. (2019); Hendrycks et al. (2020).,,
"Mixup [69] and its extensions [60,68] have been effective at improving confidence calibration [58], and to some extent, generalization [68, 69], in the balanced setting.",,
"Among calibration methods in the literature, we explore mixup [69] for two main reasons: (1) mixup has shown to improve calibration in the balanced setting [58] and to some extent in the longtailed setting [71].",,
"More broadly, there has been much recent work develop methods to improve calibration for deep learning models, including augmentation-based training [Thulasidasan et al., 2019, Hendrycks et al., 2019b], self-supervised learning [Hendrycks et al., 2019a], ensembling [Lakshminarayanan et al., 2017],…",,
"The strong performance of MixUp on CIFAR-100 is probably due to it being particularly well-suited to the hierarchical nature of the classes [23, 28].",,
"Two examples are label smoothing [28] and mixup [47] which are originally proposed to improve generalization [37, 46] and adversarial robustness [55], respectively.",,
"probability estimates in the literature, including but not limited to post-processing [6, 38], Bayesian approximation [2, 5], regularization [28, 47], and deep ensemble [22].",,
"CPC was compared to several popular single-model calibration baselines: vanilla DNN, temperature scaling [6], MC dropout [44], label smoothing [28, 46], and mixup [47, 55].",,
"These approaches encompass Monte-Carlo Dropout (MCDropout) [10], Maximum Class Probability (MCP) [17], Trust Score [20].",,
"Some recent research indicates that soft-label augmentation techniques, such as label smoothing [209] and mixup [210,211], can effectively mitigate the over-confidence problem, thus helping network calibration.",,
"Furthermore, we apply mixup [41, 33], an effective data augmentation method.",,
"Nevertheless, this issue is distinct from the field of out-of-distribution (OOD) samples, where a rich body of research exists already [33, 36, 20, 35, 42, 34].",,
"MixUp [41, 33] is a method, which trains a neural network on convex combinations of pairs of examples and their labels; thus, favoring simple linear behavior in-between training examples.",,
"Various methods have been proposed to improve DNN’s calibration in training [33, 54, 55, 56] or by a post-processing module after training [31, 32, 57].",,
"As such, the authors used a form of ECE in their evaluation that measures the miscalibration of the most probable class output by the classifier, a choice mirrored in a number of subsequent works (Thulasidasan et al., 2019; Müller et al., 2019; Mukhoti et al., 2020; Alexandari et al., 2020).",,
"As such, the authors used a form of ECE in their evaluation that measures the miscalibration of the most probable class output by the classifier, a choice mirrored in a number of subsequent works (Thulasidasan et al., 2019; Müller et al., 2019; Mukhoti et al., 2020; Alexandari et al., 2020).",,
"Training-time interventions in our experiments include: Label smoothing (LS) (Szegedy et al., 2016; Müller et al., 2019), Mix-up (MU) (Zhang et al., 2018; Thulasidasan et al., 2019), and Focal loss (FL) (Lin et al., 2017; Mukhoti et al., 2020).",,
", 2019), Mix-up (MU) (Zhang et al., 2018; Thulasidasan et al., 2019), and Focal loss (FL) (Lin et al.",,
"model regularization [11], [12], this process strongly perturbs the semantic information from reliably labeled samples.",,
"[8] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.",,
"Despite this, MixUp has been explored for NLP tasks with substantial success using hidden state representations (Verma et al., 2019).",,
"Manifold MixUp (M-MixUp) (Verma et al., 2019) generates additional samples by interpolating random training samples in the feature space (obtained from the task-specific layer on top of the BERT pre-trained language model).",,
"Another interesting approach is the use of mixup [28,32] to improve OoD detection performance.",,
Authors of [28] find that mixup trained networks are significantly better calibrated and are less prone to over-confident predictions on out-ofdistribution and random-noise data.,,
"Data augmentation (Thulasidasan et al., 2019; Hendrycks* et al., 2020; Cubuk et al., 2020; Chen et al., 2020a,b) represents a second class of approach that improves the representation learning of neural networks by explicitly injecting expert knowledge about the types of surface-form perturbations…",,
"[78] presents work on calibrating a model by training with Mixup, a data augmentation technique where new samples are generated by combining images and their associated labels.",,
Reference [5] focuses on Mixup’s effects of improving calibration and predictive uncertainty.,,
"calibration [5], robustness [6], [7] and generalization [6],",,
"lasidasan provides several methods for this, including one which modifies the training of a neural network for this robustness [8, 9].",,
"Most modern DNNs, when trained for classification in a supervised learning setting, are trained using one-hot encoding that have all the probability mass centered in one class; the training labels are thus zero-entropy signals that admit no uncertainty about the input [51].",,
"feature layout indicates that we can learn a more compact and disjoint decision boundary, which has been evaluated to be critical in machine learning applications such mixup [66,49] and uncertainty estimation in deep learning [15].",,
"DNNs trained with this technique are typically more generalizable and calibrated [39], whose predictions tend to be less overconfident.",,
", 2012), stochastic inputs in Mixup training (Zhang et al., 2018; Thulasidasan et al., 2019), stochastic computational processes in variational inference algorithms (Kingma and Welling, 2014; Salimans et al.",,
"…include the probabilistic computational unit built in fuzzy logic (Novák et al., 2012), stochastic inputs in Mixup training (Zhang et al., 2018; Thulasidasan et al., 2019), stochastic computational processes in variational inference algorithms (Kingma and Welling, 2014; Salimans et al., 2015),…",,
"For example, Thulasidasan et al. (2019) investigated the impact of mixup for model calibration of NLU but only explored in-domain settings with simple deep learning architecture such as CNNs. Kong et al. (2020) explored BERT calibration using mixup as a regularization component on in-domain and…",,
"While simple to implement, mixup has been shown to improve both predictive performance and model calibration, particularly on image classification tasks due to its regularization effect through data augmentation (Thulasidasan et al., 2019).",,
"• Mixup (Zhang et al., 2018; Thulasidasan et al., 2019): Mixup augments training data by linearly interpolating randomly selected training samples in the input space.",,
"Empirically, Mixup is helpful for robust representation learning, and it alleviates the overconfident problems and the failure of distribution shift settings as well as the in-distribution accuracy [20].",,
"For example, [15] shows that neural networks trained with mixup are significantly better calibrated and less prone to over-confident predictions on random noise data.",,
A previous study [15] shows that relatively small values of α ∈ [0.,,
"This is because that deep networks are likely to memorize some specific training statistics, resulting in overconfident prediction and poor generalization ability [4].",,
Many works have pointed out Softmax layer overconfidence as an open issue in the field of deep learning [20]–[23].,,
"Other approaches aim to calibrate models during training, using well-chosen metrics [39, 29] or through data augmentation [48].",,
"The DNNs trained with such samples are significantly better calibrated [38], i.",,
"It was shown that the mixup was beneficial to avoid overconfident predictions in several tasks such as image classification [19], [20], object detection [21], text classification [20] and semantic segmentation [22].",,
"MixUp was shown to have two main effects: (1) it diversifies the training images and thus enlarges the training distribution on the vicinity of each training sample [6] and (2) it improves the network calibration [25, 57], reducing the overconfidence in recent classes.",,
"This quantification has been observed to be inaccurate, and several methods have been developed to improve it (Platt, 1999; Guo et al., 2017; Szegedy et al., 2016; Zhang et al., 2020; Thulasidasan et al., 2020; Mukhoti et al., 2020; Thagaard et al., 2020), including Bayesian neural networks (Gal & Ghahramani, 2016; Wang et al.",,
"The first group smooths the target 0/1 labels in order to prevent output estimates from collapsing to 0/1 (Mukhoti et al., 2020; Szegedy et al., 2016; Zhang et al., 2018; Thulasidasan et al., 2020).",,
"…to be inaccurate, and several methods have been developed to improve it (Platt, 1999; Guo et al., 2017; Szegedy et al., 2016; Zhang et al., 2020; Thulasidasan et al., 2020; Mukhoti et al., 2020; Thagaard et al., 2020), including Bayesian neural networks (Gal & Ghahramani, 2016; Wang et al.,…",,
The calibration of a model can be measured by the expected calibration error (ECE) [17] and the overconfidence error (OE) [29].,,
"to be overconfident when trained with hard labels [52], [53].",,
"According to some recent machine learning studies, the uncertainty estimations of many widely used machine learning classifiers are not reliable [20; 12; 32; 36].",,
"mixup is proven to be effective on balanced dataset due to its improvement of calibration [58, 18], but it is unsatisfactory in LT scenarios (see in Tab.",,
"Classification calibration [18, 58] represents the predicted winning Softmax scores indicate the actual likelihood of a correct prediction.",,
[58] point out that the effectiveness of mixup in balanced datasets originates from superior calibration modification.,,
"[58] Sunil Thulasidasan, Gopinath Chennupati, Jeff A.",,
"However, the authors in [58] illustrate that such interpolation without labels is negative for classification calibration.",,
", the predicted confidence indicates actual accuracy likelihood [18, 58].",,
The confidence data is obtained by the average Softmax winning score in a test mini-batch [58].,,
"mixup [64] and its extensions [59, 63, 12] are effective feature improvement methods and contribute to a well-calibrated model in balanced datasets [58, 65], i.",,
"Enhancement [58], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209]",,
"data augmentation [195], ensembling with leaving-out strategy [196], adversarial training [197], [198], [199], [200], [214], stronger data augmentation [201], [202], [203], [204],",,
"…technique has been proved to work very well for augmenting images and texts, where Mixup has been successfully deployed to mix two unrelated images (Zhang et al. 2018a; Thulasidasan et al. 2019) or two semantically different words in sentences (Guo, Mao, and Zhang 2019a; Guo 2020).",,
"Nevertheless, such “counter-intuition” technique has been proved to work very well for augmenting images and texts, where Mixup has been successfully deployed to mix two unrelated images (Zhang et al. 2018a; Thulasidasan et al. 2019) or two semantically different words in sentences (Guo, Mao, and Zhang 2019a; Guo 2020).",,
Training with these linear interpolations and the soft labels yields a smooth decision boundary in the embedding space which has shown to improve the calibration of the uncertainty estimates obtained from the outputs [33].,,
"Indeed, they generally tend to be over-confident, particularly when employing softmax activation functions [Thulasidasan et al., 2019].",,
"The empirical advantages of Mixup training have been affirmed by several follow-up works (He et al., 2019; Thulasidasan et al., 2019; Lamb et al., 2019; Arazo et al., 2019; Guo, 2020).",,
"Mixup [1], which generates new examples by combining random data pairs and their labels, has shown promising performance on model generalization [3] and calibration [4].",,
"The method has been successfully used in many supervised learning and semi-supervised learning tasks [6, 10, 14, 16, 18, 26, 62, 69].",,
"Several works [6], [11], [16], [17], [18], [19], [20], [21] consider either algorithmic improvements or application specific challenges associated to uncertainty quantification.",,
"Further, it will be interesting to study whether NFM may also lead to better model calibration by extending the analysis of [66, 83].",,
"[66] Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.",,
"Since the pioneering work of [8] and [5], introduced to improve the generation accuracy, recently another family of methods include using soft labels during training for improving the uncertainty estimation of neural networks [6, 9, 27].",,
"tion [6, 9] which are employed during training.",,
"In this article, we identify that the over-confidence in deep radar classifiers, which emanates from using hard labels, can be fixed using soft labels [6, 7, 8, 9] and propose two novel heuristics to compute sample-specific smoothing factors to refine the hard labels.",,
"Despite its simplicity, it has been shown that optimizing fθ on mixed-up data leads to better generalization and improves model calibration [26].",,
"Unlike OOD generalization tasks, which primarily focus on performance under distribution shifts, the OOD detection community [211, 152, 98, 244] concentrates more on detecting samples from unseen distributions.",,
"As label mixing is a form of label smoothing [17], label mixup is beneficial to classification tasks and has achieved both better generalization and increased model calibration in classification applications across various data domains such as images, audio, text, and tabular data [16, 18].",,
"Similar to label mixup, the success of loss mixup is highly sensitive to the mixing parameter λ [19, 18, 20].",,
[32] proposed to use mix-up augmentation to improve the model calibration.,,
"In particular, label smoothing modifies the ground-truth labels by fusing them with a uniform distribution, essentially forcing neural networks to produce ‘more flattened’ probabilities; whereas Mixup is a data augmentation method that randomly mixes two instances at both the image and label space, with a byproduct effect of improving calibration.",,
"Regularization methods, such as label smoothing [36] and Mixup [37], have also been demonstrated effective in improving calibration.",,
"Baselines We compare our approach with nine baseline methods: MC-Dropout [7], Temperature Scaling [14], Mixup [37], Label Smoothing [36], TrustScore [20], JEM [12], DBLE [40], and OvA DM [34].",,
"These include: consistency regularization of self-supervised learning [19] and the MixUp data augmentation algorithm [20], both implemented in the MixMatch approach.",,
[278] regularized it by using a data-agnostic data augmentation technique named,,
"According to [278], the label smoothing resulting from mixup training can be viewed as a form of entropy-based regularization resulting in inherent calibration of networks trained",,
"binning based calibration measure [283], [15], [274], [275], [278], [47].",,
"1 [48] 2 [47] 3 [276] 4[15], [68], [274] 5 [31], [275] 6 [272], [209], [273], [274], [16] 8 [277] 9 [268], [270] 10 [278], [279], [234], [280] 11 [269], [266], [11], [271], [279]",,
", 2018), calibration, and predictive certainty (Thulasidasan et al., 2019).",,
"It is also reported in Zhang et al. (2018) that Mixup helps with stability, adversarial robustness (Zhang et al., 2018), calibration, and predictive certainty (Thulasidasan et al., 2019).",,
"Recently, representation learning has known major breakthroughs due to the advance in the field of contrastive learning [4, 5, 6, 12, 14, 23].",,
"Previous self-supervised contrastive learning methods [4, 6, 12, 14] indicate that the use of a batch normalization layer after the first fully connected layer has shown to generate more powerful representations.",,
"Techniques in this category include mixup training [Thulasidasan et al., 2019], pre-training [Hendrycks et al.",,
"To avoid possible misleading, maxl zl is referred to as winning score v (i.e., v = maxl zl) [Thulasidasan et al., 2019] hereinafter.",,
"Techniques in this category include mixup training [Thulasidasan et al., 2019], pre-training [Hendrycks et al., 2019a], label-smoothing [Müller et al., 2019], data augmentation [Ashukha et al., 2020], self-supervised learning [Hendrycks et al., 2019b], Bayesian approximation [Gal and Ghahramani,…",,
"Enforcing this strict linearity is expected to guide a model to behave linearly in between the data instances and to be robust to adversarial examples [29, 30].",,
"5SYNTHetic collection of Imagery and Annotations
Meta learning in the graceful degradation problem is most applicable to the quick adaptation of models upon encountering data outside of their training distribution, this might be fast video segmentation of new object as in Thulasidasan et al. (2019).",,
"However, their method is designed for data augmentation to enhance in-distribution performance and requires corresponding combinations in the label space (Thulasidasan et al., 2019).",,
"…modern neural networks to be poorly calibrated, as suggested previously (Guo et al., 2017; Lakshminarayanan et al., 2017; Malinin & Gales, 2018; Thulasidasan et al., 2019; Hendrycks et al., 2020b; Ovadia et al., 2019; Wenzel et al., 2020; Havasi et al., 2021; Rahaman & Thiery, 2020; Leathart &…",,
"This suggests that there may be no continuing trend for highly accurate modern neural networks to be poorly calibrated, as suggested previously (Guo et al., 2017; Lakshminarayanan et al., 2017; Malinin & Gales, 2018; Thulasidasan et al., 2019; Hendrycks et al., 2020b; Ovadia et al., 2019; Wenzel et al., 2020; Havasi et al., 2021; Rahaman & Thiery, 2020; Leathart & Polaczuk, 2020).",,
"Many strategies have been proposed to improve model calibration such as post-hoc rescaling of predictions (Guo et al., 2017), averaging multiple predictions (Lakshminarayanan et al., 2017; Wen et al., 2020), and data augmentation (Thulasidasan et al., 2019; Wen et al., 2021).",,
"…networks can be surprisingly poor, despite the advances in accuracy (e.g. Guo et al. 2017; Lakshminarayanan et al. 2017; Malinin & Gales 2018; Thulasidasan et al. 2019; Hendrycks et al. 2020b; Ovadia et al. 2019; Wenzel et al. 2020; Havasi et al. 2021; Rahaman & Thiery 2020; Leathart &…",,
"Other works have corroborated some of these findings (e.g., Thulasidasan et al. 2019; Wen et al. 2021).",,
"In fact, over the last few years, there have been many reports that calibration of modern neural networks can be surprisingly poor, despite the advances in accuracy (e.g. Guo et al. 2017; Lakshminarayanan et al. 2017; Malinin & Gales 2018; Thulasidasan et al. 2019; Hendrycks et al. 2020b; Ovadia et al. 2019; Wenzel et al. 2020; Havasi et al. 2021; Rahaman & Thiery 2020; Leathart & Polaczuk 2020).",,
"TREC is a commonly used dataset to evaluate mixup methods in sentence classification (Guo et al., 2019; Thulasidasan et al., 2019).",,
has shown to improve classifier’s calibration and reduced prediction uncertainity in [73].,,
"This proves [73, 46] the better generalization and",,
Training with MixUp data augmentation [281] is also found to beneit model calibration [225].,,
The work of [40] and [4] both directly evaluated Mixup’s effect on OOD detection.,,
"Other uncertainty estimation methods such as [18, 34, 10] can also be used to estimate the uncertainty in conjunction with our proposed method.",,
"Mixup has also been widely used in other learning tasks such as semi-supervised learning [16], neural network calibration [17], and adversarial defense [18].",,
"through data augmentation, better calibrated DNNs (Mixup [23]) or alleviate the over-confidence of the model (CutMix [24]).",,
"• Mixup As shown in Thulasidasan et al. [2019], Mixup can be an effective OoD detector, so we also use this as one of our baselines.",,
[300] discovered that mix-up training [296],,
"Sampling-free uncertainty estimation [21] and data augmentation [22], [23] are candidates.",,
"Neural network prediction scores are well known to not be well calibrated, a subject of recent investigation by Thulasidasan et al. (2019).",,
"Several studies have been conducted on confidence calibration (Guo et al. 2017; Zhang, Dalca, and Sabuncu 2019; Thulasidasan et al. 2019; Wan et al. 2018; Kull et al. 2019).",,
"Starting with this paper, several confidence calibration methods are proposed (Zhang, Dalca, and Sabuncu 2019; Thulasidasan et al. 2019; Wan et al. 2018; Kull et al. 2019).",,
"Uncertainty calibration performance is measured using the root-mean-square calibration error (RMSE), Expected Calibration Error (ECE) [43], [44], and Overconfidence Error (OE) [44].",,
"DNNs tend to predict over-confidently in classification tasks [47], mixup methods can significantly alleviate this problem.",,
We used object recognition datasets (CIFAR-10 and CIFAR-100 [8]) and a fashion-product recognition dataset (Fashion MNIST [21]) as in the previous study [17].,,
"As reported in [17], Mixup substantially reduces the ECE compared with the baseline method, but its ECE still increases to some extent when the training dataset becomes small-scale.",,
"MixConf basically follows the scheme of Mixup, which is known to contribute to model’s calibration [17], but is more carefully designed for confidence calibration.",,
"Subsequent work by (Thulasidasan et al., 2019) showed that mixup training and the soft decision boundary has the effect of improving network calibration.",,
Thulasidasan et al. (2019) show that MixUp can improve calibration for CNN based models.,,
"Mixup has successfully been used as a form of data augmentation in image classification, improving generalization and calibration [32, 26].",,
"In standard training (ST), mixup has been widely used to improve the generalization (Zhang et al., 2018; Thulasidasan et al., 2019; Berthelot et al., 2019; 2020; Kim et al., 2021; Zhang et al., 2021b).",,
", 2018), and its predictions tend to be overconfident (Malkin and Bilmes, 2009; Thulasidasan et al., 2019).",,
"Large-scale pretrained models are not well calibrated (Jiang et al., 2018), and its predictions tend to be overconfident (Malkin and Bilmes, 2009; Thulasidasan et al., 2019).",,
"When trained naively with cross entropy loss on unambiguously annotated data (examples with a single label), models generate a over-confident distribution (Thulasidasan et al., 2019) putting a strong weight on a single label.",,
"When
trained naively with cross entropy loss on unambiguously annotated data (examples with a single label), models generate a over-confident distribution (Thulasidasan et al., 2019) putting a strong weight on a single label.",,
"Similar approaches where the proxies or auxiliary data is created using tools such as generative adversarial networks are reported in [22, 14, 27, 31].",,
"Since the output prediction with mixup augmentation is better calibrated (Thulasidasan et al., 2019) we use relaxed thresholds for τp (0.50) and κp (0.10).",,
"The literature on general confidence scoring is rich and continually evolving; the most interesting research avenues involve Bayesian averaging [26], generative models [27, 28], input perturbations [29, 30], exploiting inner activations [31, 32].",,
Mixup can be regarded as a kind of data augmentation method and it often enhances the generalization performance of CNNs. Mixup can also ease the over-confident prediction problem for deep neural networks (Thulasidasan et al. 2019).,,
"Therefore, mixup involves the representation space
unseen during normal training, and raises the generalization of deep neural networks significantly, especially on small datasets (Thulasidasan et al. 2019).",,
"However, α greater than 1 tends to perform better in some cases (Thulasidasan et al. 2019).",,
Mixup can also ease the over-confident prediction problem for deep neural networks (Thulasidasan et al. 2019).,,
"Therefore, mixup involves the representation space unseen during normal training, and raises the generalization of deep neural networks significantly, especially on small datasets (Thulasidasan et al. 2019).",,
"Research in computer vision has shown that improving model calibration improves adversarial robustness as well as out-of-distribution detection (Hendrycks and Gimpel 2017; Thulasidasan et al. 2019; Hendrycks, Lee, and Mazeika 2019).",,
"In [30], for instance, has been measured the robustness and calibration of Mixup training [31] showing improved results over a baseline model.",,
", mixup[9, 14], label smoothing [5], multiscale [2], various data augmentations [6], can bring accuracy improvements.",,
Mixup (Zhang et al. 2018) and its variants (Verma et al. 2018; Thulasidasan et al. 2019) are a recently proposed approach to improve model robustness and generalization by training a model on convex combinations of data sample pairs and their labels.,,
2018) and its variants (Verma et al. 2018; Thulasidasan et al. 2019) are a recently proposed approach to improve model robustness and generalization by training a model on convex combinations of data sample pairs and their labels.,,
Data augmentation methods include Mixup [19] and AugMix [21].,,
[39] discovered that mixuptraining [52] with label smoothing can significantly improve model calibration.,,
"This is because the commonly used cross-entropy loss is known to be highly overconfident [27, 40].",,
"As the commonly used cross-entropy loss is known to be highly overconfident [27, 40], LossNet tends to produce polarized results, and high weights could be assigned to some noisy data.",,
It is increasingly being recognized that decision-making systems must be both accurate and calibrated (Wallace and Dahabreh 2012; Guo et al. 2017; Thulasidasan et al. 2019; Huang et al. 2020).,,
It is increasingly being recognized that decision-making systems must be both accurate and calibrated (Wallace and Dahabreh 2012; Guo et al. 2017; Thulasidasan et al. 2019; Huang et al. 2020).,,
"Furthermore, [39] notices that label smoothing during mixup training has a calibration effect which regularizes over-confident predictions.",,
"Furthermore, [38] notices that label smoothing during mixup training has a calibration effect which regularizes over-confident predictions.",,
[236] investigated the predictive uncertainty and calibration of models trained with,,
"Different from other approaches that aim to retrain a model on the augmented training set (Patel et al., 2019; Thulasidasan et al., 2019), our proposed algorithm does not change the predictions and thus retains the original prediction accuracy while adjusting
the confidence of the predictions.",,
"Several manifold-based confidence calibration have been proposed (Bahat and Shakhnarovich, 2020; Thulasidasan et al., 2019; Patel et al., 2019; Lee et al., 2017; Verma et al., 2019).",,
"Bahat and Shakhnarovich (2020) augments test data using transformations to calibrate confidence, while Thulasidasan et al. (2019) and Patel et al. (2019) augment data by interpolating existing data and using an auto-encoder based model, respectively.",,
"Different from other approaches that aim to retrain a model on the augmented training set (Patel et al., 2019; Thulasidasan et al., 2019), our proposed algorithm does not change the predictions and thus retains the original prediction accuracy while adjusting the confidence of the predictions.",,
"To address this issue, techniques that utilize redundancy in the example space have been proposed (Bahat and Shakhnarovich, 2020; Thulasidasan et al., 2019; Patel et al., 2019).",,
"Most of these techniques augment the training dataset with examples on the same manifold and re-train a model on the augmented dataset (Thulasidasan et al., 2019; Patel et al., 2019).",,
"…overconfidence is caused by the training samples with same winning scores due to the one-hot labels, and adding noise perturbation in the training process is a way to mitigate aleatoric uncertainty, we apply mix-up (Zhang et al., 2017; Thulasidasan et al., 2019) to jointly address the two issues.",,
", 2019), the existing metrics directly or indirectly depend on winning score, which is the maximum probability in a semantic vector (softmax vector from the last layer of a DNN model) (Thulasidasan et al., 2019).",,
"…et al., 2018; Wang et al., 2019; Shen et al., 2019; Xiao and Wang, 2019; Kumar et al., 2019), the existing metrics directly or indirectly depend on winning score, which is the maximum probability in a semantic vector (softmax vector from the last layer of a DNN model) (Thulasidasan et al., 2019).",,
"Since the overconfidence is caused by the training samples with same winning scores due to the one-hot labels, and adding noise perturbation in the training process is a way to mitigate aleatoric uncertainty, we apply mix-up (Zhang et al., 2017; Thulasidasan et al., 2019) to jointly address the two issues.",,
"Besides, Overconfidence Error is proposed by applying winning score as confidence and penalizing samples with confidence values greater than accuracy values (Thulasidasan et al., 2019).",,
[46] showed that mixed up training can improve calibration and predictive uncertainty of models.,,
"…1, 2.5, 5}; for VAT, we search the perturbation size in {10−3, 10−4, 10−5} as in (Jiang et al., 2020); for Mixup, we search the interpolation parameter from {0.1, 0.2, 0.3, 0.4} as suggested in (Zhang et al., 2018; Thulasidasan et al., 2019); for Manifold-mixup, we search from {0.2, 0.4, 1, 2, 4}.",,
"•Mixup (Zhang et al., 2018; Thulasidasan et al., 2019) augments training data by linearly interpolating training samples in the input space.",,
"…Adversarial Training (VAT) (Miyato et al., 2018) introduces a smoothness-inducing adversarial regularizer to encourage the local Lipschitz continuity of DNNs. •Mixup (Zhang et al., 2018; Thulasidasan et al., 2019) augments training data by linearly interpolating training samples in the input space.",,
[46] demonstrated the robustness of deep models with the use of mixup.,,
"However, the same idea can be used on metadata that we’d like to balance uncertainty estimates, e.g., gender and age groups.
et al. (2020) report 1.7% ECE with Rank-1 Bayesian neural nets and 3.0% with Deep Ensembles; Thulasidasan et al. (2019a) report 3.2% for ResNet-50 with Mixup, 2.9% for ResNet-50 with an entropy-regularized loss, and 1.8% for ResNet-50 with label smoothing.",,
"It has been a key factor driving state-of-the-art: for example, Mixup (Zhang et al., 2018; Thulasidasan et al., 2019a), AugMix (Hendrycks et al., 2020), and test-time data augmentation (Ashukha et al., 2020).",,
"This is counterintuitive as we would expect Mixup, which improves calibration of individual models (Thulasidasan et al., 2019a), to also improve the calibration of their ensemble.",,
"…uncertainty estimates, e.g., gender and age groups.
et al. (2020) report 1.7% ECE with Rank-1 Bayesian neural nets and 3.0% with Deep Ensembles; Thulasidasan et al. (2019a) report 3.2% for ResNet-50 with Mixup, 2.9% for ResNet-50 with an entropy-regularized loss, and 1.8% for ResNet-50 with…",,
"Ensembles are the among the most known and simple approaches to improving calibration (Ovadia et al., 2019; Lakshminarayanan et al., 2017), and Thulasidasan et al. (2019b) showed that Mixup improves calibration in a single network.",,
"Mixup was shown to be effective for generalization and calibration of deep neural networks (Zhang et al., 2018; Thulasidasan et al., 2019b).",,
", 2019) and confidence calibration (Thulasidasan et al., 2019).",,
"Not only improving the generalization on the supervised task, it also improves adversarial robustness (Zhang et al., 2018; Pang et al., 2019) and confidence calibration (Thulasidasan et al., 2019).",,
"In this paper, inspired by the success of mix-up training [34] in recent studies [29, 30, 32], we propose Mix-up Contrast (MixCo) as an extension of contrastive learning approach.",,
"Mix-up training consistently improved the performance in various studies [30, 32].",,
"It has been empirically shown to substantially improve test performance and robustness to adversarial noise of state-of-the-art neural network architectures (Zhang et al., 2018; Lamb et al., 2019; Thulasidasan et al., 2019; Zhang et al., 2018; Arazo et al., 2019).",,
"In addition, the interpolation technique has also been used in semi-supervised learning (Berthelot et al. 2019) and also used to improve the robustness (Li, Socher, and Hoi 2020), uncertainty (Hendrycks et al. 2019), and calibration (Thulasidasan et al. 2019) of DNN classifiers.",,
"2019), and calibration (Thulasidasan et al. 2019) of DNN classifiers.",,
"hnique has also been used in semi-supervised learning (Berthelot et al. 2019) and also used to improve the robustness (Li, Socher, and Hoi 2020), uncertainty (Hendrycks et al. 2019), and calibration (Thulasidasan et al. 2019) of DNN classiﬁers. Our Framework From KD to L2RKD Hinton et al. (Hinton, Vinyals, and Dean 2015) propose KD which minimizes the output probability differences between a student and a teacher over dat",,
[30] show that networks trained with mixup are better calibrated.,,
[30] found that CNNs trained with mixup are better calibrated.,,
"Also, since the ground-truth labels are represented as one-hot-coded vectors, the cross-entropy loss has the tendency to make the model overconfident since it only focuses on the predicted probability corresponding to the ground-truth label (Thulasidasan et al. 2019).",,
"Following Guo et al. (2017) and Thulasidasan et al. (2019), softmax predictions are grouped into M interval bins of equal size.",,
"Following the calibration metrics in Guo et al. (2017) and Thulasidasan et al. (2019), we evaluate the calibration of the model in Figure 4.",,
"Mixup can improve model accuracy [29], model calibration [26], and model robustness to certain types of image corruptions [5].",,
"The effect of this training is a substantial improvement in model calibration and accuracy on large-scale image classification tasks [30,26,5].",,
"While this process is intuitively obvious to humans, neural networks behave abnormally in many cases, making overconfident mistakes when encountering confusing or unknown inputs [3], [5].",,
[23] demonstrate that Mixup is also useful for neural network calibration.,,
"The trainable methods are not compared in this work because the literature shows that they have a worse or similar calibration performance with temperature scaling [15, 21, 23].",,
"In this section, we introduce some existing calibration methods, including temperature scaling [19, 16], entropy regularization [14], MMCE regularization [15], label smoothing [20, 21], and Mixup training [22, 23].",,
"Some other works explored within-training strategies that can provide high-quality model uncertainty, such as label smoothing [21], dropout [6], mixup [33], Bayesian models [16], etc.",,
"Following the uncertainty calibration approaches [7,33], we also investigate the relationship between statistical metrics (e.",,
[33] proves its strong uncertainty calibration capability beyond its label smoothing effects.,,
"As the model is trained on noisy web labels, we employ mixup [39], which is known as an effective regularization to make DNNs less prone to over-confident predictions and predicted scores of DNNs better calibrated to the actual confidence of a correct prediction [33].",,
"Since the original classification network could easily obtain high confidence, in which the generated CAM only attends to small discriminative object parts, we utilize mixup data augmentation to calibrate the uncertainty in prediction [38].",,
"First, inspired by the mixup data augmentation in [49], we observe that including mixup could effectively calibrate the model uncertainty on overconfident predictions [38] and in return enables the model to attend to more object regions.",,
"In this paper, we propose to integrate the idea of mixup data augmentation [49], thereby calibrating the uncertainty in prediction [38] as well as allowing the model to attend to other regions of the image.",,
"Numerous Mixup variants [3, 14, 37, 38, 40, 47] have been proposed to extend mixup for better prediction of uncertainty and calibration of the DNNs.",,
"Additionally, mixup has the property of curbing confirmation bias by enforcing label smoothness by combining yi and yj as noted by [29].",,
It has been shown that models trained with Mixup is robust toward out-of-distribution data [10] and is beneficial for the uncertainty calibration of a network [28].,,
"…have been carried out for improving uncertainty calibration on modern neural networks on tasks such as image classification (Guo et al., 2017; Thulasidasan et al., 2019) and anomaly detection (Snoek et al., 2019), the relationship between uncertainty calibration and efficiency of active…",,
"Uncertainty calibration in AL Even though a large number of studies have been carried out for improving uncertainty calibration on modern neural networks on tasks such as image classification (Guo et al., 2017; Thulasidasan et al., 2019) and anomaly detection (Snoek et al.",,
"Based on the suggestions and findings in other papers [23,22], for our experiments we set α = 0.",,
"Apart from improving the classification performance on various image classification benchmarks [23], Mixup also leads to better calibrated deep-learning models [22].",,
"Mixup also shed lights upon other learning tasks such as semi-supervised learning [33,1], adversarial defense [31] and neural network calibration [30].",,
• Number of epochs : 30 • Loss function : Binary Cross Entropy • Mixup beta distribution [17] : α = β = 0.,,
"In addition, the mixup method [17] was used.",,
A Mixup data augmentation [23] using an alpha of 0.,,
", 2018) and recently found to be able to improve calibration in (Thulasidasan et al., 2019), (6) Confidence-calibrated adversarial training (CCAT) (Stutz et al.",,
"…mixup, which is a data augmentation technique originally proposed in (Zhang et al., 2018) and recently found to be able to improve calibration in (Thulasidasan et al., 2019), (6) Confidence-calibrated adversarial training (CCAT) (Stutz et al., 2020), a method builds on adversarial training by…",,
Label smoothing Label smoothing is originally proposed in Szegedy et al. (2016) and is shown to be effective in improving the quality of uncertainty estimates in Müller et al. (2019); Thulasidasan et al. (2019).,,
"Recently, mixup training (Zhang et al., 2018) has been shown to improve both models’ generalization and calibration (Thulasidasan et al., 2019), by preventing the model from being over-confident in its predictions.",,
"To find the best hyperparameter for label smoothing, previous methods (Szegedy et al., 2016; Thulasidasan et al., 2019) sweep in a range and choose the one that has the best validation",,
"Note that AR-AdaLS is only trained on the clean training data without any data augmentation compared to mixup (Thulasidasan et al., 2019) and CCAT (Stutz et al., 2020).",,
"Note that AR-AdaLS is only trained on the clean training data without any data augmentation compared to mixup (Thulasidasan et al., 2019) and CCAT (Stutz et al.",,
"To find the best hyperparameter for label smoothing, previous methods (Szegedy et al., 2016; Thulasidasan et al., 2019) sweep in a range and choose the one that has the best validation
1Note, predicted confidence is not a good indicator for splitting the training dataset as the model can easily…",,
"For example, models are often miscalibrated where the predicted confidence is not indicative of the true likelihood of the model being correct (Guo et al., 2017; Thulasidasan et al., 2019; Lakshminarayanan et al., 2017; Wen et al., 2020; Kull et al., 2019).",,
", 2018) has been shown to improve both models’ generalization and calibration (Thulasidasan et al., 2019), by preventing the model from being over-confident in its predictions.",,
"Recently, mixup training [37] has been shown to improve both models’ generalization and calibration [32], by preventing the model from being over-confident in its predictions.",,
"To find the best hyperparameter for label smoothing, previous methods [30, 32] sweep in a range and choose the one that has the best validation performance.",,
"Label Smoothing Label smoothing is originally proposed in [30] and is shown to be effective in improving the quality of uncertainty estimates in [20, 32].",,
"For example, models are often miscalibrated where the predicted confidence is not indicative of the true likelihood of the model being correct [8, 32, 16, 33, 15].",,
"DNNs are not only overconfident on the data they are trained on but also on unseen out-of-distribution data [9,25].",,
"We compare the four variants of our proposed method with L1 distance, L2 distance, Autoencoder distance (AE) and Word Embedding distance (WE) to the vanilla training using one-hot labels, as well as various techniques that improve confidence calibration: temperature scaling (TS) [7], uniform label smoothing [24,18], mixup training [25], Dirichlet calibration with off-diagonal regularization (Dir-ODIR) [10], and ensemble temperature scaling (ETS) [31].",,
"Non-post-hoc methods are mostly based on adapting the training procedure, including modifying the training loss [17,27], label smoothing [24,18], and data augmentation [25,29].",,
"Since the discovery of this challenging problem, several methods [31,18,27,25] have been explored and empirically shown to improve confidence calibration performance on the predictions, which we refer to as prediction calibration, for which only the model’s prediction (the winning class) and its associated confidence (the maximum softmax score) are considered.",,
Mixup [29] trains a classifier not only on the training data but also on linear interpolations of random pairs of samples and their labels.,,
"We compare our proposed method to the vanilla training using one-hot labels, as well as three other techniques that improve confidence calibration: temperature scaling [10], uniform label smoothing [28, 20], and mixup training [29].",,
"Studies have shown that DNNs are not only overconfident on the data they are trained on but also on unseen out-of distribution data [12, 29].",,
"…et al., 2018), scalable Gaussian processes (Milios et al., 2018), sampling-free uncertainty estimation (Postels et al., 2019), data augmentation (Patel et al., 2019; Thulasidasan et al., 2019; Yun et al., 2019; Hendrycks et al., 2020) and ensemble distribution distillation (Malinin et al., 2020).",,
", 2018) which is a data augmentation shown to improve calibration (Thulasidasan et al., 2019).",,
", 2017); and 3) using Mixup data augmentation (Zhang et al., 2018; Thulasidasan et al., 2019).",,
"A WRN CIFAR100 classifier is trained in three modes: 1) no during-training calibration; 2) using entropy regularization (Pereyra et al., 2017); and 3) using Mixup data augmentation (Zhang et al., 2018; Thulasidasan et al., 2019).",,
"Additionally, we adopt Mixup (Zhang et al., 2018) which is a data augmentation shown to improve calibration (Thulasidasan et al., 2019).",,
", 2019), data augmentation (Patel et al., 2019; Thulasidasan et al., 2019; Yun et al., 2019; Hendrycks et al., 2020) and ensemble distribution distillation (Malinin et al.",,
"Among the former, the main idea is to increase the entropy of the classifier to avoid overconfident predictions, which is accomplished via modifying the training loss [12, 15, 23], label smoothing [16, 21], and data augmentation techniques [25, 28, 32].",,
Mixup is a data augmentation procedure proposed by [23] that is known to improve calibration ([19]) when applied to standard models.,,
"To this end, the works in computer vision tasks with well-established benchmark studies(Thulasidasan et al., 2019; Snoek et al., 2019) has shown that Bayesian learning is beneficial for better generalization to OOD and corrupted samples.",,
"Recent works [3, 18, 19] discover the hidden gems of label smoothing [20], mixup [21], and adversarial training [22] on improving the calibration performance and the uncertainty representation ability.",,
This interaction between label smoothing (due to output Mixup) and Jacobian regularization (due to input Mixup) may explain why Mixup on inputs only performs poorly compared to Mixup on both inputs and outputs [31].,,
"These include improved calibration [31], robustness to input adversarial noise [36], and robustness to label corruption [36].",,
"Another promising approach could be using recent data augmentation techniques [66], [67] or strategies based on pretrained models [42].",,
"By using an implicit bias that linear interpolations of data should lead to predictions that are linearly interpolated in the target space, Mix Up enables generation of well-calibrated models whose generalization performance is slightly better (Thulasidasan et al., 2019).",,
"…to Data Augmentation techniques, there have been attempts to explain the latent effect of data augmentation using mathematical formulations, such as in (Chen et al., 2019; Thulasidasan et al., 2019; He et al., 2019), and for Mixed Sample Data Augmentation Techniques in (Harris et al., 2020).",,
"Specific to Data Augmentation techniques, there have been attempts to explain the latent effect of data augmentation using mathematical formulations, such as in (Chen et al., 2019; Thulasidasan et al., 2019; He et al., 2019), and for Mixed Sample Data Augmentation Techniques in (Harris et al.",,
"We also investigated the calibration of our scNym models by comparing the prediction confidence scores to prediction accuracy (Thulasidasan et al., 2019).",,
"data augmentation techniques [25] and regularization strategies [21] to more sophisticated methods that quantify the epistemic (or model) uncertainties and aleatoric (or data) uncertainties for calibrating model confidences [7,8,12,22].",,
"But fine-grained testing does not work well when comparing mixup methods and non-mixup methods, since mixup is better class-calibrated [41].",,
This overconfidence can occur even with randomly labeled training data as deep networks are likely to just memorize the training statistics [36].,,
"Many efforts have been made to understand the generalization performance of deep learning [43, 10, 23, 2, 38, 21, 36, 45].",,
"Why Auxiliary Classifiers? Several works have found that deep networks are prone to over-confident predictions, and this hinders a network from learning generalization [23, 45, 36].",,
"classification predictions about an input, thus causing loss in the generalization performance [23, 45, 20, 36].",,
"56]. The basic idea is to generate new training data-label pairs by convex combinations of training samples. Several studies demonstrated it’s benet for various tasks such as calibrating uncertainty [42] and domain adaptation for images [23,51,54]. 3 Approach In this section, we present the main building blocks of our approach. We rst describe our general pipeline and training procedure, and then exp",,
"62 average ECE) than in [30], while they report a 2.",,
"Motivated by the fundamentals and good performance of Mixup, a very recent work [30] has studied how Mixup affects the uncertainty quantification and the calibration performance on DNN.",,
"In the experimental section, we show that some models trained with Mixup do not necessarily improve the calibration, as recently noted in [30].",,
"Finally, on the side of DA strategies, [30] measure the robustness and calibration of Mixup training and [24] propose On-Manifold Adversarial Data Augmentation, which attempts to generate challenging examples by following an on-manifold adversarial attack path in the latent space of a generative model.",,
"By comparing with the results reported in [30], we can conclude that Mixup behaves particularly well in CIFAR100, probably because the intersection between classes can be explained through a linear relation.",,
"In general, our results contrast with those reported in [30] where they provide general improvement in calibration performance due to Mixup.",,
"62 average ECE) than in [30], while they report a 2.",,
"Motivated by the fundamentals and good performance of Mixup, a very recent work [30] has studied how Mixup affects the uncertainty quantification and the calibration performance on DNN.",,
"In the experimental section, we show that some models trained with Mixup do not necessarily improve the calibration, as recently noted in [30].",,
"Finally, on the side of DA strategies, [30] measure the robustness and calibration of Mixup training and [24] propose On-Manifold Adversarial Data Augmentation, which attempts to generate challenging examples by following an on-manifold adversarial attack path in the latent space of a generative model.",,
"By comparing with the results reported in [30], we can conclude that Mixup behaves particularly well in CIFAR100, probably because the intersection between classes can be explained through a linear relation.",,
"In general, our results contrast with those reported in [30] where they provide general improvement in calibration performance due to Mixup.",,
"Data augmentation methods [30, 34] overcome overfitting by enriching the training data with new artificially generated pseudo data points and labels.",,
"Here we follow the calibration literature [30, 7, 14] and use the negative log likelihood (NLL) loss, i.",,
"Calibration: A recent study (Thulasidasan et al., 2019) showed that DNNs trained with Mixup are significantly better calibrated than DNNs trained in a regular fashion.",,
"We measure the Expected Calibration Error (ECE) (Thulasidasan et al., 2019; Guo et al., 2017) of the proposed method, following (Thulasidasan et al.",,
"Although still in its early phase, the above efforts (Zhang et al., 2017b; Verma et al., 2019; Pang* et al., 2020; Thulasidasan et al., 2019) also indicate a trend to view Mixup from perspectives of robustness and calibration.",,
", 2017) of the proposed method, following (Thulasidasan et al., 2019): predictions (total N predictions) are grouped into M interval bins (Bm) of equal size.",,
"We measure the Expected Calibration Error (ECE) [26, 8] of the proposed method, following [26].",,
", 2020) and (Thulasidasan et al., 2019), in fact, have inferences that motivate the need to consider a latent Mixup space to address a model’s robustness and predictive uncertainty.",,
"Other efforts related to Mixup (Thulasidasan et al., 2019) have shown that Mixup-trained networks are better calibrated i.",,
Calibration error: A recent study [42] showed that DNNs trained with Mixup are significantly better calibrated than DNNs trained in a regular fashion.,,
"We measure the Expected Calibration Error(ECE) [42,16] of our trained networks, following [42]: predictions (total N predictions) are grouped into M interval bins (Bm) of equal size.",,
Other efforts on Mixup [42] have shown that Mixup-trained networks are significantly better calibrated than ones trained in the regular fashion.,,
"[14, 15] and Label Smoothing [16, 17] that were part of high performance deep networks for classification were later shown empirically to achieve calibration.",,
"Several works [10, 19, 22, 6, 29, 17, 13, 25, 18] aim to address this calibration challenge.",,
"To do this, we use ResNet-110 and Wide-ResNet-26-10 trained on CIFAR-10 and consider the SVHN [23] test set and CIFAR-10-C [9] with Gaussian noise corruption at severity 5 as OoD data.",,
"Furthermore, we empirically observe that models trained using focal loss are not only better calibrated under i.i.d. assumptions, but can also be better at detecting OoD samples which we show by taking CIFAR-10 as the in-distribution dataset and SVHN and CIFAR-10-C as out-of-distribution datasets, something which temperature scaling fails to achieve.",,
"More advantages of focal loss: Behaviour on Out-of-Distribution (OoD) data: A perfectly calibrated model should have low confidence whenever it misclassifies, including when it encounters data which is OoD [34].",,
"Finally, we also make the interesting observation that whilst temperature scaling may not work for detecting out-ofdistribution (OoD) samples, our approach can.",,
"Since focal loss has implicit regularisation effects on the network (see §4), we investigate if it helps to learn representations that are more robust to OoD data.",,
"While many approaches aim to address this network overconfidence problem (Blundell et al. 2015; Gal and Ghahramani 2016; Lee et al. 2018; Thulasidasan et al. 2019), BAYES-TREX is complementary to these efforts.",,
"While many approaches aim to address this neural network overconfidence problem (e.g. Thulasidasan et al., 2019; Lee et al., 2017; Gal & Ghahramani, 2016; Blundell et al., 2015), our work is complementary to these efforts.",,
", 2018) and its variants (Thulasidasan et al., 2019; Verma et al., 2018), and Label smoothing (Shafahi et al.",,
", 2017) or by intelligently varying the inputs (Thulasidasan et al., 2019).",,
"However, deep networks become overconfident from overfitting, which can be partially addressed by the usage of normalization and weight decay (Guo et al., 2017) or by intelligently varying the inputs (Thulasidasan et al., 2019).",,
This is a surprising result since it has been empirically showed in existing works [22] that augmentation strategies such as mixup often produce highly,,
"Recently, in [22], it was found that mixup regularization led to improved calibration in the resulting model.",,
"Surprisingly, state-of-the-methods such as mixup, which produce highly calibrated models [22] (in 2(a), mixup achieves the lowest ECE at 0% compression), do not always produce tickets that are inherently well-calibrated.",,
Figure 1 displays this phenomenon which has been replicated following the details provided by [7].,,
In this work we aim to replicate the results reported by [7] on their analysis of the effect of Mixup [5] on a network’s calibration.,,
"Overfitting to training data with one-hot encoded or hard labels [18], and over-confidence of ReLU networks for out-of-data inputs [5] have been identified as potential root causes for this behavior.",,
"In [18] it was shown that Mixup not only improves generalization, but also yields well-",,
"Mixup has been shown to yield several benefits, such as reducing overfitting and better calibrating the confidence of deep learning models [11, 19].",,
", 2017), which performs augmentation by taking a weighted average of two input images, was proposed and shown to improve model performance (Liang et al., 2018; Thulasidasan et al., 2019).",,
"More recently, Mixup (Zhang et al., 2017), which performs augmentation by taking a weighted average of two input images, was proposed and shown to improve model performance (Liang et al., 2018; Thulasidasan et al., 2019).",,
"Finally, intrinsically uncertainty-aware neural networks is a very active research field and independent and concurrent studies include new takes on Bayesian neural networks (Joo, Chung, and Seo 2020; Chan et al. 2020) and an extension of MixUp (Zhang, Kailkhura, and Han 2020).",,
"We also compare the additional baselines verified uncertainty calibration (VUC) (Kumar, Liang, and Ma 2019), and MixUp (Thulasidasan et al. 2019); to illustrate that the different modeling assumptions of OOD detection methods do not translate into calibrated predicted uncertainty under domain drift, we also jointly trained a classifier and a GAN (Lee et al. 2017).",,
"Considering perturbations at maximum strength (epsilon 90), FALCON is the only model yielding uncertainty-aware confidence scores at a median of less than 0.5 (for VUC and MixUp see Appendix).",,
"Our findings for baseline methods confirm their results, in particular for deep ensembles and SVI (they did not consider EDL, MNF, VUC and MixUp).",,
"More recently, Thulasidasan et al. (2019) have shown that using MixUp training, where label- and input smoothing is performed, yields good results for in-domain calibration.",,
"We also compare the additional baselines verified uncertainty calibration (VUC) (Kumar, Liang, and Ma 2019), and MixUp (Thulasidasan et al. 2019); to illustrate that the different modeling assumptions of OOD detection methods do not translate into calibrated predicted uncertainty under domain…",,
"We also compare the additional baselines verified uncertainty calibration (VUC) (Kumar, Liang, and Ma 2019), and MixUp (Thulasidasan et al. 2019); to illustrate that the different modeling assumptions of OOD detection methods do not translate into calibrated predicted uncertainty under domain drift, we also jointly trained a classifier and a GAN (Lee et al.",,
", 2018) and its variants (Verma et al., 2018; Thulasidasan et al., 2019) are a recently proposed approach to improve model robustness and generalization by training a model on convex combinations of data sample pairs and their labels.",,
"Mixup (Zhang et al., 2018) and its variants (Verma et al., 2018; Thulasidasan et al., 2019) are a recently proposed approach to improve model robustness and generalization by training a model on convex combinations of data sample pairs and their labels.",,
"Another promising approach could be using recent data augmentation techniques [59], [60] or strategies based on pretrained models [37].",,
"uts to decide works even better when many classes are presented. We speculate that recent advances in data augmentation techniques may help to improve IsoMax+ES OOD detection performance even further [52], [53]. 4.3 Robustness Analysis Fig. 3 presents OOD detection performance of SoftMax and IsoMax losses in many models (DenseNet and ResNet), metrics (AUROC and TNR@TPR95), and datasets (SVHN, CIFAR10,",,
"However, we speculate this could also be achieved in a better way using isotropic regularization or special data augmentation techniques (Thulasidasan et al., 2019; Yun et al., 2019) to avoid the need for out-of-distribution or adversarial samples.",,
"We speculate that recent advances in data augmentation techniques may help to improve IsoMax+ES OOD detection performance even further [51], [52].",,
"(7) As shown in [26], overconfidence in deep neural networks is a consequence of training on hard labels and it is the label smoothing effect from randomly combining yp and yq during mixup training that reduces prediction confidence and improves",,
"To deal with this issue, we propose to use mixup augmentation [25] as an effective regularization that helps calibrate deep neural networks [26] and, therefore, alleviates confirmation bias.",,
"undesirable behaviors [21], [24] and has been shown to improve calibration and OoD performance [24].",,
"…Müller et al., 2019), Entropy Regularized Loss (ERL; Pereyra et al., 2017), Virtual Adversarial Training (VAT; Miyato et al., 2018), and (5) Data-augmentation: Mixup (Zhang et al., 2018), Manifold-Mixup (M-Mixup; Verma et al., 2019), and Manifold-regularization (Mregularization; Kong et al., 2020).",,
"They also combine this approach with mix-up (Thulasidasan et al., 2019) and a distinctiveness score based on the MD.",,
"…is a strong yet computationally intensive baseline (Ashukha et al., 2020), and results of another recently proposed computationally intensive method called MSD (He et al., 2020) that leverage “mix-up” (Thulasidasan et al., 2019), “self-ensembling”, MD,
and the MC dropout (all layers are activated).",,
"They also use mix-up (Thulasidasan et al., 2019) to generate additional training instance representations that help to capture aleatoric uncertainty, self-ensembling, MC dropout, and a distinctiveness score to measure the",,
"They also use mix-up (Thulasidasan et al., 2019) to generate additional training instance representations that help to capture aleatoric uncertainty, self-ensembling, MC dropout, and a distinctiveness score to measure the epistemic uncertainty.",,
", 2020) that leverage “mix-up” (Thulasidasan et al., 2019), “self-ensembling”, MD,",,
"Mixup (Zhang et al., 2018) and its extensions (Verma et al., 2019; Yun et al., 2019) have been shown very effective for improving model calibration (Thulasidasan et al., 2019) and generalization in the presence of balanced datasets.",,
"Although Mixup (Zhang et al., 2018) was introduced to alleviate memorization for over-parameterized models (Arpit et al., 2017), it was later shown to improve calibration in the balanced setting (Thulasidasan et al., 2019).",,
", 2017), it was later shown to improve calibration in the balanced setting (Thulasidasan et al., 2019).",,
", 2019) have been shown very effective for improving model calibration (Thulasidasan et al., 2019) and generalization in the presence of balanced datasets.",,
"diction emanating from the desirable regularization effects it induces (Carratino et al., 2020; Zhang et al., 2018; Thulasidasan et al., 2019).",,
"a broad range of tasks ranging from computer vision (Zhang et al., 2018; Thulasidasan et al., 2019; Carratino et al., 2020; Wang et al., 2020a) to natural language processing (Guo et al.",,
"It has been empirically shown that mixup can hone the accuracy and calibration of the pre-
9266
diction emanating from the desirable regularization effects it induces (Carratino et al., 2020; Zhang et al., 2018; Thulasidasan et al., 2019).",,
"While mixup is making significant inroads in a broad range of tasks ranging from computer vision (Zhang et al., 2018; Thulasidasan et al., 2019; Carratino et al., 2020; Wang et al., 2020a) to natural language processing (Guo et al., 2019; Guo, 2020; Chen et al., 2020; Yin et al., 2021; Kong et al.,…",,
"To improve the diversity and variation of the perturbed samples (thus increasing the learnable information from a limited-size buffer), we investigate the role of MixUp [58, 67, 68], a data augmentation technique applied together with RAR — we find that it brings substantial improvements when there are strict buffer size constraints.",,
"Moreover, we conduct an ablation study showing that the key components such as replay samples selection strategy, sample pairing & adversarial perturbation, MixUp, etc, each bring appreciable improvements.",,
"The proposed MixUp strategy is different from [21, 40] as we apply mixup among the replay samples and then generate RAR perturbed samples anchored around them.",,
"Moreover, we study the role of MixUp in increasing the variation of replay augmentations, which significantly improves CL in the small buffer regime.",,
"The proposed use of MixUp among the replay samples before RAR is able to increase the variation of RAR perturbed samples, which is essential to CL with a small buffer.",,
"Empirically, Mixup is helpful for robust representation learning, and it alleviates the overconfident problems, and the failure of distribution shift settings as well as the in-distribution accuracy [34].",,
"Other works along the track also show improvements on various discriminative learning tasks [20,38,40,34].",,
Table 3 lists the over-confidence error (OE); the equation for OE is presented in Thulasidasan et al. (2019).,,
"Thulasidasan et al. (2019) find that for image classification, using mixup training improves calibration evaluated by the ECE metric.",,
"Fortunately, training good classifiers is easy nowadays with AutoML [Erickson et al., 2020] and versatile techniques for calibration, data augmentation, and transfer learning [Thulasidasan et al., 2019].",,
"Moreover, we evaluated the ability to separate TPs and FPs by evaluating the area under the receiver operator characteristic (AU-ROC) applied in [37, 5].",,
"Model averaging can yield well-calibrated confidence [4, 5] and is one of the state-of-the-art methods for detecting FPs caused by out-of-distribution examples [4, 3].",,
"Specifically, we evaluated the calibration error using measures, such as the negative log likelihood (NLL) applied in [4, 5, 31], expected calibration error (ECE) applied in [13, 8, 12], and Brier score (BS) applied in [4].",,
"Second, despite this simplicity, mixup is a powerful and popular training-time method that has been leveraged to address model fairness [9], improve model calibration [60, 73], and increase model robustness via regularizing the form of category boundaries learned implicitly [72].",,
"It is noteworthy that the condition in above lemma is easy to satisfy since the model prone to over-confident (Thulasidasan et al., 2019), thus, f(y = k|α(xi ); θ)/f(y = k|α(xi ); θ) is relatively small in real tasks.",,
"It is noteworthy that the condition in above lemma is easy to satisfy since the model prone to over-confident (Thulasidasan et al., 2019), thus, f(y = k′|α(xui ); θ)/f(y = k|α(xui ); θ) is relatively small in real tasks.",,
"For MX, we use α = 0.2 based on the results provided by (Thulasidasan et al. 2019; Singh and Bay 2020).",,
• Mixup (MX): Thulasidasan et al. (2019) showed that the data augmentation approach of Mixup also helps in calibrating a model.,,
"For ERL, we use the strength to be 0.1 based on the experiments of Thulasidasan et al. (2019).",,
Mixup (Zhang et al. 2018; Thulasidasan et al. 2019) and AugMix (Hendrycks et al.,,
The strength of the entropy regularizer in ERL is set to 0.1 based on the experiments of Thulasidasan et al. (2019).,,
Mixup (Zhang et al. 2018; Thulasidasan et al. 2019) and AugMix (Hendrycks et al. 2020) combine data augmentation and regularization.,,
A surprising observation to note is the poor performance of MX. MX as shown by Thulasidasan et al. (2019) performs well on out-of-distribution detection.,,
This is partly true in practice as for all deep neural networks the problem of calibration entails over-confident predictions(Thulasidasan et al. 2019).,,
"For LS, we use = 0.1 as utilized by Müller et al. (2019) and Thulasidasan et al. (2019).",,
"Thulasidasan et al. (2019) has demonstrated Mixup’s ability to distinguish ood samples however, we believe that natural shift is a weaker notion of data shift than ood evaluation and MX fails to provide any benefit in this regard.",,
"해당 방법은 간단하지만, 효과적인 데이터 증강 기법으로 알려져 있으며 Mixup 방법으로 생성된 데이터를 학습에 사용하면 심층신경망의 정확도와 신뢰 점수를 일치 하게 하는 교정 (Calibration)에 효과가 있음이 입증되었다 [14].",,
", 2019) and confidence calibration (Thulasidasan et al., 2019).",,
"Not only improving the generalization on the supervised task, it also improves adversarial robustness (Pang et al., 2019) and confidence calibration (Thulasidasan et al., 2019).",,
"Additionally, mixup has the property of curbing confirmation bias by enforcing label smoothness by combining yi and yj as noted by [73].",,
"Moreover, in [36], the impact of MixUp data augmentation on the model uncertainty estimation (also known as model calibration) is assessed.",,
"[53] evaluate mixup training, which convexly combines different images (and their labels) in the training In dataset to create new images (and labels).",,
The work of [39] and [4] both empirically evaluated Mixup’s effect on OOD detection.,,
"Relative Uncertainty Learning Inspired by the relativity of the uncertainty concept and the mixup method [53, 42, 46], we mix two different facial features according to their uncertainty values which enables the FER model to learn uncertainty through the relativity of different samples.",,
"Other works along the track also show improvements on various discriminative learning tasks [24, 36, 40, 43].",,
"Different from post-hoc calibration methods, another line of research aims to learn calibrated networks during training by modifying the training process [29, 12, 8].",,
[29] found that DNNs trained with mixup are significantly better calibrated than DNNs trained in the regular fashion.,,
"Also the influence of mixup [18] is investigated, which can improve the calibration [17] and performs a data-level change compared to the ensembles.",,
Calibration Deep neural networks tend to predict overconfidently (Thulasidasan et al. 2019).,,
"Therefore, Bayesian techniques have known a rising interest in medical imaging for classification [12], segmentation [15,22] and registration [18].",,
"But unlike Nketia and colleagues [22] who studied cellular segmentation, the confidence scores we use are derived from a deep BNNs rather than handcrafted.",,
[22] introduces an uncertainty metric based on distribution similarity of the two most probable classes.,,
"Predicting such uncertainty measures helps interpreting the output of machine learning programs [6], which is especially useful in the medical imaging domain [14,22,23] where explainability is crucial [8].",,
"[22] considered the Bhattacharya coefficient, since it is interpretable (0: certain, 1: uncertain), Eq.",,
The problem of uncertainty quantification has also been addressed using variational Bayesian methods [22].,,
"The works that are most relevant to our method are Mixup (Thulasidasan et al., 2019; Zhang et al., 2018) and variance-weighted confidence-integrated Loss (VWCI, Seo et al. (2019)).",,
"Regarding mixing-based techniques as a strong data augmentation scheme, Thulasidasan et al. (2019) show that the data augmentation alone without mixed labels can substantially improve the prediction accuracy, but not the predictive uncertainty.",,
"Furthermore, Thulasidasan et al. (2019) reports that Mixup training encourages that the output of DNN, the estimated label distributions, serves as a better indicator of the actual likelihood of a correction prediction.",,
"The works that are most relevant to our method are Mixup (Thulasidasan et al., 2019; Zhang et al., 2018) and variance-weighted confidence-integrated Loss (VWCI, Seo et al.",,
"Notably, this simple learning procedure results in robustness toward adversarial examples (Zhang et al., 2018) and improving calibration (Thulasidasan et al., 2019).",,
"Recently, Thulasidasan et al. (2019) have empirically shown the network trained with Mixup gives better-calibrated results.",,
"For the quantitative analysis of the confidence calibration, we used two popular metrics, the expected calibration error (ECE, Naeini et al. (2015)) and the overconfidence error (OE, Thulasidasan et al. (2019)).",,
"Especially, using a large α value degrades the accuracy largely in Mixup Thulasidasan et al. (2019); Zhang et al. (2018).",,
", 2018) and improving calibration (Thulasidasan et al., 2019).",,
"One possible explanation for this is under-fitting since its complicate training prevents sufficient convergence for the conventional learning procedure (Thulasidasan et al., 2019).",,
"Lately, Thulasidasan et al. (2019) empirically show that the label smoothing effect is a key factor for achieving the accurate predictive uncertainty.",,
"Thulasidasan et al. (2019) demonstrate that neural networks trained with mixup are significantly better calibrated under dataset shift, and are less prone to over-confident predictions on out-of-distribution data.",,
"(2018), can consistently improve classification accuracy and further has been shown to be able to help with calibration in (Thulasidasan et al., 2019).",,
It has been shown by Müller et al. (2019); Thulasidasan et al. (2019) that label smoothing can also effectively improve the quality of a model’s uncertainty estimates.,,
This trade-off between clean accuracy and calibration of mixup is also observed in other datasets and networks in Figure 2(j) in Thulasidasan et al. (2019).,,
"Overview of mixup Mixup, originally proposed by Zhang et al. (2018), can consistently improve classification accuracy and further has been shown to be able to help with calibration in (Thulasidasan et al., 2019).",,
[19] show that mixup can improve the calibration of neural networks.,,
"Mixup [25] combines random pairs of images and their labels during training, originally aimed at increased performance but it has recently shown to improve the calibration of DNNs [23].",,
"Deep ensemble We train M standard DNNs independently of each other following [13] and combine the predictions as
p(y = k|x, θ) = 1 M M∑ m=1 pm(y = k|x, θm) (5)
Mixup Recently proposed as a simple method by [25] for training better DNNs where two random input samples (xi, xj) and their corresponding labels (yi, yj) are combined using:
x̃ = λxi + (1− λ)xj ỹ = λyi + (1− λ)yj
(6)
where λ ∈ [0, 1] determines the mixing ratio of the linear interpolation. λ is drawn from a symmetric Beta distribution Beta(α, α), where α controls the strength of the input interpolation and the label smoothing.",,
Multiple popular methods have been proposed for quantifying predictive uncertainty for better calibration and robustness under distributional shifts and OOD inputs in deep neural networks (DNNs).,,
"[15], follow-up work developed improved variants of temperature scaling [32] or new types of scaling [27, 28] as well as novel training procedures altogether [50].",,
"Furthermore, it has been observed that the success of mixup is highly sensitive to the shape of the mixing distribution [12, 9, 13].",,
"When used to train deep neural networks on classification tasks, mixup has been shown to achieve both better generalization and increased model calibration across a range of data domains: images, audio, text, and tabular data [1, 9].",,
"The trainable methods are not compared in this work because the literature shows that they have a worse or similar calibration performance with temperature scaling [16, 22, 33].",,
"In this section, we introduce some existing calibration methods, including temperature scaling [4, 7], entropy regularization [24], MMCE regularization [16], label smoothing [22, 32], and Mixup training [33, 39].",,
[33] demonstrate that Mixup is also useful for neural network calibration.,,
