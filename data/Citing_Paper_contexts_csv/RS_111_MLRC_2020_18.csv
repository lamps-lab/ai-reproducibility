text,label_score,label
"Motivated by prior research [5, 6], this study aims to develop a method that captures the inductive bias of energy changes in rigid bodies as external conditions vary while preserving the high-precision modeling of 6-DoF equations for rigid bodies and the high-precision forward and backward sliding along the temporal dimension.",,
"Building upon, the authors in [5, 6] proposed a deep generative model termed the Hamiltonian generative network (HGN), which can learn the Hamiltonian dynamics of continuous-time evolution systems, exhibiting features such as time reversibility and smooth temporal interpolation.",,
"Some further leverage Hamiltonian separability [19], [21] by using a Leapfrog integrator.",,
"[47] use a latent Hamiltonian neural network in a VAE to learn dynamics without control, prior knowledge of the configuration-space structure or dimension.",,
"Previous efforts in learning dynamics from images [23, 47, 4, 54] consider only 2D planar systems (e.",,
"The model is compared to three baseline models: (1) an LSTM-baseline, (2) a Neural ODE [12]-baseline, and (3) the HGN [47] model.",,
"The authors of [54, 4, 47] leverage Hamiltonian and Lagrangian neural networks to learn the dynamics of 2D rigid bodies (e.",,
"We evaluate our model and compare to three baseline models: (1) recurrent model (LSTM [26]), (2) NeuralODE ([12]), (3) HGN ([47]).",,
"Previous work [54, 47, 4] has made significant progress in using physics-based priors to learn dynamics from images of 2D rigid bodies, such as a pendulum.",,
"This is a generalization of the existing literature where dynamics of canonical Hamiltonian systems are learned with the canonical symplectic form as the physics prior [23, 16, 13, 47].",,
"The combination of deep learning with physics-based priors allows models to learn dynamics from high-dimensional data such as images [54, 47, 4].",,
"[47], we use a latent Hamiltonian neural network to learn dynamics.",,
"This formulation has been used by several authors to learn unknown dynamics: the Hamiltonian structure (canonical symplectic form) is used as a physics prior and the unknown dynamics are uncovered by learning the Hamiltonian [23, 56, 47].",,
The authors in (Toth et al. 2019; Saemundsson et al. 2020; Higgins et al. 2021) circumvent this challenge by using a standard Gaussian prior and a phase-space of substantially higher dimension than the dynamical system underlying the video.,,
2020; Zhong and Leonard 2020) for video prediction and (Toth et al. 2019; Saemundsson et al. 2020) for variational autoencoding (VAE) (Kingma and Welling 2013) based video generation.,,
"Note that we do not compare against HGN since it is designed to model a single system, and color is considered a system parameter.",,
"As evidenced by Figure 6, our model produces qualitatively better videos than HGN.",,
"We compare the performance of our model against the HGN, and MoCoGAN baseline models on the CCC dataset.",,
"HGN (Toth et al. 2019): A VAE-based video generation
approach in which the latent vector is interpreted as an element of the phase-space and propagated forward in time with an HNN (Greydanus, Dzamba, and Yosinski 2019) cell.",,
"2020; Zhong and Leonard 2020), or select the dimension to be arbitrarily large for model flexibility (Toth et al. 2019; Saemundsson et al. 2020).",,
2019) and in VAE-based video generation pipelines where ground truth position-momentum values are assumed to be unknown (Toth et al. 2019; Saemundsson et al. 2020).,,
"HGN (Toth et al. 2019): A VAE-based video generation approach in which the latent vector is interpreted as an element of the phase-space and propagated forward in time with an HNN (Greydanus, Dzamba, and Yosinski 2019) cell.",,
The work in (Toth et al. 2019) and (Gordon and Parde 2021) are most similar to ours.,,
"While videos generated with our model have a foreground more similar to that of the real data, those generated with HGN have a more similar background.",,
"There is a significant difference in the FVD score of the leading model (HGN), and our model on the Three-body case.",,
"We empirically evaluate our model on two versions of the Toy Physics dataset (Toth et al. 2019), one with constant physical parameters and colors and another where the physical parameters and colors vary.",,
We attribute the discrepancy in FVD score to the difference in the background color of videos generated with our model and HGN.,,
(Toth et al. 2019) use the Hamiltonian formalism as an inductive bias for VAEbased video generation.,,
The Hamiltonian formalism has been used as an inductive bias in physics-guided video generation in several recent works (Toth et al. 2019; Saemundsson et al. 2020; Zhong and Leonard 2020; Higgins et al. 2021).,,
HGN and MoCoGAN are trained using the hyperparameters provided in the original paper.,,
"Following HGN we evaluate all models on the version of this dataset generated without friction, with constant physical quantities across trajectories, and with constant color.",,
"We use the pytorch implementation of HGN introduced in (Rodas, Canal, and Taschin 2021).",,
"There are also numerous pieces of research [22, 45, 8, 43] focusing on recovering the Hamiltonian, and predicting the dynamics of certain physical systems based on observed trajectories.",,
"[23] Peter Toth, Danilo J Rezende, Andrew Jaegle, Sébastien Racanière, Aleksandar Botev, and Irina Higgins.",,
"Structure-preserving dense networks: For dense networks, it is relatively straightforward to parameterize reversible dynamics, see for example: Hamiltonian neural networks [19, 20, 21, 22], Hamiltonian generative networks [23], Hamiltonian with Control (SymODEN) [24], Deep Lagrangian networks [25] and Lagrangian neural networks [26].",,
"In order to improve the stabilty of the HNNs, Chen et al. (2020), and Toth et al. (2020) introduced an other physical constraint to the HNNs by applying symplectic integrator for deriving states from HNNs.",,
"Initial works related to the discovery of Lagrangian can be linked to Hamiltonian Neural Networks (HNN) [12, 13].",,
"For learning the Hamiltonian and Lagrangian directly in Cartesian coordinates, Constrained Hamiltonian Neural Networks (CHNNs) and Constrained Lagrangian Neural Networks (CLNNs) were proposed in [16].",,
", 2019) or Hamiltonian Generative Networks (Toth et al., 2020).",,
"Exploiting the Newtonian evolution in classical Mechanics, Neural Hamiltonian Flows (NHF) (Toth et al., 2020) are NF models that use Hamiltonian transformations.",,
"…`(d|qT ) + log g(p)] dqT dpT + cst
= ∫ Π0(q0,p0) [log π0(q0) + log f(p0|q0)− log π0(Tq(q0,p0))− log `(d|Tq(q0,p0))− log g(Tp(q0,p0)] dq0dp0 + cst.
(7)
We can also adapt the ELBO from (Toth et al., 2020) to our inference framework:
lnπ0(q0) = ln ∫ Π0(q0,p0)dp0
= ln
∫ Π0(q0,p0)
f(p0|q0)…",,
"The dynamics is integrated in phase-space with the Leapfrog integrator (Toth et al., 2020).",,
"Including physical prior knowledge into neural networks may be another solution to understand the model (Raissi et al., 2019; Toth et al., 2020).",,
"Such toy example, similarly studied in (Toth et al., 2020), is interesting in the sense that it will allow us to discuss various aspects, from memory usage to interpretability.",,
"By adding artificial momenta pT (Toth et al., 2020), the distribution modeled by our NHF is m(qT ) = ∫ M(qT ,pT )dpT = ∫ Π0(T (qT ,pT ))dpT .",,
"As generative models, they have been used to sample from 2D distributions (Toth et al., 2020).",,
"In previous work (Toth et al., 2020), authors have proposed to parameterize each potential by a neural network.",,
"They come with performance similar to the ones obtained with Real-NVPs in sampling 2D distributions (Toth et al., 2020).",,
"To alleviate these issues, Neural Hamiltonian Flows (NHF, Toth et al., 2020) is a NF technique that uses a series of Hamiltonian transformations as normalizing flows.",,
"Furthermore, even if they have been numerically shown to transfer multimodality from the target distribution to the potential energy in some cases (Toth et al., 2020), this property is not guaranteed.",,
"By adding artificial momenta pT (Toth et al., 2020), the distribution modeled by our NHF is m(qT ) = ∫ M(qT ,pT )dpT = ∫ Π0(T −1(qT ,pT ))dpT .",,
"Multiple architectures have been proposed, such as Hamiltonian Neural Networks (Greydanus et al., 2019) or Hamiltonian Generative Networks (Toth et al., 2020).",,
"If the kinetic energy is chosen to be a MLP (Toth et al., 2020), then the model contains two black-boxes that are not easy to interpret a priori, namely the kinetic and potential energies K and V .",,
"…(Cranmer et al., 2020; Lutter et al., 2019; Zhong and Leonard, 2020; Allen-Blanchette et al., 2020; Greydanus et al., 2019; Matsubara et al., 2020; Toth et al., 2020; Finzi et al., 2020), and it also enables learning control-oriented dynamics models (Zhong et al., 2020a,b; Roehrl et al.,
2020;…",,
"This approach allows for the inclusion of general forms physics knowledge into data-driven models , such as for so-called Lagrangian and Hamiltonian neural networks (Cranmer et al., 2020; Lutter et al., 2019; Zhong and Leonard, 2020; Allen-Blanchette et al., 2020; Greydanus et al., 2019; Matsubara et al., 2020; Toth et al., 2020; Finzi et al., 2020), and it also enables learning control-oriented dynamics models (Zhong et al.",,
"Neural network-based methods for modeling continuous-time ODEs require derivative regression [14] or computationally expensive numerical integration to solve the ODEs [7, 40, 51, 30, 8].",,
"Neural networks, such as recurrent neural networks [16, 6], and neural ordinary differential equations (ODEs) [7], have been used for modeling black-box nonlinear dynamical systems given time-series data.",,
"Existing neural network-based models in continuous time, such as neural ODEs, require high computational cost for training since they need to backpropagate through an ODE solver or solve an adjoint ODE for each training epoch.",,
"Weak form learning [43, 9] has been proposed for efficient training of neural ODEs.",,
"Here, ODESolve(PΘ(·,u),x, t, T ) is a numerical solution to the ordinary differential equation specified by PΘ(x,u) over the window of time [t, T ].",,
"In this work, we use fixed-timestep RK4 to evaluate ODESolve(·) in all experiments.",,
"More closely related to our work, several recent papers also study neural ODEs that have a port-Hamiltonian structure (Zhong et al., 2020; Desai et al., 2021; Eidnes et al., 2023; Duong and Atanasov, 2021).",,
"(3)
Finally, we search for local minima of L(Θ,D) using gradient-based techniques, where ∇ΘL(Θ,D) may be computed using either direct automatic differentiation through ODESolve(·), or using the adjoint sensitivity method (Pontryagin, 1987; Chen et al., 2018).",,
"Of particular relevance to our work, Hamiltonian neural networks use the Hamiltonian formulation of dynamics to inform the structure of a neural ODE (Greydanus et al., 2019; Matsubara et al., 2020; Toth et al., 2020; Finzi et al., 2020).",,
"By using neural networks to parametrize differential equations, as opposed to directly fitting the available trajectory data, NODEs allow the user to harness an existing wealth of knowledge from applied mathematics, physics, and engineering (Djeumou et al., 2022a; Cranmer et al., 2020; Lutter et al., 2019; Gupta et al., 2020; Roehrl et al., 2020; Zhong et al., 2021b; Shi et al., 2019).",,
"The output x̂T := PHNNΘ(x,u, t, T ) of the PHNN parametrized by Θ is then given by ODESolve(PΘ(·,u),x, t, T ) ≈ x + ∫ T t PΘ(xs,u)ds, where we use the subscript notation xt to denote x(t).",,
"We note that the particular algorithm used to evaluate ODESolve(·) influences the model’s accuracy and the computational cost of forward evaluations of the model (Djeumou et al., 2022b).",,
"In particular, neural ordinary differential equations (NODEs) (Chen et al., 2018) provide a natural approach to incorporate physicsbased knowledge as inductive bias in deep learning (Zhong et al., 2021a; Rackauckas et al., 2020).",,
"Inspired by the Hamiltonian mechanics, a Hamiltonian neural network (HNN) has been proposed, where the output represents the Hamiltonian dynamics, through which energy conservation is explicitly enforced [24].",,
"The proposed Hamiltonian generative network has been applied to density estimation, leading to a neural Hamiltonian flow [24].",,
Scientific Knowledge Mathematical Equations [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50]–[52] [53] [54] [55],,
"Hamiltonian functionality that enforces energy conservation has attracted much attention [24], [25], [30], [31].",,
"Moreover, NLD can be viewed in a line of physics inspired neural network models such as (Cranmer et al., 2020; Greydanus et al., 2019; Toth et al., 2020; Botev et al., 2021).",,
"There is a great deal of work designing specific neural architectures that naturally obey Hamiltonian equations and Lagrangian equations [67], [68], [134], [135].",,
"For PDEs with Dirichlet conditions, they sample a dataset of collocation points from Ω and ∂Ω, i.e.{xi} ⊂ Ω
7 Neural Solver Method Description Representatives Loss Reweighting Grad Norm GradientPathologiesPINNs [43] NTK Reweighting PINNsNTK [44] Variance Reweighting Inverse-Dirichlet PINNs [45] Novel Optimization Targets Numerical Differentiation DGM [46], CAN-PINN [47], cvPINNs [48] Variantional Formulation vPINN [49], hp-PINN [50], VarNet [51], WAN [52] Regularization gPINNs [53], Sobolev Training [54]
Novel Architectures
Adaptive Activation LAAF-PINNs [55], [56], SReLU [57] Feature Preprocessing Fourier Embedding [58], Prior Dictionary Embedding [59] Boundary Encoding TFC-based [60], CENN [61], PFNN [62], HCNet [63]
Sequential Architecture PhyCRNet [64], PhyLSTM [65] AR-DenseED [66], HNN [67], HGN [68] Convolutional Architecture PhyGeoNet [69], PhyCRNet [64], PPNN [70]
Domain Decomposition XPINNs [71], cPINNs [72], FBPINNs [73], Shukla et al. [74]
Other Learning Paradigms Transfer Learning Desai et al. [75], MF-PIDNN [76]Meta-Learning Psaros et al. [77], NRPINNs [78]
TABLE 2: An overview of variants of PINNs.",,
"HGN
(Toth et. al.)",,
HGN [135] combines generative models such as variational auto-encoders (VAE) [141] and Hamiltonian neural networks to model time-dependent systems with uncertainty.,,
"Hamiltonian generative networks [135] build an autoencoder to map images to latent physical variable, and use Hamiltonian canonical equations to predict the changes in the physical variables.",,
"Among them, we can cite [15, 16, 17, 18, 19, 20], to mention but a few.",,
The combination of deep learning with physics-based models allows models to learn dynamics from high-dimensional data such as images (Allen-Blanchette et al. 2020; Zhong and Leonard 2020; Toth et al. 2020).,,
"Approaches that model the dynamics of the system (Li et al. 2020; Zhong and Leonard 2020; Allen-Blanchette et al. 2020; Toth et al. 2020) learn dynamics from image-state data but only for either 2D planar systems or systems with dynamics in R, using pixel images.",,
"contributions to the study of learning dynamics from images (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2020; Allen-Blanchette et al. 2020; Zhong and Leonard 2020) have evaluated their models on pixel image sequences of 2D planar dynamics.",,
"This formulation has been used by several authors to learn unknown dynamics: the Hamiltonian structure (canonical symplectic form) is used as a physics prior and the unknown dynamics are uncovered by learning the Hamiltonian (Greydanus, Dzamba, and Yosinski 2019; Zhong, Dey, and Chakraborty 2020b; Toth et al. 2020).",,
"Their datasets include the pixel pendulum, Acrobot, cart-pole (Zhong and Leonard 2020), as well as 2-body and 3-body problems (Toth et al. 2020).",,
Previous work (Allen-Blanchette et al. 2020; Zhong and Leonard 2020; Toth et al. 2020) has made significant progress in learning dynamics from images of planar rigid bodies.,,
"Due to its simplicity and the elegance of the idea, HNN has been applied on a wide range of tasks and neural network architectures [11, 12, 13], and even on dissipative systems by adding a dissipation term [14, 15].",,
"There have been a lot of follow-up works that build on the foundation of HNN and expand it to different settings including dissipative systems [5, 10], generative networks [6] and graph networks [7], while others try to improve HNN by simplifying it [11, 12].",,
"A popular workaround is to learn the Hamiltonian [2, 6, 7] or Lagrangian [3] of the energy-conserving system with a neural network, then get the dynamics as the derivatives of the learned quantity.",,
Even Generative Neural Networks exist under this Hamiltonian prism [81].,,
"Examples of existing work for Hamiltonian systems include [10,17,18,26,32,51,76,78,79,81], and while early seminal work learned Hamiltonian vector fields without truly preserving symplecticity, later results leveraged various tools including symplectic integrator [18], composition of triangular maps [32], and generating function [17] to fix this imperfection.",,
"[8] Peter Toth, Danilo J Rezende, Andrew Jaegle, Sébastien Racanière, Aleksandar Botev, and Irina Higgins.",,
"The majority of related work [1, 8, 9, 10, 11] use a variational autoencoder (VAE) framework to represent the state in a latent space embedding.",,
"The Hamiltonian expresses the total energy of the system H(q,p) = T (q,p) + V (q) [1, 8].",,
"This is similar to other reported results in literature [1, 9, 8, 10].",,
"Other work uses sequences of images as input to a model [8], or a specific velocity estimator model trained to estimate velocities from a sequence of positions [26].",,
"abstracted physical quantities that are not directly accessible from the video, more recent works directly use image data [49, 12, 22, 24, 52, 28, 59, 26, 50].",,
"For example, [16, 10] and [52] use a neural network to parameterize the Hamiltonian of a system, which relates the total energy to the change of the state.",,
"Although several learning-based approaches that infer physical models from image data have been proposed [12, 22, 24, 59, 52], existing approaches are particularly tailored towards settings with large training corpora.",,
[59] and [52] use a variational autoencoder (VAE) to predict posterior information about the initial state and combine this with an energy based representation of the dynamics and a final decoding stage.,,
"Several of the previously mentioned works model physical systems using Lagrangian or Hamiltonian energy formulations [30, 16, 11, 10, 52, 58, 28, 59], or other general physics models [26].",,
"For example, structural priors and constraints
are combined for fluid prediction (Tompson et al., 2017; Raissi et al., 2020), and Hamiltonian mechanics are used to construct non-regression losses (Greydanus et al., 2019; Chen et al., 2020; Toth et al., 2020) to learn basic laws of physics.",,
"The idea of utilizing Hamilton’s equations was successfully used to predict the dynamics of Hamiltonian systems from pixel observations [13,26,31], to build representations of molecular data [21] and it was extended to control tasks [29, 31] and metalearning [20].",,
"Many extensions of HNNs [5, 9, 25, 26] use more advanced numerical integrators combined with the Neural ODE approach [3] to model the evolution of the system state in time.",,
The analysis [32] of several numerical integrators when applied to HNNs shows that non-symplectic integrators cannot guarantee the recovery of true Hamiltonian H and the prediction accuracy obtained with a symplectic integrator depends on the integrator accuracy order.,,
"In Hamiltonian neural networks (HNNs), the law of the energy conservation is in-built in the structure of the dynamics model and therefore it is automatically satisfied.",,
"[154] proposes Neural Hamiltonian Flow (NHF), which is a powerful normalising flow model using Hamiltonian dynamics as the invertible function to model expressive densities.",,
Energy Conservation Law [117][118][109][154][134],,
"Hamilton Generative Model(HGN) [Toth et al., 2019] proposes a Variational Autoencoder(VAE) to accommodate high-dimensional observations (such as images), and assumes hidden states are governed by Hamiltonian system.",,
"Flow Model: HGN [Toth et al., 2019] proposes a simple modification of HNN that changes it to Neural Hamiltonian Flow (NHF) model.",,
", 2020] Standard Quadrature Canonical/Angle No RK HGN [Toth et al., 2019] Standard Quadrature Canonical/Pixel Yes Leapfrog",,
"For graph data, Hamiltonian ODE Graph Network (HOGN) [Sanchez-Gonzalez et al., 2019] combines HNN with graph neural network by the following formulation:
HGN(q,p) = GNu(q,p, c;φ) fHOGNq̇,ṗ (q,p) ≡ ( ∂HGN ∂p ,−∂HGN ∂q ) = (q̇, ṗ)
(q,p)n+1 = RK ( ∆t, (q,p)n, f HOGN q̇,p ) ,
where GN denotes a graph network and RK is Runge-Kutta integrator (can be replaced with symplectic integrators).",,
"One can model the driving force of the system directly [11, 13], focus on the Hamiltonian [14, 15], or the Lagrangian [6, 16].",,
"Invariances [38] [38] [38], [38] [41], [40], [42] [40], [84] [41], [42], [84] [84] [84]",,
"Toth et al. (2019) introduced Hamiltonian Generative Networks (HGNs), a class of generative models that can learn time-reversible Hamiltonian dynamics in an abstract phase space, starting from image inputs.",,
"Recent work by Greydanus et al. (2019), Toth et al. (2019) and others has shown that this function can be used to incorporate physical constraints, like conservation of energy, into deep neural networks.",,
"Some Hamiltonian methods (Toth et al., 2019; Yildiz et al., 2019) also model the dynamics of high dimensional sequential data in a latent space.",,
"Recently Toth et al. (2019) developed the Hamiltonian generative network (HGN), where they proposed to learn a Hamiltonian from image sequences.",,
Higgins et al. (2018); Toth et al. (2019); Botev et al. (2021) have discussed the benefits of such inductive biases for learning disentangled representation.,,
"…Generative Networks (HGNs) are capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions, and the Neural Hamiltonian Flow (NHF), which uses Hamiltonian dynamics to model expressive densities [Toth et al., 2019].",,
"Subsequent work uses invertible ResNet, optimal transport theory, among other techniques to further improve the performance of CNFs [1, 2, 4, 11, 18, 20, 44, 48, 51].",,
"To overcome these problems, we therefore combine latest advances in physics-enhanced Neural Networks based on dynamic invariants such as the Hamiltonian [1, 2, 3] or Lagrangian [4, 5] NNs with additional inductive bias to gain better predictive capabilities and to reduce the requisite training data.",,
"Recently, a body of work has emerged that brings these well-established principles of modelling dynamics from physics – such as the conservation of energy, and numerical formulations from the theory of differential equations – to neural network architectures [49, 20, 9, 3, 58, 8, 4, 31, 50, 43, 56, 12, 17, 24, 14, 32, 11, 45, 57, 54, 48, 21].",,
"employed by [49, 9, 45, 3, 58]), where the model is evaluated on how well it can reproduce the same trajectory length T as was used for training, albeit using test data.",,
HGN HGN [49] is a generative model that aims to learn Hamiltonian dynamics from pixel observations x.,,
"Currently only a handful of methods exist for learning dynamics with physical priors from pixels [49, 9, 45, 3, 58].",,
"We use these measures to identify a set of hyperparameters and architectural modifications that significantly improves the performance of Hamiltonian Generative Networks (HGN) [49], an existing state of the art model for recovering Hamiltonian dynamics from pixel observations, both in terms of long time-scale predictions, and interpretability of the learnt latent space.",,
For the HGN model we use the leap-frog integrator.,,
"What makes HGN and LGN models an attractive option is that when they do learn a dataset well, their long extrapolations are consistently good, unlike those for the other models which have larger variances (shown in brackets in Table 1).",,
The encoder and decoder networks are implemented as convolutional modules with leaky relu activations similar to the original HGN paper.,,
"The network architecture used forF is equivalent to that used for the kinetic and potential networks in the HGN, with the only difference being that it outputs not a scalar, but a vector of the same dimensionality as the input.",,
We implement the dynamics module with Hamiltonian (HGN) and Lagrangian (LGN) priors by using two neural networks to parameterise the kinetic energy T and the potential energy V .,,
"We use a Variational Autoencoder (VAE) [38, 39] as the basis for our models, inspired by the Hamiltonian Generative Network (HGN) [11].",,
"In terms of backward extrapolations, HGN and LGN do well.",,
"Unlike HGN and LGN that use MLPs to parametrise the functions that give rise to the time derivatives of the latent state, here the time evolution of the latent state is directly parameterised by an MLP: ṡ=F(s) (see Figure 1).",,
"Both the HGN and LGN models show good promise, especially in terms of getting good backward extrapolation performance effectively “for free”, however, they are still somewhat lagging behind the Neural ODE [TR], which implements the same inductive biases of modeling continual dynamics,
predicting state update residuals and modelling the dynamics forward and backwards in time, but constrained in a different manner.",,
"To address this issue, a number of approaches have been proposed that augment their physics-inspired models of dynamics with encoder/decoder modules for inferring the low-dimensional states from high-dimensional pixel observations [11, 24, 25, 22, 21].",,
"This intuition has been referred to as the Hamiltonian manifold hypothesis [11], which conjectures that natural images lie on a low-dimensional manifold embedded within a high-dimensional pixel space and natural sequences of images trace out paths on this manifold that follow the equations of an abstract Hamiltonian.",,
"To reduce the plethora of architectural choices we restrict our investigation to models which are Markovian and which treat the latent representation as a single vector without making any further assumptions or introducing further inductive biases (for example some previous works treat the latent representation as a spatial image or a graph [11, 36, 37, 4]).",,
"First, Section 4.4.1 describes how energy-conserving dynamics can be enforced by encoding the problem using Hamiltonian or Lagrangian mechanics.",,
"For instance, we relate energy-conserving numerical solvers to Hamiltonian NNs, whose goal is to encode energy conservation, and we discuss concepts such as numerical stability and solver convergence, which are crucial in long-term prediction using NNs.",,
"We start with the Hamiltonian defined as
H (x ) = T (x ) −V (x ), (14)
where x = [q,p] represents the concatenated state vector of generalized coordinates q and generalized momentap.",,
"Despite their mathematical elegance, deriving analytical Hamiltonian and Lagrangian functions for complex dynamical systems is a grueling task.",,
"The main advantage of Hamiltonian [41, 124] NNs and the closely related Lagrangian [21, 77] NNs is that they naturally incorporate the preservation of energy into the network structure itself.",,
"A similar concept to that of Hamiltonian and Lagrangian NNs involves learning neural surrogates for potential energy functionsV (x ) of a dynamical system, where the primary difference with Hamiltonians and Lagrangians is that the kinetic terms are
ACM Computing Surveys, Vol. 55, No. 11, Article 236.",,
"Specifically, the goal is to train an NN to approximate the Hamiltonian/Lagrangian of the system, as shown in Figure 18.",,
"In physics, a special class of closely related functions, called Hamiltonian and Lagrangian functions, has been developed for describing the total energy of a system.",,
4.4.1 Hamiltonian and Lagrangian Networks.,,
"To improve the performance, others have introduced various inductive biases such as Hamiltonian NODE architecture [142] or penalizing higher-order derivatives of the NODEs in the
ACM Computing Surveys, Vol. 55, No. 11, Article 236.",,
Both Hamiltonian H and Lagrangian L are defined as a sum of total kineticT and potential energy V of the system.,,
"[5, 17, 46] introduce non-regression loss functions inspired by Hamiltonian mechanics [19].",,
"Introduction of physical knowledge prior [41], [42], [43], [44], [45] has also shown to enhance the modeling and control of latent dynamics.",,
"Data-driven methods allow, essentially, to learn and explore dynamical systems from given data alone [5, 6, 7].",,
"…works either use the Lagrangian or the Hamiltonian formulation of dynamics to inform the structure of a neural ODE, as in (Cranmer et al. 2020; Lutter, Ritter, and Peters 2019; Roehrl et al. 2020) vs. (Greydanus, Dzamba, and Yosinski 2019; Matsubara, Ishikawa, and Yaguchi 2020; Toth et al. 2020).",,
"Unsupervised system identification from vision is a recent area of research that removes the requirements for trajectory data, with approaches including unsupervised physical parameter estimation [24, 31, 40], structured latent space learning [19, 25, 32], and Hamiltonian/Lagrangian learning [18, 51, 58].",,
"Reversible bracket formalisms (e.g. Hamiltonian/Lagrangian mechanics) have been shown effective for learning reversible dynamics (Toth et al. 2019; Cranmer et al. 2020; Lutter, Ritter, and Peters 2018; Chen et al. 2019; Jin et al. 2020; Tong et al. 2021; Zhong, Dey, and Chakraborty 2021; Chen and…",,
"Parameterization techniques that preserve physical structure include Hamiltonian neural networks (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019), Lagrangian neural networks (Cranmer et al.",,
"Parameterization techniques that preserve physical
structure include Hamiltonian neural networks (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019), Lagrangian neural networks (Cranmer et al. 2020; Lutter, Ritter, and Peters 2018), port-Hamiltonian neural networks (Desai et al. 2021), and…",,
"Hamiltonian/Lagrangian mechanics) have been shown effective for learning reversible dynamics (Toth et al. 2019; Cranmer et al. 2020; Lutter, Ritter, and Peters 2018; Chen et al. 2019; Jin et al. 2020; Tong et al. 2021; Zhong, Dey, and Chakraborty 2021; Chen and Tao 2021; Bertalan et al. 2019), while dissipative metric bracket extensions provide generalizations to irreversible dynamics in the metriplectic formalism (Lee, Trask, and Stinis 2021; Desai et al.",,
"Therefore, our future work may focus on building a stable version of ODE2VAE and increasing the interpretability of the latent representations through using arbitarary Lagrangians or Hamiltonians [10, 11], and learning disentangled latent representations with weak supervision [12].",,
"Concretely, it has been shown that physicallyinformed learning biases embedded in networks, such as Hamiltonian mechanics [6, 8], Lagrangians [9, 10], Ordinary Differential Equations (ODEs) [11], physicsinformed networks [12, 13], generative networks [14], and Graph Neural Networks [15, 16] can significantly improve learning and generalization over vanilla neural networks in complex physical domains.",,
"While there is a plethora of works devoted to learning conservative phenomena, based upon Hamiltonian or Lagrangian descriptions (see, among others, [9], [56], [57], [58], [59], [60]), very little has been investigated for learning dissipative phenomena.",,
"[37] show that, instead of from state trajectories, the Hamiltonian function can be learned from high-dimensional image observations.",,
"Structure preserving neural networks A thorough accounting of works embedding structurepreservation into neural networks include pioneering works for Hamiltonian neural networks [4, 40], followed by development of Lagragian neural networks [41, 5] and neural networks that mimic the action of symplectic integrators [6, 7, 8].",,
"To name a few, generative [28], recurrent [9] and constrained [33] versions, as well as Lagrangian Neural Networks [10] have been proposed.",,
"Another interesting direction to pursue would be to apply novel physics-informed neural network architectures[63, 64] to resolve Hamiltonians of systems with many degrees of freedom (such as molecules) using time-resolved HHG spectra, such as those obtained from solids driven by mid-IR fields [24].",,
"In the application of neural networks to model physical systems, several authors have also constructed equivariant (or invariant) models by incorporating equations of motion - in either the Hamiltonian or Lagrangian formulation of classical mechanics - to accommodate the learning of system dynamics and conservation laws [71, 72, 73].",,
"…connect with work on inferring dynamics with neural networks such as (Battaglia et al., 2016) in the same way as HNNs. Natural extensions of our current work can include the application on graph neural network based approaches (Sanchez-Gonzalez et al., 2019) and in flows (Toth et al., 2020).",,
"Weak form learning of GHNNs While derivative regression [5, 15–17, 20] and state regression [14, 18, 21] are well-known in the deep learning literature, learning ODEs from the weak form of the governing equations has only been used in the context of sparse basis function regression as far as the authors are aware [8, 29].",,
[21] independently developed methods for learning a Hamiltonian decomposition of an ODE.,,
"Making use of such priors allows models in this class to exhibit desirable properties by construction, such as being strictly Hamiltonian [16, 17, 21] or globally stable [15].",,
"Recent work on physics-based deep learning injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al., 2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly conserving physical quantities, or with ground truth supervision (Asenov et al.",,
"…injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al.,
2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly conserving physical quantities, or with…",,
"…studied in the context of differentiable physics simulation Degrave et al. (2016); de Avila Belbute-Peres et al. (2018); Cranmer et al. (2020b); Toth et al. (2020); Greydanus et al. (2019); Sanchez-Gonzalez et al. (2019), we focus on more challenging systems which have not been studied in…",,
"Many recent methods have used the Lagrangian formulation [14], [15] and also the closely related Hamiltonian formulation [16], [17], as physics-based priors for learning dynamics models.",,
"Many recent methods have used the Lagrangian formulation [14], [15] and also the closely related Hamiltonian formulation [16], [17], as physics-based priors for learning dynamics models.",,
"For example, the network may incorporate inductive bias corresponding to the physical laws of interaction and motion [25, 6, 107, 16, 92, 32].",,
"…et al., 2019) and an independent work (Bertalan et al., 2019), SRNN (Chen et al., 2020), SympNets (Jin et al., 2020), and (Lutter et al., 2019; Toth et al., 2020; Zhong et al., 2020; Wu et al., 2020; Xiong et al., 2021), all of which, except SympNets, are related to learning some quantity…",,
"(53)
The semiclassical neural network is a type of learnable Hamiltonian flow (Bondesan & Lamacraft, 2019; Rezende et al., 2019; Toth et al., 2020).",,
"We did not choose a specific model but let fP be a trainable Hamilton’s equation as in [39, 11].",,
"It may happen when the semantics of zP are not necessarily grounded, for example, when fP is a trainable Hamilton’s equation like in Toth et al. (2020).",,
"Toth et al. (2020) propose Hamiltonian generative networks, where a sequence of the latent variable is governed by the Hamiltonian mechanics with a learned Hamiltonian, the encoder infers the initial condition of the sequence, and the learned decoder works as an observation function.",,
"We did not choose a specific model but let fP be a trainable Hamilton’s equation as in Toth et al. (2020); Greydanus et al. (2019):
fP
([ pT qT ]T) = [ −∂H∂q T ∂H ∂p T ]T , (24)
where p ∈ Rdy is a generalized position, q ∈ Rdy is a generalized momentum, andH : Rdy ×Rdy → R is a Hamiltonian.",,
"While direct comparison is impossible due to the differences of the problem settings, the baseline methods we examined (listed below) are similar to some existing methods [4, 46, 39, 20, 47].",,
"Experiments on Human Locomotion
Physics model We modeled fP with a trainable Hamilton’s equation as in Toth et al. (2020); Greydanus et al. (2019):
fP
([ pT qT ]T , zP ) = [ −∂H∂q T ∂H ∂p T ]T , (26)
where p ∈ Rdy is a generalized position, q ∈ Rdy is a generalized momentum, andH : Rdy ×Rdy → R is…",,
[39] propose a model where the latent variable sequence is governed by the Hamiltonian mechanics with a neural Hamiltonian.,,
", when fP is a neural Hamilton’s equation [39].",,
"Capability of learning such an observation function has already been investigated (e.g., Toth et al., 2020).",,
"…fB works as an observation function that changes signal’s modality (Greydanus et al., 2019; Lutter et al., 2019; Yıldız et al., 2019; Linial et al., 2020; Toth et al., 2020; Cranmer et al., 2020; Saemundsson et al., 2020); and
• fB works as a learnable ensemble of physics models (Sengupta et…",,
"The HNNs have been developed through combining graph networks for learning interacting systems (Sanchez-Gonzalez et al., 2019) or generative networks for learning Hamiltonian from high-dimensional data (Toth et al., 2020).",,
", 2019) or generative networks for learning Hamiltonian from high-dimensional data (Toth et al., 2020).",,
"many useful properties of the Hamiltonian (Toth et al., 2020; Chen et al., 2020; Zhong et al., 2020a; Sanchez-Gonzalez et al., 2019; Jin et al., 2020).",,
"HNN and its variants have been shown to be effective in learning
many useful properties of the Hamiltonian (Toth et al., 2020; Chen et al., 2020; Zhong et al., 2020a; Sanchez-Gonzalez et al., 2019; Jin et al., 2020).",,
In Toth et al. (2019) HNNs were extended to latent variable models.,,
"One example involves the learning of invariant quantities via their Hamiltonian or Lagrangian representations [13, 14, 15, 16, 17].",,
"A particular interest was given to the automatic learning of equivariances in dynamical systems through their Hamiltonian [14, 15, 16, 17] or Lagrangian [13] formulations.",,
"As example applications, to solve the dynamics modeling problems, some works have introduced Hamiltonian dynamics (Greydanus et al., 2019; Toth et al., 2019; Zhong et al., 2019).",,
"In some works, in order to solve machine learning problems such as sequence prediction or reinforcement learning, neural networks attempt to learn a data symmetry of physical systems from noisy observations directly (Greydanus et al., 2019; Toth et al., 2019; Zhong et al., 2019; Sanchez-Gonzalez et al., 2019).",,
"…in order to solve machine learning problems such as sequence prediction or reinforcement learning, neural networks attempt to learn a data symmetry of physical systems from noisy observations directly (Greydanus et al., 2019; Toth et al., 2019; Zhong et al., 2019; Sanchez-Gonzalez et al., 2019).",,
"Other examples of dynamical systems inspired models include the learning of invariant quantities via their Hamiltonian or Lagrangian representations [37, 22, 10, 71, 58].",,
"More general formulations such as Hamiltonian (Greydanus et al., 2019; Toth et al., 2019) and Lagrangian",,
"More general formulations such as Hamiltonian (Greydanus et al., 2019; Toth et al., 2019) and Lagrangian
Dynamics (Lutter et al., 2019) are currently restricted in terms of the dimension of the dynamical system.",,
"In [14], the authors use the symplectic leapfrog integrator and show improved performance over a forward Euler integrator; however, this comparison does not distinguish between gains due to symplecticness and gains due to the second-order accuracy of leapfrog.",,
[71] 2020 DD NN  × Cranmer et al.,,
"Additional details on ARMs with linear transformed NNs is found in [22, 68, 23, 72, 67, 71] (cf.",,
"In other developments, autoencoder-based HNNs (AE-HNNs) [15] and Hamiltonian Generative Networks (HGNs) [19] were proposed to learn and predict the images of mechanical systems, which can be seen as Hamiltonian systems on manifolds embedded in high-dimensional spaces.",,
"Based on HNNs, other models were proposed to tackle problems in generative modeling [16], [19] and continuous control [20].",,
"In recent works [14]–[19], the primary focus has been to solve the inverse problem, i.",,
"Most of them are done in a supervised way [1, 2, 3, 4].",,
"To create a Hamiltonian neural network [8,19,15,2,3]",,
"In particular, Hamiltonian neural networks [8,19,15,2, 3] exploit the symplectic structure of conservative systems to forecast dynamics that mix order and chaos [6], even in very high dimensions [16].",,
"[19] introduced Hamiltonian Generative Networks (HGN), which harnessed the Hamiltonian flow without assuming canonical coordinates, but their statistical loss function was a complicated difference of posterior and prior probability distributions.",,
"In (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019; Bertalan et al. 2019), the authors model the underlying dynamics of sequential data assuming the Hamiltonian structure.",,
"[35] developed the Hamiltonian Generative Network (HGN), learning Hamiltonian dynamics from high-dimensional observations.",,
"This gives us a volume-preserving normalizing flow, similar to (Dinh et al., 2015; Toth et al., 2020).",,
"enforcing conservation laws in subdomains [19], hyperbolic conservation laws [41], Hamiltonian mechanics [42,43], symplectic structures [44,45], Lagrangian mechanics [46] and metriplectic structures [47]) and we believe that adapting/extending ideas of these approaches potentially mitigates the limitation of data-driven surrogate modelling approaches.",,
"More works on learning Hamiltonian systems can be found in [11, 34, 43] and references cited therein.",,
"More works on learning Hamiltonian systems can be found in [13, 37, 46] and references cited therein.",,
"Through TorchDyn neural differential equations and derivative models, e.g (Greydanus et al., 2019; Toth et al., 2019; Massaroli et al., 2020b; Lutter et al., 2019; Cranmer et al., 2020; Massaroli et al., 2020a; Li et al., 2020), including yet–to–be–published combinations, can effortlessly be…",,
"Notably, this allows for an out–of–the–box definition of Hamiltonian CNFs (Toth et al., 2019) and other unpublished variants.",,
"g (Greydanus et al., 2019; Toth et al., 2019; Massaroli et al., 2020b; Lutter et al., 2019; Cranmer et al., 2020; Massaroli et al., 2020a; Li et al., 2020), including yet–to–be–published combinations, can effortlessly be obtained by ad hoc primitives in combination with the rich PyTorch (Paszke et al.",,
"Continuous normalizing flows An additional fundamental member of the continuous– depth framework, continuous normalizing flows (CNFs) (Chen et al., 2018; Grathwohl et al., 2018) are treated as first–class primitives.",,
"C on
tr ol
A d jo in t In te gr al
L os s
Model ANODE (Dupont et al., 2019)
Higher-Order (Massaroli et al., 2020b)
Galërkin Neural ODEs (Massaroli et al., 2020b) Stacked Neural ODEs (Massaroli et al., 2020b)
Hamiltonian (Greydanus et al., 2019)
Lagrangian (Lutter et al., 2019; Cranmer et al., 2020)
Stable Neural Flows (Massaroli et al., 2020a)
Graph Neural ODEs (Poli et al., 2019)
CNF (Chen et al., 2018)
FFJORD (Grathwohl et al., 2018)
RNODE (Finlay et al., 2020)
Figure 3: Support in torchdyn of different SOTA models.",,
: Denton & Fergus (2018); Villegas et al. (2019); Weissenborn et al.,,
"Moreover, some of them use special ODE functions such as Hamilton’s equations to incorporate physical properties to neural network structurally [15, 42, 32, 7, 40].",,
"Neural ODE and its applications [6, 5, 15, 42, 32, 7, 40], alias ODE networks (ODENs), tackle these issues by learning the governing equations, rather than the state transitions directly.",,
", ODE [6], Hamiltonian [15, 32, 40, 42, 7], and other domain knowledge [36, 37, 27, 29].",,
"Recent works [15, 42, 32, 7, 40] apply the Hamiltonian mechanics to ODE networks, and succeed in enforcing the energy conservation as well as the accurate time evolution of classical conservative systems.",,
"In this space, we primarily see efforts to learn classical Hamiltonians from time series [4, 5, 9, 22, 28, 30, 38, 46, 49] as well as efforts to learn quantum Hamiltonians or potentials for timeindependent problems [2, 3, 15, 20, 24].",,
"Some methods bypass this problem by learning energetic invariants of the system [16, 17, 18] or exploiting the symplectic structure of the problem [19, 20], reporting promising and interpretable results for Hamiltonian dynamics.",,
Hamiltonian Generative Network [6] learns Hamiltonian dynamics from images.,,
Baselines We set up two baseline models: HGN [6] and PixelHNN [3].,,
Hamiltonian Generative Network (HGN) [6] learns Hamiltonian dynamics from image sequences.,,
"In the case of natural sciences, applying ML to physics is not new, several works have been reported (Toth et al. 2019; Greydanus, Dzamba, and Yosinski 2019; Cranmer et al. 2020; Tong et al. 2020) where different authors have combined",,
"In particular, there is a considerable amount of literature where authors have endowed neural networks with classical Hamiltonian mechanics (Toth et al. 2019; Greydanus, Dzamba, and Yosinski 2019; Tong et al. 2020; Iten et al. 2020; Bondesan and Lamacraft 2019; Zhong, Dey, and Chakraborty 2019; Chmiela et al. 2017); conservation of energy and irreversibility in time are the key features of such networks.",,
"In the case of natural sciences, applying ML to physics is not new, several works have been reported (Toth et al. 2019; Greydanus, Dzamba, and Yosinski 2019; Cranmer et al. 2020; Tong et al. 2020) where different authors have combined ML with Hamilton’s equations of motion to generate trajectories that obey energy conservation principles and classical physical laws.",,
"For these methods, the authors used a flow based method (Toth et al. 2019; Jimenez Rezende and Mohamed 2015) to increase the expressivity of their variational family of density matrices.",,
"In particular, there is a considerable amount of literature where authors have endowed neural networks with classical Hamiltonian mechanics (Toth et al. 2019; Greydanus, Dzamba, and Yosinski 2019; Tong et al. 2020; Iten et al. 2020; Bondesan and Lamacraft 2019; Zhong, Dey, and Chakraborty 2019;…",,
"This model is a combination of a Hamiltonian Neural Network [45, 46] and GN.",,
[36] proposed Hamiltonian generative networks to learn the Hamiltonian governing the evolution of a physical system.,,
"Hamiltonian Neural Networks have been augmented to form other architectures as well, such as Hamiltonian Generative Networks [47].",,
", later developed Hamiltonian Generative Network (HGN), which is capable of consistently learning Hamiltonian dynamics from high-dimensional observations without restrictive domain assumptions [38].",,
", see [6], [31], [32], [33], [34], and [35].",,
"developed the Hamiltonian Generative Network (HGN), learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions [45].",,
"Hamiltonian system is one of the expressions of classical mechanics and has been applied to a wide range of physics fields from celestial mechanics to quantum field theory [2, 35, 38], and there are also important applications for machine learning [4, 23, 36, 39, 41].",,
"The Hamiltonian operator in physics is the primary tool for modeling the time evolution of systems with conserved quantities, but until recently the formalism had not been integrated with NNs. Greydanus et al. [112] design a NN architecture that naturally learns and respects energy conservation and other invariance laws in simple mass-spring or pendulum systems.",,
"For example, in NNs, weights are often initialized according to a random distribution prior to training.",,
"For example, RNNs encode temporal invariance and CNNs can implicitly encode spatial translation, rotation, and scale invariance.",,
"This is taken a step further in Toth et al. [268], where they show that not only can NNs learn the Hamiltonian, but also the abstract phase space (assumed to be known in Greydanus et al. [112]) to more effectively model expressive densities in similar physical systems and also extend more generally to other problems in physics.",,
"There is a vast amount of other work using physics-guided architecture towards solving PDEs and other PDE-related applications as well which are not included in this survey (e.g. see ICLR workshop on deep learning for differential equations ([5]))
A recent direction also relating to conserved or invariant quantities is the incorporation of the Hamiltonian operator into NNs [64, 112, 268, 317].",,
"A similar transfer and adapt approach is seen in Lu et al. [180], but for an ensemble of NNs transferred from related tasks.",,
"More specifically, they demonstrated the encoding of translational symmetries, rotational symmetries, scale invariances, and uniform motion into NNs using customized convolutional layers in CNNs that enforce desired invariance properties.",,
"A recent direction also relating to conserved or invariant quantities is the incorporation of the Hamiltonian operator into NNs [60, 107, 255, 302].",,
A recent approach is seen in geophysics where researchers use NNs for the waveform inversion modeling to find subsurface parameters from seismic wave data.,,
"Another common technique in inverse modeling of images (e.g. medical imaging, particle physics imaging), is the use of CNNs as deep image priors [271].",,
"[255], where they show that not only can NNs learn the Hamiltonian, but also the abstract phase space (assumed to be known in Greydanus et al.",,
"Recently, the Hamiltonian-parameterized NNs above have also been expanded into NN architectures that perform additional differential equation-based integration steps based on the derivatives approximated by the Hamiltonian network [61].",,
"This can take place in many ways, including using domain-informed convolutions for CNNs, additional domain-informed discriminators in GANs, or structures informed by the physical characteristics of the problem.",,
"In a general setting, Wang et al. [281] show how spatiotemporal models can be made more generalizable by incorporating symmetries into deep NNs.",,
"They define a parabolic CNN inspired by anisotropic filtering, a hyperbolic CNN based on Hamiltonian systems, and a second order hyperbolic CNN. Hyperbolic CNNs were found to preserve the energy in the system as intended, which set them apart from parabolic CNNs that
, Vol. 1, No. 1, Article .",,
"Their idea is to use NNs to discover hidden signs of ""simplicity"", such as symmetry or separability in the training data, which enables breaking the massive search space into smaller ones with fewer variables to be determined.",,
"In a pioneering work by Ruthotto et al [236], three variations of CNNs are proposed to improve classifiers for images.",,
"To do this, they substitute NN layers into an unrolled version of an existing solution framework which drastically reduced the overall computational cost due to the fast forward evaluation property of NNs, but kept information of the underlying physical models of power grids and of physical constraints.",,
"Schutt et al. [245] proposes continuous-filter convolutional (cfconv) layers for CNNs to allow for modeling objects with arbitrary positions such as atoms in molecules, in contrast to objects described by Cartesian-gridded data such as images.",,
The modular and flexible nature of NNs in particular makes them prime candidates for architecture modification.,,
"In particular, NN solvers can reduce the high computational demands of traditional numerical methods into a single forward-pass of a NN. Notably, solutions obtained via NNs are also naturally differentiable and have a closed analytic form that can be transferred to any subsequent calculations, a feature not found in more traditional solving methods [159].",,
"This is similar to the common application of pre-training in computer vision, where CNNs are often pre-trained with very large image datasets before being fine-tuned on images from the task at hand [259].",,
"Though these loss functions are mostly seen in common variants of NNs, they are also be seen in architectures such as echo state networks.",,
Lagergren et al. [160] expand on this by using ANNs to construct the dictionary of functions.,,
"Recent work by Greydanus et al. (2019), Toth et al. (2019), and Chen et al. (2019) built on previous approaches of endowing neural networks with physical priors by demonstrating how to learn invariant quantities by approximating a Hamiltonian with a neural network.",,
This was the core motivation behind Hamiltonian Neural Networks by Greydanus et al. (2019) and Hamiltonian Generative Networks by Toth et al. (2019).,,
"…parallel, physics and machine learning have been forging strong ties mostly based upon the Hamiltonian theory and the integration of ordinary differential equations in the latent space to describe the evolution of dynamical systems (Chen et al. (2018), Toth et al. (2019), Greydanus et al. (2019)).",,
"The molecule has 66 dimensions in x, and we augment it with 66 auxiliary dimensions in a second channel v, similar to “velocities” in a Hamiltonian flow framework (Toth et al., 2019), resulting in 132 dimensions total.",,
"to “velocities” in a Hamiltonian flow framework [42], resulting in 132 dimensions total.",,
"…2019; Greydanus et al., 2019; Rezende et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2020; Toth et al., 2020; Zhong et al., 2020), with further applications in image prediction (Greydanus et al., 2019), generative modeling (Toth et al., 2020) and continuous control (Zhong et al., 2020).",,
"…systems from data (Bertalan et al., 2019; Greydanus et al., 2019; Rezende et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2020; Toth et al., 2020; Zhong et al., 2020), with further applications in image prediction (Greydanus et al., 2019), generative modeling (Toth et al.,…",,
"k-based models have been proposed to identify the Hamiltonian systems from data (Bertalan et al., 2019; Greydanus et al., 2019; Rezende et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2020; Toth et al., 2020; Zhong et al., 2020), with further applications in image prediction (Greydanus et al., 2019), generative modeling (Toth et al., 2020) and continuous control (Zhong et al., 2020). These learning model",,
"Learning physically relevant concepts of the two body problem was recently solved both by Hamiltonian Neural Networks [12, 13] and by very problem specific VAE architectures [14].",,
"Relatedly, deep-learning-based approaches for enforcing conservations laws include (1) designing neural networks that can learn arbitrary conservation laws (hyperbolic conservation laws (Raissi, Perdikaris, and Karniadakis 2019), Hamiltonian dynamics (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019; Chen et al. 2019), Lagrangian dynamics (Cranmer et al.",,
"…laws (hyperbolic conservation laws (Raissi, Perdikaris, and Karniadakis 2019), Hamiltonian dynamics (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019; Chen et al. 2019), Lagrangian dynamics (Cranmer et al. 2020)), or (2) designing a loss function or adding an extra neural network…",,
"This idea also applies to learning the conserved quantities from images (Toth et al., 2020).",,
"For example, [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40] exploit Lagrangian or Hamiltonian mechanics to learn an energy-conserving system based on position, momentum, and the derivatives thereof along trajectories.",,
"This problem setup is distinct from that of HNN [33], HGN [36], or Symplectic ODE-Net [35].",,
"These methods are successfully applied to a variety of tasks, such as the generative tasks [17] and the dynamics reconstruction [20, 15], sharing the similar design idea where utilisation of an appropriate loss function enforces the model to nearly obey the physical principles.",,
"We can easily extend our proposed approach to learn Hamiltonians from high-dimensional data (such as images) by combining an autoencoder with an SSGP, as in [14, 42].",,
"Recently, neural networks have been proposed [15, 16] that not only learn the dynamics of the system but also",,
"Recent research [15, 16] features artificial neural networks that incorporate Hamiltonian structure to learn",,
HGN maps a sequence to a latent representation and then projects it to the phase space to unroll the dynamics using an ODE integrator with Hamilton’s equation.,,
"In a very recent work, Hamiltonian generative network (HGN) [8] proposed to learn Hamiltonian from image sequences.",,
"Identifying fundamental symmetries is essential for developing expressive deep generative models (DGMs) that understand the motion constraints and can generalise beyond the training data [7, 8].",,
"Some Hamiltonian methods (Toth et al., 2019; Yildiz et al., 2019) also model the dynamics of high dimensional sequential data in a latent space.",,
"(Higgins et al., 2018; Toth et al., 2019; Botev et al., 2021) have discussed the benefits of such inductive biases for learning disentangled representation.",,
"Recently Toth et al. (2019) developed the Hamiltonian generative network (HGN), where they proposed to learn a Hamiltonian from image sequences.",,
"We will also investigate the integration of a physical model that uses physics as a model prior as in [26, 22, 27].",,
"In [3] and its variants[11, 1, 12, 8], the Hamiltonian function can be approximated by neural networks, Hθ, called Hamiltonian Neural Networks (HNN).",,
"In parallel, physics and machine learning have been forging strong ties based for example on Hamiltonian theory and the integration of ordinary differential equations in the latent space to describe the evolution of dynamical systems (Chen et al. (2018); Toth et al. (2019); Greydanus et al. (2019)).",,
"Recent work on physics-based deep learning injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al., 2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly conserving physical quantities, or with ground truth supervision (Asenov et al.",,
"…injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al., 2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly conserving physical quantities, or with…",,
"…studied in the context of differentiable physics simulation Degrave et al. (2016); de Avila Belbute-Peres et al. (2018); Cranmer et al. (2020b); Toth et al. (2020); Greydanus et al. (2019); Sanchez-Gonzalez et al. (2019), we focus on more challenging systems which have not been studied in…",,
"• Predicting the coordinates (q,p) from images has been pursued in (Greydanus et al., 2019; Toth et al., 2019).",,
"Here this would add an additional network before our input with the target output (q,p) as was explicitly demonstrated to work in (Toth et al., 2019).",,
", later developed Hamiltonian Generative Network (HGN), which is capable of consistently learning Hamiltonian dynamics from high-dimensional observations without restrictive domain assumptions (38).",,
"Recently, some works leveraged data specific knowledge to shape the prediction function, for example imposing specific fluid dynamic [26] or Hamiltonian constraints [11, 31].",,
