text,label_score,label
"We compare ENG-based attacks with four variants of anchor attack (AA), a recent heuristic generic poisoning attack on classical fair machine learning (Mehrabi et al., 2021).",,
"Recently researchers successfully attacked classical fair machine learning methods such as fair logistic regression (Mehrabi et al., 2021) and exacerbated bias in model predictions, thereby hurting fairness.",,
"These attacks have been successfully applied to classical fair machine learning (Chang et al., 2020; Solans et al., 2021; Mehrabi et al., 2021), but the non-convexity of neural networks and expensive influence function computations make them unsuitable for poisoning FRL.",,
"Heuristics such as label flipping (Mehrabi et al., 2021) lack a direct connection to the attack goal, thereby having no success guarantee and often performing unsatisfactorily.",,
"We follow previous attacks on classical fair machine learning (Chang et al., 2020; Mehrabi et al., 2021) and suppose a worst-case threat model, which has full knowledge and control of the victim model, as well as the access to part of the training samples.",,
"Despite the success of fair machine learning methods, not much is known about their vulnerability under data poisoning attacks until very recent studies (Chang et al., 2020; Solans et al., 2021; Mehrabi et al., 2021).",,
"However, sensitive attributes are sometimes indirectly represented by other information within the data, and they could still learn to discriminate even when the sensitive attribute is no longer present [31, 32].",,
Solans et al. (2020) and Mehrabi et al. (2021b) made the first attempt to propose novel ways to generate adversarial samples taking into account fairness objectives to disturb the training process and exacerbate bias on clean testing data.,,
"However, machine learning models have shown biased predictions against disadvantaged groups on several real-world tasks (Larson et al., 2016; Dressel and Farid, 2018; Mehrabi et al., 2021a).",,
"Some work discusses the problem of fairness poisoning attack during training (Solans et al., 2020; Mehrabi et al., 2021b); however, it is not clear how fairness attack would influence the predicted soft labels, and the relationship between fairness and accuracy attack/robustness remains unclear.",,
"Another line of research is supporting robustness in fair training, including handling noisy groups (Celis et al., 2021; Wang et al., 2020) or poisoning attacks (Mehrabi et al., 2021; Solans et al., 2020).",,
"A data poisoning attack can malfunction the system or exhaust resources, and decreases the system’s qualities, such as performance, safety, and fairness [59, 60].",,
"Fair methods have shown to be vulnerable to data corruption [75] and, in the light of this issue, there is an emerging research line concerned about the development of fair classifiers that are robust to such adversity.",,
"For example, [15] develops a gradient-based poisoning attack, [16] presents anchoring attack and influence attack, [17] provides three online attacks based on different group-based fairness measures, and [18] shows that adversarial attacks can worsen the model’s fairness gap on test data while satisfying the fairness constraint on training data.",,
"Note that φ can be defined to be any fairness utility metric, such as Balance and Entropy [Chhabra et al., 2021a, Mehrabi et al., 2021a].",,
"To our best knowledge, although there are a few pioneering attempts toward fairness attack [Mehrabi et al., 2021b, Solans et al., 2020], all of them consider the supervised setting.",,
"For this reason, it is of paramount importance to ensure that decisions derived from such predictive models are unbiased and fair for all individuals treated [Mehrabi et al., 2021a].",,
"Although there are some pioneering attempts on fairness attacks against supervised learning models [Solans et al., 2020, Mehrabi et al., 2021b], unfortunately, none of these works propose defense approaches.",,
"Arguably [26], and [30] are the most related to our work, as they aim to design data poisoning attacks targeting fairness but not on graph data.",,
"[26] propose two families of data poisoning attacks targeting fairness (anchoring and influence attacks), which inject poisoned data points aiming to degrade",,
"We base our research on two separate yet related streams of research: adversarial attacks on GNNs that aim to reduce GNN classification accuracy [7], [23], [36], [39], [44], [45], and attacks on fairness in the context of classical machine learning [5], [26], [27], [30].",,
"A growing body of work has been studying the robustness of more traditional machine learning models to attacks on fairness [5], [26], [27], [30].",,
"To the best of our knowledge, research in this domain is very scarce for fairness attacks [17, 22] compared to backdoor attacks and BlindSpot is the first technique leveraging fairness attacks for watermarking models.",,
[17] and consists in distorting the watermarked model’s decision boundary by generating poisoned points near specific target points to bias (proportionally to a sensitivity bias s) the outcome.,,
", 2018; Zhang and Bareinboim, 2018; Zhang and Ntoutsi, 2019) and fairness reducing attacks (Hua et al., 2021; Mehrabi et al., 2020; Solans et al., 2020).",,
"In these attacks, the adversary supplies poisoned training data, which then results in models that either compromise the accuracy of targeted subgroups [48, 64] or exacerbate pre-existing unfairness between sub-",,
"data [108], [135], and cannot be directly grafted to the graph-structured data.",,
"For instance, the attacker may poison the training data to degrade the fairness of the model [129, 167] or mislead the GNN explainer model [53].",,
"Recent works proposed online gradient descent algorithms for poisoning attacks against FERM, with respect to various fairness notions [11, 33, 44].",,
"…having adversarial clients, or in general they did not consider cases where auditing and verification is needed, such as cases where the client data itself might be intentionally biased or poisoned (Mehrabi et al. 2021b; Solans, Biggio, and Castillo 2020) and can corrupt the final global FL model.",,
"the test, we needed different varieties of clients, the cooperative clients who would train less biased models, the uncooperative clients who would intentionally train their models on skewed/imbalanced data to bias their trained models (Mehrabi et al. 2021b; Solans, Biggio, and Castillo 2020; Zhang and Zhou 2019) and normal clients.",,
"…of clients, the cooperative clients who would train less biased models, the uncooperative clients who would intentionally train their models on skewed/imbalanced data to bias their trained models (Mehrabi et al. 2021b; Solans, Biggio, and Castillo 2020; Zhang and Zhou 2019) and normal clients.",,
"…train their models on imbalanced (Zhang and Zhou 2019), poisoned (Mehrabi et al. 2021b; Solans, Biggio, and Castillo 2020), or poor quality data (Mehrabi et al. 2021a) that can contribute to the unfairness of their models, we decided to dedicate a central validation set for the server using…",,
"2021a), or poisoned data-points from data poisoning attacks against fairness (Mehrabi et al. 2021b; Solans, Biggio, and Castillo 2020).",,
"…and regression (Agarwal, Dudik, and Wu 2019) in general ML, and many other NLP applications, such as translation (Basta, Costa-jussà, and Fonollosa 2020), language generation (Liu et al. 2020), named entity recognition (Mehrabi et al. 2020), and commonsense reasoning tasks (Mehrabi et al. 2021c).",,
"To be able to satisfy different objectives including existing statistical fairness measures and be able to detect and downweight the effect of uncooperative or adversarial clients who might train their models on imbalanced (Zhang and Zhou 2019), poisoned (Mehrabi et al. 2021b; Solans, Biggio, and Castillo 2020), or poor quality data (Mehrabi et al.",,
"2021), they either considered satisfying such metrics locally by trusting the clients which might not be effective in case of having adversarial clients, or in general they did not consider cases where auditing and verification is needed, such as cases where the client data itself might be intentionally biased or poisoned (Mehrabi et al. 2021b; Solans, Biggio, and Castillo 2020) and can corrupt the final global FL model.",,
"…detect and downweight the effect of uncooperative or adversarial clients who might train their models on imbalanced (Zhang and Zhou 2019), poisoned (Mehrabi et al. 2021b; Solans, Biggio, and Castillo 2020), or poor quality data (Mehrabi et al. 2021a) that can contribute to the unfairness of their…",,
"Thus, although most of the previous work in fair federated learning focused on having a framework in which clients with different data distributions can be treated fairly and similarly to each other, not much attention has been given to standard statistical fairness metrics with regards to the existing sensitive attributes in the data and the destructive outcomes the unfair FL model can have in the existence of adversarial, uncooperative, or unfair clients who can train unfair models by poisoning their data instances (Mehrabi et al. 2021b).",,
"Similar to federated learning, research in fair Machine Learning (ML) and Natural Language Processing (NLP)
has gained significant popularity in recent years (Mehrabi et al. 2021a).",,
"This framework is also able to identify uncooperative or adversarial clients who might inject poisoned, unfair, or poor quality models to the overall FL system (Mehrabi et al. 2021b; Solans, Biggio, and Castillo 2020).",,
"…fairness metrics with regards to the existing sensitive attributes in the data and the destructive outcomes the unfair FL model can have in the existence of adversarial, uncooperative, or unfair clients who can train unfair models by poisoning their data instances (Mehrabi et al. 2021b).",,
"Even if the clients may not be adversarial, chances are that some clients may be training their models on an unintentionally biased data (Mehrabi et al. 2021a) that can corrupt the overall FL model.",,
"Explanations generated by our framework, which complement existing approaches in XAI, are crucial for helping system developers and ML practitioners to debug ML algorithms by identifying data errors and bias in training data, such as measurement errors andmisclassifications [35, 42, 94], data imbalance [27],missing data and selection bias [29, 61, 62], covariate shift [74, 82], technical biases introduced during data preparation [85], and poisonous data points injected through adversarial attacks [36, 43, 64, 83].",,
"The most relevant classes of attacks are based on data poisoning [18, 84], which injects a minimum set of synthetic data points into the training data to compromise the performance or fairness of a model trained on the contaminated data [43, 64, 83].",,
"However, anomaly detection fails in the presence of sophisticated attacks that are targeted at deteriorating model accuracy and/or fairness [36, 43, 64, 83].",,
"Interestingly, the above optimization problem is a bi-level optimization [30], similar to attack optimizations formulated for fairness degrading attacks for supervised learning models [13, 14].",,
"Similar to our work in this paper, most fairness degrading attack problems for these learning models are also bi-level optimization problems [13, 14], which are generally challenging to solve.",,
"There have been a few works on attacking fair machine learning models very recently (Chang et al. 2020; Solans, Biggio, and Castillo 2020; Roh et al. 2020; Mehrabi et al. 2020).",,
Mehrabi et al. (2020) also focused on attacking FML models trained with fairness constraint of demographic disparity.,,
Mehrabi et al. (2020) also focused on demographic disparity and presented anchoring attack and influence attack.,,
"Data poisoning attacks that target fairness controls have been recently developed [78,54].",,
"If the training data is not representative of the actual data distribution, e.g. it is noisy, biased, or has been manipulated, then fairness-enforcing mechanisms fall short (Kallus et al., 2020; Mehrabi et al., 2021b).",,
"…harmful is impossible to solve in general, see e.g. Charikar et al. (2017)
Fairness-aware learning In the last years, a plethora of algorithms have been developed that are able to learn classifiers that are not only accurate but also fair, see, for example Mehrabi et al. (2021a) for an overview.",,
• random anchor (RA0/RA1): these adversaries follow the protocol introduced in Mehrabi et al. (2021b).,,
"In fact, recent work has demonstrated empirically that strong poisoning attacks can negatively impact the fairness of specific learners based on loss minimization (Solans et al., 2020; Chang et al., 2020; Mehrabi et al., 2021).",,
"In particular, Solans et al. (2020), Chang et al. (2020) and Mehrabi et al. (2021) consider practical, gradient-based poisoning attacks against machine learning algorithms.",,
"[39] Ninareh Mehrabi, Muhammad Naveed, Fred Morstatter, and Aram Galstyan.",,
"Recent works have proposed poisoning attacks on fairness [39, 52].",,
We conducted a reproducibility study of the paper Exacerbating Algorithmic Bias through Fairness Attacks [11].,,
"Although the response was259 fairly quick, we were prompted to check the existing code in-depth, while further communication was discouraged.260
7 Conclusion261
In this reproduction study, we extensively reviewed the paper Exacerbating Algorithmic Bias through Fairness Attacks.262 We provided a clear foundation, upon which we described the proposed data poisoning attacks, namely the influence263 attack on fairness and the anchoring attack, as well as the experimental setup of the original paper.",,
"Figure 1: Impact on performance and fairness of a logistic regression classifier, using the attacks proposed in [11] and other state-of-the-art methods, for increasing ε values.",,
"Current research is primarily focused on adversarial attacks 35 targeting the performance of machine learning systems [3, 10], but recent studies indicate that adversarial attacks can 36 also be used to target fairness [11, 12, 13].",,
"[Re] Exacerbating Algorithmic Bias through Fairness Attacks
Anonymous Author(s) Affiliation Address email
Reproducibility Summary1
Scope of Reproducibility2
We conducted a reproducibility study of the paper Exacerbating Algorithmic Bias through Fairness Attacks [11].3 According to the paper, current research on adversarial attacks is primarily focused on targeting model performance,4 which motivates the need for adversarial attacks on fairness.",,
"FIFs do not only allow practitioners to identify the features to act up on but also to quantify the effect of various affirmative [8, 19, 23, 45–48] or punitive actions [21, 32, 42] on the resulting bias.",,
"Recently, Mehrabi et al. (Mehrabi et al., 2021) propose two families of attacks targeting fairness measures, which urges us to audit algorithm fairness carefully.",,
"(Mehrabi et al., 2021) propose two families of attacks targeting fairness measures, which urges us to audit algorithm fairness carefully.",,
Another limitation is that of robustness and further research on adversarial attacks on fairness [38] should be investigated.,,
"In the experiment, we use the Drug consumption dataset, as used in (Mehrabi et al., 2020; Donini et al., 2020).",,
"In particular, (Solans et al., 2020; Chang et al., 2020; Mehrabi et al., 2020) considering practical, gradientbased poisoning attacks against fairness-aware learners.",,
"This effect has been observed multiple times in the literature [22, 32, 35, 39].",,
"Research reproducibility: Given the increasing number of papers on fairness in AI [6, 32, 48, 49, 63, 65, 79, 105], software engineering [10, 14, 16, 94], and other conferences [2, 18, 52], reproducibility becomes increasingly important [4, 78].",,
"In particular, Solans et al. (2020); Chang et al. (2020); Mehrabi et al. (2020) consider practical, gradient-based poisoning attacks against machine learning algorithms.",,
