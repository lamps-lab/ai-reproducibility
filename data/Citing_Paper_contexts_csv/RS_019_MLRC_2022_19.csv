text,label_score,label
"Agarwal et al. [2022] introduced a hierarchical shrinkage procedure to regularize decision tree models. When viewing the tree model as a linear regression onto orthogonal basis elements ψl indexed by the internal nodes of the tree, they showed that their procedure was equivalent to performing ridge regression instead of linear regression on this basis. Since the amount of shrinkage in ridge regression is controlled by a single parameter, theirs is therefore a more constrained form of shrinkage compared to what arises from stacking nested subtrees. The latter optimizes the amount of shrinkage over M different parameters, each controlling the amount of shrinkage over a separate block of the regression coefficients. On the other hand, hierarchical shrinkage allows the basis elements ψl to be unnormalized, with ‖ψl‖(2) = Nl/n, where Nl is the number of samples contained in node l. The ridge regression shrinkage factor for the coefficient of ψl is then Nl Nl+λ , which means that splits on nodes with fewer samples are penalized more strongly. A theoretical or empirical performance comparison of the two forms of shrinkage is left for future work. Agarwal et al. [2022] also showed empirically that the best performing tree model after shrinkage is usually much deeper than the best performing tree model prior to shrinkage.",,
Agarwal et al. [2022] introduced a hierarchical shrinkage procedure to regularize decision tree models.,,
"Agarwal et al. [2022] introduced a hierarchical shrinkage procedure to regularize decision tree models. When viewing the tree model as a linear regression onto orthogonal basis elements ψl indexed by the internal nodes of the tree, they showed that their procedure was equivalent to performing ridge regression instead of linear regression on this basis. Since the amount of shrinkage in ridge regression is controlled by a single parameter, theirs is therefore a more constrained form of shrinkage compared to what arises from stacking nested subtrees. The latter optimizes the amount of shrinkage over M different parameters, each controlling the amount of shrinkage over a separate block of the regression coefficients. On the other hand, hierarchical shrinkage allows the basis elements ψl to be unnormalized, with ‖ψl‖(2) = Nl/n, where Nl is the number of samples contained in node l. The ridge regression shrinkage factor for the coefficient of ψl is then Nl Nl+λ , which means that splits on nodes with fewer samples are penalized more strongly. A theoretical or empirical performance comparison of the two forms of shrinkage is left for future work. Agarwal et al. [2022] also showed empirically that the best performing tree model after shrinkage is usually much deeper than the best performing tree model prior to shrinkage. In other words, by allowing a more nuanced bias-variance tradeoff, shrinkage allows for a “larger model” and makes use of features whose inclusion would not be justified otherwise. Breiman [1996]’s empirical findings for stacked subtrees present a similar story.",,
", M, the leaf of Tk containing x is an ancestor of that containing x in TM, this has the effect of shrinking the predictions over each leaf in TM to its ancestors’ values, similar to the method of Agarwal et al. [2022].",,
"By incorporating shrinkage using ridge regression [1] as opposed to OLS as the GLM within the MDI+ framework, we are able to mitigate this instability and regain the added benefits of including the raw feature and LOO evaluation, as illustrated by the strong performance of MDI+ (ridge+raw+loo).",,
"The starting point of our framework is a recently discovered connection between decision trees and linear models [38, 1].",,
", 2018), and “honest” random forests (Agarwal et al., 2022).",,
"…as non-parametric models such as reproducing kernel Hilbert spaces (RKHSs; Gretton et al., 2012), the Highly-Adaptive Lasso (Benkeser and Van Der Laan, 2016), the neural tangent kernel space of infinite-width neural networks (Jacot et al., 2018), and “honest” random forests (Agarwal et al., 2022).",,
Other recent studies have improved trees after fitting through regularization [45] or iterative updates [46].,,
", 2021), particularly for rule-based models (Friedman et al., 2008; Agarwal et al., 2022; Tan et al., 2022; Lin et al., 2020), without degrading interpretability.",,
"Recent works have focused on improving the predictive performance of intrinsically interpretable methods (Ustun & Rudin, 2016; Ha et al., 2021), particularly for rule-based models (Friedman et al., 2008; Agarwal et al., 2022; Tan et al., 2022; Lin et al., 2020), without degrading interpretability.",,
"methods that extract rulesets from a trained RF, such as C443 [17], and SIRUS [23], or that manipulate tree learners such as HS [25].",,
"Finally, a global approach is represented by ‘‘Hierarchical Shrinkage’’ [25] (HS) algorithm.",,
"Finally, we consider Hierarchical Shrinkage [25] (HS)method in its default configuration.",,
"The selection narrows down to C443 [17], SIRUS [23], RuleCOSI+ [24] and HS [25].",,
", 2021; Singh & Gao, 2022) or simply using a transparent model in the first place (Breiman et al., 1984; Tan et al., 2022; Singh et al., 2021; Agarwal et al., 2022).",,
"A different approach involves distilling information into a transparent model (Tan et al., 2018; Ha et al., 2021; Singh & Gao, 2022) or simply using a transparent model in the first place (Breiman et al., 1984; Tan et al., 2022; Singh et al., 2021; Agarwal et al., 2022).",,
"We consider 4 impelementations of decision tree algorithms, namely XGBoost [9], scikit-learn [11], Generalized Optimized Sparse Decision Tree (GODST) [10], and Hierarchical Shrinkage [1].",,
"Other transparent models Rule-based methods, such as trees [7, 18], regularized trees [19], rule sets [20], lists [21, 22], and tree sums [11] perform well for a variety of tasks, but are often less effective in NLP tasks, where the number of required rules tends to grow with the size of the vocabulary.",,
