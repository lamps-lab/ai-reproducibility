text,label_score,label,target_predict,target_predict_label
[19] demonstrate the manipulability of attention-based explanations and Wang et al.,,,0,not_related
These approaches involve recording the self-attention maps generated by the self-attention heads of the last block in the ViT model during inference.,,,0,not_related
"Despite these advancements, there are few contributions exploring the explainability of the ViT series of models.",,,0,not_related
"B. ViT Explainability
Currently, there remain few studies focusing on the explainability of methods belonging to the ViT family.",,,0,not_related
"First, we extract tLS from ViT.",,,1,related
"This approach consists of two main parts: generating alternative activation maps M and calculating the class-aware weighting scores w to extract class-aware patch tokens tc.
Generating alternative activation maps M : As discussed in III-A, ViT utilizes discrete tokens to convey information.",,,0,not_related
V iT (¬∑) denotes the output vector of the ViT model.,,,1,related
"Most existing approaches only consider the direct use of the raw-attention map corresponding to the class token in the multi-head self-attention (MHSA) module to directly generate explainability maps in ViT [22], [23], [24].",,,0,not_related
"Some approaches have been proposed to generate explainability maps directly from the raw-attention map corresponding to the cls-token [22], [23], [24].",,,0,not_related
"We firstly generate the patch tokens tLS by removing the last layer class token tLcls from the output of the last layer normalization
4 ‚ãØ Transform er layers ‚ãØ ‚ãÆ
ViT‚Ä¢
‚ãØ
s
‚ãØ
‚Äúzebra‚Äù ‚Äúelephant‚Äù
C classes
M
ùíïùíïùë∫ùë∫ùë≥ùë≥
P
G
w
ViT(X)
ViT(P)
X
‚ãØ
Projection+ Position em
bedding
Reshape normalization
Hadamard product
Cosine similarity
√ó
Columnwise
Generate graph
Graph cut
ùíïùíïùíÑùíÑ
R-Out
Cut
ViT Share weights
Upsampling
Fig.",,,1,related
"To compute the weight scores w for each perturbation map Pi, we input both the perturbation map matrix P and the original image X into the pre-trained ViT model.",,,1,related
"In the Vision Transformer (ViT) architecture, each transformer block follows a specific arrangement of components.",,,0,not_related
"2) Implementation Details: In our experiments, we used the same pre-trained ViT-base model as the backbone for our explainability maps tests to ensure fairness.",,,1,related
"This type of explainability is particularly relevant for models with complex structures, such as Convolutional Neural Networks (CNNs) [6], [7], [8], [9], [10] and Vision Transformers (ViTs) [11], [12], [13], [14], [15], [16].",,,0,not_related
", raw-attention [22], [23], [24], rollout[25], grad-cam[30], and Hila‚Äôs method[26].",,,0,not_related
The emergence of Vision Transformers (ViTs) has revolutionized computer vision.,,,0,not_related
"A. Vision transformer (ViT)
The ViT model is a popular approach for image classification tasks that uses a transformer-based architecture.",,,0,not_related
"This sensitivity makes them susceptible to manipulations designed to hide the use of problematic features in the AI decision-making and deceive the human overseeing the system [7, 11, 28, 65, 92].",,,0,not_related
"However, using information from raw attention poses limitations to the complete use of the structural characteristics of vision transformers, which include multiple learning modules [24].",,,0,not_related
"(Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2020), and on applying gradient-based methods (Li et al.",,,0,not_related
"This line of research includes a large body of work focusing on the analysis of the attention mechanism
(Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2020), and on applying gradient-based methods (Li et al., 2016a; Sundararajan et al., 2017) to obtain input attribution scores.",,,0,not_related
"In doing so, we avoid the non-identifiability problem of transformer models (Pruthi et al., 2019).",,,1,related
"In spite of a number of supporters initially for this approach, there has been a recent wave of detractors of attentionbased explanations (Jain and Wallace, 2019; Pruthi et al., 2019; Serrano and Smith, 2019).",,,0,not_related
"Because we are factorizing h, we can generate explanations on embeddings without needing to deal with the complexities of attention layers (Pruthi et al., 2019); nor do we have to deal with the nonidentifiability of transformer models (Brunner et al.",,,1,related
"Recent debates have shown that attention maps and attribution methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019).",,,0,not_related
"can be deceivable, determining the importance of the output only based on attention weights is not explainable [4].",,,0,not_related
"However, recent studies [14, 30, 42] showed that attention is not an explanation.",,,0,not_related
"Past work has used attention weights as a mechanism of providing explanations, but it has been observed that such explanations may not always be faithful to predictions [12, 17, 41].",,,0,not_related
"For example, using attention mechanisms may highlight associations entirely unrelated to a model‚Äôs output [44].",,,0,not_related
[28] use explanation-supervised models to substitute human participants in artificial simulatability studies to assess the quality of explanations.,,,0,not_related
"Recently, there has been promising progress on the topic of explanation supervision in the domains of image processing [19,29,2] and natural language processing [10,28,37].",,,0,not_related
The use of attention weights for explaining NLP models has been extensively debated and the general conclusion seems to point to the negative side (Jain and Wallace 2019; Serrano and Smith 2019; Pruthi et al. 2020; Bastings and Filippova 2020).,,,0,not_related
"Incorporating prior knowledge through selective attention is widely explored in natural language processing, especially in
recent NLP models with attention mechanism (Lin et al., 2016; Sukhbaatar et al., 2019; Pruthi et al., 2020; Beltagy et al., 2020; Wang et al., 2022).",,,0,not_related
"all modalities and textual data in particular (Pruthi et al., 2020; Ribeiro et al., 2016; Lundberg and Lee, 2017).",,,0,not_related
"ExAI is witnessing endeavors in
Papers accepted at Findings of Empirical Methods in Natural Language Processing (EMNLP), 2022.
all modalities and textual data in particular (Pruthi et al., 2020; Ribeiro et al., 2016; Lundberg and Lee, 2017).",,,0,not_related
", 2014) are widely used as an interpretability tool, but these attention maps are often unfaithful and unreliable (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020).",,,0,not_related
"Explainable NLP Heat maps generated from attention values from the models (Bahdanau et al., 2014) are widely used as an interpretability tool, but these attention maps are often unfaithful and unreliable (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020).",,,0,not_related
"While our study design does not explicitly account for this, even if perceptions vary at the instance level, our findings suggest that reliance would depend on the inclusion of sensitive features, which research has shown to be an unreliable signal for assessing algorithmic fairness [4, 26, 50, 54, 67, 73, 79].",,,0,not_related
", through adversarial attacks on explanation methods [24, 54, 79, 95].",,,0,not_related
", the exclusion of information that is evidently indicative of a person‚Äôs demographics, is neither necessary nor sufficient for an algorithm to be procedurally fair [54, 67, 79] or to not display bias in terms of distributive fairness [4, 26, 50, 73].",,,0,not_related
"Pruthi et al. (2020) again refutes argument (b), showing that with a simple training objective, they successfully guide the model to learn intended adversarial attention
distributions.",,,0,not_related
"For example, Pruthi et al. (2020) show that it is possible to attention weights can be a deceiving explanation to end-users.",,,0,not_related
"This suggests that although attention signifies the importance of the input modalities, it alone may not be enough for predicting DTAs or potential mechanism of action [36], [37].",,,0,not_related
", 2018, 2019), a lot of work questions attention explainability (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019; Serrano and Smith, 2019; Bastings and Filippova, 2020; Pruthi et al., 2020).",,,0,not_related
"‚Ä¶is not reliable: although there is evidence that attention can play recognizable roles (Voita et al., 2018, 2019), a lot of work questions attention explainability (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019; Serrano and Smith, 2019; Bastings and Filippova, 2020; Pruthi et al., 2020).",,,0,not_related
[83] manipulate the attention weights to whitewash problematic tokens in explanations that afect the model fairness or accountability.,,,0,not_related
"[91] (2015) Weights visualization on recognizing textual entailement X Dominance of the last output vector over attention in some cases [27] (2017) Layer-wise relevance propagation (LRP) [6] X Importance of LRP to further interpret the attention weights and the internal workings of transformers [116] (2019) Alignment between syntactic dependency and attention through visualization and aggregation X (1) Disproportionality between heads targeting POS, (2) Capturing of longer-distance relationships by deeper layers and (3) Moderate correlation between distance and entropy of attention [47] (2019) Correlation and counterfactuals X No frequent correlation between attention weights and gradient-based measures of feature importance [13] (2019) Aggregation of context into hidden tokens X Preservation of the token identiiability throughout the model and decrease of information identiiability with depth [83] (2020) Diminishing attention weights of impermissible tokens X Attention-based explanations can be deceived especially within the fairness context",,,0,not_related
"However, attention is only a small part of the overall computation and can be easily manipulated to hide model biases [56].",,,0,not_related
"Reliability of visualizations: There has been recent work examining the reliability of model interpretability methods for real-world practitioners (Pruthi et al., 2020; Srinivas and Fleuret, 2020).",,,0,not_related
"Although its use as an interpretability method has been criticized (Jain and Wallace 2019; Serrano and Smith 2019; Pruthi et al. 2020), using also the Euclidean norms of the vectors computed across each attention head (Kobayashi et al. 2020), the attention vector-norms method from now on, has been‚Ä¶",,,0,not_related
"Although its use as an interpretability method has been criticized (Jain and Wallace 2019; Serrano and Smith 2019; Pruthi et al. 2020), using also the Euclidean norms of the vectors computed across each attention head (Kobayashi et al.",,,0,not_related
"Lastly, our approach provides some quantitative argument for the validity of attention-based studies (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Pruthi et al., 2020) and expands on earlier works looking beyond attention weights (Kobayashi et al.",,,0,not_related
"Attention-based DNNs are growing in popularity [6, 37] with attention weights being used as explanations; however, recent works [29, 47, 45] show the limitations of such explanations.",,,0,not_related
"[32] apply a masked strategy for ‚Äúdeceiving‚Äù, which improves the attention‚Äôs reliability.",,,0,not_related
"Explanations can be misleading Deriving conclusions from existing findings is further complicated by evidence that explanations can mislead people‚Äôs beliefs [6, 25, 33], even in cases where there is no intention to manipulate [12].",,,0,not_related
[33] manipulate attention-based explanations such that people can be deceived into thinking that a model does not rely on sensitive information (e.,,,0,not_related
"While there have been recent advances in explaining neural network predictions [98], researchers have also demonstrated the ability to fool attention-based interpretation techniques [144].",,,0,not_related
"erefore, we set penalties to suppress the corresponding attention weight [29].",,,1,related
"But there is an ongoing debate ‚ÄúIs attention interpretable"" (Pruthi et al., 2020; Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Vashishth et al., 2019).",,,0,not_related
"A prominent line of research has investigated the faithfulness of attention weights (Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2020; Wiegreffe and Pinter, 2019; Madsen et al., 2021b) with contradictory conclusions.",,,0,not_related
"While the aforementioned work modified attention weights in a post-hoc manner after a model was trained, Pruthi et al. (2020) proposed to modify attention weights during model learning and produced models whose actual weights could lead to deceived interpretations. Wiegreffe and Pinter (2019) argued the validity of the claim in prior work (Jain and Wallace 2019) and proposed alternative experimental design to test when/whether attention can be used as explanation.",,,0,not_related
"While the aforementioned work modified attention weights in a post-hoc manner after a model was trained, Pruthi et al. (2020) proposed to modify attention weights during model learning and produced models whose actual weights could lead to deceived interpretations.",,,0,not_related
"Attention-based methods try to understand the network logic behind the Transformer-based models mainly from analyzing their self-attention maps [11], [43].",,,0,not_related
"Inspecting the attention scores is a common method of explaining a model‚Äôs prediction that has been called into question in recent years (Pruthi et al., 2020; Serrano and Smith,
2019).",,,0,not_related
"Inspecting the attention scores is a common method of explaining a model‚Äôs prediction that has been called into question in recent years (Pruthi et al., 2020; Serrano and Smith, 2019).",,,0,not_related
"Explanation robustness and sensitivity are two desired properties and have been mostly studied on images [1], [11], [55], [48], [34] and texts [34], but none on graphs.",,,0,not_related
"Similarly, other work has shown that it is possible to systematically manipulate attention while still retaining the same prediction [30].",,,0,not_related
"In addition, recent works [7, 20, 35, 38, 44] also find that the same prediction on an input could be generated by totally different attentions, which limits its applicability to explaining neural predictions.",,,0,not_related
"Recent methods for explaining predictions made by deep learning models considered explanations computed through convolutional layers [3, 26, 27, 33] (in vision models), and Transformer based architectures [11, 16, 17, 30, 37, 37, 40, 43] (in NLP models).",,,0,not_related
"While the authors in [18] argue that attention scores sometimes does not interpret model predictions faithfully, other works show that attention scores do offer plausible and meaningful interpretations that are often sufficient and correct [30, 39, 43].",,,0,not_related
"We notice that the use of attention weights for interpretability is a contentious topic in the literature [17, 32, 47].",,,0,not_related
"There has been a growing debate on the question of whether attention is interpretable [164, 143, 86, 203, 193, 30].",,,0,not_related
"[164, 143, 86, 30] hold negative attitude towards attention‚Äôs interpretability.",,,0,not_related
"In the NLP literature, the faithfulness of attention is regularly debated (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Vashishth et al., 2019; Serrano and Smith, 2019; Pruthi et al., 2020), often with contradicting conclusions.",,,0,not_related
"Much recent work in NLP has been devoted to investigating the faithfulness of attention as an interpretability method (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Vashishth et al., 2019; Serrano and Smith, 2019; Pruthi et al., 2020; Meister et al., 2021).",,,0,not_related
Pruthi et al. (2020) perform a similar analysis but report a contradictory finding.,,,0,not_related
"In Pruthi et al. (2020), the authors found that attention scores can be manipulated to deceive human decision makers.",,,0,not_related
"Following Pruthi et al. (2020), we use the biographies (DeArteaga et al., 2019) to predict whether the occupation is a surgeon or physician (non-surgeon).",,,1,related
Pruthi et al. (2020) also provides a pre-specified list of impermissible tokens 6 that a robust model should assign low attention scores to.,,,0,not_related
Pruthi et al. (2020) derived an occupation dataset to study the gender bias in NLP classification tasks.,,,0,not_related
even though multiple heads can be pruned without reducing the accuracy [21].,,,0,not_related
[21] showed how attention heads can be pruned; we follow up with an even bigger reduction in attention but show that key elements can still be preserved.,,,0,not_related
"This could also be further underlined by the findings and hypotheses of [4], [8], [11], [21].",,,0,not_related
instead of relying on the attention mechanism [27].,,,0,not_related
"As attention is not guaranteed to reflect how important a modality is [27], we guide the modality attention to be similar to human perceived modality informativeness and also evaluate how similar the predicted modality attention is to the perceived modality informativeness.",,,0,not_related
"of attention visualization may be limited in explaining particular predictions, depending on the task, attention can be quite useful in explaining the model‚Äôs overall predictions [39, 40, 41].",,,0,not_related
"Some works have criticized the use of attention weights as model explanations (Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2020), demonstrating that attention weights distributions can be modified without affecting the final prediction.",,,0,not_related
"‚Ä¶have typically been used as a tool for evaluating explanation faithfulness generated by attention (Kennedy et al., 2020; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Ghorbani et al., 2019) while others used auxiliary objectives to improve the faithfulness of explanations generated by‚Ä¶",,,0,not_related
"typically been used as a tool for evaluating explanation faithfulness generated by attention (Kennedy et al., 2020; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Ghorbani et al., 2019) while others used auxiliary objectives to improve the faithful-",,,0,not_related
Pruthi et al. (2020) show similar outcomes by manipulating attention to attend to uninformative tokens.,,,0,not_related
"The functionally-groundedness of input feature explanations have recived a lot of attention and discussion, however there is still little consensus on what is functionally-grounded or how to even measure it [3, 11, 54, 60, 73, 76, 89, 106, 122, 130].",,,0,not_related
"On the other hand, there are also dissenting papers showing that attention weights are not (or only weakly) correlated with feature importance measures such as gradientbased methods [26, 55], and that there exist alternative ‚Äúcounterfactual‚Äù attention distributions that yield equivalent model predictions [26, 44, 50, 22].",,,0,not_related
"MFN is highly relied on attention mechanism, whose effect improvement is limited until the mask method is proposed [16].",,,0,not_related
"Model interpretability for NLP has been intensively studied in the past few years (Ribeiro et al., 2016; Alvarez-Melis and Jaakkola, 2018; Jacovi et al., 2018; Chen et al., 2020a; Jacovi and Goldberg, 2020; DeYoung et al., 2020; Pruthi et al., 2020; Ye et al., 2021).",,,0,not_related
‚Ä¶Grimsley et al. (2020) found evidence that causal explanations are not attainable from attention layers over text data; Jacovi and Goldberg (2020) explored the faithfulness of attention heatmaps; Pruthi et al. (2020) showed that attention masks can be trained to give deceptive explanations.,,,0,not_related
"‚Ä¶al. (2020) have described the potential for human attention supervision to address the validity of attention as a faithful, human-like explanation for model decisions while Pruthi et al. (2019) have discussed the potential for deception by manipulating attention to make models appear less biased.",,,0,not_related
"[1, 30, 61, 70, 84]) and determining when attention can be used as an explanation [84].",,,0,not_related
"inherent uncertainty [1], [19], [27], [49].",,,0,not_related
"However, humans find it hard to make sense of soft masks and instead prefer sparse binary masks or hard masks [1], [19], [27], [49].",,,0,not_related
"And we follow the (Pruthi et al. 2020) to carve out a binary classification task of distinguishing between surgeons and (non-surgeon) physicians, where a majority of surgeons (> 80%) in the dataset are male.",,,1,related
"To enhance the gender bias, (Pruthi et al. 2020) further downsample minority classes ‚Äî female surgeons, and male physicians by a factor of ten.",,,0,not_related
"Model Architecture We train a simple embedding attention model (Pruthi et al. 2020), where the attention is directly over word embeddings (128 dimensions).",,,1,related
"And we follow the (Pruthi et al. 2020) to carve out a binary classification task of distinguishing between surgeons and (non-surgeon) physicians, where a majority of surgeons (> 80",,,1,related
"Attention-based interpretation can also be unreliable and manipulable to the point of deceiving practitioners, as Pruthi et al. (2020) and Jain and Wallace (2019) show.",,,0,not_related
", 2015) model the selection using a conditional importance distribution over the inputs, but the resulting explanations are noisy (Jain and Wallace, 2019; Pruthi et al., 2020).",,,0,not_related
"Attention mechanisms (Bahdanau et al., 2015) model the selection using a conditional importance distribution over the inputs, but the resulting explanations are noisy (Jain and Wallace, 2019; Pruthi et al., 2020).",,,0,not_related
"While removing the attention score normalization via softmax is not in line with an intuitive interpretation of attention as distributing ‚Äúfractions‚Äù of an overall attention budget among inputs, a growing body of work shows that the attention weights distribution does not directly correlate with predictions (Jain & Wallace, 2019; Pruthi et al., 2020; Brunner et al., 2020).",,,0,not_related
"‚Ä¶with an intuitive interpretation of attention as distributing ‚Äúfractions‚Äù of an overall attention budget among inputs, a growing body of work shows that the attention weights distribution does not directly correlate with predictions (Jain & Wallace, 2019; Pruthi et al., 2020; Brunner et al., 2020).",,,0,not_related
", 2020) and is faithful to the underlying computation, providing benefits even when omitted during inference (Pruthi et al., 2020).",,,0,not_related
"‚Ä¶since the attention mechanism (Bahdanau et al., 2015) provides plausible insight into what tokens the model considers relevant for a prediction (Galassi et al., 2020) and is faithful to the underlying computation, providing benefits even when omitted during inference (Pruthi et al., 2020).",,,0,not_related
Pruthi et al. (2020) also investigate the ability of attention weights to provide plausible explanations.,,,0,not_related
"Past studies proposed various, and sometimes conflicting, criteria for such interpretation [17, 25, 35], but their correctness is unclear.",,,0,not_related
"Due to its powerful ability, the attentionmechanism has been widely used in various neural network based applications such as language understanding tasks [11],[28], computer vision problems [32],[17].",,,0,not_related
"However, learned attention weights are often uncorrelated with word importance, which is calculated using the gradient-based method [14], and perturbations to the attention mechanisms may interfere with the interpretation [10, 28].",,,0,not_related
"Also, Baan et al. showed that MHA is at least partly interpretable even though multiple heads can be pruned without reducing the accuracy (Pruthi et al. 2019).",,,0,not_related
showed that MHA is at least partly interpretable even though multiple heads can be pruned without reducing the accuracy (Pruthi et al. 2019).,,,0,not_related
"2018), for inducing trustworthiness (Heuer and Breiter 2020) and have also explored the possibility of false privacy guarantees (Pruthi et al. 2019).",,,0,not_related
"Researchers have previously investigated the utility of explanations for various stakeholders (Preece et al. 2018), for inducing trustworthiness (Heuer and Breiter 2020) and have also explored the possibility of false privacy guarantees (Pruthi et al. 2019).",,,0,not_related
"This discussion has continued in the interpretable ML literature, with methods demonstrating how attention mechanisms can be useful or deceptive (Zhong et al., 2019; Grimsley et al., 2020; Jain et al., 2020; Pruthi et al., 2020).",,,0,not_related
"‚Ä¶research, and although there exist important concerns about the interpretability of attention distributions in transformers (Brunner et al., 2019; Pruthi et al., 2020), methods based on gradient attribution (Pascual et al., 2020) or on attention flow (Abnar and Zuidema, 2020) can provide‚Ä¶",,,0,not_related
"cerns about the interpretability of attention distributions in transformers (Brunner et al., 2019; Pruthi et al., 2020), methods based on gradient attribution (Pascual et al.",,,0,not_related
"However, it remains uncertain whether attention weights can provide reliable insights into the decision process of the model (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Pruthi et al., 2020b).",,,0,not_related
"However, such interpretations were shown to be unreliable (Serrano and Smith, 2019; Pruthi et al., 2020) and unfaithful (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019).",,,0,not_related
"For example, in text classification, while standard models typically attend to salient (high gradient influence) words (Serrano and Smith, 2019a), recent work constructs accurate models that attend to irrelevant words instead (Wiegreffe and Pinter, 2019a; Pruthi et al., 2020).",,,0,not_related
"For example, Jain and Wallace (2019); Serrano and Smith (2019b); Wiegreffe and Pinter (2019b); Pruthi et al. (2020) all show that information flows across hidden states.",,,0,not_related
"However, recent work raises concerns about the interpretability of attention distributions [3], [12], [21], [24].",,,0,not_related
"Though ubiquitous, token scores may not always reflect their real importance (Pruthi et al., 2020).",,,0,not_related
"‚Ä¶to/from Human As for human consuming attention as explanation, there has been criticism that unsupervised attention weights are too poorly correlated with the contribution of each word for machine decision (or, unfaithful) (Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2019).",,,0,not_related
"tion of each word for machine decision (or, unfaithful) (Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2019).",,,0,not_related
"Conversational Question Answering (ConvQA) is a new question answering task that requires a comprehension of the context, which has recently received more and more attention (Zhu et al., 2018; Qu et al., 2019a; Qu et al., 2019b; Meng et al., 2019; Pruthi et al., 2020).",,,0,not_related
"The robustness of explanations is studied in [1, 10, 30, 44, 48], but none of them is for Shapley values on MRF.",,,0,not_related
"The above answer to the T-Contrast questions can be used to study the robustness of Shapley values when the input MRF is (adversarially) perturbed, a situation that has been studied for other explanations in [1, 10, 30, 44, 48].",,,0,not_related
2017) models in an attempt to make it more interpretable (Pruthi et al. 2020; Brunner et al. 2020).,,,0,not_related
"‚Ä¶for NLP models have focused on explaining individual predictions by using gradient-based saliency maps over the input text (Lei, Barzilay, and Jaakkola 2016; Ribeiro, Singh, and Guestrin 2018; Bastings, Aziz, and Titov 2019) or interpreting attention (Brunner et al. 2020; Pruthi et al. 2020).",,,0,not_related
"Prior interpretability techniques for NLP models have focused on explaining individual predictions by using gradient-based saliency maps over the input text (Lei, Barzilay, and Jaakkola 2016; Ribeiro, Singh, and Guestrin 2018; Bastings, Aziz, and Titov 2019) or interpreting attention (Brunner et al. 2020; Pruthi et al. 2020).",,,0,not_related
Recent work has manipulated attention in Transformer (Vaswani et al. 2017) models in an attempt to make it more interpretable (Pruthi et al. 2020; Brunner et al. 2020).,,,0,not_related
"This complete reliance on attention mechanism is problematic as the MFN assumes synchronous inputs which is hard to achieve in real-world scenarios, and more importantly, the reliability of attentionmemory to discover inter-modal interactions is questionable as shown with deceptive attention masks in [7].",,,0,not_related
"Their complete reliance on attention scheme is problematic due to two reasons: 1) they have deceptive attention masks (in MFN), and hence it is obscure whether the gain in prediction is attributable to inter-modal interactions [7] and, 2) the role of training dynamics (in MARN) instead of multiple-heads [8].",,,0,not_related
"Pruthi et al. (2020) manipulate attention distributions in
an end-to-end fashion; we focus on manipulating gradients.",,,1,related
"This metric estimates the extent to which the model is attributing its predictions to gender (an unbiased model should have less of this attribution), and is similar to the measure of bias used by Pruthi et al. (2020).",,,0,not_related
"For example, adversaries can control attention visualizations (Pruthi et al., 2020) or black-box explanations such as LIME (Ribeiro et al., 2016; Slack et al., 2020).",,,0,not_related
"A.4 Biosbias Details
We follow the setup of Pruthi et al. (2020) and only use examples with the labels of ‚Äúphysician‚Äù and ‚Äúsurgeon‚Äù.",,,1,related
"For example, adversaries can control attention visualizations (Pruthi et al., 2020) or black-box explanations such as LIME (Ribeiro et al.",,,0,not_related
"Finally, Pruthi et al. (2020) propose a method to produce deceptive attention weights.",,,0,not_related
"But these weights do not reliably correlate with model predictions, making them unsuitable for explainability (Pruthi et al., 2020; Serrano and Smith, 2019; Jain and Wallace, 2019).",,,0,not_related
"Depending on the task and model architecture, attention may have more or less explanatory power for model predictions [35, 51, 57, 71, 79].",,,0,not_related
"Recent work on the faithfulness of attention heat-maps (Baan et al., 2019; Pruthi et al., 2019; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) or saliency distributions (AlvarezMelis and Jaakkola, 2018; Kindermans et al., 2019) cast doubt on their faithfulness as‚Ä¶",,,0,not_related
"The work of Pruthi et al. (2019) emphasizes the threat of interpreting models through attention weights, as they show a regularization term can be introduced to guide the attention weights away from focusing on subsets of words while retaining model accuracy, implying that models which exploit bias‚Ä¶",,,0,not_related
"However, the above formulation (referred in literature as ‚ÄúLuong attention‚Äù) is most widely used in text classification tasks (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Pruthi et al., 2020).",,,0,not_related
"Recently, Pruthi et al. (2020) conjecture that attention offers benefits during training; our work explains, and provides empirical evidence to support the speculation.",,,0,not_related
"Although it is wrong to equate attention with explanation (Pruthi et al., 2019; Jain and Wallace, 2019), it can offer plausible and meaningful interpretations (Wiegreffe and Pinter, 2019; Vashishth et al., 2019; Vig, 2019).",,,0,not_related
"Although it is wrong to equate attention with explanation (Pruthi et al., 2019; Jain and Wallace, 2019), it can offer plausible and meaningful interpretations (Wiegreffe and Pinter, 2019; Vashishth et al.",,,0,not_related
"For example, without faithful explanations, we cannot know whether a model is exploiting sensitive features such as gender (Pruthi et al., 2020).",,,1,related
"In pa ticular, existing feature attributio methods m y n t provide robust, faithful explanations (Feng et al., 2018; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Brunner et al., 2020; Zhong et al., 2019; Pruthi et al., 2020).",,,0,not_related
"the models (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Vashishth et al., 2019).",,,0,not_related
"Thus, it is more similar to attention as an explanation (Serrano and Smith, 2019; Jain and Wallace, 2019; Pruthi et al., 2019), where the attention structure does not necessarily reveal what tokens are used for prediction.",,,0,not_related
"Recent work (Brunner et al., 2020; Pruthi et al., 2019) has raised concerns about the interpretability of attention maps as representative of global context aggregation.",,,0,not_related
"The ability of attention distributions to provide explanations has been the target of a number of studies (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Pruthi et al., 2019).",,,0,not_related
"6 HTA: Local Validation The ability of attention distributions to provide explanations has been the target of a number of studies (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Pruthi et al., 2019).",,,0,not_related
"Pruthi et al. (2019) show that, just as in other attention models, it is possible to manipulate self-attention in transformers in order to generate different attention masks that cause only a small drop in performance.",,,0,not_related
"However, recent studies (Brunner et al., 2020; Pruthi et al., 2019) question the ability of attention maps to provide a faithful explanation of the inner workings of transformer models.",,,0,not_related
"Whether for attention (Baan et al., 2019; Pruthi et al., 2019; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019), saliency methods (Alvarez-Melis and Jaakkola, 2018; Kindermans et al.",,,0,not_related
"‚Ä¶the following challenge to the community: We must develop formal definition and evaluation for faith-
9Whether for attention (Baan et al., 2019; Pruthi et al., 2019; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019), saliency methods (Alvarez-Melis and Jaakkola,‚Ä¶",,,1,related
"the models (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Vashishth et al., 2019).",,,0,not_related
"It has been actively discussed so far whether the attention weights can be interpreted to explain
the models (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Vashishth et al., 2019).",,,0,not_related
[22] examined how attention-based methods could be fooled.,,,0,not_related
"In other related work, Pruthi et al. (2019) show that one can easily learn to deceive using attention weights.",,,0,not_related
"However, attention weights do not in general provide faithful explanations for predictions (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2019; Brunner et al., 2019; Moradi et al., 2019; Vashishth et al., 2019).",,,0,not_related
"However, their reliability has been questioned (Jain and Wallace 2019; Pruthi et al. 2020).",,,0,not_related
"According to various studies (Jain and Wallace 2019; Serrano and Smith 2019; Pruthi et al. 2020), standard attention modules noisily predict input importance; the weights cannot provide safe and meaningful explanations.",,,0,not_related
"‚Ä¶state activation (Karpathy, Johnson, and Li 2015; Li et al. 2016; Montavon, Samek, and MuÃàller 2018), attention weights (Jain and Wallace 2019; Serrano and Smith 2019; Pruthi et al. 2020), and learned sparse and interpretable word vectors (Faruqui et al. 2015b,a; Herbelot and Vecchi 2015).",,,0,not_related
"Moreover, (Pruthi et al. 2020) showed that standard attention modules can fool people into thinking that predictions from a model biased against gender minorities do not rely on the gender.",,,0,not_related
"2016; Montavon, Samek, and M√ºller 2018), attention weights (Jain and Wallace 2019; Serrano and Smith 2019; Pruthi et al. 2020), and learned sparse and interpretable word vectors (Faruqui et al.",,,0,not_related
"As it is proven possible to incorporate constraints on attention while maintaining satisfactory performance [25,12,31], we propose three approaches for enforcing plausibility constraints on attention maps, namely, sparsity regularization, semi-supervised learning, and supervised learning.",,,1,related
"Again, is TracIn intrinsic or posthoc?",,,1,related
"resentations [38], clustering [39, 40, 41], probing classifiers [16], concept activation [18], representer point selection [42], TracIn [43], and uptraining [44].",,,0,not_related
"For a real example, consider TracIn [43], in which influence functions are estimated across various training check points.",,,0,not_related
"2Examples of local methods include gradients [21, 22, 23], LRP [24], deep Taylor decomposition [25], integrated gradients [26, 27], DeepLift [28], direct interpretation of gate/attention weights [29], attention roll-out and flow [30], word association norms and analogies [31], time step dynamics [32], challenge datasets [33, 34, 35, 36], local uptraining [19], and influence sketching and influence functions [37]; examples of global methods include unstructured pruning, lottery tickets, dynamic sparse training, binary networks, sparse coding, gate and attention head pruning, correlation of representations [38], clustering [39, 40, 41], probing classifiers [16], concept activation [18], representer point selection [42], TracIn [43], and uptraining [44].
maintain.3 Moreover, for a method to be posthoc means different things to local and global methods.",,,0,not_related
"consider TracIn [43], in which influence functions are estimated across various training check points.",,,0,not_related
"In 538 fact, Pruthi et al. (2020) cast serious doubts on us- 539 ing attention maps as a way for users to audit expla- 540 nations in the context of fairness.",,,0,not_related
"While we believe these tools will eventually help stakeholders gain a deeper understanding of multimodal models as a step towards reliable real-world deployment, we believe that special care must be taken in the following regard to ensure that these interpretation tools are reliably deployed: Reliability of visualizations: There has been recent work examining the reliability of model interpretability methods for real-world practitioners [82, 94].",,,0,not_related
"[82] Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C Lipton.",,,0,not_related
"Reliability of visualizations: There has been recent work examining the reliability of model interpretability methods for real-world practitioners [15, 53, 65, 82, 94].",,,0,not_related
", 2014) are widely used as an interpretability tool, but these attention maps are often unfaithful and unreliable (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020).",,,0,not_related
"Explainable NLP Heat maps generated from attention values from the models (Bahdanau et al., 2014) are widely used as an interpretability tool, but these attention maps are often unfaithful and unreliable (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020).",,,0,not_related
"Following the work, there have been a number of efforts to obtain both faithful and plausible explanations of Transformers using attention [61, 62].",,,0,not_related
"However, recent studies argued that attention cannot be used as a reliable tool to explain the behavior of models (Jain and Wallace, 2019; Pruthi et al., 2020; Bastings and Filippova, 2020).",,,0,not_related
34 The paper Learning to Deceive with Attention-Based Explanations [8] aims to prove that attention weights do not 35 convey true interpretability.,,,0,not_related
"These papers often provide results that either prove a32 correlation between the attention weights and predictions [11][10], or the ambiguity between attention weights and the33 performance of the rest of the model [6][7], or somewhere in between [9].34
The paper Learning to Deceive with Attention-Based Explanations [8] aims to prove that attention weights do not35 convey true interpretability.",,,0,not_related
[8] introduce a method for manipulating attention that penalizes the attention weights of impermissible 52 tokens.,,,0,not_related
In: arXiv preprint232 arXiv:2004.14243 (2020).233 [8] Danish Pruthi et al. ‚ÄúLearning to Deceive with Attention-Based Explanations‚Äù.,,,1,related
"[8] add a penalty R to the loss function that is used for the specific task, resulting in a total cost of 58 L‚Ä≤ = L+R.",,,1,related
Our results reproduce Pruthi et al. (2020)‚Äôs finding that models can learn to deceive.,,,1,related
"Our results reproduce Pruthi et al. (2020)‚Äôs finding that models can learn to deceive. Jain and Wallace (2019) note that for attention to be an explanation, a different configuration of attention weights for the same piece of text should lead to different predictions. The research which we have reproduced implies that the same accuracy (hence prediction) can be maintained while explicitly changing the configuration of attention weights. The implications are clear; either it is providing further evidence for why attention should not be thought of as an explanation, supporting Serrano and Smith (2019)‚Äôs findings that attention weights can be largely zeroed out without affecting accuracy.",,,0,not_related
Pruthi et al. (2020) challenge the usage of attention-based explanation through a series of experiments using classification and sequence-to-sequence (seq2seq) models.,,,0,not_related
"Acknowledging the debate, Pruthi et al. (2020) whose work we seek to reproduce, examine whether models can learn to deceive, by adding a penalty to the loss function that punishes the model when attention is paid to impermissible tokens.",,,1,related
Table 3: Classification results from Table 3 in Pruthi et al. (2020) with cell scheme author | reproduced for all models except BERT(HgFc) which follows cell scheme author | replicated.,,,1,related
Pruthi et al. (2020) argue that the accuracy of the Embedding and BiLSTM models could have been greatly impacted by the lambda parameter because those models might be under-parameterised for the SST-Wiki dataset.,,,0,not_related
Especially the reported mean accuracy by Pruthi et al. (2020) shows no significant difference to our reported values for all seq2seq tasks except for the baseline experiments with uniform and no attention for the tasks sequence copy and sequence reverse.,,,0,not_related
"Our results reproduce Pruthi et al. (2020)‚Äôs finding that models can learn to deceive. Jain and Wallace (2019) note that for attention to be an explanation, a different configuration of attention weights for the same piece of text should lead to different predictions.",,,1,related
"Pruthi et al. (2020) did not report accuracies for the translation task in their original paper, but they provided us with
additional raw data which also contained the accuracy scores from their experiments.",,,1,related
Seq2seq Pruthi et al. (2020) provide a bidirectional and unidirectional Gated Recurrent Unit (GRU) with dot-product attention respectively for their encoder-decoder model tackling seq2seq tasks.,,,1,related
"In this reproducibility report we run four classification tasks and four sequence-to-sequence tasks to test the claims made by Pruthi et al.. For the classification experiments, three attention-based models are trained and evaluated on four classification tasks. The four classification experiments consist out of three binary classification task and one multiclass classification task. For the sequence-to-sequence experiments, an encoder-decoder model with varying attention mechanisms is trained and evaluated on four tasks. Three of these tasks are toy datasets created by Pruthi et al., the fourth task is an English to German machine translation task (More information in section 3). Further on in the report, we display the results obtained by conducting these experiments and compare them to the results reported by Pruthi et al. (Section 4 & 5). We cannot reproduce one of the binary classification tasks from the paper of Pruthi et al., because they do not have permission to share this private dataset. Therefore, we substitute this dataset for a multiclass classification dataset. As Wiegreffe and Pinter (2019) state, complex networks can produce outputs which can easily be aggregated to form the same binary prediction.",,,1,related
"However, Pruthi et al. (2019) show that the attention weights can easily be manipulated during training without a significant change in performance.",,,0,not_related
Sentiment Analysis + Wikipedia sentences Pruthi et al. used the binary version of the Stanford Sentiment Treebank Socher et al. (2013). Pruthi et al.,,,1,related
"First, different evaluation methods may give rise to contradictory conclusions, which has caused many debates, such as the argument in using the magnitudes of attention weights as interpretations for transformer-based models (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019; Pruthi et al., 2019; Bastings and Filippova, 2020) and the rebuttal of gradient-based methods (Alvarez-Melis and Jaakkola, 2018; Ghorbani et al.",,,0,not_related
"‚Ä¶in using the magnitudes of attention weights as interpretations for transformer-based models (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019; Pruthi et al., 2019; Bastings and Filippova, 2020) and the rebuttal of gradient-based methods (Alvarez-Melis and Jaakkola, 2018; Ghorbani et al.,‚Ä¶",,,0,not_related
"At the same time, these interpretation methods have received much scrutiny, arguing that the interpretations are fragile or unreliable (Alvarez-Melis and Jaakkola, 2018; Pruthi et al., 2019; Wang et al., 2020; Slack et al., 2020).",,,0,not_related
"At the same time, these interpretation methods have received much scrutiny, arguing that the interpretations are fragile or unreliable (Alvarez-Melis and
Jaakkola, 2018; Pruthi et al., 2019; Wang et al., 2020; Slack et al., 2020).",,,0,not_related
"However their contributions like training with an auxilary atomic-task supervi-
sion for improved faithfulness are specific to the context of NMNs. Pruthi et al. (2020) demonstrate that it is possible to train a model that produces a deceptive attention mask, questioning the use of attention‚Ä¶",,,0,not_related
"However their contributions like training with an auxilary atomic-task supervi-
sion for improved faithfulness are specific to the context of NMNs. Pruthi et al. (2020) demonstrate that it is possible to train a model that produces a deceptive attention mask, questioning the use of attention weights as explanation from the fairness and accountability perspective.",,,0,not_related
"We use unweighted average recall (UAR) as our evaluation metric for the emotion classification to account for class imbalance (Rosenberg, 2012).",,,1,related
"As shown in [10], the MHA is at least partly interpretable even though multiple heads can be pruned without reducing the accuracy [15].",,,0,not_related
"Inspecting the attention scores is a common method of explaining a model‚Äôs prediction that has been called into question in recent years [19,26].",,,0,not_related
"[13] Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, and Finale Doshi-Velez.",,,0,not_related
"Thus to restrict the model to only consider some specific alignments, we intuitively mask co-attention matrices AP and AH following Serrano and Smith (2019); Pruthi et al. (2020).",,,1,related
Authors have exploited these vulnerabilities to make discriminatory models pass algorithmic audits [18] and appear fair in user studies [13].,,,0,not_related
"[7], Jain and Wallace [8] illustrated that there are limitations in using attentions as explanations, in terms of being incapable to provide fully faithful explanations concerning the model decisions.",,,0,not_related
"Moreover, Pruthi et al. (2019) demonstrates how easy it is to manipulate the Attention weights.",,,0,not_related
"‚Ä¶it has been reported that learned attention weights are often uncorrelated with the word importance calculated through the gradientbased method (Simonyan et al., 2013), and perturbations to the attention mechanisms may interfere with interpretation (Jain and Wallace, 2019; Pruthi et al., 2019).",,,0,not_related
", 2013), and perturbations to the attention mechanisms may interfere with interpretation (Jain and Wallace, 2019; Pruthi et al., 2019).",,,0,not_related
"However, different layers vary in their behaviors [23, 24], and it has been shown that attention alone can be deceiving when used for interpretability and explanation [25].",,,0,not_related
"[50] demonstrate that it is possible to train a model that produces a deceptive attention mask, questioning the use of attention weights as explanation from the fairness and accountability perspective.",,,0,not_related
