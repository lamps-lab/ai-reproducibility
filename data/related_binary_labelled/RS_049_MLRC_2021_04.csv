text,label_score,label,target_predict,target_predict_label
"Although the roll-out of learned weights from attention mechanisms has been used as a proxy to approximate in a post-hoc fashion which regions of the input space support the prediction [17], the extent to which this holds true is still a matter of debate [1, 5].",,,0,not_related
Method Drop (↓) Inc (↑) Coher (↑) Compl (↓) ADCC (↑) Drop (↓) Inc (↑) Coher (↑) Compl (↓) ADCC (↑) Relevance[Chefer et al. [2021]] 55.,,,1,related
"Chefer et al. [2021] improved the visualization quality of attention-rollout and enabled generating class specific activation map with relevance information but it requires gradient information so it cannot be used for supporting explainability in inference time. And, all the existing techniques require the accessibility to the all attention layer’s activation in a model and it requires deeper model dependency. To overcome the limitations of existing XAI techniques for ViT, we propose an accurate and efficient reciprocal information-based approach. Our method utilizes reciprocal relationship between new spatially masked feature inputs (positional token masking) and network’s prediction results. By identifying these relations, we can generate visual explanations that provide insights into the model’s decision-making process without using attention layers’ information and assuming their internal relationship as previous researches. Our approach not only improves the interpretability of ViT models but also enables users to use XAI result in their inference system without trainable model. We evaluate our method on a range of ViT classification models and demonstrate its effectiveness in generating high-quality visual explanations that aid in understanding the models’ behavior and its accuracy in measuring Average Drop-Coherence-Complexity (ADCC) score suggested by Poppi et al. [2021]. The main contributions of this paper include:",,,1,related
Chefer et al. [2021] improved the visualization quality of attention-rollout and enabled generating class specific activation map with relevance information but it requires gradient information so it cannot be used for supporting explainability in inference time.,,,0,not_related
"The relevancy map loss, uses a CLIP-based relevancy [11] to provide rough estimation for the localization map",,,1,related
"Attention rollout [1, 9] is a popular way to understand whether attention modules can provide such explanations [19, 34].",,,0,not_related
"Both methods have been applied to derive the relevance score of inputs in Transformer-based models (Wu & Ong, 2021; Chefer et al., 2021).",,,0,not_related
"For the intepretation of the attention in ShE, SE, DE, BERT, and its variants we used the tool developed by (Chefer et al., 2021).",,,1,related
"The current effective methods are Rollout [1], TransAtt [7], GradCAM [30], PRLP [38] and GenAtt [6].",,,1,related
"Chefer et al. (Chefer, Gur, and Wolf 2021) allocated local correlations based on deep Taylor decomposition, subsequently propagating these correlations across layers, involving both attention and residual connections.",,,0,not_related
"Class-specific interpretability can be obtained by extensions like Contrastive-LRP (CLRP) and Softmax-Gradient- LRP (SGLRP) where the results of the class to be visualized are contrasted with the results of all other classes, to emphasize the differences and produce a class-dependent heatmap [100].",,,0,not_related
"associations among features and output, lack of propagated relevancy through the attention layers partially addressed through a new layer-propagation strategy, and unstable interpretability with covariate shift [100, 101].",,,0,not_related
"Similar variants such as positive / negative perturbations (Chefer et al., 2021a) or using masking in (Hase et al., 2021) have also been used for evaluating the explainability of methods.",,,0,not_related
"…short-term memory (LSTM) architectures (Li et al., 2016; Arras et al., 2017; Kádár et al., 2017) and more complex state-of-the-art transformerstack-based architectures (Guan et al., 2019; Wallace et al., 2019; De Cao et al., 2020; Chefer et al., 2021b; Hase et al., 2021; Feldhus et al., 2021).",,,0,not_related
• Exploiting the intepretability capabilites of the transformer model to provide explanations of its results [9].,,,1,related
"Additionally, the transformer model offers inherent interpretability properties [9], allowing for explaining algorithms to provide insight about how a certain result was achieved.",,,0,not_related
upon previous research in this field [57] and presents a graph Transformer relevancy map.,,,0,not_related
"methods, which are IG [30], BlurIG [64], GuidedIG [35], I-GOS [31], LIME [46], XRAI [38], GradCAM [6], GradCAM++ [28], Score-CAM [39], Transformer Explainability [65].",,,0,not_related
Transformer Explainability [65] propose a novel way to compute relevancy for the transformer network.,,,0,not_related
Hybrid models can potentially be also used to improve model interpretability [22-25].,,,0,not_related
[15] is adopted to interpret the transformer model and highlight important words.,,,0,not_related
"For the text modality, we employ the interpretability of transformers approach [15].",,,1,related
3) The perturbation test: This test consists of two experiments: Most Relevant First Perturbation (MRFP) and Least Relevant First Perturbation (LRFP) as described in the work by Hila’s method [46].,,,0,not_related
"[65] leverage a transformer interpretability method [218] to generate coarse relevance maps for each category, which are then refined by test-time-augmentation",,,0,not_related
"Vision transformers [61, 62], on the other hand, have demonstrated superior performance and should be investigated [63] further.",,,0,not_related
Recent work has introduced the assignment of a local relevancy score [21].,,,0,not_related
", by analyzing gradients [4, 47, 53] and attentions [11, 31, 59].",,,0,not_related
"[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 74, 75, 76, 77] I think the formatting may have gotten screwed up (or Gerrit made it look ugly) [ ] below assignments also should be removed",,,0,not_related
"To improve explainability using attention-based mechanisms, recent works have proposed transformer-based sequence-to-sequence models [47], [48].",,,0,not_related
"Many studies focus on visualizing the attention maps to interpret the principle of the network, especially for the transformer modules [1,6].",,,0,not_related
"Unlike TextShield, which relies on computationally intensive saliency factors [9] and lacks the utilization of attention cues inherent in transformer architecture, ITDT is specifically designed to tackle the challenges posed by large and complex transformer-based models.",,,0,not_related
"In order to verify the effectiveness of our proposed DynaSlim, we leverage a local relevancebased calculating method [26] to visualize the significant parts of the image that lead to a certain classification.",,,1,related
Attention map visualization on ImageNet-1K with [26].,,,0,not_related
[461] show that Transformer attention is often fragmented and does not provide a robust explanation.,,,0,not_related
"We report the additional visual explanations and evaluations on various methods and models, including GradCAM, FullGrad, GradCAM++, WGradCAM, RAP, RSP, AGF, SGLRP, and transformer interpretability (Chefer, Gur, and Wolf 2021).",,,1,related
"An architecture that will be explored is transformers [9], which in this context can be employed for the unsupervised matching between symbolic annotations and audio features.",,,0,not_related
[33] proposed a visualization method for Transformer networks that integrates attention and correlation scores into multiple attention modules and includes normalization terms for non-parametric layers.,,,0,not_related
"In [23], the authors employed LRP-based relevance to compute scores for each attention head in layers of a transformer which obtains state-of-the-art results, although, this method is not applicable to CNNs.",,,0,not_related
"However, few studies have actually delved into what it is that the models actually learn (Raghu et al., 2021; Nguyen et al., 2021; Chefer et al., 2021) and even fewer have examined point clouds in particular (Zhang et al., 2019; Tayyub et al., 2022).",,,0,not_related
"Other works (Tayyub et al., 2022; Chefer et al., 2021) utilize gradient-based methods in order to visualize the receptive fields or the relevancy of input patches towards the model’s decision.",,,0,not_related
"However, few studies have actually delved into what it is that the models actually learn (Raghu et al., 2021; Nguyen et al., 2021; Chefer et al., 2021) and even fewer have examined point clouds in particular (Zhang et al.",,,0,not_related
", heatmaps [5, 6] and input masks [7, 8]), instead of providing a mechanistic understanding of its inner-workings.",,,0,not_related
"Most available methods concentrate on visualizing attention scores [20] or reconstructing the attention flow [21], while other methods were introduced solely for visualizing the saliency maps of Transformers working with images [22].",,,0,not_related
We adopt a recent Transformer visualization method [4] to visualize the Transformer-based video encoder of our CMMT model.,,,1,related
"For instance, we could pick a particular neuron in the embedding and trace back what parts of a particular input sequence excite that neuron using methods such as layer-wise relevance propagation [Bach et al., 2015, Chefer et al., 2021].",,,1,related
"While they can make accurate predictions, understanding why they made a certain prediction is not straightforward, which could be problematic in a healthcare setting where interpretability is often necessary for clinicians to trust and act on model predictions [33].",,,0,not_related
(2021) [3] RobustViT [4] ICE-f (Ours) ICE (Ours) Pixel-wise accuracy Mean IoU Pixel-wise accuracy Mean IoU Pixel-wise accuracy Mean IoU Pixel-wise accuracy Mean IoU Pixel-wise accuracy Mean IoU Pixel-wise accuracy Mean IoU Pixel-wise accuracy Mean IoU,,,0,not_related
", relevance propagation rule, integration of propagation information, relevance, and attention scores) and solved some issues due to dependence on non-positive values and skip connections propagated in the learning process caused by the structural characteristics of vision transformers [3].",,,0,not_related
Later studies have evaluated the degree to which each attention head contributes to performance [36] or integrated the relevance and attention scores in layers through the proposal of a relevance propagation rule [3].,,,0,not_related
"However, GradCAM has not been effectively applied to explainability visualization for vision transformers because of the structural nature of the transformers, which classify image classes using [CLS] tokens [3].",,,0,not_related
"Despite the advantage of this optimization, challenges to explainability visualization for vision transformers remain given their structural characteristics [3, 4].",,,0,not_related
The effect described is also different than the relevancy maps obtained by explainability methods such as GradCAM [31] or recent transformer explainability methods [8].,,,0,not_related
"Note that DTA differs from the co-attention introduced in prior works [5, 6], wherein both cases, the attention is computed based on a specific task.",,,0,not_related
"This deviation from the target distribution causes the attention-attenuation problem [4] between generation chains of R and G, leading to the generation of images that diverge from the reference.",,,0,not_related
"Explainable deep learning methods have recently been effective in visualizing the internal decision-making processes of CNN and Transformer models [7], [18], [9].",,,0,not_related
"Inspired by recent work on Transformer-based image classification [29, 30], we propose a framework for regression tasks operating on molecular strings, and develop an explainable AI technique for chemical language models, using solely the model without external tools or information.",,,1,related
"Recent approaches overcome this limitation by considering all Transformer components and aggregating importance throughout the layers while retaining context [28, 29].",,,0,not_related
"There have been several methods proposed in the literature for generating attention visualizations in classification tasks, such as Attention Roll-Out [31] and Gradient Attention Roll-Out [32].",,,0,not_related
[50] reassigned a trainable relevancy map to the input image and propagate it through all the self-attention layers.,,,0,not_related
"Therefore, the relevance map that corresponds to the CLS token links each of the tokens to the CLS token, and the strength of this link can be intuitively considered as an indicator of the contribution of each token to the classification [60].",,,0,not_related
(b) Bottom: feature map visualization using Transformer Attribution method [60].,,,0,not_related
"[5] also argues that the intermediate artifacts of self-attention, such as queries and keys, are underexplored.",,,0,not_related
"In addition, considering the architecture difference between CNNs and transformers, some works [5, 33] adopt the Deep Taylor Decomposition [36] principle, based on which attention and relevancy scores are integrated across multiple layers for generating class-aware activation maps.",,,0,not_related
"For explaining BERT, we employ the transformer visualization method proposed in Chefer et al. (2021) to map back from the [CLS] activation concepts to input tokens.",,,1,related
"Similarly, the codebase used for replicating the visualization method (Chefer et al., 2021) and the baseline method (Chen et al., 2018) are licensed under the MIT license, which allows for redistribution of the code.",,,1,related
"We use the transformer visualization approach (Chefer et al., 2021) and Grad-CAM (Selvaraju et al., 2017), which rely on the gradients generated from the red path.",,,1,related
"With the trained BrainNPT model, for any given brain network, we can obtain the relevance scores of corresponding ROI with LRP methods.",,,1,related
The Transformer models with the <cls> classification embedding vectors have been proven to be interpretable using self-attribution scores [40] or layer-wise relevance propagation (LRP) [41] in NLP or CV domain.,,,0,not_related
"In addition, the proposed model could be interpreted by layer-wise relevance propagation (LRP), which is one of the most prominent explanation techniques for deep neural networks.",,,0,not_related
"The BrainNPT contains <cls> classification embedding vector, the Transformer block, and fully connected layers, and it is able to use LRP to calculate the relevance scores of each ROI for the classification results.",,,1,related
"Based on LRP for interpretation of BrainNPT, we could obtain the local relevance for an input sample using deep Taylor decomposition method [42].",,,1,related
"For a trained BrainNPT model, its relevance scores of ROIs can be calculated based on LRP for Transformer’s self-attention, FFN, GELU activation function and MLP. Finally, by the chain rule, the relevance score of each ROI can be estimated in the input brain network.",,,0,not_related
"Moreover, the pre-training strategies and the influence of the parameters of the model were further analyzed, and the trained BrainNPT model was interpreted by LRP.",,,0,not_related
"Considering the similarity between BrainNPT with BERT [7] and ViT [21], we adapted an LRP based interpretation method for the BrainNPT model to explore which ROIs in the brain networks have the key impact on classification.",,,1,related
"Interpretability is also another crucial criteria, especially in sensitive scenarios like medical applications, and despite some efforts to improve it[13, 38], investigation of interpratibility of attention-based architectures is more challenging compared to CNNs[36].",,,0,not_related
"The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al. [2020], natural language processing Ismail et al. [2021], Zhang et al.",,,0,not_related
"The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al. [2020], natural language processing Ismail et al. [2021], Zhang et al. [2021], graph structure analysis Funke et al. [2022], and time-series analysis Liang et al. [2020], Ismail et al. [2020]. The ROAR protocol has been established as the primary evaluation methodology in Meng et al.",,,0,not_related
"The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al.",,,0,not_related
"The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al. [2020], natural language processing Ismail et al. [2021], Zhang et al. [2021], graph structure analysis Funke et al. [2022], and time-series analysis Liang et al.",,,0,not_related
"The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al. [2020], natural language processing Ismail et al. [2021], Zhang et al. [2021], graph structure analysis Funke et al.",,,0,not_related
"The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al. [2020], natural language processing Ismail et al. [2021], Zhang et al. [2021], graph structure analysis Funke et al. [2022], and time-series analysis Liang et al. [2020], Ismail et al.",,,0,not_related
"The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al. [2020], natural language processing Ismail et al.",,,0,not_related
", CAM [49], and GradCAM [33] for convolutional neural networks; [7] for Transformers [40].",,,0,not_related
"Therefore, we employ an improved Layer-wise Relevance Propagation (LRP) method [16] which integrates the weighted attention relevance for each MSA block.",,,1,related
"In addition, we employ an improved Layer-wise Relevance Propagation (LRP) method [16] for the interpretation of the spectral transformer.",,,1,related
"According to [16], the relevance rule can be described as",,,0,not_related
"Gradient-based methods have also been used to visualize ViT models [73, 13].",,,0,not_related
"[5], Attention Rollout [1], and Grad-CAM [19].",,,0,not_related
"The former is characterized by the use of the gradients of a model’s output with respect to a layer’s input as an indicator of importance [19, 21, 22], while the latter relies on the Deep Taylor Decomposition method [16] to recursively break down the model’s output into the contributions of each layer [3, 5, 20].",,,0,not_related
"Recently, a Layerwise Relevance Propagation for Transformers (TransLRP) approach [6] was introduced to",,,0,not_related
utilized a perturbation metric that demonstrated the superiority of their method over others significantly [6].,,,0,not_related
"We aim to perform a quantitative evaluation of the interpretation method adapted specifically to ViTs [6], in comparison to model-agnostic [25] and attention-based interpretation methods [4], on the example of medical imaging.",,,1,related
For TransLRP we utilize the implementation from the original work [6].,,,1,related
"As a current state-of-the-art approach to explaining ViTs, we rely on the TransLRP algorithm proposed in [6].",,,1,related
"Several XAI methods have been proposed or adapted for ViTs [1, 6], yet a rigorous and standardized evaluation of these methods in terms of their quality of explanations is still lacking.",,,0,not_related
"The problem of noisy activation is pervasive across various research papers, including explainability methods for convolutional neural networks (CNNs) [47,56], vision transformers [9], and CLIP [31,10].",,,0,not_related
"with previous explainability works, including similarity map of original CLIP, Grad-CAM [47] for CNN, pLRP [28] implemented by [9] for multiple layers, Bi-",,,0,not_related
CLIP; see results of Bi-Modal [8] built upon explainability of ViT [9] and gScoreCAM [10] for CLIP-based localization in Fig.,,,1,related
Recent methods [9] have focused on explainability for vision transformers [16] based on self-attention and gradient.,,,0,not_related
"[9] proposed a new method for information propagation within Transformer model components based on LRP attribution, which comprehensively understands the decision-making and inference processes within the model.",,,0,not_related
"[9, 8], however, individual attention maps provide limited representation of the overall behavior of the model.",,,0,not_related
"In transformer-based architectures, the extracted attention maps from each layer represent a good indicator of support activations for the final classification, but they do not reflect the combined attention scores and the other components of the transformer model [9].",,,0,not_related
"Consequently, the class-regional token attention relevance scores [2] are observed in left Fig.",,,0,not_related
"(a) Relevance maps [2] visualizing attention between [CLS] and regional token (Left: vanilla ViT, Right: SAT).",,,0,not_related
"Visualizing attention is the basis of saliency map approaches specific to Computer Vision for Vision Transformers [24, 25].",,,0,not_related
6 with the Transformer model’s attention visualization tool provided by Chefer [30].,,,0,not_related
We show the attention distribution of ViT and SViT on images in Fig.6 with the Transformer model’s attention visualization tool provided by Chefer [30].,,,1,related
"Specifically, we compare the results using raw attention, CAM (Zhou et al., 2016), and Transformer-LRP (Chefer et al., 2021) in Table 4.",,,1,related
", 2016), and Transformer-LRP (Chefer et al., 2021) in Table 4.",,,0,not_related
"To figure out how interpretability contributes to the model prediction, we adopt the positive and negative perturbation tests presented in (Chefer et al., 2021) to give both quantitative and qualitative evaluations on the interpretability.",,,1,related
"Decomposition-based approaches (Bach et al., 2015; Montavon et al., 2017; Chefer et al., 2021) propagate the final prediction to the input following the Deep Taylor Decomposition (Montavon et al., 2017).",,,0,not_related
"3 EVALUATION OF THE INTERPRETABILITY To figure out how interpretability contributes to the model prediction, we adopt the positive and negative perturbation tests presented in (Chefer et al., 2021) to give both quantitative and qualitative evaluations on the interpretability.",,,1,related
"Although Luo et al. (2016) propose to measure the ERF for CNNs, it cannot be directly implemented to Transformer-base models.",,,0,not_related
"Decomposition-based approaches (Bach et al., 2015; Montavon et al., 2017; Chefer et al., 2021) propagate the final prediction to the input following the Deep Taylor Decomposition (Montavon et al.",,,0,not_related
"As most previous methods focus on CNNs, Chefer et al. (2021) propose ViT-LRP tailored for vision Transformers.",,,0,not_related
"…challenging vision related tasks such as image segmentation [Amit et al. 2021], domain adaptation [Song et al. 2022], image editing [Avrahami et al. 2022; Hertz et al. 2022; Tumanyan et al. 2022a], personalization [Gal
et al. 2022, 2023; Ruiz et al. 2022], and explainability [Chefer et al. 2021].",,,0,not_related
", 2022), data augmentation (Shorten & Khoshgoftaar, 2019), interpretability (Chefer et al., 2021), self-supervised pretraining (He et al.",,,0,not_related
"…of fast-evolving and well-studied computer vision techniques in the time series domain, such as better model architecture (Liu et al., 2022), data augmentation (Shorten & Khoshgoftaar, 2019), interpretability (Chefer et al., 2021), self-supervised pretraining (He et al., 2022), to name a few.",,,0,not_related
"3 Self-Attention Self-Attention (SA) draws on the attention mechanism of the human brain when looking at objects, and only selects some key information inputs for processing to improve the efficiency of the neural network[16], [17].",,,0,not_related
"Some more recent works have also proposed versions of post-hoc algorithms tailored for the transformer model.(27,28)",,,0,not_related
"For instance, in [38], layer-wise relevance propagation is applied to transformers, and [8]",,,0,not_related
mean-intersection-over-union (mIoU) metric [72] and compare with eight explanation approaches.,,,0,not_related
", CAM or Grad-CAM) to other backbones like Transformer [9, 36] and graph neural networks [3, 12, 55].",,,0,not_related
"While most of these visualization approaches to interpretation of model predictions were originally developed for image classification models, they have been extended or modified for other tasks [11, 72] or other deep learning models [9, 38].",,,0,not_related
"For the
other type of deep learning model backbone Transformer and its variants (e.g., ViT [14], Swin Transformer [42]), since most items in the input sequence at each model layer correspond to components (e.g., words for a sentence input, image patches for an image input) of the original input, the final model prediction also largely depends on the collection of local features of the original input.",,,0,not_related
"In contrast, for each representative baseline method, the importance maps often
change over model backbones and even may not work for the Transformer backbone ViT and SwinT.",,,0,not_related
"Substantial efforts are often required to adapt one visualization approach to various tasks (e.g., image caption) with different model backbones (e.g., Transformer backbone) or input formats (e.g., sequence of items).",,,0,not_related
"Because the proposed PAMI framework can consider the
well-trained model as a black-box, it can potentially work for various backbone structures (e.g., both CNN and Transformer backbones).",,,0,not_related
"In contrast, the majority of interpretation methods were proposed for the CNN backbone, and specific modifications are often required when applying existing interpretation methods (e.g., CAM or Grad-CAM) to other backbones like Transformer [9, 36] and graph neural networks [3, 12, 55].",,,0,not_related
"[9] Hila Chefer, Shir Gur, and Lior Wolf.",,,0,not_related
"In Figure 8, we further conduct visualization experiments(Chefer, Gur, and Wolf 2021) to show the effectiveness of X-ReID.",,,1,related
"Explainability refers to the ability of ViT to provide insights into its decisionmaking process, which is crucial for building trust in the model [1, 4, 47].",,,0,not_related
"Specifically, we use IntGrad (Sundararajan et al., 2017) with n=32 steps, ‘Input×Gradient’ (IxG), cf. Adebayo et al. (2018), as well as an adapted GradCAM (Selvaraju et al., 2017) as in Chefer et al. (2021).",,,1,related
The significant gains in quantitative interpretability metrics reported by Chefer et al. (2021) highlight the importance of such holistic explanations.,,,0,not_related
"Second, we evaluate two pixel perturbation metrics, cf. Chefer et al. (2021).",,,1,related
"In response, various improvements over attention rollout have been proposed, such as GradSAM (Barkan et al., 2021) or an LRP-based explanation method (Chefer et al., 2021), that were designed to more accurately reflect the computations of all model components.",,,0,not_related
", 2021) or an LRP-based explanation method (Chefer et al., 2021), that were designed to more accurately reflect the computations of all model components.",,,0,not_related
"Further, we evaluate architecture-agnostic methods such as Integrated Gradients (IntGrad) (Sundararajan et al., 2017), adapted GradCAM (Selvaraju et al., 2017) as in Chefer et al. (2021), and ‘Input×Gradient’ (IxG), cf. Adebayo et al. (2018).",,,1,related
"In contrast to attention explanations, which are not class-specific (Chefer et al., 2021), we find the model-inherent explanations of B-cos ViTs to be highly detailed and class-specific.",,,1,related
"For the last, we rely on the implementation provided by Chefer et al. (2021).",,,1,related
"For all these transformer-specific explanations, we rely on the implementation provided by Chefer et al. (2021).",,,1,related
"However, as transformers consist of many additional components, explanations derived from attention alone have been found insufficient to explain the full models (Bastings & Filippova, 2020; Chefer et al., 2021).",,,0,not_related
"First, we follow Chefer et al. (2021) and evaluate common transformerspecific explanations such as the attention in the final layer (FinAtt), attention rollout (Rollout) (Abnar & Zuidema, 2020), a transformer-specific LRP implementation (CheferLRP) proposed by Chefer et al. (2021), ‘partial…",,,1,related
"The configurations of the ViTs follow the conventional specifications for ViTs of size Ti, S, and B, cf. Chefer et al. (2021).",,,1,related
"On the conventional ViTs, we further evaluate LRP-based explanations: partial LRP (pLRP) Voita et al. (2019) and the transformer-specific LRP adaptation by Chefer et al. (2021) (CheferLRP).",,,1,related
"For all models, we rely on the implementation by Chefer et al. (2021), which we use unchanged for the conventional ViTs and modify as we describe below for the B-cos ViTs (C.1.1).",,,1,related
"However, instead of deriving an explanation ‘post-hoc’ as in Chefer et al. (2021), we explicitly design our models to be holistically explainable.",,,1,related
"…explanations such as the attention in the final layer (FinAtt), attention rollout (Rollout) (Abnar & Zuidema, 2020), a transformer-specific LRP implementation (CheferLRP) proposed by Chefer et al. (2021), ‘partial LRP’(pLRP) (Voita et al., 2019), and ‘GradSAM’ (Barkan et al., 2021).",,,1,related
"Transformer visualizations are not limited to the only ones listed here as new techniques are continually suggested in scientific publications which shows how versatile Transformer models are (Chefer et al., 2021).",,,0,not_related
"ViT-B
Fake-CAM [39] 62.8 54.0 57.7 47.9 99.8 28.6 0.87
Grad-CAM [46] 79.6 74.3 29.4 45.0 58.1 31.0 3.27 Grad-CAM++ [11] 84.2 80.6 14.8 23.8 51.4 27.3 4.15 Score-CAM [56] 77.6 71.6 46.0 54.3 66.1 33.1 3.14 XGrad-CAM [20] 82.0 76.9 19.6 41.3 52.8 28.5 3.31 Layer-CAM [26] 70.7 63.9 20.6 50.5 60.7 32.6 1.44 ExPerturbation [18] 71.5 64.9 35.9 44.6 62.3 35.3 1.34 RawAtt [16] 72.4 64.8 18.5 50.4 55.4 31.6 1.68 Rollout [1] 67.6 58.8 36.9 50.7 57.8 30.0 1.16 TIBAV [12] 70.1 63.1 26.6 58.8 66.1 35.0 1.23 Opti-CAM (ours) 64.4 54.6 54.5 48.0 58.2 28.7 0.98
DeiT-B
Fake-CAM [39] 61.4 54.0 57.7 47.9 99.8 28.7 0.83
Grad-CAM [46] 65.5 60.3 44.3 47.2 62.8 30.2 1.20 Grad-CAM++ [11] 70.6 67.2 34.3 43.6 57.7 30.3 2.14 Score-CAM [56] 79.9 76.2 31.9 43.8 63.4 32.2 3.14 XGrad-CAM [20] 82.0 78.4 19.5 44.1 53.4 28.8 3.03 Layer-CAM [26] 80.2 77.3 17.6 50.8 62.7 35.1 3.15 ExPerturbation [18] 69.9 64.3 36.2 44.2 63.1 35.5 1.16 RawAtt [16] 73.5 68.2 5.9 48.1 46.5 27.3 1.91 Rollout [1] 63.9 57.0 27.8 47.9 36.5 27.2 0.94 TIBAV [12] 68.2 62.2 28.1 59.6 64.1 33.5 1.08 Opti-CAM 62.3 55.1 53.9 48.0 55.1 28.8 0.84
Table A8: Localization metrics with ViT and DeiT on ImageNet validation set.",,,0,not_related
"TIBAV [12], which is designed for transformers, outperforms the other methods on DeiT and ViT.",,,0,not_related
"METHOD DEIT-B VIT-B
I↑ D↓ I↑ D↓
Fake-CAM [39] 57.5 34.2 57.4 33.3
Grad-CAM [46] 61.8 17.5 62.9 19.8 Grad-CAM++ [11] 60.5 21.9 56.7 29.3 Score-CAM [56] 60.6 24.4 66.5 15.1 XGrad-CAM [20] 55.2 31.1 55.6 26.5 Layer-CAM [26] 61.6 21.2 62.9 14.6 ExPerturbation [18] 62.1 27.0 64.4 18.4 RawAtt [16] 56.3 29.3 62.2 17.9 Rollout [1] 56.7 32.8 64.8 15.2 TIBAV [12] 63.7 16.3 66.1 14.1 Opti-CAM (ours) 59.2 22.8 60.5 22.0
Table A6: I/D: insertion/deletion [36] scores on ImageNet validation set; ↓ / ↑: lower / higher is better.
observation holds for deletion.",,,1,related
"For transformer models, we also compare against raw attention [16], rollout [1] and TIBAV [12]7.",,,1,related
TIBAV [12] uses both instance-specific and class-specific information.,,,0,not_related
"As an active research topic, current attention visualization methods mainly focus on dot-product attention [1, 11].",,,0,not_related
"The interpretability of the Transformer-based model is closely tied to the basis of this model, which is the mechanism of attention [4], as the attention matrix provides deep insight into the object and relations in it [5].",,,0,not_related
The comparison of attention maps in FBKD and FBKD-ProC-KD (ours) by using the Transformer Interpretability method [43].,,,1,related
"We provide the attention relevance maps [9] for the same image as shown in Figure 13 in the main paper, but for all three classes present in the image, in Figure 28.",,,1,related
Attention relevance (as in [9]) can significantly change at different patch sizes for both ViT and FlexiViT.,,,0,not_related
Attention relevance patterns across scales We find that decreasing the patch size results in attention relevance [9] to concentrate into a larger number of smaller areas throughout the image.,,,0,not_related
Top: Attention relevance (as in [9]) can significantly change at different patch sizes.,,,0,not_related
"2 [9] Hila Chefer, Shir Gur, and Lior Wolf.",,,0,not_related
"D.2 Measuring negative contribution While Shapley values estimate both the positive and the negative contributions of input tokens towards the model prediction – which is relevant for foil words –, attention (Chefer et al., 2021a) allows for positive-only relevance assessments.",,,0,not_related
"In Figures 10 and 11, we have visualised CLIPs attention-based relevancy for the image-caption and foil examples shown in Figures 2 to 7 using the method of Chefer et al. (2021a).",,,1,related
"…to assign relevancy values for image and text tokens, research strives to generate simple explanations that represent the most important tokens and tend to inhibit the rest, as can be seen on the progress from Chefer et al. (2021b) to Chefer et al. (2021a) (cf. Figure 4 in Chefer et al. (2021a)).",,,0,not_related
"CAM-, or propagation-based methods [39].",,,0,not_related
"However, some of these methods are classagnostic in practical applications [39].",,,0,not_related
"Propagation-based methods [39, 44-51] mostly rely on the deep Taylor decomposition (DTD) framework [44].",,,0,not_related
Raghu et al. (2021) compare how the internal representation structure and use of spatial information differs between ViTs and CNNs. Chefer et al. (2021) produce ‘image relevance maps’ (which resemble saliency maps) to promote interpretability of ViTs.,,,0,not_related
"Furthermore, the final layer in ViTs appears to behave as a learned global pooling operation that aggregates information from all patches, which is similar to its explicit averagepooling counterpart in CNNs.",,,0,not_related
"In other words, ViTs learn to preserve spatial information,
despite lacking the inductive bias of CNNs. Spatial information in patches of deep layers has been
explored in Raghu et al. (2021) through the CKA similarity measure, and we further show that spatial information is in fact present in individual channels.",,,0,not_related
"After probing the role of spatial information, we delve into the behavioral differences between ViTs and CNNs.",,,0,not_related
"When performing activation maximizing visualizations, we notice that ViTs consistently generate higher quality image backgrounds than CNNs.",,,1,related
"As extensive work has been done to understand the workings of convolutional networks, including similar feature visualization and image reconstruction techniques to those used here, we may be able to learn more about ViT behavior via direct comparison to CNNs.",,,0,not_related
Chefer et al. (2021) produce ‘image relevance maps’ (which resemble saliency maps) to promote interpretability of ViTs.,,,0,not_related
"Given their rapid proliferation, there is naturally great interest in how ViTs work and how they may differ from CNNs.",,,0,not_related
"We visualize the saliency maps [12] of DeiT-S and OAMixer on top of it, trained on ImageNet9.",,,1,related
"First, the advance of unsupervised [11, 36, 50] and weakly-supervised [12, 43, 60] saliency detection significantly reduced the labeling cost of objects.",,,0,not_related
The last two columns are the attention maps extracted by the method [93] to demonstrate the focus area of the model without and with the proposed AP modules.,,,0,not_related
"C V
] 2
7 N
ov 2
terpretability methods, like CAM [46] and Transformerinterpretability [5], can support such an argument, such as in the work of [61].",,,1,related
"terpretability methods, like CAM [46] and Transformerinterpretability [5], can support such an argument, such as in the work of [61].",,,0,not_related
"Finally, we perform visualization experiments using the (Chefer, Gur, and Wolf 2021) method to show the focused areas of the model.",,,1,related
"When ViT first demonstrate its powerful performance to outperform previous CNN-based baselines [8], its unique model structure has attracted extensive focus from researchers to understand its interpretability from different aspects, including: observing the attention map of Transformer outputs [3,15], computing the relevancy of different attention heads in Transformer networks [5, 26].",,,0,not_related
We applied the method described in [6] to interpret the learned representations from three selfsupervised learning approaches.,,,1,related
"Furthermore, we visualize the attention from the pre-trained weights using the method described in [6].",,,1,related
"In comparison with CNNbased neural networks, the recently developed attention-based transformer models represent a potential paradigm change in the middle of the 2020s.(25) When compared with CNNs, the recovered features from transformers can more accurately reflect long-range dependency within the sequence, and they also carry more semantic information.",,,0,not_related
"The flexibility of Transformers for various input modalities [226] and the inherent interpretability of learned attention scores [227], [228] lend themselves well to the self-driving domain.",,,0,not_related
"…DeepLift, LRP, etc. (Simonyan et al., 2014; Smilkov et al., 2017; Shrikumar et al., 2017; Sundararajan et al., 2017; Xu et al., 2020; Selvaraju et al., 2017; Bach et al., 2015; Shrikumar et al., 2016; Chefer et al., 2021; Montavon et al., 2017; Shrikumar et al., 2017; Schwab & Karlen, 2019).",,,0,not_related
The recent approaches presented in [23] and [24] go beyond interpretability solely based on attention weights and generate model explanations by utilizing all components in the transformer architecture.,,,0,not_related
"In the future, other than class activation maps, we will seek to explore the explainability for Vision Transformers in multi-label classification tasks, with the help of self-attention derived from the Transformer architectures [12, 79, 1, 13].",,,1,related
’s method [190] can reach better performance.,,,0,not_related
[190] proposed a novel method for computing the correlation of the Transformers network.,,,0,not_related
[190] proposed to explore attention visualization to interpret the principle of Transformer.,,,0,not_related
"For non-parametric layers (add layer and matrix multiplication), the authors of [7].",,,0,not_related
"To circumvent the black-box nature of the Deep Neural Networks, recent studies have tried explaining their decisions [8, 9, 10, 11, 7].",,,0,not_related
The computation of the weighted attention map is inspired by [15] and [7].,,,0,not_related
"On the contrary, their method is class-specific, as [7] integrates the relevance scores with the gradient of attention w.",,,0,not_related
Our method is compared with the state-of-the-art explainers with a recently adapted version of Relevance Propagation for the transformers [7] being amongst them.,,,1,related
"In [16], the authors of [7] extended their work for coattention methods performing multimodal input (images and text) and encoder-decoder attention.",,,0,not_related
"Heat maps built upon explanation maps are obtained on a number of state-of-the-art methods such as Grad-Cam [10], adapted relevance propagation [7], rollout method [15], and our self-attention weighted method (SAW).",,,0,not_related
"al [7] proposed a transformer explanation inspired by LRP [11] method, which was developed for explaining the decisions of CNNs.",,,0,not_related
"In particular, Yuan et al. (2021) and Chefer et al. (2021) are promising examples of approaches based, partially or entirely, on gradient information.",,,0,not_related
"In particular, there have been some approaches based, partially or entirely, on gradient information, such as GradCam Yuan et al. (2021); Chefer et al. (2021).",,,0,not_related
"Specifically, given a vision transformer model F and a sampled pair (x,y), we first acquire the class activated matrix C of the b−th block of F following [6], which can be formulated as:",,,1,related
"In this experiment, we verify the improvement of AdsCVLR compared with single modal models on these “hard” samples through a visualization method [1].",,,1,related
"We make a visualization on each level in Figure 6, we use the method proposed in [40,41] to generate the visualization maps.",,,1,related
"Ablation experiments and a variety of representation analyses have been applied to understand the role of the attention mechanism in NLP tasks as well as in transformer-based vision models (Chefer et al., 2021; Manning et al., 2020; Michel et al., 2019; Voita et al., 2019).",,,0,not_related
"Besides, Zabari & Hoshen (2021) obtains segments via interpretability Chefer et al. (2021b) based on gradient, but owing to the limited localization quality, additional unsupervised segmentation algorithm is required.",,,1,related
"To get rid of costly pixel-level annotations, some works generate the segments by retrieval Shin et al. (2022), grouping Xu et al. (2022) or unsupervised segmentation Zabari & Hoshen (2021) with gradient-based interpretability method Chefer et al. (2021b).",,,1,related
"We firstly compare our dense ITSM with smoothed min pooling with gradient based visualization method Bi-Module Chefer et al. (2021a), and our method performs much better than it.",,,1,related
"Head operations commonly found in the literature are averaging [6, 12, 33] and summing [34, 35] the attention matrices of each head.",,,0,not_related
"A recent method that does not solely rely on raw attention to provide explanations, is combining relevance and gradient information [6].",,,0,not_related
Operations concerning the resulting matrices of self-attention layers include averaging [35] and multiplying [6].,,,0,not_related
Other transformer-specific interpretability approaches combine attention with gradient information [6] or compute new attentions based on the network’s residual connections [7].,,,0,not_related
"To obtain the final interpretation vector, a common approach is to consider the attention that each input token receives from the special [CLS] token that is prepended at the beginning of sequences in text classification tasks [6, 12].",,,0,not_related
[10] expand the class-agnostic self-attention of rollout to class-specific.,,,0,not_related
"[9] (Bi-Model) is the latest method based on [10] for transformer, and [33] (Grad-CAM) is the most used gradient based method.",,,0,not_related
"Note, Bi-Model [9] is based on the latest explainability method [10] for ViT.",,,1,related
"Besides, Bi-Model [9] based on [10] treats CLIP as ViT [14] and explains it with self-attention, even the quality of self-attention in CLIP is bad (see Fig.",,,0,not_related
"Most existing methods for visual explainability are designed for convolutional networks [41, 44] or vision transformers [9, 10].",,,0,not_related
"Another explainable method [9] treats CLIP as vision transformer [10] and relies on the self-attention of ViT [14] to explain the images, followed by rollout [1] to expand classagnostic attention map into class-specific.",,,0,not_related
The literature (Chefer et al. 2021) proposed a DTD-based decomposition algorithm to solve the problems caused by residual connection and matrix multiplication operations.,,,0,not_related
The power of these methods has led to their adoption in the field of language and vision (Chefer et al. 2021; Lu et al. 2019).,,,0,not_related
"[28], which leads to low credibility of the final results.",,,0,not_related
"Here the method described in [3] is used, the corresponding values per token are referred to as “attention weights” here.",,,1,related
"For ViTs, some reasons the decision-making process via gradients [10, 14, 15], attributions [6, 41] and redundancy reduction [26].",,,0,not_related
"To visualize the regions of the AS-OCT images that contributed to the model’s decisions, Gradient-weighted Class Activation Mapping (Grad-CAM) [39] will be extracted from the first LN of the last block of the transformer encoder.",,,0,not_related
"And [49], [80], [219] expanded on this approach toward the goal of multi-step attribution across multiple layers.",,,0,not_related
"Recent studies have therefore been attempting to improve the quality and sharpness of the maps by focusing on objects, for example, by combining Grad-CAM and LRP [25], applying LRP to Vision Transformers [5], improving ABN with Score-CAM [26], and even human intervention [33] or additional supervision [27].",,,0,not_related
The latest researches’ results [60]–[65],,,0,not_related
"In contrast to pure self-attention based ViTs, the enhanced visual interpretability of hybrid ViTs [28, 54] allows us to extract these salient regions utilizing general methods such as gradient based class activation maps (Grad-CAM) [4, 38].",,,0,not_related
"It is a interesting future direction to investigate how different VLMs [7–11], their training procedure [43], and different relevancy approaches [18, 22, 44, 45] affect the performance of different downstream 3D scene understanding tasks.",,,0,not_related
"Our intuition is that the neural activation map of even a partially trained classification network can better localize some part of an object [40,4] than using naive score averaging.",,,0,not_related
"Although it is wrong to equate attention scores with explanation [44], it can offer plausible and meaningful interpretations [45], [46].",,,0,not_related
[13] proposed a layer-wise relevance propagation (LRP) method by introducing a relevancy propagation rule that is applicable to both positive and negative contributions.,,,0,not_related
[13] proposed a layer-wise relevance propagation (LRP) method to compute different relevance of the attention heads throughout the transformer.,,,0,not_related
"Alternatives include methods to compute and propagate trained attention-based token relevancy scores (Chefer et al., 2021), or to generate higher-level conceptual explanations (Rigotti et al., 2021).",,,0,not_related
"Alternatives include methods to compute and propagate trained attention-based token relevancy scores (Chefer et al., 2021), or to generate higher-level conceptual explanations (Rigotti et al.",,,0,not_related
"For each layer, we present the patch-to-patch attention matrix (size: 180×180) calculated by the rollout method in [32].",,,1,related
[32] enhanced this method by adding an additional identical matrix before multiplication to simulate the effect of residual connection of MSA.,,,0,not_related
(d) visualizes the impact of each spatial location on the final prediction in the DeiT-S model [47] using the visualization method proposed in [4].,,,0,not_related
"It is also quite challenging to interpret vision transformers’ decisions [66], e.",,,0,not_related
"To investigate the performance of our model, we used the class activation map (CAM) [55] to visualize the attention maps generated by our ACSI-Net.",,,1,related
"With the introduction of the deep neural network model with attention, its attention weights also play an important role in the XAI field[23,50,6].",,,0,not_related
"The third term considers the explainability map of CLIP, given the input image and the caption [13] as a guide for the foreground mask.",,,0,not_related
"We present visualizations of target class activation maps using the recent Transformer Explainability [2]
for several images in Figure 5 to showcase the behavior of SPViT.",,,1,related
Figure 5: Visualization using Transformer Explainability [2].,,,0,not_related
We present visualizations of target class activation maps using the recent Transformer Explainability [2] for several images in Figure 5 to showcase the behavior of SPViT.,,,1,related
"These methods achieve mixed results in quantitative benchmarks, whether for object localization or the removal of influential features [42, 25, 49, 10, 31], and they are somewhat insensitive to the randomization of model parameters [2].",,,0,not_related
"Among the baselines, RISE and LRP remain most competitive, but ViT Shapley performs best for both datasets.",,,0,not_related
"Among those shown here, LRP is most similar to ViT Shapley, but they disagree in several cases.",,,0,not_related
"Attention last This approach calculates the attention directed from each image token into the class token in the final self-attention layer, summed across attention heads [1, 10].",,,0,not_related
"Other methods modify the gradient backpropagation algorithm to generate attribution scores that satisfy certain properties [4, 53, 3], including the layer-wise relevance propagation (LRP) approach that was recently extended to transformers [10].",,,0,not_related
"Recent work has disputed the role of attention as an indicator of feature importance [51, 29, 63, 10], and we find in our experiments that attention is a poor proxy for the effect of removing features from a model.",,,0,not_related
"For attention-based methods, we use attention rollout [1] and the last layer’s attention directed to the class token [1, 10].",,,1,related
"Layer-wise relevance propagation (LRP) Originally described as a set of constraints for a modified backpropagation routine [4], LRP has since been implemented for a variety of network layers and architectures, and it was recently adapted to ViTs [10].",,,0,not_related
"We show results for attention last, attention rollout, Vanilla Gradients, Integrated Gradients, SmoothGrad, LRP, leave-one-out, and ViT Shapley only; we excluded VarGrad, GradCAM and RISE because their results were less visually appealing.",,,1,related
We used an implementation provided by prior work [10].,,,1,related
"Our evaluation was conducted on a GeForce RTX 2080 Ti GPU, with minibatches of 16 samples for attention last, attention rollout and ViT Shapley; batch size of 1 for Vanilla Gradients, GradCAM, LRP, leave-one-out and RISE; and internal minibatching for SmoothGrad, IntGrad and VarGrad (implemented via Captum [35]).",,,1,related
"Similar to prior work [10], we did not use attention flow [1] due to the computational cost.",,,1,related
"(↑)
Attention last - - - Attention rollout - - -
GradCAM 0.021 (0.002) 0.005 (0.000) -0.672 (0.015) IntGrad 0.008 (0.001) 0.004 (0.000) 0.294 (0.022) Vanilla 0.006 (0.001) 0.020 (0.001) -0.682 (0.015) SmoothGrad 0.006 (0.001) 0.006 (0.001) -0.683 (0.015) VarGrad 0.006 (0.001) 0.006 (0.001) -0.680 (0.015) LRP 0.004 (0.001) 0.022 (0.001) -0.680 (0.015)
Leave-one-out 0.013 (0.002) 0.003 (0.000) -0.017 (0.028) RISE 0.023 (0.003) 0.002 (0.000) -0.681 (0.015) ViT Shapley 0.093 (0.004) 0.001 (0.000) 0.672 (0.014)
Random 0.005 (0.001) 0.005 (0.001) -",,,1,related
"RISE and LRP tend to be the most competitive baselines, and perhaps surprisingly, certain other methods fail to outperform a random baseline (GradCAM, SmoothGrad, VarGrad).",,,0,not_related
"Next, for gradient-based methods, we use Vanilla Gradients [54], Integrated Gradients [57], SmoothGrad [55], VarGrad [25], LRP [10] and GradCAM [50].",,,1,related
"We used existing LRP and GradCAM implementations for ViTs [10, 23], and the remaining gradient-based methods were run using the Captum package [35].",,,1,related
"Following the evaluation scheme in (Samek et al., 2016; Feng et al., 2018; Chefer et al., 2020), given an input x and an attribution map, we rank the map elements by ascending importance.",,,1,related
Metrics and dataset are taken from [9].,,,0,not_related
"the combination of gradients and attention values has been shown to produce a viable interpretation of the model’s prediction [8, 9].",,,0,not_related
"Second, we conduct segmentation tests following [9] to assess the effect of our method on the level of agreement between the relevancy maps and the foreground segmentation maps.",,,1,related
"Segmentation tests Since our motivation is to encourage the relevance to focus less on the background and more on as much of the foreground as possible, we test the resemblance of the resulting relevance maps to the segmentation maps following [9].",,,1,related
We visualize the activated area of our M3T network based on transformer interpretability technique [7].,,,1,related
"Third, we visualize the activated area in 3D MRI images the transformer interpretability methods [7].",,,1,related
[9] study relevancy for Transformer networks in computer vision by assigning local relevance based on the Deep Taylor Decomposition principle [27].,,,0,not_related
[24] visualize attention layer in an image more clearly.,,,0,not_related
"For ViT, attention visualization and interpretability [24] are important factors in its success, but for MLP-Mixer and similar methods, it’s unclear what are learned from training.",,,0,not_related
"Our technique was inspired by the recent work by Chefer and colleagues [39], who used",,,0,not_related
"Following the propagation procedure of relevance and gradients by Chefer and colleagues [39], GraphCAM computes the gradient ∇ A(l) and layer relevance R(nl ) with respect to a target class for each attention map A(l), where nl is the layer that corresponds to the softmax operation in Eq.",,,0,not_related
"Prior works [6, 63, 1, 14] using transformer networks in natural language have re-purposed the attention weights in the later layers as an mechanism to introspect model logic.",,,0,not_related
"Another work uses a Deep Taylor Decomposition approach to visualize portions of input image leading to a particular ViT prediction (Chefer et al., 2021).",,,0,not_related
"Note that our shared attention differs from the coattention introduced in prior works [7], where the value and key are passed via a skip connection from the encoder layers.",,,1,related
"Regarding the DeiT, we refer the reader to [216], 1335 which proposed a framework to generate LRP attributions for 1336 Transformer-based architectures.",,,1,related
"From increasing users’ trust [9, 27, 39] and improving interpretability [4, 10, 19, 20, 24] to debugging [31].",,,0,not_related
Class-specific behavior is also introduced using Contrastive-LRP [12] and Softmax-Gradient LRP [15].,,,0,not_related
"Post-hoc work [1, 7] has interpreted and visualized",,,0,not_related
"In the second row we observe that the negative contributions from the noisy background observed in the attention maps of DeiT-B + pADL + AR are alleviated in ViTOL-GAR/LRP.
Visualization on CUB: In Figure 5, first row, we showcase five random example images from the CUB dataset.",,,1,related
"For each of the example images, we visualize the baseline DeiT-B with AR, GAR and LRP in the second, third and fourth columns, and, DeiT-B + p-ADL with AR, ViTOL with LRP and ViTOL with GAR attention maps in the final three columns.",,,1,related
"In Section 4, we also showcase results for an alternative post-hoc approach called Layer Relevance Propagation [7].",,,1,related
We observe that our approaches with a DeiT-B backbone with p-ADL + (a) GAR and (b) LRP significantly outperform the other WSOL approaches.,,,1,related
[7] proposed an alternate method of assigning local relevance based on the DTD principle to generate class dependent attention maps.,,,0,not_related
"[7], this overlooks the fact that GELU [14] activation is used in all intermediate layers.",,,0,not_related
[7] examined the problems in attention rollout and proposed a method that,,,0,not_related
"In the second row, we overlay the attention maps obtained from ViTOL-LRP for these images.",,,1,related
We compare this against attention rollout (AR) mechanism [1] and layer relevance propogation (LRP) [7] for generating class dependent attention maps.,,,1,related
"Training and Testing details: On ImageNet-1K, for baseline models, we use the DeiT-B and DeiT-S ImageNet pre-trained weights and evaluate on all the methods, namely, AR, GAR and LRP as stated in Table 1, 2 and 3.",,,1,related
"We observe that attention maps generated for ViTOL with GAR/LRP show dependency with the class, are noisefree and cover the complete object of interest.",,,1,related
"Other ablations: Some other ablation studies include i) comparison of GAR and LRP across different transformer backbones, ii) detailed study of patch drop masks for the pADL layer and iii) effect of changing embedding drop rate
and drop threshold hyper-parameters for p-ADL layer.",,,0,not_related
For more details we refer the reader to [7].,,,1,related
"However, the literature on Transformer explainability is relatively sparse and most methods focus on pure self-attention architectures [1, 8].",,,0,not_related
"Attention Visualization: In order to visualize the parts of the facial image that contributes to the category clarification, we apply the visualization method of [6] to visualize the attention maps in the transformer.",,,1,related
An identity matrix I is added to avoid self-inhibition of patches [40].,,,0,not_related
"Unlike conventional gradient-based visualization techniques [39], we use an attention-oriented visualization similar to a visual work [40] to highlight the FT patches the model pays the most attention to by inferring both the gradient information and the relevance from the final classification decision to each attention layer.",,,0,not_related
"In brief, Zabari & Hoshen (2021) create a set of query-driven relevance maps for an image, coupled with transformer interpretability methods (Chefer et al., 2021).",,,0,not_related
"Attempts [55, 42, 9] have been made to interpret image classification models.",,,0,not_related
[9] went beyond attention visualization by adopting gradients and the propagation of relevancy scores.,,,0,not_related
"[7] proposed a method for visualizing self-attention models by calculating a LRP [1]-based relevancy score for each attention head in each layer, and propagating relevancies through the network.",,,0,not_related
"More recent work visualizes the attention maps in transformers [8,9,47].",,,0,not_related
We visualize the attention maps of transformers using Transformer Explainability [6].,,,1,related
"12, we visualize the attention maps of different transformer models on spoof images using Transformer Explainability [6].",,,1,related
"7, we visualize the attention maps of different transformers on spoof images using Transformer Explainability [6].",,,1,related
One future direction is to interpret the Transformer by visualizing the attention heatmaps based on the deep Taylor decomposition principle [70].,,,0,not_related
"On the other hand, explaining trained ViTs requires non-trivial and sophisticated methods [4] following the trend of eXplainable AI (XAI) [18] that has been extensively studied with convolutional neural networks.",,,0,not_related
"As pointed out in the Improved LRP [4], reducing the explanation to only the attentions scores may be myopic since many other components are ignored.",,,0,not_related
"As pointed out in the Im-
proved LRP [4], reducing the explanation to only the attentions scores may be myopic since many other components are ignored.",,,0,not_related
Tools of interpreting ViTs in [6] are adopted to produce visualization.,,,0,not_related
Figure 5 is generated by an advanced ViT interpretable approach [6].,,,1,related
", [4] proposed the visualization method of attention weight based on a specific formulation while maintaining the total relevancy in each layer.",,,0,not_related
"Finally, Abnar and Zuidema (2020) proposed the attention rollout method, which measures the mixing of information by linearly combining attention matrices, a method that has been extended to Transformers in the visual domain (Chefer et al., 2021a,b).",,,0,not_related
"Considering the powerful function of the skip-connection [27], we concatenate the two outputs together and utilize the convolution with 1 kernel size to align the scale of the feature map.",,,1,related
"The derived transformer models have shown a flexible adoption in many fields [26], [27].",,,0,not_related
"Unlike original LRP and [7], where the decomposition starts from the classifier output corresponding to the target class, we have a similarity model that rather measures how similar graph embeddings of the time-snapshot graphs Gt andGt+1 are.",,,1,related
"In order to explain the output and pave the way for a better explanation of our model, we utilize the idea from [7].",,,1,related
"On the other hand, the transformer model enjoys higher visual interpretability by the virtue of its inherent selfattention block [22]–[24].",,,0,not_related
"The distributed representations learned by such models result in significant performance gains on a range of tasks, however come with the drawback of storing world knowledge implicitly within their parameters, making post-hoc modification [8] and interpretability [4] challenging.",,,0,not_related
"‘Generic Attention Explainability’ (GAE) by Chefer et al. (2021a) propagates attention gradients together with gradients from other parts of the network, resulting in state-of-the art performance in explaining Transformer architectures.",,,0,not_related
"Other approaches to LRP / gradient propagation in Transformer blocks can be found in (Chefer et al., 2021a;b), where the relevancy scores are obtained by combining attention scores with LRP or attention gradients.",,,0,not_related
"New explanation techniques, such as attention rollouts (Abnar & Zuidema, 2020), or other generic ways to aggregate attention information (Chefer et al., 2021a;b) have also been developed for a similar purpose.",,,0,not_related
Combining Q16 with explainable AI methods such as [Chefer et al. 2021] to explain the reasons is likely to improve the datasheet.,,,0,not_related
"…this, many works hence develop explanation methods by combining attention weights with additional attributions, such as the gradients (Chefer et al., 2021a; Hao et al., 2021), layer-wise relevance (Abnar & Zuidema, 2020; Chefer et al., 2021b), or the norm of
input vectors (Kobayashi et al., 2020).",,,0,not_related
"Correspondence to: Haoliang Li <haoliang.li1991@gmail.com>.
dararajan et al., 2017; Smilkov et al., 2017; Shrikumar et al., 2017; Chefer et al., 2021a; Hao et al., 2021; Kobayashi et al., 2020).",,,1,related
"In line with this, many works hence develop explanation methods by combining attention weights with additional attributions, such as the gradients (Chefer et al., 2021a; Hao et al., 2021), layer-wise relevance (Abnar & Zuidema, 2020; Chefer et al., 2021b), or the norm of
input vectors (Kobayashi et…",,,0,not_related
"…we particularly adopt four explanation methods: Partial Layer-wise Relevance Propagation (PLRP) (Voita et al., 2019), Attention Rollout (Abnar & Zuidema, 2020), Transformer Attention Attribution (TransAtt) (Chefer et al., 2021b), and Generic Attention Attribution (GenAtt) (Chefer et al., 2021a).",,,1,related
"Note that in transformer-based architectures, these methods are implemented based on the last attention layer’s output following (Chefer et al., 2021a).",,,0,not_related
proposed an explainability method in which relevancy is assigned to attention maps and then propagated throughout all blocks [7].,,,0,not_related
"Few works have tried to interpret Transformers further than this for vision [207], and so far within the literature of VTs we only find a limited subset of works that visualize these attention activations for specific samples [66], [69], [99], [130].",,,0,not_related
"As stated in [76], using solely the attention map to explain the model’s reasoning is naive, since there are a lot more layers and processes in a model that contribute to its decisions.",,,0,not_related
We adopt the method presented in [4] which employs Deep Taylor Decomposition to calculate local relevance and then propagates these relevancy scores through the layers to generate a final relevancy map.,,,1,related
Class wise visualisation of ImageNet-1K images with method presented in [4].,,,0,not_related
"Previous work has used Rollout to produce outputagnostic visualizations of the attention weights [14, 8].",,,0,not_related
"lowed by methods focused on networks trained for classification [8, 7].",,,0,not_related
"used as the foundation for output-specific visualizations for Transformer networks used for classification [8, 7].",,,0,not_related
"Take Transformers [26] for example, a naive approach of averaging the attention layer weights for each token would lead to the dilution of signal and would not consider the interaction between layers [27].",,,0,not_related
"The token representations [84,87,10,5] in early and middle layers are insufficiently encoded, which makes token pruning quite difficult.",,,0,not_related
"However, unlike our mechanism, selfattention layers do not distinguish between classes on the same image without additional steps [10].",,,0,not_related
"Techniques have been proposed [3, 16], but they enable the analysis of influential tokens from the input text, rather than words, or they display multiple attention weights relative to the different layers and attention mechanisms inside the model.",,,0,not_related
We contribute to the field by building on the method in [3] to associate consolidated attention weights to words containing the tokens and extending this analysis to the whole test set so that one can leverage this knowledge to assess the behavior of a stance classifier in terms of domain characteristics.,,,1,related
"To get the attention weights for each token, we use the modified LRP technique proposed in [3].",,,1,related
The technique proposed in [3] leverages LRP (Layer-wise Relevance Propagation) to overcome this limitation and summarizes the attention weights using information related to both the relevance and gradient.,,,0,not_related
The attention weights are obtained using the adapted LRP technique proposed in [3] from the results obtained by the classifier.,,,1,related
"The interpretability mechanism expands the attention weights consolidation proposed in [3] in two ways: a) relating tokens to their original words in each instance; and b) proposing an attention score that is significant with regard to a documents set, referred",,,0,not_related
"It extends the technique of [3], which consolidates at instance level the attention weights of the relevant tokens throughout the whole network.",,,0,not_related
"Averaging over the models, we’d achieve 59.74% accuracy with the BM-GAE model and 58.33% using the TRF method compared with 51.04% average accuracy amongst all E-BERT injection models as seen in Table 3.",,,1,related
"We see that for 7 of the 9 models, questions which include E-BERT entities amongst their top 5 using BM-GAE provide better accuracy than those using the TRF method.",,,1,related
The method uses the model’s attention layers to produce relevancy maps for each of the interactions between the input modalities in the network and is a generalization of TRF [4] without Layer-wise Relevance Propagation [1] which itself was shown to be effective on single-modality Transformers that utilize self-attention such as VilBERT[14].,,,0,not_related
"TRF EXPLANATION
NERagro
NERper
KVQAmeta
OKVQA, both without the need to redo any costly pre-training.",,,0,not_related
"[421]:
KVQAmeta
TRF EXPLANATION
NERagro
NERper
entity Fisher Morgan10 is in fact both a singer and actor, the later which KVQAmeta is actually the only model to predict correctly.",,,0,not_related
Over all models E-BERT entities appear in the top 5 most important tokens using TRF more than BM-GAE (10.35 vs 8.59,,,1,related
We extract visual and text explanations using BM-GAE and TRF on our KVQA models.,,,1,related
"FOR METHODS THAT PURSUE OPTIMAL CLASSIFICATION PERFORMANCE WITHOUT A SPECIFIED ATTENTION VISUALISATION MECHANISM, WE RESORT TO GRADCAM [68] AND TRANSRELEVANCE [10] AS TWO GENERIC WAYS FOR CNN AND TRANSFORMER TYPE OF MODELS, WHICH COVERED MOST NETWORK BACKBONE CHOICES ACROSS DISCIPLINES IN COMPUTER VISION WORLD NOWADAYS.",,,1,related
"For works that do not specify a concrete model visualisation approach, we adopt GradCAM [68] and TransRelevance [10] as two generic ways for CNN and Transformer type of models respectively.",,,1,related
"TIBS is essentially a thresholded interpretability map, which despite its knowledge distillation from CLIP, generated relatively noisy segmentations.",,,0,not_related
Transformer Interpretability Based Segmentation (TIBS) [6] .,,,0,not_related
The mapping between the text prompts to per-category relevance maps is performed by utilizing a recently developed transformer interpretation method [6].,,,0,not_related
"The trends are similar to those of PASCAL VOC, our method improves over TIBS.",,,1,related
"To be more specific, inspired by [5], we first obtain the class token xcls i and patch token x patch i of the whole image Ii by ViT encoder:",,,1,related
"By visualizing attention maps of class activation based on method [4], in Fig.",,,0,not_related
"Following [1, 5], rollout applies matrix multiplication across all 12 blocks’ attention matrices.",,,0,not_related
"Then we follow [1, 5] to compute the attention rollout, which aggregate the attention matrices from all blocks by matrix multiplications.",,,1,related
Input saliency Bastings & Filippova (2020) and attribution-propagation Chefer et al. (2020) methods have also been studied as potential tools for model interpretability.,,,0,not_related
"The transformer encoders are arranged after a convolution feature
https://github.com/hila-chefer/Transformer-Explainability
extractor.",,,1,related
"Left to right: input image, rollout [62], raw-attention, GradCAM [72], LRP [73], partial LRP [63], and Transformer-Explainability [23].",,,0,not_related
"We also briefly describe recent developments in visualizing feature maps of ViT models [23, 62, 63], which help to better understand the working mechanism of ViT models.",,,0,not_related
"8, the latest tools specialized for MSA modules and ViT models, namely, partial LRP [63] and TransformerExplainability [23] , can generate better results for feature map visualization than the visualization methods for CNN.",,,0,not_related
"Several researchers have achieved promising progress in unveiling the power of transformer models, from such perspectives as information bottlenecks [149, 150] and better visualization tools [23, 63].",,,0,not_related
Visualization Transformer-Explainability [23] A better tool to visualize feature maps from ViT models,,,0,not_related
"Examples of XAI in research fields outside of MI include: visualizing word embeddings in Natural Language Processing [41–43], inspecting decision-making processes in reinforcement learning [44–46], visualizing pixel importances [47,48], or segmenting in computer vision [49,50].",,,0,not_related
"Our technique was inspired by the recent work by Chefer and colleagues [26], who used the deep Taylor decomposition principle to assign local relevance scores",,,0,not_related
"Following the propagation procedure of relevance and gradients by Chefer and colleagues [26], GraphCAM computes the gradient ∇A and layer relevance Rl with respect to a target class for each attention map A, where nl is the layer that corresponds to the softmax operation in Eq.",,,0,not_related
"Applying explainable AI methods such as (Chefer et al., 2021) to explain the reasoning process could lead to further improvement of the curation process.",,,0,not_related
We contribute to the field by building on the solution proposed in [Chefer et al. 2021] to associate consolidated attention weights to words containing the tokens so that one can leverage this knowledge to assess the behavior of a stance classifier in terms of the domain characteristics and the contribution of this words to the correct classification of the stances.,,,1,related
The technique proposed in [Chefer et al. 2021] leverages LRP (Layer-wise Relevance Propagation) to overcome this limitation and summarizes the attention weights using information related to both the relevance and gradient.,,,0,not_related
The interpretability mechanism proposed expands the attention consolidation proposed in [Chefer et al. 2021] by relating tokens to the original words and associating them to two word attention metrics: absolute and relative.,,,0,not_related
"Some
techniques have been proposed [Chefer et al. 2021, Vig 2019].",,,0,not_related
"To get the attention weights for each token, we use the modified LRP technique proposed in [Chefer et al. 2021].",,,1,related
The attention weights are obtained using the adapted LRP technique proposed in [Chefer et al. 2021] from the results obtained by the classifier.,,,1,related
"It extends the technique of [Chefer et al. 2021], which consolidates the attention weights of the relevant tokens throughout the whole network by relating these tokens and the respective weights to the original words of the input text.",,,0,not_related
We contribute to the field by building on the solution proposed in [Chefer et al. 2021] to associate consolidated attention weights to words containing the tokens so that one can leverage this knowledge to assess the behavior of a stance classifier in terms of the domain characteristics and the…,,,1,related
"Moreover, while the interpretability of Transformers is a research problem on its own [4], MM-MIL has an explicit mechanism for model interpretability.",,,0,not_related
"To further investigate the effectiveness of our approach, we employ the method [5] to visualize the attention maps generated by our TransFER.",,,1,related
Attention visualization [5] of different expressions on some example face images from AffectNet dataset.,,,0,not_related
Attention visualizations [5] on two example images: Sur-,,,0,not_related
"Although they achieve promising results on image classification [50, 2], visual question answering [31], and image generation [1], they cannot explain how visual similarity is composed.",,,0,not_related
"To improve the transparency of deep visual models, many efforts have been made recently by either explaining the existing models [50, 31, 1, 2] or modifying models to achieve better interpretability [46, 47].",,,0,not_related
"For overall comparisons with the state-of-the-art methods (Rao et al. 2021; Tang et al. 2021; Chen et al. 2021; Pan et al. 2021), we conduct the token selection and slow-fast token updating from the fifth layer of DeiT and the third layer (excluding the convolution layers) of LeViT, respectively.",,,1,related
"In Table 1, we compare our method with existing token pruning methods (Rao et al. 2021; Pan et al. 2021; Tang et al. 2021; Chen et al. 2021).",,,1,related
"In order to analyse the explainability properties of our proposed method, we use the Gradient Attention Rollout algorithm as outlined in [87].",,,1,related
"Visualization of different cases (normal, Pneumonia, COVID-19) considered in this study and their associated critical factors in decision making by xViTCOS as identified using the explanability method laid out in [87] for transformers [16].",,,0,not_related
"Visualization of the weights in self-attention has widely been used to provide the semantic relationships between different elements of data [27, 28].",,,0,not_related
"On the other hand, previous works [6, 8] have already shown the vulnerable interpretability of the original vision transformer, where the raw attention comes from the architecture sometimes fails to perceive the informative region of the input images.",,,0,not_related
(c) visualizes the impact of each spatial location on the final prediction in the DeiT-S model [25] using the visualization method proposed in [3].,,,0,not_related
Images from different classes are visualized in Figure 5 using Transformer Attribution method [6] on DeiT-Tiny.,,,0,not_related
"2 [6] Hila Chefer, Shir Gur, and Lior Wolf.",,,0,not_related
Visualization using Transformer Attribution [6].,,,1,related
"We use the attention as the explanation [29, 3].",,,1,related
"For the interpretability of the classification model, we adopted a visualization method of saliency map tailored for ViT suggested by (Chefer et al., 2020), which computes relevancy for Transformer network.",,,1,related
"Since each such map is comprised of h heads, we follow [5] and use gradients to average across heads.",,,1,related
", [5, 1]) heavily rely on self-attention, and do not provide adaptations to any other form of attention, which is commonly used in multi-modal Transformers.",,,0,not_related
"In some cases, when self-attention is prominent, the recent method by Chefer et al. [5] is the only method that can provide comparable results.",,,0,not_related
"6 we account for the fact that the tokens were already contextualized in previous attention layers by applying matrix multiplication with the aggregated self-attention matrix R, as done in [1, 5].",,,0,not_related
"Our explainability prescription is easier to implement than existing methods, such as [5], and can be readily applied to any attention-based architecture.",,,1,related
Following [5] we remove the negative contributions before averaging.,,,1,related
"As noted by Chefer et al [5], the computation in each attention head mixes queries, keys, and values and cannot be fully captured by considering only the inner products of queries and keys, which is what is referred to as attention.",,,0,not_related
"[5] provide a comprehensive treatment of the information propagation within all components of the Transformer model, which back-propagates the information through all layers from the decision back to the input.",,,0,not_related
"[5] demonstrates that this method fails to distinguish between positive and negative contributions to the decision, leading to an accumulation of relevancy scores across the layers in cases where these should be cancelled out.",,,0,not_related
"We present baselines of three classes, following [5]: attention map baselines, gradient baselines, and relevancy map baselines.",,,1,related
"For our gradient baselines, we use the Grad-CAM [32] adaptation described in [5], i.",,,1,related
"In contrast to these methods, Chefer et al. [5] provide a comprehensive treatment of the information propagation
within all components of the Transformer model, which back-propagates the information through all layers from the decision back to the input.",,,0,not_related
"There also exist interpretation tools that specifically leverage the transformer architecture (Chefer et al., 2021, 2020).",,,0,not_related
"As well, these ideas have also been used in Vision Transformers [48] for explaining image classification models [34, 185] or bi-modal transformer models [33].",,,0,not_related
"Furthermore, extended LRPs [34, 169] can be helpful to interpret Transformer models [45, 48, 159].",,,0,not_related
"Different from other methods relying on attention maps or heuristic propagation of attention, the method proposed by [3] assigns local relevance with deep Taylor decomposition, and propagate the local relevance throughout the layers.",,,0,not_related
Fong & Vedaldi (2017) introduce a method for explaining classifiers based on meaningful perturbation and Chefer et al. (2021) introduce a method for improving interpretation for transformer-based classifiers.,,,1,related
We also notice a recent paper [10] that develops LRPbased [2] method to compute relevance to explain the predictions of Transformer.,,,1,related
"The current literature usually analyzes the effect in an intuitive way [55], [23].",,,0,not_related
"In [9], the authors proposed a way to visualize the relevancy maps for Transformer networks.",,,0,not_related
"Specifically, we compare the results using raw attention, CAM (Zhou et al., 2016), and Transformer-LRP (Chefer et al., 2021) in Table 4.",,,1,related
", 2016), and Transformer-LRP (Chefer et al., 2021) in Table 4.",,,0,not_related
"To figure out how interpretability contributes to the model prediction, we adopt the positive and negative perturbation tests presented in (Chefer et al., 2021) to give both quantitative and qualitative evaluations on the interpretability.",,,1,related
"Decomposition-based approaches (Bach et al., 2015; Montavon et al., 2017; Chefer et al., 2021) propagate the final prediction to the input following the Deep Taylor Decomposition (Montavon et al., 2017).",,,0,not_related
"3 EVALUATION OF THE INTERPRETABILITY To figure out how interpretability contributes to the model prediction, we adopt the positive and negative perturbation tests presented in (Chefer et al., 2021) to give both quantitative and qualitative evaluations on the interpretability.",,,1,related
"Although Luo et al. (2016) propose to measure the ERF for CNNs, it cannot be directly implemented to Transformer-base models.",,,0,not_related
"Decomposition-based approaches (Bach et al., 2015; Montavon et al., 2017; Chefer et al., 2021) propagate the final prediction to the input following the Deep Taylor Decomposition (Montavon et al.",,,0,not_related
"As most previous methods focus on CNNs, Chefer et al. (2021) propose ViT-LRP tailored for vision Transformers.",,,0,not_related
"The following six explanation methods are used as the baselines for a comparison in our experiments: Raw Attention Map (RAM), Attention Rollout (Abnar & Zuidema, 2020) (Rollout), Generic Attribution (Chefer et al., 2021b) (GA), GradCAM (Selvaraju et al., 2017) (CAM), Partial LRP (Voita et al., 2019) (PLRP) and Transformer Attribution (Chefer et al., 2021a) (TA).",,,1,related
"Some attention-based explanation methods also have been proposed for the Transformers (Michel et al., 2019; Abnar & Zuidema, 2020; Chefer et al., 2021a;b; Hao et al., 2021), despite the disputation about the legitimacy of attentions being an explanation (Jain & Wallace, 2019; Wiegreffe & Pinter,…",,,0,not_related
"Transformer attribution (Chefer et al., 2021a) (TA) Transformer attribution method is a state-of-the-art class-specific explanation method for Transformer.",,,1,related
"Generic Attribution (Chefer et al., 2021b) generalizes the idea of Rollout and adds the gradient information to each attention map, while Transformer Attribution (Chefer et al., 2021a) exploits LRP (Binder et al., 2016) and gradients together for getting the explanations.",,,1,related
"Generic attribution (Chefer et al., 2021b) (GA) Generic attribution extends the usage of Transformer attribution to co-attention and self-attention based models, such as VisualBERT, LXMERT etc. and propose a more generic relevancy update rule.",,,1,related
"…a comparison in our experiments: Raw Attention Map (RAM), Attention Rollout (Abnar & Zuidema, 2020) (Rollout), Generic Attribution (Chefer et al., 2021b) (GA), GradCAM (Selvaraju et al., 2017) (CAM), Partial LRP (Voita et al., 2019) (PLRP) and Transformer Attribution (Chefer et al., 2021a) (TA).",,,1,related
"We show in Figure 1 some visualizations with Generic Attribution to different types of Vision Transformers (Dosovitskiy et al., 2021; He et al., 2022).",,,1,related
"Following previous works (Abnar & Zuidema, 2020; Chefer et al., 2021a;b; Samek et al., 2017; Vu et al., 2019; DeYoung et al., 2020), we prepare three types of tests for the trustworthiness evaluation:
Perturbation Tests gradually mask out the tokens of input according to the explanation results and…",,,1,related
"…baselines for a comparison in our experiments: Raw Attention Map (RAM), Attention Rollout (Abnar & Zuidema, 2020) (Rollout), Generic Attribution (Chefer et al., 2021b) (GA), GradCAM (Selvaraju et al., 2017) (CAM), Partial LRP (Voita et al., 2019) (PLRP) and Transformer Attribution (Chefer et…",,,1,related
"Transformer
Attribution (Chefer et al., 2021a) and Generic Attribution (Chefer et al., 2021b) combine the gradients with layer-wise relevance propagation (Binder et al., 2016) (LRP) or attention maps along a rolling out path, and eliminate the negative components in each attention block.",,,1,related
"Our approach outperforms other strong baselines (e.g., (Abnar & Zuidema, 2020; Chefer et al., 2021a;b)) through quantitative metrics and qualitative visualizations, and shows better applicability to various settings.",,,1,related
"Following the work of Chefer et al. (2021a), we use a weighted gradient map of the last attention block, which corresponds to the [CLS] token .",,,1,related
"…especially popular in computer vision, the more advanced methods have been successfully implemented and validated for transformer models for NLP by Chefer et al. (2021), who showed that their improved implementation of LRP gives better classspecific explanations compared to roll-out because the…",,,0,not_related
Further mentions of LRP in this paper follow the implementation of Chefer et al. (2021).,,,1,related
"This implementation (Chefer et al., 2021)
2https://github.com/INK-USC/DIG 3https://github.com/cdpierse/transformers-interpret 4https://github.com/hila-chefer/Transformer-
Explainability 5For each text, we use the explanation for unseen data, meaning that the attributions were generated with the…",,,1,related
and restrain unimportant features [14].,,,0,not_related
"…of fast-evolving and well-studied computer vision techniques in the time series domain, such as better model architecture (Liu et al., 2022), data augmentation (Shorten & Khoshgoftaar, 2019), interpretability (Chefer et al., 2021), self-supervised learning (He et al., 2022), to name a few.",,,0,not_related
", 2022), data augmentation (Shorten & Khoshgoftaar, 2019), interpretability (Chefer et al., 2021), self-supervised learning (He et al.",,,0,not_related
"In the future, more advanced approaches such as [29], and [30], can be incorporated.",,,0,not_related
"Meanwhile, most Transformer explanation methods are post-hoc and focus on the vanilla ViT models in image classification [6].",,,0,not_related
The clusters are semantically meaningful (with some corresponding object parts) and show comparable or qualitatively better results to the state-of-the-art Improved LRP [6].,,,0,not_related
"On the other hand, explaining trained ViT models requires nontrivial and sophisticated methods [6] following the trend of eXplainable AI (XAI) [18] that has been extensively studied with convolutional neural networks.",,,0,not_related
"Results of Rollout [1], raw attention, GradCAM [34], LRP [5], partial LRP [41] and the Improved LRP [6] are reproduced from [6], which all use the pretained ViT-B model [12].",,,1,related
"As pointed out in the Improved LRP [6], reducing the explanation to only the attentions scores may be myopic since many other components are ignored.",,,0,not_related
There are mainly two categories of approaches: gradient based methods such as the GradCAM method [34] that is built on the CAM [59] and attribution based methods built on the deep Taylor decomposition framework [30] such as the Layerwise Relevance Propagation (LRP) method [5].,,,0,not_related
"6, to further understand the proposed method, we visualize the feature maps of our HFI-Net by using Attention Map [82] on various manipulation faces.",,,1,related
"6, to further understand the proposed method, we visualize the feature maps of our HFI-Net by using Attention Map [82] on various
manipulation faces.",,,1,related
The visualization experiments of our method via Attention Map [82].,,,1,related
"To interpret the inner working mechanism of Transformers, it is essential to understand how the information of each input token flows through each intermediate layer and finally reaches the output.",,,0,not_related
"Thus, attention alone, without considering the skip connection, is not sufficient to characterize the inner working mechanism of Transformers.",,,0,not_related
"Transformers have advanced the state-of-the-art on a variety of natural language processing tasks [1, 2] and see increasing popularity in the field of computer vision [3, 4].",,,0,not_related
"Third, the individual feature attribution-based approaches [15, 14, 29, 30] cannot capture the pairwise interactions of feature since gradients or relevance scores are calculated independently for each individual feature.",,,0,not_related
"Others [15, 14] apply LRP aiming to dissect the information flows via layer-wise back-propagation.",,,0,not_related
"While it somewhat outperforms the rollout method in specific scenarios, it is not ready to support large-scale evaluations [15].",,,0,not_related
This work addresses the major issues in generating faithful and confident explanations for Transformers via a novel attentive class activation tokens approach.,,,0,not_related
"There has been a growing body of work on using LRP to explain Transformers [14, 15].",,,0,not_related
"Although some gradient-based methods [20, 21, 22, 23] have been proposed to leverage salience for explaining Transformer’s output, most of them still focus on the gradients of attention weights, i.e., Grads and AttGrads as shown in Figure 2.",,,0,not_related
"Since there are various versions of Transformer architectures, e.g., ViT [3] and Swin Transformer [4], which are much different from Transformers used on NLP tasks, it opens up new avenues to extend our AttCAT to explain these models prediction.",,,0,not_related
"This has motivated new research on explaining Transformers output to assist trustworthy human decision-making [10, 11, 12, 13, 14, 15, 16, 17].",,,0,not_related
"[15] provide a comprehensive treatment of the information propagation within all components of the Transformer model, which back-propagates the information through all layers from the output back to the input.",,,0,not_related
The self-attention mechanism [18] in Transformers assigns a pairwise score capturing the relative importance between every two tokens or image patches as attention weights.,,,0,not_related
This observation motivates us to interpret the inner working mechanism of Transformers via disentangling the information flow Transformer.,,,0,not_related
"Among all the compared methods, the attention-based methods (i.e., RawAtt and Rollout) perform worst since attention weights alone without considering the magnitudes of feature values are not adequate to analyze the inner working mechanism of Transformers.",,,0,not_related
"Thus, the rollout operation used in [13, 15] will attenuate the impact scores at shallower layers (i.",,,0,not_related
• Our AttCAT exploits both the self-attention mechanism and skip connection to explain the inner working mechanism of Transformers via disentangling information flows between intermediate layers.,,,1,related
"In addition, the identity matrix (I) is used to avoid the self-inhibition of each patch [61].",,,0,not_related
[61] to highlight the FT patches that the model is attending to by inferring both the gradient and the relevance from the final classification decision for each attention layer.,,,0,not_related
"LRP-implementations follow [34] for ResNet and [35], [36] for transformers.",,,1,related
Fong & Vedaldi (2017) introduce a method for explaining classifiers based on meaningful perturbation and Chefer et al. (2021) introduce a method for improving interpretation for transformer-based classifiers.,,,1,related
"For example, LRP rules have been derived for LSTMs [46], networks with Batch Normalization [47, 48], Multi-Head Attention [5, 49] and Transformers [50].",,,0,not_related
"Interpretability of vision models has been an active research recently [3, 11, 46, 65].",,,0,not_related
"Using the same method with the main material [3], we also visualize the activated area of our M3T network.",,,1,related
"There is a tremendous amount of work on extracting saliency maps in a self-supervised (Voynov et al., 2021; Caron et al., 2021; Mo et al., 2021) or weakly-supervised (i.e., using class labels; Selvaraju et al. (2017); Chefer et al. (2021)) manner.",,,0,not_related
"Figure 4 visualizes the saliency maps (Chefer et al., 2021), verifying that ReMixer gives more object-centric view.",,,0,not_related
"First, the progress of supervised (He et al., 2017; Carion et al., 2020; Fang et al., 2021), weakly-supervised (Selvaraju et al., 2017; Chefer et al., 2021; Yun et al., 2021), and self-supervised (Voynov et al., 2021; Caron et al., 2021; Mo et al., 2021) detection significantly reduced the cost of…",,,0,not_related
", Vision Transformers) starkly di↵erent learning mechanisms for visual recognition than traditional CNNs, their fine-grained attentiveness, and their subsequent human-interpretability in standard image domains [14,19,21,80].",,,0,not_related
We propose to leverage the idea of relevancy scores [4] as the importance map for optimal transport distributions.,,,1,related
A class-specific visualization method for self-attention models is proposed in [4].,,,0,not_related
", 2017), attribution methods (Bach et al., 2015; Montavon et al., 2017; Nam et al., 2019; Gur et al., 2020; Chefer et al., 2021), and image manipulation methods (Fong et al.",,,0,not_related
"…(Shrikumar et al., 2017; Srinivas and Fleuret, 2019; Selvaraju et al., 2017), attribution methods (Bach et al., 2015; Montavon et al., 2017; Nam et al., 2019; Gur et al., 2020; Chefer et al., 2021), and image manipulation methods (Fong et al., 2019; Fong and Vedaldi, 2017; Lundberg and Lee, 2017).",,,0,not_related
Methods raw attention rollout[11] attribution[12] Ours Ours(lend=4) Pixel accuracy 67.,,,1,related
"Based on the idea of the rollout method, the attribution method [12] computed the relevance scores of the tokens with the layer-wise relevance propagation (LRP) [13], to visualize ViT’s decision process.",,,0,not_related
Methods raw attention rollout[11] attribution[12] Ours Ours(lend=4) Deletion 21.,,,1,related
"Note that (i) and (ii) are obtained after binarizing each visualization, which depends on the pre-set threshold (30% of the max value is used in practice [12]), while (iii) is threshold-free.",,,1,related
The Transformer attribution method [12] assigns local relevance scores based on LRP and propagates the relevance scores mixed with gradients through layers.,,,1,related
"Without early stopping using lend, our proposed approach produces better explanations than the rollout [11] and attribution [12] methods.",,,1,related
"The proposed approach produces better explanations than currently state-of-the-art algorithms [11, 12].",,,0,not_related
Methods raw attention rollout[11] attribution[12] Ours Ours(lend=4) Positive 28.,,,1,related
The attribution method [12] can extract category-related features and part of the target region but is not comprehensive and complete.,,,1,related
[12] proposed a better explanation algorithm using the product of gradients and feature attributions.,,,0,not_related
"ViT is given high expectation to improve vision tasks beyond image classification, with existing studies on generative modeling [11, 27, 28, 39], video understanding [2, 38], segmentation and detection [30, 34, 53], interpretability [1, 6, 7].",,,0,not_related
"It gives the advantage to cheaply apply CAM-like [65] method to interpret how well learned representations measure object features without needing any particular algorithms for transformer, such as [7].",,,0,not_related
"A comparison of visualization techniques was made in (Chefer et al., 2021), and they carried out experiments to test some of these methods for Vision Transformers in image classification tasks.",,,0,not_related
"The token representations [4,10,65,68] in early and middle layers are insufficiently encoded, which makes token pruning quite difficult.",,,0,not_related
