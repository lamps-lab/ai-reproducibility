text,label_score,label,target_predict,target_predict_label
"Recently, some researchers [14] have put forward a new state representation technique which exploits the spatial-temporal nature of visual observations in an RL setting.",,,0,not_related
"In particular, we follow the linear probing protocol introduced in [22] and run simple linear regression on game-state variables following the protocol of [21].",,,1,related
"We conduct linear probing tasks [22] on the derived content and style embeddings (using the 3D-SSL and Gen11 datasets, respectively) to recover relevant game state information (i.",,,1,related
"In case when K > 1, because if E[Zt+K , [Zt, Ut, ..., Ut+K‚àí1]] = E[St+K , [St, At, ..., At+K‚àí1]], then for any 1 ‚â§ k ‚â§ K, E[Zt+k, [Zt, Ut, ..., Ut+k‚àí1]] = E[St+k, [St, At, ..., At+k‚àí1]], including K = 1, by Data processing Inequality.",,,1,related
"Then, our method aims to maximize the mutual information between representations of current states paired with action sequences and representations of the corresponding future states: JTACO = I(Zt+K ; [Zt, Ut, ..., Ut+K‚àí1]) (2) Here, K ‚â• 1 is a fixed hyperparameter for the prediction horizon.",,,1,related
", Ut+K‚àí1]) (2) Here, K ‚â• 1 is a fixed hyperparameter for the prediction horizon.",,,1,related
"CPC (47), ST-DIM (2), and ATC (44) integrate temporal relationships into the contrastive loss by maximizing mutual information between current state representations (or state histories encoded by LSTM in CPC) and future state representations.",,,0,not_related
"Let K ‚àà N+, and JTACO = I(Zt+K ; [Zt, Ut, ..., Ut+K‚àí1]).",,,1,related
"In CURL (33), it treats augmented states as positive pairs, but it neglects the temporal dependency of MDP. CPC (47), ST-DIM (2), and ATC (44) integrate temporal relationships into the contrastive loss by maximizing mutual information between current state representations (or state histories encoded by LSTM in CPC) and future state representations.",,,0,not_related
"This theorem guarantees that if our mutual information objective Equation (2) is maximized, then for any two state-action pairs (s1, a1) and (s2, a2) with equivalent state and action representations, their optimal action-value functions,Q‚àó(s1, a1) andQ‚àó(s2, a2), will be equal.",,,1,related
"This is because p(Rt|St = s1, At = a1) = p(Rt|St = s2, At = a2) by Equation (18) as p(zt|St = s1) = p(zt|St = s2), p(ut|At = a1) = p(ut|At = a2).",,,1,related
"This theorem guarantees that if our mutual information objective Equation (2) is maximized, then for any two state-action pairs (s1, a1) and (s2, a2) with equivalent state and action representations, their optimal action-value functions,Q(s1, a1) andQ(s2, a2), will be equal.",,,1,related
"Subsequent works, including CPC (23), ST-DIM (2), and ATC (44), made progress to rectify this by integrating temporal elements into the contrastive loss, linking pairs of observations with short temporal intervals.",,,0,not_related
"= x)dz
Proof of Theorem 3.1: Based on the graphical model, it is clear that
max œÜ,œà I(Zt+K , [Zt, Ut, ..., Ut+K‚àí1]) = I(St+K ; [St, At, ..., At+K‚àí1]) (16)
Now define the random variable of return-to-go Rt such that
Rt = H‚àít‚àë k=0 Œ≥kRt+k (17)
Based on Proposition E.1, because
I(Zt+K ;Zt, Ut:t+K‚àí1) = I(St+K ;St, At:t+K‚àí1)
we could conclude that
I(Rt+K ;Zt, Ut:t+K‚àí1) = I(Rt+K ;St, At:t+K‚àí1) (18)
Now applying Proposition E.2, we get
Ep(zt,ut:t+K‚àí1|St=s,At:t+K‚àí1=at:t+K‚àí1)[p(Rt|Zt, Ut:t+K‚àí1)] = p(Rt|St = s,At:t+K‚àí1) (19)
As a result, when K = 1, for any reward function r, given a state-action pair (s1, a1), (s2, a2) such that œÜ(s1) = œÜ(s2), œà(a1) = œà(a2), we have Qr(s1, a1) = Ep(Rt|St=s1,At=a1)[Rt] = Ep(Rt|St=s2,At=a2)[Rt].",,,1,related
Schwarzer et al. (2020) and Anand et al. (2019) did the same in the Atari game-playing environments.,,,0,not_related
"game that is considered, according to the annotations provided in (Anand et al., 2019).",,,1,related
"Moreover, we postprocess this additional information by only selecting the subset of variables that are relevant to the
game that is considered, according to the annotations provided in (Anand et al., 2019).",,,1,related
"The distance reward ùëÖdist requires the agent location at each step, we propose to either use a specifically trained object detector, which was tested with a pre-trained FasterRCNN model [25] fine-tuned on 100 manually labelled training examples, or to use the RAM state labels provided via the AtariARI Wrapper [1].",,,1,related
"The distance reward Rdist requires the agent location at each step, we propose to either use a specifically trained object detector, which was tested with a pre-trained FasterRCNN model [25] fine-tuned on 100 manually labelled training examples, or to use the RAM state labels provided via the AtariARI Wrapper [1].",,,1,related
The required state information can be extracted directly from the ALE via an environment wrapper called Atari Annotated RAM Interface (AtariARI) [1].,,,1,related
"To tackle these problems, Anand et al. (2019) have proposed AtariARI, a wrapper around some Atari gymnasium environments, that augments the information dictionary returned at each step with the RAM position of some information.",,,0,not_related
"Nevertheless, only a few methods regarding OCR, like Anand et al. (2019) (who also introduced AtariARI), Li et al. (2017) and Lin et al. (2020b), have been tested on Atari games so far.",,,0,not_related
"Nevertheless, only a few methods regarding OCR, like Anand et al. (2019) (who also introduced AtariARI), Li et al.",,,0,not_related
"Contrary to Anand et al. (2019), which consulted commented disassemblies or source code of different Atari games for their AtariARI framework, we used other analytical techniques to understand and find where the information is stored in the RAM.",,,1,related
"Nevertheless, only a few methods regarding OCR, like Anand et al. (2019) (who also introduced AtariARI), Li et al. (2017) and Lin et al.",,,0,not_related
"To prevent representational collapse, recent studies have employed the contrastive learning (Anand et al., 2019; Laskin et al., 2020; Oord et al., 2018; Stooke et al., 2021) or architectural designs including the use of batch-normalization and stop-gradient operation (Schwarzer et al., 2020; 2021).",,,0,not_related
"To prevent representational collapse, recent studies have employed the contrastive learning (Anand et al., 2019; Laskin et al., 2020; Oord et al., 2018; Stooke et al., 2021) or architectural designs including the use of batch-normalization and stop-gradient operation (Schwarzer et al.",,,0,not_related
"To prevent representational collapse, contrastive learning (Anand et al., 2019; Laskin et al., 2020; Stooke et al., 2021; Oord et al., 2018) or batch-normalization with stop-gradient operation (Schwarzer et al.",,,0,not_related
"To prevent representational collapse, contrastive learning (Anand et al., 2019; Laskin et al., 2020; Stooke et al., 2021; Oord et al., 2018) or batch-normalization with stop-gradient operation (Schwarzer et al., 2020; 2021) are commonly employed in unsupervised representation learning.",,,0,not_related
and contrastive representation learning [38; 1; 34; 24; 36; 27].,,,0,not_related
"While feature learning of this type has found some success in the RL setting, it has been mainly limited to vision-based environments [Jaderberg et al., 2017, Oord et al., 2018, Anand et al., 2019, Laskin et al., 2020, Stooke et al., 2021, Yarats et al., 2022].",,,0,not_related
"In [1], the representations for RL algorithms are learned by maximizing mutual",,,0,not_related
"While the existing methods [1, 10, 38, 40, 43, 44, 46] feeds the stacked frames to the encoder at once, which can be viewed as an early fusion [32], our method generates the set of the latent representations individually with the encoder.",,,1,related
"To address this challenge, a number of deep RL approaches [1,10,38,40,43,44,46] leverage the recent advance of self-supervised learning which effectively extracts highlevel features from raw pixels in an unsupervised fashion.",,,0,not_related
Previous SSL methods in [2] have used a single output head with 256 hidden units.,,,0,not_related
"results of using a single output head with 256 hidden units, which are taken from [2].",,,0,not_related
The performance of DIM-UA and other SRL methods is evaluated on the 19 games of the AtariARI benchmark.,,,0,not_related
"State representation learning (SRL) [2, 16, 22] focuses on learning representations from input data that are typically collected in a reinforcement learning (RL) environment.",,,0,not_related
"There are five categories of state variables in AtariARI [2], which are agent localization (Agent Loc.",,,0,not_related
The data for pretraining and probing are collected by an RL agent running a certain number of steps using a random policy since it was found that the samples collected by a random policy could be more favorable than those collected by policy gradient policies for the SSL methods [2].,,,1,related
"We experiment with our UA paradigm using the SRL algorithm ST-DIM [2], and propose DIM-UA.",,,1,related
(2) We compare DIM-UA with other SRL algorithms using samples collected from 19 Atari games of the AtariARI benchmark and illustrate that our algorithm achieves the best performance in terms of F1 scores and accuracy.,,,1,related
Our contribution is summarized as follows: (1) We modify the state-of-the-art SRL algorithm ST-DIM [2] with our UA paradigm and introduce a new algorithm called DIM-UA.,,,1,related
We present our UA paradigm for representation learning using an unbalanced atlas and design a new SRL method based on the UA paradigm.,,,1,related
We demonstrate that our paradigm improves the state-of-the-art SRL methods significantly by following the SSL experimental pipeline and comparing the probe F1 scores and accuracy.,,,1,related
"‚Ä¶policy gradient
Policy Gradient Methods in the Presence of Symmetries and State Abstractions
learning (Hjelm et al., 2018; Liu and Abbeel, 2021), or self-supervised learning (Anand et al., 2019; Sinha et al., 2021; Hansen et al., 2020; Hansen and Wang, 2021; Fan et al., 2021).",,,0,not_related
", 2018; Liu and Abbeel, 2021), or self-supervised learning (Anand et al., 2019; Sinha et al., 2021; Hansen et al., 2020; Hansen and Wang, 2021; Fan et al., 2021).",,,0,not_related
"Another body of related work on decoupling representation learning from RL primarily revolves around the use of contrastive learning (Anand et al., 2019; Wu et al., 2019; Stooke et al., 2021; Schwarzer et al., 2021b; Erraqabi et al., 2022).",,,0,not_related
"Anand et al. (2019) proposed ST-DIM, a collection of temporal contrastive losses operating on image patches from environmental observations.",,,0,not_related
"CURL [31] proposed a contrastive loss between two different data-augmentation of the same observation, while CPC [32] and ST-DIM [16] proposed different variations of temporal contrastive loss between two augmented observations separated by small time steps.",,,0,not_related
"To further improve sample efficiency, we use contrastive representation learning by maximizing the temporal mutual information between embeddings of consecutive time steps [16], [17], [18].",,,1,related
"We experimented with different self-supervised losses (MSE, ST-DIM [1] and VICReg [8]), covering both contrastive and non-contrastive approaches.",,,1,related
"Self-supervised learning also started to be used in the field of state representation learning [28], it has proven to be a suitable method for creating the feature space [1] and has also found its use in reinforcement learning [48, 17].",,,0,not_related
SND-STD method uses the Spatio-Temporal DeepInfoMax (ST-DIM) algorithm [1] (the simple diagram can be found in Figure 4) leveraging multiclass N -pair losses [46]:,,,0,not_related
The details of this algorithm are provided in [1].,,,1,related
"Our experiments revealed that if the ST-DIM algorithm works on an
incomplete dataset that takes on new samples (the authors probably did not test it in such conditions), there is an instability and an exponential increase of activity in the feature space at certain moments.",,,1,related
"SND-STD method uses the Spatio-Temporal DeepInfoMax (ST-DIM) algorithm [1] (the simple diagram can be found in Figure 4) leveraging multiclass N -pair losses [46]:
LGL = ‚àí I‚àë i=1 J‚àë j=1 log exp(gi,j)‚àë s‚àót‚ààSnext exp(gi,j)
(5)
LLL = ‚àí I‚àë i=1 J‚àë j=1 log exp(fi,j)‚àë s‚àót‚ààSnext exp(fi,j)
(6)
where f(.)",,,1,related
"Our version uses the state st and its successor st+1 (the same as ST-DIM, the simple diagram can be found in Figure 4) instead of two augmentations of the same state.",,,1,related
7) of the ST-DIM algorithm itself has an expansive effect and the addition of another expansive term led to problems with the uncontrolled expansion of the feature space.,,,0,not_related
ST-DIM [21] uses temporal and contrastive losses to operate on local features of the intermediate layer within the encoderwithout data augmentation.,,,0,not_related
ST-DIM [21] uses temporal and contrastive losses to operate on local features of the intermediate layer within theencoderwithoutdataaugmentation.,,,0,not_related
"2), (vi) predictive information [138,150,151], (vii) bisimulation [137,152] and (viii) asymmetric training by taking advantage of simulation [153].",,,0,not_related
"reconstructing the observation (to ignore irrelevant visual aspects) [162, 51, 76, 67, 109], to predicting forward dynamics (to capture what constrains movement) [162, 67] or inverse dynamics (to recover actions from observations) [126], to enforcing behavioural similarity between observations [173, 59, 11], to contrastive losses [158, 94, 8, 154], and many",,,0,not_related
"Feature Embedding (Unsupervised) œÜR(oR) = ~ œÜR(oR) ‚àà Rd May learn wrong disentangled information d, Low by design If disentangled information complete, easy If disentangled information incomplete, hard May be interpretable to designer [97, 154, 145, 173, 76, 67, 94, 8, 70]",,,1,related
"+20 ] (8) To train the critic, we symlog transform the targets R t and then twohot encode them into a soft label for the softmax distribution produced by the critic.",,,1,related
"Several works have addressed the problem of finding compact state representations from raw sensory streams in a model-free MDP setting [3], [4], [5], [6].",,,0,not_related
"Some of the recent unsupervised state representations are Time Contrastive Network (TCN) [32], conditional models of Noise Contrastive Estimation (CM-NCE) [33], SpatioTemporal Deep InfoMax (ST-DIM) [18], Balanced View Spatial Deep InfoMax (BVS-DIM) [22], CURL [6] and augmented temporal contrast (ATC) [21].",,,0,not_related
Some researchers used state variables of video games as an evaluation methods such as [18] and [22] while other used reinforcement learning tasks such as [6] and [21].,,,0,not_related
"In ST-DIM, œÑ = 1 while in ATC they used œÑ = 3.",,,1,related
"There in no modification of equation 5 since it is adjusted with p = 2 ‚àó (0.5‚àí r) when we have r   0.5
xbmn(i, j) :=
{ x‚àó1(i, j) if Ci,j = true
xmin(i, j) otherwise (8)
xbmx(i, j) :=
{ x‚àó2(i, j) if Ci,j = true
xmax(i, j) otherwise (9)
In ST-DIM and ATC the temporally close frames are taken as the view of one another.",,,1,related
"However, learning in ST-DIM and ATC is limited to time sequenced samples, and we hypothesize the success of representation learning in ST-DIM and ATC depends on the similarity factor of these temporal observations.",,,0,not_related
"Video games and reinforcement learning based control tasks are useful candidates for evaluating representation learning algorithms [12], [18]‚Äì[21].",,,0,not_related
Devising a generic way of evaluation of the general goodness of the representation is however essential [18].,,,0,not_related
"On the other hand, ST-DIM and BVS-DIM made evaluation of the learned representations by using state variables of the game environments.",,,0,not_related
"Since œÑ is considered as a factor of spatio-temporal similarity-contrast, there was no possible values to choose for œÑ other than integers in ST-DIM and ATC.",,,0,not_related
"Various unsupervised/self-supervised auxiliary objectives like autoencoders [34, 45, 83], forward [31] and inverse dynamics [56], spatio-temporal mutual information maximization [2, 35], contrastive learning [32, 33, 66, 85], derive supervision from the agent‚Äôs own experience.",,,0,not_related
"[2] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C√¥t√©, and R.",,,0,not_related
"Some common auxiliary tasks in RL include: (1) future prediction [26, 15, 46, 13, 32], which predicts an agent‚Äôs future state conditioned on its current state and the actions taken, (2) inverse dynamic [15, 46, 32], which predicts the actions taken between two states, and (3) contrastive learning [3, 36], which applies contrastive learning to refine the state representation.",,,0,not_related
"This protocol is related to the one used in [1], but we probe not only the encoder output, but also the predictor output.",,,1,related
"These games are the only two from our datasets with a fixed number of objects, which allows for a very simple policy architecture, and for which perfect information obtained from the RAM using [1] is available.",,,0,not_related
The first was to use the RAM hacking approach of AtariARI [1].,,,0,not_related
"[1] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C√¥t√©, and R.",,,0,not_related
"Representation Learning World Model (Ha & Schmidhuber, 2018) Reconstruction 3 7 ST-DIM (Anand et al., 2019) Forward Pixel Prediction 3 7 ATC (Stooke et al.",,,0,not_related
"These approaches usually incorporate temporal information, aiming to distinguish between sequential and non-sequential states (Anand et al., 2019; Stooke et al., 2021).",,,0,not_related
"Furthermore, recent work has shown significance of learning representations through self supervised objectives in RL [3, 55], often as a pre-training phase [56, 70], which can help for both exploration [36] and control [71].",,,0,not_related
"[3] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C√¥t√©, and R.",,,0,not_related
"We mostly used the default parameters from the Atari agent in (Hafner et al., 2020) but increased the RSSM recurrent state size and tuned the KL and entropy scales to the new environment.",,,1,related
"Deep reinforcement learning (RL) has made tremendous progress in recent years, outperforming humans on Atari games (Mnih et al., 2015; Badia et al., 2020), board games (Silver et al., 2016; Schrittwieser et al., 2019), and advances in robot learning (Akkaya et al., 2019; Wu et al., 2022).",,,0,not_related
The differences from the parameters of the original DreamerV2 Atari model (Table D.1 in Hafner et al. (2020)) are shown in bold face.,,,1,related
"Unsupervised representations are commonly evaluated by probing (Oord et al., 2018; Chen et al., 2020; Gregor et al., 2019; Anand et al., 2019), where a separate network is trained to predict relevant properties from the frozen representations.",,,0,not_related
"A standardized probe benchmark is available for Atari (Anand et al., 2019), but those tasks require almost no memory.",,,0,not_related
[28] systematically evaluate the importance of key representational biases encoded by DQN‚Äôs network by proposing simple linear representations.,,,0,not_related
"When learning a predictive information representation between a state, action pair and its subsequent state, the learning task is equivalent to modeling environment dynamics [4].",,,0,not_related
"While existing studies of learning predictive information representations in RL [2, 3, 4, 8, 9] have largely been limited to learning single-task specialist agents in simulated environments such as DM-Control [10] and Atari [11], our hypotheses can be seen as extending the generalization results in Lee et al.",,,0,not_related
"Previous studies [17, 2, 4, 3, 9, 18, 8, 19] have shown that predictive information [1] is an effective auxiliary or representation learning objective for RL agents or planning.",,,0,not_related
"(2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered.",,,0,not_related
"Along similar lines, (56; 2; 54; 19; 21; 18; 36; 1) have begun to explore contrastive systems and binary classifiers to do imitation learning and RL.",,,0,not_related
"One must (1) generate a good idea, (2) conduct mathematical analyses or experiments, (3) write a paper, and finally, (4) respond to reviewers in a satisfactory manner.",,,0,not_related
"CPC [73] and ST-DIM [30] use temporal contrastive losses to maximize the MI between the previous state embedding and a future embedding several time steps later, but they both do not leverage DA to transform the observations.",,,0,not_related
"Instead of maximizing the MI between the current state and the future state using the InfoNCE loss [73, 30, 38], SPR [23] produces state representations by minimizing the prediction error between the true future states and the predicted future states using an explicit multi-step DM.",,,1,related
"generate fully imagined trajectories [39], model multi-agent interactions [22], learn competitive policies through selfplay [23], imagine goals in goal-conditioned policies [40], [41], meta-reinforcement learning [42], and offline reinforcement learning [43].",,,0,not_related
"In classical RL [26], where behaviour is learned from a given reward function, mutual information objectives are commonly used to find compact state representations that increase performance by discarding irrelevant information [29, 3, 37, 24, 22].",,,0,not_related
"Another possible path would be using previously proposed benchmarks like the AtariARI benchmark (Anand et al., 2020), which tries to evaluate representations using the RAM states as ground truth labels.",,,0,not_related
"Another possible path would be using previously proposed benchmarks like the AtariARI benchmark Anand et al. (2020), which tries to evaluate representations using the RAM states as ground truth labels.",,,0,not_related
"Pretraining representations Previous work has explored, similarly to our approach, pretraining representations using self-supervised methods which led to great data-efficiency improvements in the fine-tuning phase (Schwarzer et al., 2021b; Zhan et al., 2020) or superior results in evaluation tasks, like AtariARI (Anand et al., 2020).",,,0,not_related
"‚Ä¶unlike images from datasets like ImageNet or MSCOCO, observations from reinforcement learning environments share similarities in more dimensions, for example, time (Stooke et al., 2021; Anand et al., 2020), semantics (Fan et al., 2022; Zhong et al., 2022), and behavior (Agarwal et al., 2021a).",,,0,not_related
"‚Ä¶work has explored, similarly to our approach, pretraining representations using self-supervised methods which led to great data-efficiency improvements in the fine-tuning phase (Schwarzer et al., 2021b; Zhan et al., 2020) or superior results in evaluation tasks, like AtariARI (Anand et al., 2020).",,,0,not_related
"Several works have addressed the problem of finding compact state representations from raw sensory streams in a model-free MDP setting [3], [4], [5], [6].",,,0,not_related
"Our work is also related to unsupervised representation learning methods based on mutual information estimation [33], [34].",,,1,related
"This objective has previously been used in the context of augmented multiscale DIM (AMDIM) [41], cross-modal DIM (CM-DIM) [51], and spatio-temporal DIM (ST-DIM) [52].",,,0,not_related
"Previously, augmented multiscale DIM (AMDIM) [41], cross-modal DIM (CM-DIM) [50, 51], and spatio-temporal DIM (ST-DIM) [52] used local intermediate representation of convolutional layers to capture multi-scale relationships between multiple views, modalities or time frames.",,,0,not_related
"The CC objective has been used in AMDIM [41], CM-DIM [51], and ST-DIM [52].",,,0,not_related
"[49] introduced a new contrastive loss to learn better representation in a fully unsupervised setting, while in [45], the authors adapted the BYOL objective [50] for learning the visual encoder in parallel with the RL objective, which led to state-of-the-art results on the Atari benchmark.",,,0,not_related
"5) and momentum encoders [83] were used in combination with SAC and Proximal Policy Optimization (PPO) [84] by [48], SAC and RainbowDQN [85] by CURL [44], distributional Q-learning by [86], and PPO by [87] for continuous and discrete control problems.",,,0,not_related
"STDIM [3], ATC [46] and BVS-DIM [35] incorporate temporal information in their contrastive objective, adapting similar techniques from the unsupervised video representation learning [45].",,,0,not_related
"Some prior work [41, 21, 3] evaluate the quality of their pretrained representations by probing for ground truth state variables such as agent/object locations and game scores.",,,0,not_related
"Contrastive losses (Oord et al., 2018; Anand et al., 2019; Srinivas et al., 2020; Stooke et al., 2021), which encourage similar states to be closer in embedding space, where the notion of similarity is usually defined in terms of temporal distance (Anand et al.",,,0,not_related
"‚Ä¶et al., 2021), which encourage similar states to be closer in embedding space, where the notion of similarity is usually defined in terms of temporal distance (Anand et al., 2019; Sermanet et al., 2018) or image-based data augmentations (Srinivas et al., 2020), also show promising performance.",,,0,not_related
", 2021), which encourage similar states to be closer in embedding space, where the notion of similarity is usually defined in terms of temporal distance (Anand et al., 2019; Sermanet et al., 2018) or image-based data augmentations (Srinivas et al.",,,0,not_related
"Contrastive losses (Oord et al., 2018; Anand et al., 2019; Srinivas et al., 2020; Stooke et al., 2021), which encourage similar states to be closer in embedding space, where the notion of similarity is usually defined in terms of temporal distance (Anand et al., 2019; Sermanet et al., 2018) or‚Ä¶",,,0,not_related
"Finally, object-level world-models have been used to constrain action-spaces, and states in robotics [85,86] and control domains [87,12,88].",,,0,not_related
"For learning representations from image-based RL tasks, CPC (van den Oord et al., 2018), ST-DIM (Anand et al., 2019), DRIML (Mazoure et al., 2020), CURL (Laskin et al., 2020), and SPR (Schwarzer et al., 2021) learn representations by optimizing various temporal contrastive losses.",,,0,not_related
"To address these sample complexity demands, a recent line of work (van den Oord et al., 2018; Anand et al., 2019; Mazoure et al., 2020; Yang & Nachum, 2021) has incorporated advances in unsupervised representation learning from the supervised learning literature into developing RL agents.",,,0,not_related
"We compare our representational objective against CURL and SPR in Section 6, and demonstrate that under linear evaluation protocol, ours outperform both CURL and SPR. Note, we refer the readers to Schwarzer et al. (2021) for comparisons between SPR and CPC, ST-DIM, and DRIML.",,,1,related
"Extensive work [2, 3, 12] in dissimilar domains of AI and games such as player experience modeling, general gameplaying or content generation make use of the internal state of the game [1, 9] obtained from the game engine.",,,0,not_related
"[1], we evaluate how well the learned representations have captured information relevant to the game",,,0,not_related
"[65] that demonstrated the effectiveness of auxiliary unsupervised objectives for RL, a variety of unsupervised learning objectives have been studied, including future latent reconstruction [27, 66, 67, 68], bisimulation [69, 70], contrastive learning [71, 72, 73, 74, 75, 30, 76, 29, 77, 78], keypoint extraction [79, 80, 81, 82], world model learning [8, 9, 10, 55] and reconstruction [83, 84].",,,0,not_related
"Self-supervised representation learning aims to group states together without loss of that property (Anand et al. 2019; Kipf, van der Pol, and Welling 2020).",,,0,not_related
"3 While most methods generate the ‚Äúpositive‚Äù examples via data augmentation, some methods generate similar examples using different camera viewpoints of the same scene [109, 121], or by sampling examples that occur close in time within time series data [4, 95, 109, 117].",,,0,not_related
"Even when the state representation is trained parallel with the agent as in [27] or when unsupervised learning is used as in [1,30], the training is still time-consuming since multiple convolutional networks are trained.",,,0,not_related
"This evaluation approach, called linear probing [2], has been used to evaluate representations of Atari games.",,,0,not_related
[2] proposed a self-supervised learning method which utilizes,,,0,not_related
"application tasks in games research‚Äîincluding gameplaying, modeling user behavior, and content generation‚Äîuse image representations of games containing information about critical factors describing the current state of the game [2].",,,0,not_related
"This approach, however, has two core limitations: first, it requires time-distributed images as the method‚Äôs loss function incorporates temporal difference between the game‚Äôs images and, second, it presents results on basic Atari games that are restricted to simple and abstract 2D grid environments, which are not representative of most modern era games.",,,0,not_related
"As mentioned earlier, recent work in self-supervised representation learning considered 2D game environments such as Atari [2].",,,0,not_related
"To solidify these conclusions, further investigation would be required with other contrastive and non-contrastive SSL methods such as Barlow Twins [5], SimSiam [6], VICReg [7],
and DINO [38], as well as time-distributed SSL approaches such as ST-DIM [2].",,,0,not_related
"More recently, Anand et al. [2] proposed a self-supervised learning method which utilizes
the spatial and temporal relations between frames of different Atari games to learn important visual features of the game‚Äôs image.",,,0,not_related
Following the principles of [2] the evaluation takes place by quantifying the capacity of a linear model to recover or predict the internal game state variables based on the derived SSL representations.,,,0,not_related
"Indicatively, a number of recent studies have effectively used ConvNets with reinforcement learning for playing Atari games [18]‚Äì[20] by mapping the game‚Äôs raw image to an action to be performed for maximizing the game‚Äôs score.",,,0,not_related
"be required with other contrastive and non-contrastive SSL methods such as Barlow Twins [5], SimSiam [6], VICReg [7], and DINO [38], as well as time-distributed SSL approaches such as ST-DIM [2].",,,0,not_related
"The internal state of Atari games is described via discrete variables, and thus linear probing in [2] evaluated the capacity of a linear model to predict the class of the internal game
state variables.",,,0,not_related
"The internal state of Atari games is described via discrete variables, and thus linear probing in [2] evaluated the capacity of a linear model to predict the class of the internal game",,,0,not_related
"In this paper, we use a Contrastive Predictive Coding (CPC) algorithm (Oord et al., 2018) which was shown useful for finding predictive latent variables (Anand et al., 2019; Henaff, 2020; Yan et al., 2020).",,,1,related
"‚Ä¶the CPC algorithm, (Oord et al., 2018) is used for finding predictive latent representations, which is useful for data-efficient image recognition (Henaff, 2020) and learning world-models
that supports robotic object manipulation (Yan et al., 2020) and playing Atari games (Anand et al., 2019).",,,0,not_related
"‚Ä¶2010; Lange et al., 2012; Yarats et al., 2021b), contrastive objectives (van den Oord et al., 2018; Laskin et al., 2020a), data augmentation (Yarats et al., 2021a), and mutual information maximization (Anand et al., 2019; Choi et al., 2021) to learn representations for downstream control.",,,0,not_related
"CPC [32], CPC|Action [17] and ST-Dim [3] propose different variants of temporal contrastive losses, however these methods add complexity and require a sequence of images for training.",,,0,not_related
"Minimizing the contrastive loss is known to be less computationally costly and can produce good representations from high-dimensional inputs (Oord et al., 2018; Anand et al., 2019; Chen et al., 2020; Laskin et al., 2020; van der Pol et al., 2020a), thus we use it here.",,,0,not_related
"which no additional annotation is needed for the representation learning, the network is trained to produce consistent embeddings for semantically similar input (positives), contrasting them with other, more distant samples (negatives) [6], [8]‚Äì [10].",,,0,not_related
"Following CPC concepts, ST-DIM [8] used both patch structure and temporal positions of observations, aligning nearby frames.",,,0,not_related
"According to the different objectives of auxiliary tasks, existing auxiliary tasks in model-free RL frameworks can be roughly categorized into three types, reconstruction-based [7, 11], prediction-based [10, 15, 34] and contrastive-learning based [1, 18, 22].",,,0,not_related
"The main idea of most contrastive-learning based auxiliary tasks is to hope that the representation of anchor sample is closer to the representation of positive samples and farther from the representation of negative samples in the latent vector space [1, 2, 4, 22, 30].",,,0,not_related
"‚Ä¶al., 2019; 2021), reconstruction (Yarats et al., 2021c), future representation prediction (Gelada et al., 2019; Schwarzer et al., 2021a), bisimulation (Castro, 2020; Zhang et al., 2021), and contrastive learning (Oord et al., 2018; Anand et al., 2019; Mazoure et al., 2020; Srinivas et al., 2020).",,,0,not_related
", 2021), and contrastive learning (Oord et al., 2018; Anand et al., 2019; Mazoure et al., 2020; Srinivas et al., 2020).",,,0,not_related
"However, despite the loss functions operating directly on the agent‚Äôs representation, the representations themselves are rarely evaluated beyond evaluating the returns, with some exceptions (Anand et al., 2019; Zhang et al., 2020; Mazoure et al., 2021; Lyle et al., 2021).",,,0,not_related
"Usable annotation is completely absent from the Atari benchmark (Bellemare et al., 2013), though wrappers have been created to aid in representation learning research (e.g., Anand et al., 2019), which has meaning only within a single task ‚Äúgame‚Äù.",,,0,not_related
"(Zhang et al., 2016; Anand et al., 2019).",,,0,not_related
"sigmoid (max{Wsi + b}), to predict VQA-style questions in the form ‚Äúis there a (size, color, material, shape) object in the image?‚Äù (Zhang et al., 2016; Anand et al., 2019).",,,0,not_related
"Contrastive learning (Oord et al., 2018; He et al., 2020) is always used as an unsupervised learning approach to extract useful representations from high-dimensional data, which has been applied successfully to image recognition (Henaff, 2020) and RL (Anand et al., 2019; Laskin et al., 2020).",,,0,not_related
", 2020) is always used as an unsupervised learning approach to extract useful representations from high-dimensional data, which has been applied successfully to image recognition (Henaff, 2020) and RL (Anand et al., 2019; Laskin et al., 2020).",,,0,not_related
"‚Ä¶Guo et al., 2020; Lee et al., 2020a;b; Schwarzer et al., 2021a; Yu et al., 2021) and contrastive learning of instance discrimination (Laskin et al., 2020b) or (spatial -) temporal discrimination (Oord
et al., 2018; Anand et al., 2019; Stooke et al., 2020; Zhu et al., 2020; Mazoure et al., 2020).",,,0,not_related
", 2020b) or (spatial -) temporal discrimination (Oord et al., 2018; Anand et al., 2019; Stooke et al., 2020; Zhu et al., 2020; Mazoure et al., 2020).",,,0,not_related
"This idea is further explored in ATC [Stooke et al., 2021] and STDIM [Anand et al., 2019] where a temporal contrast is adopted instead.",,,0,not_related
", 2021] and STDIM [Anand et al., 2019] where a temporal contrast is adopted instead.",,,0,not_related
"When unsupervisedly training a low-level layer, at each time step, a random action was taken of a game (Anand et al., 2019), then minibatches of size 32 were randomly extracted from a buffer of 10(6) recent frames to train the network using eq.",,,0,not_related
", computer vision [1, 2, 3], natural language [4, 5], code [6], reinforcement learning [7, 8, 9], and graphs [10].",,,0,not_related
"We also experiment with a temporal contrastive objective which treats pairs of observations close in time as positive examples and un-correlated timestamps as negative examples [2, 52, 23].",,,0,not_related
(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018). Pretraining and transfer learning have also been used for neuro/brain imaging Mensch et al.,,,0,not_related
"(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018). Pretraining and transfer learning have also been used for neuro/brain imaging Mensch et al. (2017); Thomas et al. (2019); Mahmood et al. (2020); Li et al. (2018). In this study, we propose a novel pre-training algorithm involving GNNs which we call Multinetwork InfoMax (MIM).",,,0,not_related
"(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018). Pretraining and transfer learning have also been used for neuro/brain imaging Mensch et al. (2017); Thomas et al. (2019); Mahmood et al. (2020); Li et al. (2018). In this study, we propose a novel pre-training algorithm involving GNNs which we call Multinetwork InfoMax (MIM). MIM uses self-supervised learning with a mutual information objective to learn data representations. These representations are then used for downstream task of classification using small labeled dataset. We show that using pre-training we can bypass the need to acquire large labelled training datasets and achieve higher classification performance than non pre-trained counter parts. Prior work on self-supervised pre-training for GNNs Bojchevski and G√ºnnemann (2018); Davidson et al.",,,0,not_related
(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018). Pretraining and transfer learning have also been used for neuro/brain imaging Mensch et al. (2017); Thomas et al. (2019); Mahmood et al. (2020); Li et al.,,,0,not_related
(2019); Devlin et al. (2018); Lugosch et al. (2019) traditionally employ unsupervised/self-supervised pre-training Erhan et al.,,,0,not_related
(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018). Pretraining and transfer learning have also been used for neuro/brain imaging Mensch et al. (2017); Thomas et al.,,,0,not_related
(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018).,,,0,not_related
"(2019); Devlin et al. (2018); Lugosch et al. (2019) traditionally employ unsupervised/self-supervised pre-training Erhan et al. (2010). Furthermore, self-supervised methods with mutual information objective are able to preform competitively with supervised methods Oord et al.",,,0,not_related
(2018); Bachman et al. (2019) and are suitable for a number of applications Anand et al.,,,0,not_related
(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018). Pretraining and transfer learning have also been used for neuro/brain imaging Mensch et al. (2017); Thomas et al. (2019); Mahmood et al.,,,0,not_related
"Previous works in RL have constructed various auxiliary tasks to learn better representations, such as methods based on temporal structures (Aytar et al., 2018) and local spatial structures (Anand et al., 2019).",,,0,not_related
", 2018) and local spatial structures (Anand et al., 2019).",,,0,not_related
"Adopting the self-supervised objective [26, 39, 2, 34], most approaches in this field consider learning features [18, 41] of high-dimensional (eg, image-based) states in the environment, then (1) adopt the non-parametric measurement function to acquire rewards [23, 33, 42, 53, 44, 32] or (2) enable policy transfer [23, 16, 17, 13, 22] over the learned features.",,,0,not_related
"Self-supervised learning of visual representations from unlabeled images is a fundamental task of machine learning, which establishes various applications including object recognition [1, 2], reinforcement learning [3, 4], out-of-distribution detection [5, 6], and multimodal learning [7, 8].",,,0,not_related
"This information is exposed in the RAM state of the Atari emulator, as shown in Anand et al. (2019).",,,1,related
"[50], wherein data was collected by a purely random policy, which may well fail to explore many relevant regions of the games.",,,0,not_related
"[50]‚Äîhowever, as we require slot-level rather than flat embeddings, the final layers of our encoder are different.",,,1,related
"The learned representation should facilitate efficient downstream learning [19, 25, 26] and exhibit better generalization [27, 28].",,,0,not_related
Anand et al. (2019) learn state representation for an RL agent in an unsupervised setting and introduce a set of probe tasks to evaluate the representation learnt by agents.,,,0,not_related
"Anand et al. (2019) show that representations from encoders trained with ST-DIM contain a great deal of information about environment states, but they do not examine whether or not representations learned via their method are, in fact, useful for reinforcement learning.",,,0,not_related
"Two works similar to ours, Anand et al. (2019) and Stooke et al. (2021), propose reward-free temporalcontrastive methods to pretrain representations.",,,0,not_related
"These factors have led to Atari‚Äôs extensive use for representation learning and exploratory pretraining (Anand et al., 2019; Stooke et al., 2021; Campos et al., 2021), and specifically Atari 100k for data-efficient RL (e.",,,0,not_related
"These factors have led to Atari‚Äôs extensive use for representation learning and exploratory pretraining (Anand et al., 2019; Stooke et al., 2021; Campos et al., 2021), and specifically Atari 100k for data-efficient RL (e.g., Kaiser et al., 2019; Kostrikov et al., 2021; Schwarzer et al., 2021),‚Ä¶",,,0,not_related
"And it has achieved remarkable success in unsupervised learing task in images [6], videos [2] and natural language processing [4].",,,0,not_related
"Contrastive loss objectives typically aim to distinguish either sequential states from non-sequential ones (Shelhamer et al., 2016; Anand et al., 2019; Stooke et al., 2020), real states from predicted ones (Van den Oord et al.",,,0,not_related
"Contrastive loss objectives typically aim to distinguish either sequential states from non-sequential ones (Shelhamer et al., 2016; Anand et al., 2019; Stooke et al., 2020), real states from predicted ones (Van den Oord et al., 2018), or determine whether two augmented views came from the same or‚Ä¶",,,0,not_related
"Similarly, CPC [38], ST-DIM [1], DRIML [34], ATC [42] and PI-SAC [31] maximize the mutual information between the current state and the future state by using InfoNCE [38, 42], Deep InfoMax [20, 1, 34], or Conditional Entropy Bottleneck [9, 31].",,,0,not_related
"Previous works have demonstrated that good auxiliary supervision can significantly improve agent learning, like leveraging image reconstruction [49], the prediction of future states [41, 13, 30, 40], maximizing Predictive Information [38, 1, 34, 42, 31], or promoting discrimination through contrastive learning [29, 59, 32, 25].",,,0,not_related
"(i) Auxiliary task based methods introduce auxiliary task to help representation learning of the states [21, 49, 41, 13, 30, 40, 38, 1, 34, 42, 31, 29, 59, 32].",,,0,not_related
"For example, successor representations [32, 68] orthogonalize the features of the representation and standard temporal contrastive losses use the representation for imitation [59], end-to-end task solving [2, 62] or flat exploration [69, 28].",,,0,not_related
", where pixel similarities in consecutive frames in Atari games can help decode object identities [15], or the underlying objectlayout map [16].",,,0,not_related
"To do so, we propose Cross Trajectory Representation Learning (CTRL), which applies a novel self-supervised learning (SSL) objective to pairs of trajectories drawn from the agent‚Äôs policies.",,,1,related
"The end result is an agent whose encoder maps behaviorally similar trajectories to similar representations without directly referencing reward, which we show improves ZSG performance over using pure RL or RL in conjunction with other unsupervised or SSL methods.",,,1,related
"While successful in their own way, prior works that combine SSL with RL do so by applying known SSL algorithms [e.g., from vision, 22, 42, 6, 11, 21, 19] to RL in a nearly off-the-shelf manner, predicting state representations within a given trajectory, only potentially using other trajectories as counterexamples in a contrastive loss.",,,0,not_related
"SSL formulates objectives by generating different views of the data, which are essentially transformed versions of the data, e.g., generated by using data augmentation or by sampling patches.",,,0,not_related
"Our main contributions are as follows:
‚Ä¢ We introduce Cross Trajectory Representation Learning (CTRL), a novel SSL algorithm for RL that defines an auxiliary objective across trajectories, drawing samples for a SSL predictive task by leveraging assignments from an online clustering algorithm.",,,1,related
"We also compare with two SSL-based auxiliary objectives: CURL [38], a common SSL baseline which contrasts augmented instances of the same state; and Proto-RL [47], which we adapt for this generalization setting and denote PPO+Sinkhorn.",,,1,related
We then compare to several unsupervised and SSL auxiliary objectives used in conjunction with PPO.,,,1,related
"Therefore, CTRL has a second step: drawing inspiration from Mine Your Own View [MYOW, 5], it selects (mines) representational nearest neighbors from different, nearby clusters and applies a predictive SSL objective to them.",,,1,related
A recent thorough comparison of SSL and bisimulation frameworks also suggests that self-supervised objectives can outperform bisimulation-based algorithms on offline tasks [46].,,,0,not_related
"A successful class of models that incorporate unsupervised objectives to improve RL use self-supervised learning (SSL) [3, 38, 27, 36, 39].",,,0,not_related
CTRL does this by using a predictive SSL objective between representations of ‚Äúsimilar‚Äù trajectories drawn from RL policies.,,,0,not_related
"In these experiments, we employ a probing technique similar to the one described in Anand et al. (2019).",,,1,related
"For example, (Anand et al., 2020) introduce a method that learns state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations.",,,0,not_related
"We find that the objects extracted from the self-supervised attention masks are reasonably focused on salient objects, as compared to both the ground truth objects extracted from (Anand et al. 2019) and the Transporter (Kulkarni et al.",,,1,related
We compute these metrics using the predicted object locations and the ground truth locations from Anand et al. (2019).,,,1,related
"We find that the objects extracted from the self-supervised attention masks are reasonably focused on salient objects, as compared to both the ground truth objects extracted from (Anand et al. 2019) and the Transporter (Kulkarni et al. 2019) method.",,,1,related
"Despite being a relatively new area of study, it has been the basis for many excellent works such as contrastive predictive coding [20, 43], representation learning using Deep InfoMax (DIM) [4, 21, 59] or momentum contast [18], learning invariances using Augmented Multiscale DIM [9] or Contrastive MultiView Coding [56] etc.",,,0,not_related
"Despite being a relatively new area of study, it has been the basis for many excellent works such as contrastive predictive coding [20, 43], representation learning using Deep InfoMax (DIM) [4, 21, 59] or momentum contast [18], learning invariances using Augmented Multiscale DIM [9] or Contrastive MultiView Coding [56] etc.
Previous deep learning approaches have shown promises in identifying COVID-19 cases from chest radiography images [5, 13, 14, 19, 22, 46, 61‚Äì64, 71, 72], please refer to Table¬†1.",,,0,not_related
"Representation learning in RL: Recently, using unsupervised/self-supervised representation learning methods to improve sample efficiency and/or performance in RL has gained increased popularity [2, 25, 37, 39, 40].",,,0,not_related
"Such an interpretable and layered model of images could be beneficial for a plethora of applications like object discovery [16, 6], image edition [75, 21], future frame prediction [74], object pose estimation [57] or environment abstraction [1, 47].",,,0,not_related
The generalization of this idea as a self-supervised task(also called auxiliary-task and pretext) is a relatively new paradigm in machine learning literature [14][2].,,,0,not_related
[51] introduces a new contrastive loss to improve sample efficiency in Atari benchmark.,,,0,not_related
"In contrast with [RUMS18], who use a heuristic to find the agent‚Äôs position from the screen‚Äôs pixels, we use the Atari annotated RAM interface wrapper [ARO19].",,,1,related
"[ARO19] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C√¥t√©, and R Devon Hjelm.",,,0,not_related
Anand et al. (2019) provide a method for learning representations via loss functions that operate directly on the hidden layers of encoder neural networks.,,,0,not_related
The evaluation metrics are the number of ram states visited using [2] and the downstream zero shot performance on Atari game.,,,0,not_related
", 2018) and local spatial structure (Anand et al., 2019) has been leveraged for state representation learning via contrastive losses.",,,0,not_related
"Contrastive learning has seen dramatic progress recently, and been introduced to learn state representation (Oord et al., 2018; Sermanet et al., 2018; Dwibedi et al., 2018; Aytar et al., 2018; Anand et al., 2019; Srinivas et al., 2020).",,,0,not_related
"Temporal structure (Sermanet et al., 2018; Aytar et al., 2018) and local spatial structure (Anand et al., 2019) has been leveraged for state representation learning via contrastive losses.",,,0,not_related
"CPC (van den Oord et al., 2018), CPA|action (Guo et al., 2018), ST-DIM (Anand et al., 2019), and DRIML (Mazoure et al., 2020) apply different forms of temporal contrastive losses to learn predictions in a latent space.",,,0,not_related
"CPC [30], CPC|action [10], ST-DIM [1], DRIML [19], and ATC [25] apply different forms of temporal contrastive losses to learn predictions in a latent space.",,,0,not_related
"Mutual information estimation [10] has inspired a number of successful uses for single-view data, such as deep infomax (DIM) [11], contrastive predictive coding (CPC) [12]) and for multi-view data, such as augmented multiscale DIM (AMDIM) [13], contrastive multiview coding (CMC) [14], SimCLR [15]) image classification, reinforcement learning (spatio-temporal DIM (ST-DIM) [16]) and zero-shot learning (class matching DIM (CM-DIM) [17], [18]).",,,0,not_related
"Further, AMDIM [13], ST-DIM [16], and CMDIM [17] are referred to as XX, which captures a CrossConvolution-to-Representation relation and as CC, which captures a Convolution-to-Convolution relation.",,,0,not_related
"infomax (DIM) [11], contrastive predictive coding (CPC) [12]) and for multi-view data, such as augmented multiscale DIM (AMDIM) [13], contrastive multiview coding (CMC) [14], SimCLR [15]) image classification, reinforcement learning (spatio-temporal DIM (ST-DIM) [16]) and zero-shot learning (class matching DIM (CM-DIM) [17], [18]).",,,0,not_related
"Methods for accomplishing this goal are being developed in the emerging field of state representation learning (Anand et al., 2019; Higgins et al., 2018b; Jaderberg et al., 2016; Lesort et al., 2018; van denOord et al., 2019; Srinivas et al., 2020; Zhang et al., 2020).",,,0,not_related
"‚Ä¢ CSS [1, 3]: Single-view CSS loss using Eq.",,,0,not_related
"Many current CSS approaches [3, 16, 1] first take a reference input I, a positive sample I, and n ‚àí 1 negative samples Ik for 1 ‚â§ k < n, and then minimize the loss",,,0,not_related
"By contrast, popular approaches to using CSS [16, 1, 21, 3] neither learn time-invariant components nor try to reconstruct the input images.",,,0,not_related
"ch so is to treat a video frame close to the one of interest as positive sample, and a temporally-distant one as negative sample [1, 21].",,,0,not_related
"While [7] exploits depth, other works [21, 1, 3] leverage CSS, with [21, 1] using it in single-view videos.",,,0,not_related
"However, we diverge from standard CSS formulations [8], [10], [11], [12] in the two following aspects: 1) Instead of applying the con-",,,1,related
"CSS [10], [11]: Single-view CSS loss using Eq.",,,0,not_related
"While [22] takes depth as input in addition to RGB, other works [10], [11], [12] leverage CSS, with [12] and [11] using it for only single-view RGB videos.",,,0,not_related
"In our context, one way to do so is to treat a video frame close to the one of interest as positive, and a temporally-distant one as negative [11], [12].",,,0,not_related
"Given a reference input I, a positive sample I√æ, and n 1 negative samples I k for 1 k < n, standard CSS approaches [8], [10], [11] minimize the loss",,,1,related
[5] by introducing regression and non-linear probing.,,,0,not_related
"[5] formulate the task of predicting each state variable as separate 256-way classification problem, as each byte of RAM can represent 256 possible values, regardless of the nature of each variable.",,,0,not_related
[5] introduced the Atari Annotated RAM Interface (AtariARI) to enable the evaluation of state representation,,,0,not_related
"A final approach is to ‚Äúprobe‚Äù the learned representations by training small regression or classification models to predict ground-truth data from learned representations [5], [13].",,,0,not_related
"For our investigations, we assess the quality of the encoding of important state variables for each game by employing a novel evaluation method that probes the contents of the learnt state representations using ground truth state information provided by the Atari Annotated RAM Interface [5].",,,1,related
To evaluate the quality of the learned representations we proposed and utilised novel extensions to an evaluation method that probes the representations using the AtariARI [5].,,,1,related
Section III describes our extensions to the approach for evaluating state representation learning methods proposed in [5].,,,1,related
[5] used such a method for evaluating learned representations for Atari games using linear classification probes (single layer neural networks).,,,0,not_related
"Comparison to pixel-based world models Aligned with prior studies of VAE losses on Atari [3], our attempts to use a world model based on pixel reconstruction (e.",,,0,not_related
ST-DIM [16] and CM-DIM [17] apply this in turn to reinforcement learning and zero-shot learning respectively.,,,0,not_related
"The corresponding objective function can cause the model to waste its capacity on reconstructing unimportant features such as static backgrounds and noise, while ignoring visually small but important details in its learned representation (Anand et al., 2019; Kipf et al., 2019).",,,0,not_related
"A problem of this objective is that it can cause the model to waste its capacity on reconstructing unimportant features, such as static backgrounds, while ignoring the visually small but important details in its learned representation (Anand et al., 2019; Kipf et al., 2019).",,,0,not_related
[1] have proposed to maximize the mutual information between representations of two consecutive frames of the environment.,,,0,not_related
"In future work, we want to incorporate methods for unsupervised state representation learning (Burgess et al., 2019; Anand et al., 2019).",,,1,related
"2018), or used as an auxiliary task to study representations for high-dimensional data (Srinivas, Laskin, and Abbeel 2020; Anand et al. 2019).",,,0,not_related
"In RL, it has been used to extract reward signals in latent space (Sermanet et al. 2018; Dwibedi et al. 2018), or used as an auxiliary task to study representations for high-dimensional data (Srinivas, Laskin, and Abbeel 2020; Anand et al. 2019).",,,0,not_related
"An example of this is Spatiotemporal DeepInfomax (ST-DIM) [2], where two mutual information objective functions are defined.",,,0,not_related
Whereas the ST-DIM section is based on Anand et al.‚Äôs [2] previous work and aims to encode the high level game variables.,,,0,not_related
‚Äôs [2] previous work and aims to encode the high level game variables.,,,0,not_related
"ATC requires a model to associate observations from nearby time steps within the same trajectory (Anand et al., 2019).",,,0,not_related
"ST-DIM (Anand et al., 2019) introduced various temporal, contrastive losses, including ones that operate on ‚Äúlocal‚Äù features from an intermediate layer within the encoder, without data augmentation.",,,0,not_related
"We also compare against Spatio-Temporal Deep InfoMax (ST-DIM), which uses temporal contrastive losses with ‚Äúlocal‚Äù features from an intermediate convolution layer to ensure attention to the whole screen; it was shown to produce detailed game-state knowledge when applied to individual frames (Anand et al., 2019).",,,1,related
"(Mazoure et al., 2020) provided extensive analysis pertaining to InfoNCE losses on functions of successive time steps in MDPs, including local features in their auxiliary loss (DRIML) similar to ST-DIM, and finally conducted experiments using global temporal contrast of augmented observations in the Procgen (Cobbe et al., 2019) environment.",,,0,not_related
"‚Ä¶against Spatio-Temporal Deep InfoMax (ST-DIM), which uses temporal contrastive losses with ‚Äúlocal‚Äù features from an intermediate convolution layer to ensure attention to the whole screen; it was shown to produce detailed game-state knowledge when applied to individual frames (Anand et al., 2019).",,,0,not_related
Such contrastive predictive coding has been successfully used to find useful latent representations of ATARI games [13] and deformable objects [14].,,,0,not_related
"Simpler probing tasks (Belinkov and Glass, 2019; Jawahar et al., 2019; Anand et al., 2019) test the performance of classifiers that train on the",,,0,not_related
"Simpler probing tasks (Belinkov and Glass, 2019; Jawahar et al., 2019; Anand et al., 2019) test the performance of classifiers that train on the representation learnt by a model.",,,0,not_related
"Due to the similarity between RL and dialogue, we draw inspirations from Anand et al. (2019)‚Äôs probing tasks on game playing agent.",,,1,related
Anand et al. (2019) learn state representation for an RL agent in an unsupervised setting and introduce a set of probing tasks to evaluate the representation learnt by agents.,,,0,not_related
"No pre-training 78¬± 4 79¬± 7 48¬± 4 12¬± 1 57¬± 1 37¬± 4 10¬± 1 18¬± 2 Image Net 86¬± 1 89¬± 3 66¬± 1 12¬± 0 21¬± 3 19¬± 3 13¬± 5 23¬± 2 Contrastive (ST-DIM) 73¬± 2 84¬± 4 71¬± 3 10¬± 1 66¬± 5 49¬± 2 17¬± 0 21¬± 1 Forward 68¬± 3 84¬± 3 68¬± 8 9¬± 1 49¬± 5 37¬± 3 18¬± 5 13¬± 2 Inverse 73¬± 4 82¬± 2 61¬± 3 8¬± 1 83¬± 2 67¬± 8 26¬± 8 8¬± 0 Behavior Cloning (BC) 91¬± 1 91¬± 4 68¬± 5 8¬± 1 83¬± 3 61¬± 4 25¬± 4 8¬± 1 CILRS 0.8.4 [11] 97¬± 2 83¬± 0 42¬± 2 47 66¬± 2 49¬± 5 23¬± 1 64 LBC [34] 97¬± 1 93¬± 1 71¬± 5 N/A 100¬± 0 94¬± 3 51¬± 3 N/A
Table 3: Comparison of action-based pre-training with baselines (top) and other methods from the literature (bottom).",,,0,not_related
"Further, our approach, improves over other recent pre-training proposals such as contrastive methods [16] and even over ImageNet (supervised) pre-training.",,,0,not_related
"The constrastive method, ST-DIM, shows promising results for the binary affordances but results on very poorly relative angle estimations.",,,0,not_related
"In fact, we include in our study a recent contrastive method, ST-DIM, designed in the context of playing Atari games [16], adapted
to actions required for driving.",,,1,related
"In short, ST-DIM is trained to answer if two frames are consecutive or not, without any action-related information
Binary Affordances Relative Angle (œàt) Pre-training Pedestrian (hp) Vehicle (hv) Red T.L. (hr) Left Turn Straight Right Turn
No pre-training 26¬± 0 50¬± 1 42¬± 0 11.38¬± 0.18 1.85¬± 0.03 24.68¬± 0.03 ImageNet 37¬± 2 75¬± 0 47¬± 0 11.69¬± 0.57 2.83¬± 0.07 25.55¬± 0.10 Contrastive (ST-DIM) 47¬± 1 53¬± 0 53¬± 0 10.43¬± 0.21 2.62¬± 0.03 18.75¬± 0.23 Forward 50¬± 0 63¬± 0 60¬± 0 5.35¬± 0.03 0.52¬± 0.00 6.61¬± 0.03 Inverse 49¬± 0 78¬± 0 70¬± 0 3.57¬± 0.03 0.46¬± 0.00 3.78¬± 0.06 Behavior Cloning (BC) 47¬± 0 81¬± 0 75¬± 0 4.89¬± 0.03 1.24¬± 0.03 6.25¬± 0.10
Table 1: Linear probing results.",,,0,not_related
"For ST-DIM, as expected, the difference between random and expert policy is smaller since it is not based on action.",,,0,not_related
"For better analysis, we divide the relative heading angle into three cases, left turn, straight and right turn,
Binary Affordances Relative Angle (œàt) Pre-training Pedestrian (hp) Vehicle (hv) Red T.L. (hr) Left Turn Straight Right Turn
No pre-training 26¬± 0 50¬± 1 42¬± 0 11.38¬± 0.18 1.85¬± 0.03 24.68¬± 0.03 Contrastive (ST-DIM) 41¬± 0 62¬± 1 63¬± 1 9.01¬± 0.46 2.77¬± 0.18 18.37¬± 0.45 Contrastive Random (ST-DIM) 39¬± 1 73¬± 1 47¬± 0 9.70¬± 0.41 2.98¬± 0.11 15.89¬± 0.41 Forward 50¬± 0 51¬± 0 58¬± 0 4.87¬± 0.00 0.52¬± 0.00 6.07¬± 0.06 Forward Random 20¬± 1 38¬± 0 16¬± 0 11.54¬± 0.03 1.20¬± 0.00 19.14¬± 0.00 Inverse 45¬± 0 66¬± 0 73¬± 0 3.02¬± 0.03 0.42¬± 0.03 5.06¬± 0.17 Inverse Random 26¬± 0 49¬± 0 59¬± 0 8.50¬± 0.53 1.45¬± 0.03 13.14¬± 0.34
Table 2: Linear probing results comparing encoders trained with random policy training data versus expert demonstration data.",,,1,related
"Moreover, for the Inverse, Forward, and ST-DIM strategies, we have included seldom variants which require to collect additional ‚àº 20 hours of image sequences in T1.",,,1,related
Both models also clearly outperform the constrastive-based baseline (ST-DIM) in new town.,,,0,not_related
"In fact, we include in our study a recent contrastive method, ST-DIM, designed in the context of playing Atari games [16], adapted",,,1,related
We will see how action-based representation learning outperforms ST-DIM as a self-supervised representation learning strategy to infer affordances.,,,1,related
"In addition, we have incorporated ST-DIM [16], a contrastive representation learning baseline used by agents playing Atari games.",,,1,related
"We see, the pre-trained MILC models outperform NPT and also ST-DIM based pre-trained models.",,,1,related
"Note, with very few samples, models based on the pre-trained MILC (FPT and UFPT) outperform the un-pre-trained models (NPT), ST-DIM models, autoencoder based models.",,,1,related
We compare MILC with ST-DIM based pre-training shown in [24].,,,1,related
ST-DIM [1] has been used for pre-training on unrelated data with subsequent use for classification [24].,,,0,not_related
"ST-DIM based pre-training model [24] performs reasonably well compared to autoencoder and NPT models, however, MILC steadily outperforms ST-DIM.",,,0,not_related
"They were shown to benefit structural MRI analysis [10], learn useful representations from the frames in Atari games [1] and for speaker identification [28].",,,0,not_related
"Contrastive learning is a
popular type of self-supervised learning, which has proven effective for video [Gordon et al., 2020, Knights et al., 2020] and reinforcement learning tasks [Oord et al., 2018, Anand et al., 2019, Srinivas et al., 2020, Mazoure et al., 2020, Schwarzer et al., 2020].",,,0,not_related
"[2] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C√¥t√©, and R Devon Hjelm.",,,0,not_related
"The linear probing setting is widely studied in literature [1, 21, 10] and applied in practice [4, 2].",,,0,not_related
"This is different from some other approaches [1, 30] that use contrastive methods (e.",,,0,not_related
"This work falls in another group, where future prediction is used as an auxiliary or representation learning method for model-free RL agents [1, 10, 14, 22, 29, 30, 33, 34, 35, 37].",,,0,not_related
"Following (Anand et al., 2019), we train our
model with 100,000 frames acquired with a random agent on the Atari games; an additional 50,000 frames are used for training and testing the evaluation probes.",,,1,related
"As a result, we aim to learn structured, object-centric slot representations harnessing time and using a self-supervised time-contrastive signal similar to (Anand et al., 2019; Hyvarinen & Morioka, 2017) to learn each object‚Äôs representation, but also a ‚Äúslot contrastive‚Äù signal as an attempt to force each slot to capture a unique object compared to the other slots.",,,1,related
"This is the intuition that motivates time-contrastive losses (Hyvarinen & Morioka, 2017; Anand et al., 2019; Sermanet et al., 2018); learning state representations that make it easy to predict the temporal distance between states, will potentially ensure that these representations capture time‚Ä¶",,,0,not_related
"However, most of these techniques involve generative models, which have two issues: wasted capacity on modelling spurious background pixels (Oord et al., 2018) and inability to capture small objects (Anand et al., 2019).",,,0,not_related
"This is the intuition that motivates time-contrastive losses (Hyvarinen & Morioka, 2017; Anand et al., 2019; Sermanet et al., 2018); learning state representations that make it easy to predict the temporal distance between states, will potentially ensure that these representations capture time dependent features.",,,0,not_related
"Slot Accuracy For slot accuracy, we use linear probing, a technique commonly used in self-supervised learning (Anand et al., 2019; Hjelm et al., 2018; Chen et al., 2020) and disentangling (Locatello et al.",,,0,not_related
", 2018) and contrastive approaches (Hyvarinen & Morioka, 2017; Oord et al., 2018; Anand et al., 2019) have begun to harness time in their self-supervised signal.",,,0,not_related
"The architectural details of all models are similar to (Anand et al., 2019).",,,0,not_related
"As a result, many self-supervised pretext approaches (Misra et al., 2016; Aytar et al., 2018) and contrastive approaches (Hyvarinen & Morioka, 2017; Oord et al., 2018; Anand et al., 2019) have begun to harness time in their self-supervised signal.",,,0,not_related
"‚Ä¶we aim to learn structured, object-centric slot representations harnessing time and using a self-supervised time-contrastive signal similar to (Anand et al., 2019; Hyvarinen & Morioka, 2017) to learn each object‚Äôs representation, but also a ‚Äúslot contrastive‚Äù signal as an attempt to force‚Ä¶",,,1,related
"For evaluation, we use labels from the AtariARI dataset (Anand et al., 2019), restricting ourselves to labels that correspond to the x or y coordinates of objects.",,,1,related
", 2018) and inability to capture small objects (Anand et al., 2019).",,,0,not_related
"Slot Accuracy For slot accuracy, we use linear probing, a technique commonly used in self-supervised learning (Anand et al., 2019; Hjelm et al., 2018; Chen et al., 2020) and disentangling (Locatello et al., 2018).",,,0,not_related
", 2018), ST-DIM (Anand et al., 2019) and DRIML (Mazoure et al.",,,0,not_related
"CPC (Oord et al., 2018), CPC|Action (Guo et al., 2018), ST-DIM (Anand et al., 2019) and DRIML (Mazoure et al., 2020) propose to optimize various temporal contrastive losses in reinforcement learning environments.",,,0,not_related
", 2018), ST-DIM (Anand et al., 2019) and DRIML (Mazoure et al.",,,0,not_related
"CPC (Oord et al., 2018), CPC|Action (Guo et al., 2018), ST-DIM (Anand et al., 2019) and DRIML (Mazoure et al., 2020) propose to optimize various temporal contrastive losses in reinforcement learning environments.",,,0,not_related
"In the process of improving sample efficiency, we address several important questions over prior work in auxiliary self-supervised learning, from both the supervised [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] and reinforcement learning paradigms [16, 17, 18, 19, 20, 21, 22, 23, 24, 25].",,,0,not_related
"ing [17, 19], or spatio-temporal mutual information maximization [20, 23] derive supervision from the agent‚Äôs own experience.",,,0,not_related
"In contrast to prior work, which focus on simpler and non-photorealistic environments [17, 19, 20, 21, 22], we focus on visually complex, photorealistic environments from the Gibson 3D scans [30].",,,0,not_related
of a sequence of T = 4 time-steps times the number of features provided by the ATARIARI interface [1].,,,0,not_related
"This could be alleviated by using predictive models with strong inductive biases or unsupervised representation learners, such as deep-infomax [1] or similar noise-contrastive methods.",,,0,not_related
"To assess this, we use NEAT to evolve policies for playing Atari 2600 games from the recently released Atari Annotated RAM Interface (Atari ARI) [2].",,,1,related
The Atari ARI [2] provides RAM annotations for 22 of the Atari 2600 games supported by the OpenAI Gym toolkit1.,,,0,not_related
"Recently, Spatiotemporal Deep Infomax (ST-DIM) [27] leverages temporal and spatial information in expert demonstrations to learn the state representations.",,,0,not_related
"Contrastive losses have also been constructed from the rules of physics in robotics tasks (Jonschkowski and Brock, 2015), have been applied to Atari models (Anand et al., 2019), and have combined with the above object-oriented approach (Kipf et al.",,,0,not_related
"Domains with rich observations include raw images or video frames from video games (Anand et al., 2019), robotics environments (Higgins et al., 2017), and renderings of classic control problems (Watter et al., 2015), and deep learning methods have enabled success in this space. Srinivas et al. (2020) use a contrastive learning approach to extract state representations from pixels.",,,0,not_related
"Domains with rich observations include raw images or video frames from video games (Anand et al., 2019), robotics environments (Higgins et al., 2017), and renderings of classic control problems (Watter et al., 2015), and deep learning methods have enabled success in this space. Srinivas et al. (2020) use a contrastive learning approach to extract state representations from pixels. Ha & Schmidhuber (2018) learn low-dimensional representations and dynamics which simple linear policies to achieve effective control. Hafner et al. (2019a) utilize latent imagination to learn behaviors that achieve high performance in terms of reward and sample-efficiency on several visual control tasks.",,,0,not_related
"Domains with rich observations include raw images or video frames from video games (Anand et al., 2019), robotics environments (Higgins et al.",,,0,not_related
"Domains with rich observations include raw images or video frames from video games (Anand et al., 2019), robotics environments (Higgins et al., 2017), and renderings of classic control problems (Watter et al., 2015), and deep learning methods have enabled success in this space. Srinivas et al. (2020) use a contrastive learning approach to extract state representations from pixels. Ha & Schmidhuber (2018) learn low-dimensional representations and dynamics which simple linear policies to achieve effective control.",,,0,not_related
"Domains with rich observations include raw images or video frames from video games (Anand et al., 2019), robotics environments (Higgins et al., 2017), and renderings of classic control problems (Watter et al., 2015), and deep learning methods have enabled success in this space.",,,0,not_related
"Such approaches may be viewed as postulating concrete latent relations: g(st, st+1) = c , where g is the squared distance between st and st+1 for Lcont, and a more complicated relation for [2].",,,1,related
"[2] provides such a framework for ATARI games, but it is not aimed at continuous control.",,,0,not_related
A related heuristic from [2] maximizes mutual information between parts of consecutive latent states.,,,0,not_related
"using continuity [1], mutual information with prior states [2], consistency with a forward or inverse model (see [3] for a survey).",,,0,not_related
"For the auxiliary objective, we follow a variant of Deep InfoMax [DIM, Hjelm et al., 2018, Anand et al., 2019, Bachman et al., 2019], and train the encoder to maximize the mutual information (MI) between local and global ‚Äúviews‚Äù of tuples (st, at, st+k).",,,1,related
"‚Ä¶typically either taken from different ‚Äúlocations‚Äù of the data [e.g., spatial patches or temporal locations, see Hjelm et al., 2018, Oord et al., 2018, Anand et al., 2019, H√©naff et al., 2019] or obtained through data augmentation [Wu et al., 2018, He et al., 2019, Bachman et al., 2019, Tian et‚Ä¶",,,0,not_related
"Our work, DRIML, predicts future states conditioned on the current state-action pair at multiple scales, drawing upon ideas encapsulated in Augmented Multiscale Deep InfoMax [AMDIM, Bachman et al., 2019] and Spatio-Temporal DIM [ST-DIM, Anand et al., 2019].",,,1,related
"L G
] 1
6 N
ov 2
with model-like properties, we consider a self-supervised objective derived from variants of Deep InfoMax [DIM, Hjelm et al., 2018, Bachman et al., 2019, Anand et al., 2019].",,,1,related
"The InfoMax principal has been extended to graphs [44], [40] and for state representation in reinforcement learning [2].",,,0,not_related
"[10] provides such framework for ATARI games, but it is not aimed for continuous control.",,,0,not_related
"long-studied, fast-growing field in embodied AI research (Schmidhuber, 1990; Ha and Schmidhuber, 2018; Hamrick, 2019; Anand et al., 2019; Kipf et al., 2020), and recently also in learned executors for neural programming (Kant, 2018).",,,0,not_related
"Outside of NLU, learning structured world models is a long-studied, fast-growing field in embodied AI research (Schmidhuber, 1990; Ha and Schmidhuber, 2018; Hamrick, 2019; Anand et al., 2019; Kipf et al., 2020), and recently also in learned executors for neural programming (Kant, 2018).",,,0,not_related
"The learned weights of the encoder of the VAE are frozen, and the latent input is used to train the policy for reset-free RL.",,,1,related
"Lastly, we compare algorithm performance with two ablations: running R3L without the perturbation controller (‚ÄúVICE + VAE‚Äù) and without the unsupervised learning (‚ÄúR3L w/o VAE‚Äù).",,,1,related
"We then compare with prior reset-free RL algorithms (Eysenbach et al., 2018) that explicitly learn a reset controller to alternate goals in the state space (‚ÄúReset Controller + VAE‚Äù).",,,1,related
"Fig 8 compares the performance of our method without supervised learning (‚ÄúR3L w/o VAE‚Äù) in the real world against a baseline that uses SAC for vision-based RL from raw pixels, VICE for providing rewards, and running reset-free (denoted as ‚ÄúVICE‚Äù).",,,1,related
"Note that we use a VAE as an instantiation of representation learning techniques that works well in the domains we considered, but other more sophisticated density models proposed in prior work may also be substituted in place of the VAE (Lee et al., 2019; Hjelm et al., 2019; Anand et al., 2019).",,,1,related
"B.0.1 HYPERPARAMETERS
General Standard deviation update coefficient 0.99 Image Sizes [(16, 16, 3), (32, 32, 3), (64, 64, 3)] SAC Learning Rate 3e-4 Œ≥ 0.99 Batch Size 256 Convnet Filters [(64, 64, 64), (16, 32, 64)] Stride (2, 2) Kernel Sizes (3, 3) Pooling [MaxPool2D, None] Actor/Critic FC Layers [(512, 512), (256, 256, 256)] VICE nVICE [1, 5, 10] Batch Size 128 Learning Rate 1e-4 Mixup Œ± Uniform(0, 1) Convnet Filters [(64, 64, 64), (16, 32, 64)] Stride (2, 2) Kernel Sizes (3, 3) Pooling [MaxPool2D, None] FC Layers [(512, 512), (256, 256, 256)] RND Learning Rate 3e-4 Batch Size 256 Convnet Filters (16, 32, 64) Stride (2, 2) Kernel Sizes (3, 3) Pooling [MaxPool2D, None] FC Layers [(512, 512), (256, 256, 256)] VAE Learning Rate 1e-4 Batch Size 256 Encoder (Convnet) Filters (64, 64, 32) Latent Dimension [8, 16, 32, 64] Œ≤ [1e-3, 0.1, 0.5, 1, 10] Stride (2, 2) Kernel Sizes (3, 3) Pooling [MaxPool2D, None]
The ranges of values listed above represent the hyperparameters we searched over, and the bolded values are what we use in the Section 6 experiments.",,,0,not_related
"B.0.4 VAE
We train a standard beta-VAE to maximize the evidence lower bound, given by:
Ez‚àºqœÜ(z|x)[pŒ∏(x|z)]‚àí Œ≤DKL(qœÜ(z|x) || pŒ∏(z))
To collect training data, we sampled random states in the observation space.",,,1,related
"Note that we use a VAE as an instantiation of representation learning techniques that works well in the domains we considered, but other more sophisticated density models proposed in prior work may also be substituted in place of the VAE Lee et al. (2019); Hjelm et al. (2019); Anand et al. (2019).",,,1,related
"We therefore aim to convert the vision-based learning problem into one that more closely resembles state-based learning, by training a variational autoencoder (VAE, Kingma & Welling (2013)) and sharing the latent-variable representation across the actor and critic networks (refer to Appendix B for more details).",,,1,related
"The fact that we can treat a cell as an object allows us to evaluate our error-correcting strategy as proof of concept, since unsupervised object detection for control tasks is still an nascent area of research [23]‚Äì[25].",,,1,related
"Contrastive learning has been used to extract reward signals in the latent space (Sermanet et al., 2018; Dwibedi et al., 2018; WardeFarley et al., 2018); and study representation learning on Atari games by Anand et al. (2019).",,,0,not_related
", 2018); and study representation learning on Atari games by Anand et al. (2019). World Models for sample-efficiency: While joint learning of an auxiliary unsupervised task with model-free RL is one way to improve the sample-efficiency of agents, there has also been another line of research that has tried to learn world models of the environment and use them to sample rollouts and plan.",,,0,not_related
[2] use the NCE loss to discriminate between temporally near frames and temporally far frames of ATARI gameplay but do not compare across games.,,,0,not_related
"They yield good performance for learning image [13, 31, 33, 35, 45, 52, 58, 70, 71, 77] and video [3, 28, 34, 46, 49, 54, 66, 68, 82] representations, and circumvent the need to explicitly specify what information needs to be discarded via a designed task.",,,0,not_related
"Several works have previously explored information-theoretic approaches for representation learning in the reinforcement learning context (Nachum et al., 2018; Anand et al., 2019; Lu et al., 2019).",,,0,not_related
"based on self-supervision that use alternatives to reconstruction of input states [1, 2, 4, 11, 30, 40, 53].",,,0,not_related
"Similar losses have often been used in related work [2, 11, 13, 30, 40], which we compare in Section 5.",,,0,not_related
"Certain works focus on predicting the next state using a contrastive loss [2, 30, 40], disregarding the reward function.",,,0,not_related
"For self-supervised pre-training we use Spatio-Temporal DeepInfoMax [23] to maximize predictability between current latent state and future spatial state and between consecutive spatial states (for example, on encoded time points of the resting-state fMRI (rsfMRI)).",,,1,related
"Furthermore, it influences the neuroimaging field for classification of progression to Alzheimer‚Äôs disease from sMRI [22], learning useful representation of the states from the frames in Atari games [23] and also from the speech chunks for speaker identification [24].",,,0,not_related
"Copyright 2020 by the author(s).
sentation learning for images (Doersch et al., 2015; Noroozi & Favaro, 2016; Larsson et al., 2017; Gidaris et al., 2018; Zhang et al., 2019a), natural language (Devlin et al., 2018), and video games (Anand et al., 2019).",,,0,not_related
", 2018), and video games (Anand et al., 2019).",,,0,not_related
(2018); Antonova et al. (2019) embed action sequences with a VAE.,,,0,not_related
"(Anand et al., 2019) use an MI objective to learn representations for Atari games.",,,0,not_related
Concurrent work [2] applies similar method on reinforcement learning.,,,0,not_related
"The state variables of the Atari games are the controllers of the game dynamics and the details of these state variables are provided in (Anand et al., 2019).",,,0,not_related
", 2018), TDC (Ma and Collins, 2018) and more recently, ST-DIM (Anand et al., 2019), which is the state-of-the-art baseline to our BVS-DIM method.",,,0,not_related
", 2019) and ST-DIM (Anand et al., 2019) even though the first two are general purpose representation learning methods tested on real-world data-sets such as ImageNet (Deng et al.",,,0,not_related
"For this task, and to our knowledge, the Spatio-Temporal Deep InfoMax (ST-DIM) (Anand et al., 2019) is the state-of-the-art baseline.",,,1,related
", 2019) and (Anand et al., 2019) for more details on representation learning through MI maximization between inputs and outputs.",,,0,not_related
"with InfoNCE enables the encoder to learn linearly predictable representations and helps in learning representations at the semantic level (Anand et al., 2019).",,,0,not_related
"In ST-DIM, the ground truth state information (a state label for every example frame generated from the game) has been annotated for each frame of 22 Atari games to make evaluation of the goodness of the representation (See (Anand et al., 2019)).",,,1,related
"However, the learning method of ST-DIM depends on the consecutive time step visual observations (Anand et al., 2019).",,,0,not_related
"Therefore, most recent unsupervised state representations were introduced like TCN (Sermanet et al., 2018), TDC (Ma and Collins, 2018) and more recently, ST-DIM (Anand et al., 2019), which is the state-of-the-art baseline to our BVS-DIM method.",,,0,not_related
"In Atari games, the crucial underlying generative factors of the environment are state variables which can be directly used to control the game dynamics or query the game information (Bellemare et al., 2013), (Anand et al., 2019).",,,0,not_related
"The bi-linear model in combination with InfoNCE enables the encoder to learn linearly predictable representations and helps in learning representations at the semantic level (Anand et al., 2019).",,,0,not_related
"See (Poole et al., 2019) and (Anand et al., 2019) for more details on representation learning through MI maximization between inputs and outputs.",,,0,not_related
"Our work is closely related to DIM (Hjelm et al., 2019), AMDIM (Bachman et al., 2019) and ST-DIM (Anand et al., 2019) even though the first two are general purpose representation learning methods tested on real-world data-sets such as ImageNet (Deng et al., 2009).",,,0,not_related
"Devising a generic way of measurement of the general goodness of the representation is essential (Anand et al., 2019).",,,0,not_related
"Given a reference input I, a positive sample I, and n ‚àí 1 negative samples Ik for 1 ‚â§ k < n, standard CSS approaches [6], [8], [9] minimize the loss",,,0,not_related
"In our context, one way to do so is to treat a video frame close to the one of interest as positive, and a temporally-distant one as negative [9], [10].",,,0,not_related
"However, we diverge from standard CSS formulations [6], [8], [9], [10] in the two following aspects: 1) Instead of applying the contrastive loss to the entire latent space, we enforce it only on the time-variant features, as only part of the latent features should evolve over time.",,,1,related
"While [20] takes depth as input in addition to RGB, other works [8], [9], [10] leverage CSS, with [10] and [9] using it for only single-view RGB videos.",,,0,not_related
"‚Ä¢ CSS [8], [9]: Single-view CSS loss using Eq.",,,0,not_related
"In future work, we want to incorporate methods for unsupervised state representation learning (Burgess et al., 2019; Anand et al., 2019) so CEHRL can learn from observations.",,,1,related
"Anand et al. (2019) compared unsupervised encoders‚Äô ability to represent various features of the state (e.g., number of opponent sprites).",,,0,not_related
These classifiers can be used analogously to determine if reinforcement learning agents encode tactical and strategic conceptual information (Anand et al. 2019; McGrath et al. 2021).,,,0,not_related
"Markov-CPC is a variant of the Contrastive Predictive Coding (CPC) algorithm (Oord et al., 2018) which was shown useful for finding predictive latent variables (Anand et al., 2019; Henaff, 2020; Yan et al., 2020).",,,0,not_related
"(Zhang et al., 2016; Anand et al., 2019).",,,0,not_related
"sigmoid (max{Wsi + b}), to predict VQA-style questions in the form ‚Äúis there a (size, color, material, shape) object in the image?‚Äù (Zhang et al., 2016; Anand et al., 2019).",,,0,not_related
"Along similar lines, (Srinivas et al., 2020; Anand et al., 2019; Sermanet et al., 2017; Finn et al., 2016; Ghosh et al., 2019; Eysenbach et al., 2022; Li et al., 2021; Agarwal et al., 2021) have begun to explore contrastive systems and binary classifiers to do imitation learning and reinforcement‚Ä¶",,,0,not_related
"Along similar lines, (Srinivas et al., 2020; Anand et al., 2019; Sermanet et al., 2017; Finn et al., 2016; Ghosh et al., 2019; Eysenbach et al., 2022; Li et al., 2021; Agarwal et al., 2021) have begun to explore contrastive systems and binary classifiers to do imitation learning and reinforcement learning.",,,0,not_related
"Along similar lines, (Srinivas et al., 2020; Anand et al., 2019; Sermanet et al., 2017; Finn et al., 2016; Ghosh et al., 2019; Eysenbach et al., 2022; Li et al., 2021; Agarwal et al., 2021) have begun to explore contrastive systems and binary classifiers to do imitation learning and reinforcement‚Ä¶",,,0,not_related
"Along similar lines, (Srinivas et al., 2020; Anand et al., 2019; Sermanet et al., 2017; Finn et al., 2016; Ghosh et al., 2019; Eysenbach et al., 2022; Li et al., 2021; Agarwal et al., 2021) have begun to explore contrastive systems and binary classifiers to do imitation learning and reinforcement learning.",,,0,not_related
Van den Oord et al. (2018); Guo et al. (2018); Anand et al. (2019); Mazoure et al. (2020) propose different temporal contrastive losses in reinforcement learning environments.,,,0,not_related
"Recently, multiple contrastive methods have been used to learn compact representations for predicting the next state [4, 37, 55].",,,0,not_related
"The learned latent space should contain meaningful lower-dimensional representations of our world [10] and facilitate efficient downstream learning [11, 12], exhibit better generalization [13, 14] and increased interpretability as well as allow for causal reasoning [15].",,,0,not_related
"[12] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C√¥t√©, and R Devon Hjelm.",,,0,not_related
"The works most similar to ours, Anand et al. (2019) and Stooke et al. (2020), both propose to use reward-free temporal-contrastive methods to pretrain representations.",,,0,not_related
"Anand et al. (2019) show that representations from encoders trained with ST-DIM contain a great deal of information about environment states, but they do not examine whether or not representations learned via their method are, in fact, useful for reinforcement learning.",,,0,not_related
"The learned representation should facilitate efficient downstream learning [26, 29] and exhibit better generalization [30‚Äì32].",,,0,not_related
"Learning low-dimensional representations that are capturing an environments‚Äô variations for RL agents in control scenarios is also often being described as state representation learning [75]: Methods therein are typically based on autoencoders [76‚Äì80], video prediction [81, 82] or contrastive learning [26, 83, 84].",,,0,not_related
"Using the information given in [4], we only count the RAM states corresponding to the controllable avatar.",,,1,related
"the underlying RAM states in ALE), as this is a proxy for state coverage that is agnostic to the specific reward function of the game [4].",,,0,not_related
"At each state visited by the agent evaluator during training, the agent‚Äôs state (consisting of the avatar‚Äôs x and y coordinates within the frame, and potentially also the room number in games with more than one frame in which the agent can move, such as the different rooms in Montezuma‚Äôs Revenge) is extracted from the environment‚Äôs RAM state using the RAM annotations provided by [1].",,,1,related
"In Anand et al. (2019), the representations for RL algorithms are learned by maximizing mutual information (Hjelm et al., 2019) across spatially and temporally distinct features of an encoder of visual observations.",,,0,not_related
"To address this challenge, a number of deep RL approaches (Sermanet et al., 2018; Dwibedi et al., 2018; Anand et al., 2019; Laskin et al., 2020b; Mazoure et al., 2020; Stooke et al., 2020; Schwarzer
et al., 2021) leverage the recent advance of self-supervised learning which effectively extracts‚Ä¶",,,0,not_related
"For example, imposing continuity between consecutive states [2], maximizing mutual information with prior states [3],",,,0,not_related
"To address this issue, the work of [36, 37] learn state representations by predicting the future in latent space with a probabilistic contrastive loss, while our work directs the representation learning by reducing error on a downstream target task.",,,0,not_related
This combines features of a Spatiotemporal DeepInfomax (ST-DIM) (Anand et al. 2019) and a Variational Autoencoder (VAE) (Kingma and Welling 2013).,,,0,not_related
"This is key since while the VAE section aims to purely encode visual information frame by frame, the ST-DIM section aims to learn a representation that maintains high mutual information between local features in the same spatial location in sequential frames, as well as between the global features and all the next sequential frame‚Äôs local features.",,,0,not_related
"Mutual information estimation [8] has inspired a number of successful uses to a single (DIM [9], CPC [10]) and multi-view (AMDIM [11], CMC [12], SimCLR [13]) image classification, reinforcement learning (ST-DIM [14]) and zero-shot learning (CM-DIM [15, 16]).",,,0,not_related
"Further, AMDIM [11], STDIM [14] and CM-DIM [15] incorporate Cross-Local (CL) and Cross-Spatial (CS) objectives.",,,0,not_related
"This approach was introduced as linear probing for classification [1], and later used in reinforcement learning [2]; therefore we call it linear probing accuracy.",,,0,not_related
"Recently, various different ideas to self-supervised exploration have been proposed [55, 56, 57, 58, 59, 60].",,,0,not_related
Anand et al. (2019) use the NCE loss to discriminate between temporally near frames and temporally far frames of ATARI gameplay but do not compare across games.,,,0,not_related
However it‚Äôs unclear if this extension would yield linearly interpretable representations as effectively as the bivariate case of InfoNCE as introduced by van den Oord [5].,,,0,not_related
The InfoNCE objective is to learn a score function f which maximizes the following estimate which acts as a lower bound of the mutual information between X and Y .,,,1,related
"al which contrasts the global output of the encoder at both time steps, hence creating a new global-global objective based on InfoNCE [2].",,,1,related
al use a receptive field size of 1/16 without justification in the paper [2].,,,0,not_related
"Following the work of van den Oord [5] on InfoNCE, STDIM uses a bilinear score function for both of these objectives:
gm,n(xt, xt‚Ä≤) = œÜ(xt) T ¬∑Wg ¬∑ œÜm,n(xt‚Ä≤) and fm,n(xt, xt‚Ä≤) = œÜm,n(xt)T ¬∑Wl ¬∑ œÜm,n(xt‚Ä≤) (3)
where œÜ is the output of the encoder, œÜm,n is the local feature vector produced by an intermediate convolution layer of the encoder at the (m,n) spatial location, and Wg,Wl are learned weights.",,,1,related
Doing this would require extending the definition of the InfoNCE objective to three variables.,,,1,related
"STDIM simultaneously trains two InfoNCE objectives, the global-local objective (GL) and the local-local objective (LL).",,,0,not_related
The methods introduced by Anand et. al [2] rely on the InfoNCE mutual information bound [5].,,,1,related
"Abstract In this study, we performed some ablations on the main model developed in the paper ""Unsupervised Representation Learning in Atari"" [2] as part of the 2019 NeurIPS Reproducibility Challenge.",,,1,related
al consider is using a pretrained agent with -greedy exploration added in [2].,,,1,related
"This ablation is similar to an ablation by Anand et. al which contrasts the global output of the encoder at both time steps, hence creating a new global-global objective based on InfoNCE [2].",,,1,related
"The InfoNCE objective is derived from mini-batches of consecutive observations {(xt, xt+1)i}Bi=1 given by the agent‚Äôs interactions with the environment.",,,1,related
"When training an encoder using these objectives, Wg and Wl are learned to find the maximum for each InfoNCE bound.",,,1,related
al [2] rely on the InfoNCE mutual information bound [5].,,,1,related
We also discuss whether their proposed benchmark is more effective at (1) transferring knowledge between tasks (in the same environment) and (2) learning with fewer interactions.,,,1,related
