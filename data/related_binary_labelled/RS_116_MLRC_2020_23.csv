text,label_score,label,target_predict,target_predict_label
"[20] Matthew O’Shaughnessy, Gregory Canal, Marissa Connor, Christopher Rozell, and Mark Davenport.",,,0,not_related
"Although there are some works [28, 37] that can be used to extract causal explanations, they often make strict assumptions about the underlying data format, so they cannot be compared fairly, and we put the comparison in Appendix E.",,,0,not_related
Recent works (Pearl 2009; O’Shaughnessy et al. 2020) introduce the information flow to measure the causal influence of the learned representation on the output of the predictor.,,,0,not_related
"2021a), and causal explanations (O’Shaughnessy et al. 2020; Holzinger et al. 2022), etc.",,,0,not_related
"For instance, a similar relationship between data structure and feature redundancy has been reported in various domains, including time series Radovic et al. [2017] and graph structures Liu et al.",,,0,not_related
"Neural network (NN) models enriched with causal knowledge have demonstrated their ability to achieve robustness [36], invariance [27, 9], and provide interpretable explanations for human understanding [3, 26, 17].",,,0,not_related
"In training such NN models imbued with causal knowledge, two primary tasks emerge: (1) acquiring a comprehension of causal relationships between input and output neurons [15, 21, 17], and (2) validating and explaining the acquired causal relationships [3, 16, 26].",,,0,not_related
"They often employ causal graphs, gradient-descent, discriminative and evolutionary algorithms to generate contrastive examples (CEs) while satisfying feasibility constraints (Ustun, Spangher, and Liu 2019; O’Shaughnessy et al. 2020; Goyal et al. 2019).",,,0,not_related
"In practice, one can also use the MI term I(α;Y ) as suggested in O’Shaughnessy et al. (2020).",,,0,not_related
"Note that, although we do not use “do” operator to introduce intervention, O’Shaughnessy et al. (2020) shows that I (α→ Y |do (β)) = I (α;Y |β) from the rules of do-calculus.",,,1,related
[30] developed a method for causal explaining black-box models.,,,0,not_related
"Simplifying causal metric for training the selector: We choose RED, which is an information theoretic causal strength measure to train the selector as it satisfies:  It can capture the non-linear, and complex relationship between input and output variables that is common in black-box DL models [30].",,,1,related
"Debugging and Explainability: Most of the existing works on explainability of deep networks focus on inspecting the decisions for a single image [2,8,9,15,33,37,40,41,53,56, 62,63,65,69].",,,0,not_related
"At the same time, the language of causality is advocated as a precise and powerful way of extracting explanations [18].",,,0,not_related
We follow an information-theoretical approach to measure the flow of information [18] to quantify faithfulness.,,,1,related
…the behaviors of black-box machine learning (ML) models has drawn significant attention (Ghorbani et al. 2019; Kim et al. 2018; Koh et al. 2020; Pedapati et al. 2020; Jeyakumar et al. 2020; Heskes et al. 2020; O’Shaughnessy et al. 2020; Heskes et al. 2020; Huai et al. 2019; Yao et al. 2021).,,,0,not_related
"Recently, interpreting and understanding the behaviors of black-box machine learning (ML) models has drawn significant attention (Ghorbani et al. 2019; Kim et al. 2018; Koh et al. 2020; Pedapati et al. 2020; Jeyakumar et al. 2020; Heskes et al. 2020; O’Shaughnessy et al. 2020; Heskes et al. 2020; Huai et al. 2019; Yao et al. 2021).",,,0,not_related
Variational auto-encoders (VAEs) have shown promising results in learning causal [37] and interpretable [2] representations or interception of interpretable attributes [13].,,,0,not_related
"The work in [36] relies on Variational Autoencoders [24], but is limited by experiments on artificial toy datasets.",,,0,not_related
"After localizing the features through the two methods, we perturb the salient features and observe the change in the classifier’s output on these new images, a commonly employed metric [36, 37].",,,1,related
"Generative models have been proposed to visualize classifiers [26, 31, 36, 43].",,,0,not_related
"There are several viable formalisms of causality, such as structural causal models [18, 19], Granger causality [3, 11], and causal Bayesian networks [19].",,,0,not_related
"Causality, therefore, has been a plausible language for answering such questions [11, 18].",,,0,not_related
"Following existing works [11, 18], we also evaluate the Log-odds difference to illustrate the fidelity of generated explanations in a more statistical view.",,,0,not_related
"Interpretability: Most of the existing works on post-hoc interpretability techniques focus on inspecting the decisions for a single image (Zeiler & Fergus, 2014; Mahendran & Vedaldi, 2016; Dosovitskiy & Brox, 2016; Yosinski et al., 2016; Nguyen et al., 2016; Adebayo et al., 2018; Zhou et al., 2018; Chang et al., 2019; Olah et al., 2018; Yeh et al., 2019; Carter et al., 2019; O’Shaughnessy et al., 2019; Sturmfels et al., 2020; Verma et al., 2020).",,,0,not_related
"…2014; Mahendran & Vedaldi, 2016; Dosovitskiy & Brox, 2016; Yosinski et al., 2016; Nguyen et al., 2016; Adebayo et al., 2018; Zhou et al., 2018; Chang et al., 2019; Olah et al., 2018; Yeh et al., 2019; Carter et al., 2019; O’Shaughnessy et al., 2019; Sturmfels et al., 2020; Verma et al., 2020).",,,0,not_related
"We are conducting a data collection study to observe the real-world practicality and performance of an adaptive DBS algorithm in patients with PD to observe the resulting LFP dynamics under adaptive DBS in 10 patients (Oyama et al., 2021).",,,1,related
"…has been a surge in using the ideas of causality to improve the learning and explanation capabilities of deep learning models in recent years (O' Shaughnessy et al. 2020; Suter et al. 2019; Goyal et al. 2019a,b; Chattopadhyay et al. 2019; Janzing 2019; Zmigrod et al. 2019; Pitis, Creager, and…",,,0,not_related
"explanations where decisions about single images are inspected (Zeiler & Fergus, 2014; Yosinski et al., 2016; Dosovitskiy & Brox, 2016; Mahendran & Vedaldi, 2016; Nguyen et al., 2016; Zhou et al., 2018; Olah et al., 2018; Adebayo et al., 2018; Chang et al., 2019; Carter et al., 2019; Yeh et al., 2019; Sturmfels et al., 2020; O’Shaughnessy et al., 2019; Verma et al., 2020).",,,0,not_related
"…2014; Yosinski et al., 2016; Dosovitskiy & Brox, 2016; Mahendran & Vedaldi, 2016; Nguyen et al., 2016; Zhou et al., 2018; Olah et al., 2018; Adebayo et al., 2018; Chang et al., 2019; Carter et al., 2019; Yeh et al., 2019; Sturmfels et al., 2020; O’Shaughnessy et al., 2019; Verma et al., 2020).",,,0,not_related
"…is not given, one needs to discover useful concepts for the explanation, e.g., Ghorbani et al. (2019) used segmentation and clustering, Yeh et al. (2020) retrained the classifier with a prototypical concept layer, O’Shaughnessy et al. (2020) learned the generative model with a causal objective.",,,0,not_related
"O’Shaughnessy et al. (2020) argued that compared to other metrics such as average causal effect (ACE) (Holland 1988), analysis of variance (ANOVA) (Lewontin 1974), information flow is more suitable to capture complex and nonlinear causal dependence between variables.",,,0,not_related
"Recently, O’Shaughnessy et al. (2020) proposed a learning framework that encourages the causal effect of certain latent factors on the classifier output to learn a latent representation that has causality on the prediction.",,,0,not_related
"Specifically, the standard VAE model and also O’Shaughnessy et al. (2020) assumes the independence of latent factors, which is believed to encourage
meaningful disentanglement via a factorized prior distribution.",,,0,not_related
"Next, we compare to O’Shaughnessy et al. (2020), in which we used a VAE model with ten continuous factors and encouraged three factors to have causal effects on predicted classes.",,,1,related
"Inspired by O’Shaughnessy et al. (2020), we employs a generative model to learn the data distribution while encouraging the causal influence of certain latent factors.",,,1,related
2020) or a latent factor of a generative model (O’Shaughnessy et al. 2020; Goyal et al. 2020).,,,0,not_related
"The definition of concept are various, e.g., a direction in the activation space (Kim et al. 2018; Ghorbani et al. 2019), a prototypical activation vector (Yeh et al. 2020) or a latent factor of a generative model (O’Shaughnessy et al. 2020; Goyal et al. 2020).",,,0,not_related
"As such, they usually require knowledge of the system of causal structural equations [20, 16, 25, 41] or the causal graph [26].",,,0,not_related
Another direction is to build a simpler causal tree that approximates the predictions of a more complicated network and can be said to explain its behavior (Shaughnessy et al. 2020).,,,0,not_related
"The closest method in spirit to the explanations provided by our StylEx approach is [20], though their method only works on small images, and Figure 3.",,,1,related
"• It can capture the non-linear, complex relationship between input and output variables in a black-box model [13].",,,0,not_related
"(iii) Causal influence based methods: This is a recent line of work in which causal influence is quantified using some measure and this is used for generating feature attribution maps[4, 14, 15, 13].",,,0,not_related
"Very recently, exploratory effort has been made to leverage the tools from counterfactual reasoning [22] and causal analysis [41] to derive visual explanations, but do not lend insights back to model training.",,,0,not_related
"Generative causal explanations of black box classifiers [41] are built by learning the latent factors involved in a classification, which are then included in a causal model.",,,0,not_related
Recent work has focused on learning causal graphs [12] or generating causal natural language explanations [9] from images.,,,0,not_related
"com/trends/explore?date=all&q=mnist [124, 198, 199, 200, 201, 202]",,,1,related
"com/trends/explore?date=all&q=mnist [124, 198, 199, 200, 201, 202]",,,1,related
"Most of these efforts have focused on local explanations where decisions about single images are inspected (Zeiler & Fergus, 2014; Yosinski et al., 2016; Dosovitskiy & Brox, 2016; Mahendran & Vedaldi, 2016; Nguyen et al., 2016; Zhou et al., 2018; Olah et al., 2018; Adebayo et al., 2018; Chang et al., 2019; Carter et al., 2019; Yeh et al., 2019; Sturmfels et al., 2020; O’Shaughnessy et al., 2019; Verma et al., 2020).",,,0,not_related
"…2014; Yosinski et al., 2016; Dosovitskiy & Brox, 2016; Mahendran & Vedaldi, 2016; Nguyen et al., 2016; Zhou et al., 2018; Olah et al., 2018; Adebayo et al., 2018; Chang et al., 2019; Carter et al., 2019; Yeh et al., 2019; Sturmfels et al., 2020; O’Shaughnessy et al., 2019; Verma et al., 2020).",,,0,not_related
"Generative causal explanations of black box classifiers [37] are built by learning the latent factors involved in a classification, which are then included in a causal model.",,,0,not_related
