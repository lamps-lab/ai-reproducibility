text,label_score,label,target_predict,target_predict_label
"[48] introduce Target-data Collection, which adds more favorable examples (e.",,,0,not_related
[48] append bias control tokens to the input so the generative dialogue model learns properties of gender bias.,,,0,not_related
"Dinan et al. (2020), for instance, mitigate gender bias in dialogue generation by binning each training example by the presence or absence of masculine or feminine gendered words, and appending a control token corresponding to the bin to each prompt. Xu et al. (2020) adapt Dinan et al. (2020)’s approach to provide safety guarantees for chatbot applications. The authors use safety control tokens, generated by a safety classifier that measures offensiveness, bias, and other potential harms in text. Similarly, Lu et al. (2022) score training examples with a reward function that quantifies some unwanted property, such as toxicity or bias, which is used to quantize the examples into bins.",,,0,not_related
"Also incorporating human writers, Dinan et al. (2020) investigate targeted data collection to reduce gender bias in chat dialogue models by curating human-written diversified examples, priming crowd workers with examples and standards for the desired data. Sun et al. (2023) construct example discussions that demonstrate and explain facets of morality, including fairness, using rules-of-thumb that encode moral principles and judgments. To train models that can appropriately respond and recover to biased input or outputs, Ung et al. (2022) generate a set of dialogues with example recovery statements, such as apologies, after unsafe, offensive, or inappropriate utterances. Similarly, Kim et al. (2022) generate a dataset of responses to biased or otherwise problematic statements based on rules-of-thumb that follow social norms.",,,0,not_related
"Also incorporating human writers, Dinan et al. (2020) investigate targeted data collection to reduce gender bias in chat dialogue models by curating human-written diversified examples, priming crowd workers with examples and standards for the desired data. Sun et al. (2023) construct example discussions that demonstrate and explain facets of morality, including fairness, using rules-of-thumb that encode moral principles and judgments.",,,0,not_related
"Also incorporating human writers, Dinan et al. (2020) investigate targeted data collection to reduce gender bias in chat dialogue models by curating human-written diversified examples, priming crowd workers with examples and standards for the desired data. Sun et al. (2023) construct example discussions that demonstrate and explain facets of morality, including fairness, using rules-of-thumb that encode moral principles and judgments. To train models that can appropriately respond and recover to biased input or outputs, Ung et al. (2022) generate a set of dialogues with example recovery statements, such as apologies, after unsafe, offensive, or inappropriate utterances.",,,0,not_related
"Dinan et al. (2020), for instance, mitigate gender bias in dialogue generation by binning each training example by the presence or absence of masculine or feminine gendered words, and appending a control token corresponding to the bin to each prompt.",,,0,not_related
"Also incorporating human writers, Dinan et al. (2020) investigate targeted data collection to reduce gender bias in chat dialogue models by curating human-written diversified examples, priming crowd workers with examples and standards for the desired data.",,,0,not_related
"Dinan et al. (2020), for instance, mitigate gender bias in dialogue generation by binning each training example by the presence or absence of masculine or feminine gendered words, and appending a control token corresponding to the bin to each prompt. Xu et al. (2020) adapt Dinan et al. (2020)’s approach to provide safety guarantees for chatbot applications.",,,0,not_related
"In this way, the training corpus is directly re-balanced by swapping or removing bias-related words and counterfactual data augmentation (CDA) (Zmigrod et al., 2019; Dinan et al., 2020; Webster et al., 2020; Dev et al., 2020; Barikeri et al., 2021).",,,1,related
"Downstream task analyses mostly consider shortcomings in dialogue-systems (Staliūnaitė and Iacobacci, 2020; Dinan et al., 2020a).",,,0,not_related
"The analysed bias dimension in this work is the person being spoken about (Dinan et al., 2020b), in contrast to, e.g., Excell and Al Moubayed (2021) where the bias concerns the author of a comment.",,,0,not_related
"Downstream task analyses mostly consider shortcomings in dialogue-systems (Staliūnaitė and Iacobacci, 2020; Dinan et al., 2020a).",,,0,not_related
"availability of high-quality data, there are also technical [3] and ethical challenges [20], [21].",,,0,not_related
"For example, the ConvAI2 competition at NeurIPS 2018 featured large (at the time) pre-trained Transformers being used by the top two winning teams (Wolf et al., 2019; Golovanov et al., 2020; Dinan et al., 2020b).",,,0,not_related
"…generate harmful or inappropriate content (Bender et al., 2021; Bommasani et al., 2021; Hendrycks et al., 2021; Weidinger et al., 2021; Bai et al., 2022b), including work from our group (Xu et al., 2020; Dinan et al., 2022, 2021; Smith et al., 2022a; Dinan et al., 2020a; Smith and Williams, 2021).",,,0,not_related
", 2022b), including work from our group (Xu et al., 2020; Dinan et al., 2022, 2021; Smith et al., 2022a; Dinan et al., 2020a; Smith and Williams, 2021).",,,1,related
propose a di↵erent method for evaluating gender bias in dialogue datasets[25].,,,0,not_related
", 2021), gender bias (Liu et al., 2020; Dinan et al., 2020) and other discriminated behavior (Sheng et al.",,,0,not_related
"Harmful differences in responses caused by different demographic personas are observed in well-known dialogue systems (Sheng et al., 2021; Dinan et al., 2020), including offensiveness, gender bias, race discrimination, etc.",,,0,not_related
"[6] Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston.",,,0,not_related
"A line of work focuses on automatically obtaining cleaner data [1, 46, 6].",,,0,not_related
"Previous work typically focused on some specific biases in dialogue systems, such as gender [15, 29, 30, 46], race [14, 46], social class [46] and profession [14].",,,0,not_related
"Moreover, existing works (Barikeri et al., 2021; Dinan et al., 2020; Liu et al., 2020b) only focus on English dialogue models.",,,0,not_related
Dinan et al. (2020) propose new techniques to mitigate gender bias by balancing the genderedness of generated dialogue utterances.,,,0,not_related
"TGNB Harm Evaluations in LLMs Gender bias evaluation methods include toxicity measurements and word co-occurrence in OLG [23, 25, 37, 40, 59, 61].",,,0,not_related
"Results reflect more robust pronoun consistency for binary pronouns (§4.2), the usage of generic masculine language during OLG (§4.3), less toxic language when disclosing binary gender (§5.2, §5.3), and examples of invasive TGNB commentary (§5.2).",,,1,related
"In NLG, [23] create a dataset of prompts to assess for harms in OLG across various domains (e.g., politics, occupation) using Wikipedia.",,,0,not_related
"Motivation To assess LLMs for misgendering behavior in OLG, we create an automatic misgendering evaluation tool.",,,1,related
6https://github.com/anaeliaovalle/TANGO-Centering-Transgender-NonbinaryVoices-for-OLG-BiasEval 7Addressing someone using a pronoun that does match their gender identity.,,,1,related
"2 RELATEDWORK TGNB Harm Evaluations in LLMs Gender bias evaluation methods include toxicity measurements and word co-occurrence in OLG [23, 25, 37, 40, 59, 61].",,,0,not_related
"While the tool cannot be used with multiple referents, it is a good starting point for OLG misgendering assessments.",,,0,not_related
We leverage these findings to drive our OLG harm assessment framework by asking two questions: (1) To what extent is gender non-affirmation in the form of misgendering present in models used for OLG? and (2) To what extent is gender non-affirmation in the form of negative responses to gender identity disclosure present in models used for OLG?,,,1,related
"C L
] 1
J un
we illuminate ways in which harms may manifest in OLG for members of the queer2 community, specifically those who identify as transgender and nonbinary (TGNB).",,,1,related
"[25] Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston.",,,0,not_related
This work centers the TGNB community by focusing on experienced and documented gender minoritization and marginalization to carefully guide the design of TGNB harm evaluations in OLG.,,,0,not_related
"(3) With these findings, we provide constructive suggestions for creating more gender-inclusive LLMs in each OLG experiment.",,,1,related
"In this section, we conduct OLG experiments that explore if and howmodels misgender individuals in text.",,,1,related
"Large language models (LLM) are being increasingly utilized for open language generation (OLG) in spaces such as content creation (e.g., story creation) and conversational AI (e.g., voice assistants, voice user interfaces).",,,0,not_related
"To address this gap, we center the experiences of the TGNB community to help inform the design of new harm evaluation techniques in OLG.",,,1,related
"We translate this to our work, asking if this behavior is reflected in OLG.",,,1,related
MotivationWe draw from linguistics literature to further investigate misgendering behavior in OLG.,,,0,not_related
"Moreover, there is a dearth of knowledge on how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within OLG systems.",,,0,not_related
"To determine if this behavior persists in LLMs, we create a dataset to evaluate misgendering in OLG.",,,1,related
This section measures possible harmful language in OLG across several forms of disclosing TGNB genders.,,,0,not_related
"While some of these have been studied in previous works and mitigations are included (Dinan et al., 2019), we",,,0,not_related
"While some of these have been studied in previous works and mitigations are included (Dinan et al., 2019), we acknowledge that these issues are not entirely resolved.",,,0,not_related
"Dinan et al. (2020); Zhao et al. (2017, 2018) propose approaches to mitigate gender bias in LLMs with regularization objectives and counterfactual data augmentation.",,,0,not_related
"Typical debiasing methods include counterfactual data augmentation (Zmigrod et al., 2019; Dinan et al., 2019; Webster et al., 2020; Barikeri et al., 2021), dropout regularization (Webster et al.",,,0,not_related
"Typical debiasing methods include counterfactual data augmentation (Zmigrod et al., 2019; Dinan et al., 2019; Webster et al., 2020; Barikeri et al., 2021), dropout regularization (Webster et al., 2020), self-debias (Schick et al., 2021), sentence embedding debias (Liang et al., 2020), and iterative…",,,0,not_related
Wolf et al. (2019) prepend persona sentences to personalize the history; while Su et al. (2022); Dinan et al. (2020); Keskar et al. (2019); Xu et al. (2020a) prepending task-specific signals to prompt and control the model.,,,0,not_related
"For example, these models often exhibit social biases (Barikeri et al., 2021; Dinan et al., 2020) and inappropriately align them-",,,0,not_related
"For example, these models often exhibit social biases (Barikeri et al., 2021; Dinan et al., 2020) and inappropriately align them-
∗Work done during an internship at Amazon Alexa AI.",,,0,not_related
Keskar et al. (2019) and Dinan et al. (2020) investigated using control signals to condition generation from language models.,,,0,not_related
"Work has already been done to address some of these issues particularly when it comes to character representation and dialogue (Dinan et al., 2019), however no analysis has been done on the underlying objects or learned relationships.",,,0,not_related
"Both language models and curated datasets often exhibit demographic imbalances (Dinan et al., 2020; Weidinger et al., 2021; Sheng et al., 2021).",,,0,not_related
"Controlling Dialogue Systems has been a focus of research to generate engaging responses (Ghazarian et al., 2021), prevent toxic content and biases (Dinan et al., 2020; Xu et al., 2021a), steer the conversation towards specific keywords or topics (Tang et al., 2019; Gupta et al., 2022a), and ground…",,,0,not_related
", 2021), prevent toxic content and biases (Dinan et al., 2020; Xu et al., 2021a), steer the conversation towards specific keywords or topics (Tang et al.",,,0,not_related
"…have explored multilingual scenarios (e.g., Lauscher and Glavaš, 2019; Lauscher et al., 2020c; Ahn and Oh, 2021), more fine-grained biases (Dinan et al., 2020b), and more biases, beyond the prominent sexism and racism dimensions (e.g., Zhao et al., 2018; Rudinger et al., 2018), like…",,,0,not_related
"Here, Webster et al. (2020) and Lauscher et al. (2021) rely on CDA (Zhao et al., 2018) and Dinan et al. (2020a) use control codes to guide the biases.",,,1,related
", for dialog (e.g., Sheng et al., 2019; Dinan et al., 2020a; Barikeri et al., 2021), co-reference resolution (Zhao et al.",,,0,not_related
"…2019; Qian et al., 2019; Webster et al., 2020; Nangia et al., 2020; Sap et al., 2020) and in downstream scenarios, e.g., for dialog (e.g., Sheng et al., 2019; Dinan et al., 2020a; Barikeri et al., 2021), co-reference resolution (Zhao et al., 2018), and NLI (Rudinger et al., 2017; Dev et al., 2020).",,,0,not_related
"Native speakers may find grammatical errors produced by such systems silly, jarring, or even offensive, for example if the utterance refers to an entity with the wrong tense, formality, animacy, or gender (Dinan et al., 2020).",,,0,not_related
", 2021), debiasing (Dinan et al., 2020), to improving generalization, robustness (Dhole et al.",,,0,not_related
"For instance, Dinan et al. (2020) changes gendered words in a sentence to instill gender invariance for bias mitigation.",,,0,not_related
"In comparison, rule-based models can precisely control for such behaviors, mitigate bias (Dinan et al., 2020), or introduce invariance in embedding space specific to the needs of the downstream tasks.",,,0,not_related
"If used as data augmentation for training, the transformation might mitigate gender bias, as shown in (Dinan et al., 2020).",,,0,not_related
"Data augmentation in NLP can be useful in many situations, from low resource data setting, domain adaptation (Wei et al., 2021), debiasing (Dinan et al., 2020), to improving generalization, robustness (Dhole et al., 2021).",,,0,not_related
"…Kumar et al. (2020); Bartl et al. (2020) Sen et al. (2021)
1
Prost et al. (2019); Qian et al. (2019) Emami et al. (2019); Habash et al. (2019) Dinan et al. (2020); Costa-jussà and de Jorge (2020) Basta et al. (2020)
2
Park et al. (2018); Stafanovičs et al. (2020) Saunders and Byrne (2020);…",,,0,not_related
"…al., 2019a), language models (Nadeem et al., 2021), and models for specific downstream tasks in NLP (Rudinger et al., 2018; Stanovsky et al., 2019; Dinan et al., 2020) are prone to social biases, which may have a negative impact on their performance and the social effect when applied in reality…",,,0,not_related
"Augmentation with synthetically generated data has also been explored for gender bias mitigation in dialogue (Dinan et al., 2020; Liu et al., 2020) and translation",,,0,not_related
"Augmentation Discrimination Adding synthetically generated data (Dinan et al., 2020; Liu et al., 2020; Stafanovičs et al., 2020) Toxicity Adding safer example data (Mathew et al.",,,0,not_related
"Augmentation with synthetically generated data has also been explored for gender bias mitigation in dialogue (Dinan et al., 2020; Liu et al., 2020) and translation models (Stafanovičs et al., 2020).",,,0,not_related
"For example, gender and racial biases exist in dialogue generation systems [7] and PLMs [17].",,,0,not_related
"NLG models are known to suffer from biases learnable from training or finetuning on data, such as gender bias (Dinan et al., 2020).",,,0,not_related
"NLG models are known to suffer from biases learnable from training or finetuning on data, such as gender bias (Dinan et al., 2020).",,,0,not_related
"Machine learning models have a tendency to amplify at test-time biases that exist in their training data, a problem known as bias amplification [12, 38, 22].",,,0,not_related
"Machine learning models have a tendency to amplify at test-time biases that exist in their training data, a problem known as bias amplification (Dinan et al., 2019; Leino et al., 2019; Hall et al., 2022).",,,0,not_related
"[16] use bias-controlled training to alleviate the problem of gender bias, while [17] develops a new training procedure to enhance chatbot models with crowd-workers iteratively.",,,0,not_related
"Mihaylov and Nakov, 2019) and bias (Dinan et al., 2019a), less work has studied robust learning from organic conversations with potentially adversarial feedback.",,,0,not_related
"The latter are intended to be difficult for a model to spot and understand, e.g. unsafe text that does not contain any profanity words but can be understood to be unsafe only through its deeper semantic meaning, see Dinan et al. (2019b).",,,0,not_related
"…on detecting undesirable behavior such as toxic language (Xu et al., 2020; Dinan et al., 2021), trolling (Tomaiuolo et al., 2020; Mihaylov and Nakov, 2019) and bias (Dinan et al., 2019a), less work has studied robust learning from organic conversations with potentially adversarial feedback.",,,0,not_related
"In our baseline approach, and all other subsequent approaches, we employ a 128M parameter transformer model as a classifier, using the pre-trained model from Dinan et al. (2019b).",,,1,related
We perform these experiments “zero-shot” by taking an off-the-shelf safety classifier of dialogue utterances from the existing work of Dinan et al. (2019b)4.,,,1,related
"We use the conversational safety data collected in Dinan et al. (2019b), which is a pool of 30,000 utterances, half of which is collected as standard inputs, and half where crowdworkers were asked to give difficult adversarial inputs.",,,1,related
"Finally, we add the Funpedia task (Dinan et al., 2020b) – which involves learning to produce an engaging dialogue utterance given a wikipedia sentence –
and the LIGHT (Urbanek et al., 2019) and LIGHT WILD (Shuster et al., 2021b) tasks – which are open-domain dialogue tasks grounded in a medieval…",,,1,related
"…in particular, to generate harmful or inappropriate content (Bender et al., 2021; Bommasani et al., 2021; Hendrycks et al., 2021; Weidinger et al., 2021), including work from our group (Xu et al., 2020; Dinan et al., 2022, 2021; Smith et al., 2022; Dinan et al., 2020a; Smith and Williams, 2021).",,,0,not_related
"For example, the ConvAI2 competition at NeurIPS 2018 featured large (at the time) pre-trained transformers being used by the top two winning teams (Wolf et al., 2019; Golovanov et al., 2020; Dinan et al., 2020c).",,,0,not_related
"Many recent works have also focused on the potential of conversational models for bias, either based on gender and its intersections (Dinan et al., 2020a,b; Xu et al., 2020; Smith and Williams, 2021) or several axes of demographic axis more broadly (Barikeri et al., 2021; Perez et al., 2022; Smith…",,,0,not_related
", 2021), including work from our group (Xu et al., 2020; Dinan et al., 2022, 2021; Smith et al., 2022; Dinan et al., 2020a; Smith and Williams, 2021).",,,0,not_related
"Depending on the downstream application that AlexaTM 20B is being applied to, one or several of the prior techniques from literature (Gupta et al., 2022; Dathathri et al., 2019; Dinan et al., 2019; Sheng et al., 2021; Dinan et al., 2020; Liu et al., 2020a; Sheng et al., 2019; Roller et al., 2021; Liang et al., 2021; Dinan et al., 2021; Dhamala et al., 2021; Schick et al., 2021; Ouyang et al., 2022) might be utilizable for detoxifying and debiasing the model.",,,0,not_related
"…to, one or several of the prior techniques from literature (Gupta et al., 2022; Dathathri et al., 2019; Dinan et al., 2019; Sheng et al., 2021; Dinan et al., 2020; Liu et al., 2020a; Sheng et al., 2019; Roller et al., 2021; Liang et al., 2021; Dinan et al., 2021; Dhamala et al., 2021; Schick…",,,0,not_related
"Numerous paradigms for language model debiasing were proposed, including feature extraction-based (Pryzant et al., 2020), data augmentations (Zhao et al., 2019; Lu et al., 2020; Dinan et al., 2020), or paraphrasing (Ma et al., 2020).",,,0,not_related
", 2020), data augmentations (Zhao et al., 2019; Lu et al., 2020; Dinan et al., 2020), or paraphrasing (Ma et al.",,,0,not_related
"…generated via our LLM-D-based approaches could used both to test for undesired behaviour in classifiers and potentially to mitigate that behaviour via methods such as dataset augmentation, as has been found useful in various settings, e.g. Dinan et al. (2020), Hall Maudslay et al. (2019).",,,1,related
"…used to measure and often lessen the severity of social bias in text data (Hall Maudslay et al., 2019; Prabhakaran et al., 2019; Zmigrod
et al., 2019; Dinan et al., 2020a,b; Webster et al., 2020; Ma et al., 2021; Smith and Williams, 2021; Renduchintala and Williams, 2022; Emmery et al., 2022).",,,0,not_related
"However, word list approaches are necessarily limited (Dinan et al., 2020a) and which words are included can really matter (Sedoc and Ungar, 2019).",,,0,not_related
"Dinan et al. (2020); Liu et al. (2020a,b) discuss gender bias in dialogue generation and Sheng et al. (2021b) investigates the ad hominems in dialogue responses regarding the race perspective.",,,0,not_related
"In particular, many works focus on generative models (Dinan et al., 2020a,b; Xu et al., 2021b; Kirk et al., 2021; Sheng et al., 2021b; Nozza et al., 2021; Renduchintala et al., 2021; Baheti et al., 2021; Perez et al., 2022), which are well known to pose unique challenges for automatic evaluation…",,,0,not_related
"ing the frequency of different demographic terms using a word list, for example, those signifying gender (Dinan et al., 2020a); religion, race, gender, and orientation (Barikeri et al.",,,0,not_related
"Finally, the process of assembling word lists itself can be tricky, as seed lexica often have several practical (Antoniak and Mimno, 2021) and conceptual (Dinan et al., 2020b) disadvantages, especially when they consist of paired gendered words.",,,0,not_related
"DialoGPT We specifically use a DialoGPT model tuned further on the ConvAI2 dataset (Dinan et al. 2020c, model from Smith and Williams 2021) to acclimate the model to BlenderBot-style prompts containing two sentences of persona information (Roller et al., 2021).",,,1,related
"…set of techniques for measuring bias in generated text involves computing the frequency of different demographic terms using a word list, for example, those signifying gender (Dinan et al., 2020a); religion, race, gender, and orientation (Barikeri et al., 2021); or occupations (Kirk et al., 2021).",,,0,not_related
"We collected these from existing bibliographies of surveyed papers provided in [2, 18].",,,1,related
"[18] Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston.",,,0,not_related
"As noted earlier, any system capable of generating natural language, even within the limits of fantasy domains as seen in certain games, is capable of accidental or intentional harmful and biased language use—a property which we mitigate but do not entirely eliminate through our value prior (Sheng et al., 2019; Dinan et al., 2020).",,,0,not_related
"…of generating natural language, even within the limits of fantasy domains as seen in certain games, is capable of accidental or intentional harmful and biased language use—a property which we mitigate but do not entirely eliminate through our value prior (Sheng et al., 2019; Dinan et al., 2020).",,,0,not_related
"…(Rabinovich et al., 2017; Vanmassenhove et al., 2018; Stafanovičs et al., 2020; Savoldi et al., 2021), dialogue systems (Cercas Curry et al., 2020; Dinan et al., 2020; Liu et al., 2020a,b; Sheng et al., 2021), language modeling (Lu et al., 2018; Bordia and Bowman, 2019; Sheng et al., 2019; Vig…",,,0,not_related
", 2021), dialogue systems (Cercas Curry et al., 2020; Dinan et al., 2020; Liu et al., 2020a,b; Sheng et al., 2021), language modeling (Lu et al.",,,0,not_related
"We additionally evaluated OPT-175B on the ConvAI2 hidden test set, which has never been publicly released, and achieved 10.7 ppl and .185 UF1, matching the performance of the validation set.",,,1,related
"In particular, we follow Roller et al. (2021), and evaluate on ConvAI2 (Dinan et al., 2020b), Wizard of Wikipedia (Dinan et al., 2019b), Empathetic Dialogues (Rashkin et al., 2019), and Blended Skill Talk (Smith et al., 2020).",,,1,related
"To address concerns of leakage, we searched our pre-training corpus for the first conversation in the ConvAI2 dataset, but we did not find any overlap.",,,1,related
"We see that OPT-175B significantly outperforms the alsounsupervised Reddit 2.7B model on all tasks, and
performs competitively with the fully supervised BlenderBot 1 model, especially in the ConvAI2 dataset.",,,1,related
We were somewhat surprised that the evaluations of the unsupervised OPT-175B model were as competitive as BlenderBot 1 on the ConvAI2 dataset.,,,1,related
The second conversational intelligence challenge (ConvAI2).,,,0,not_related
"We report Perplexity and Unigram F1 (UF1) overlap, following the metrics of the ConvAI2 competition (Dinan et al., 2020b).",,,1,related
"Furthermore, we evaluated OPT-175B on a subset of the ConvAI2like MultiSessionChat (MSC) dataset (Xu et al., 2021b) and obtained a perplexity of 9.7 and UF1 of .177, indicating the model is generalizing well across multiple PersonaChat-like datasets.",,,1,related
"There has been a great deal of work on mitigations for toxicity and biases (Dathathri et al., 2019; Dinan et al., 2019a; Sheng et al., 2019; Dinan et al., 2020a; Liu et al., 2019a; Krause et al., 2020; Xu et al., 2020; Liang et al., 2021; Dinan et al., 2021; Xu et al., 2021a; Dhamala et al., 2021;…",,,0,not_related
"There has been a great deal of work on mitigations for toxicity and biases (Dathathri et al., 2019; Dinan et al., 2019a; Sheng et al., 2019; Dinan et al., 2020a; Liu et al., 2019a; Krause et al., 2020; Xu et al., 2020; Liang et al., 2021; Dinan et al., 2021; Xu et al., 2021a; Dhamala et al., 2021; Schick et al., 2021; Ouyang et al., 2022).",,,0,not_related
This may indicate leakage of the ConvAI2 dataset into the general pre-training corpus or even into the validation data as evaluated in Table 2.,,,1,related
"Attempts are being made with debiasing techniques to address some of these challenges (Dinan et al., 2020).",,,0,not_related
"As noted in Dinan et al. (2020), toxicity 149 in generated dialogue may begin with biases and 150
3www.hatebase.org
offensive content in the training data, and debias-151 ing techniques focused on gender can reduce the152 amount of sexist comments generated by the re-153 sulting system.",,,1,related
"Besides the challenge of availability of data or high-quality data, there are also technical (Roller et al., 2021) and ethical challenges (Dinan et al., 2020; Javed et al., 2021).",,,0,not_related
"…the natural language processing community, work has focused on combating gender bias in co-reference resolution (Zhao et al., 2018), dialogue (Dinan et al., 2019; Lee et al., 2019; Liu et al., 2020), detection of abusive language (Park et al., 2018), machine translation (Stanovsky et al.,…",,,0,not_related
", 2018), dialogue (Dinan et al., 2019; Lee et al., 2019; Liu et al., 2020), detection of abusive language (Park et al.",,,0,not_related
"Studies have indicated that the words used in biographies about women compared to biographies about men (Dinan et al., 2019) also differs, and is reflective of gendered terminology.",,,0,not_related
"Further, generations can include toxic language and bias, especially with certain contexts and topics (Xu et al., 2020; Dinan et al., 2020).",,,0,not_related
"Approaches to mitigate bias in LMs can be broadly summarized as: (a) training or finetuning on a balanced dataset (Solaiman and Dennison, 2021; Dinan et al., 2020)), (b) attaching prefix at inference or training time (Sheng et al., 2020), and (c) using a bias or attribute classifier (e.g., toxicity…",,,1,related
"We believe both these behaviors could be dramatically reduced with adversarial data collection (Dinan et al., 2019).",,,1,related
"There are many ways to mitigate these harms, including by fine-tuning on a small, valuetargeted dataset (Solaiman and Dennison, 2021), filtering the pretraining dataset (Ngo et al., 2021), or human-in-the-loop data collection (Dinan et al., 2019; Xu et al., 2020).",,,0,not_related
"Bias amplification is concerning as it can foster the proliferation of undesired stereotypes [12, 41, 46, 47] or lead to unjustifiable differences in model accuracy between subgroups of users [7, 11].",,,0,not_related
"When the group is, for example, a gender group, an age group, or an ethnic group, such a bias can be harmful [12, 20, 38, 41, 46, 47].",,,0,not_related
"Recent work has focused on reducing gender bias in machine translation and generation [68, 66, 69, 11].",,,0,not_related
"Dialog models [84] can learn, and even amplify, biases in the training data.",,,0,not_related
"In NLP, there are studies that augment text to assess a model’s biases towards gender [31, 3] and ethnicity[26].",,,0,not_related
"Output: Unbiased text/unbiased model political bias [78], gender bias [28, 104], subjective bias [101],",,,0,not_related
"For example, in dialogue systems, researchers found the gender bias depends on utterance [49] and persona [85].",,,0,not_related
"…al., 2020; Stafanovičs et al., 2020; Saunders et al., 2021; Savoldi et al., 2021; Ciora et al., 2021), dialogue systems (Cercas Curry et al., 2020; Dinan et al., 2020; Liu et al., 2020a,b; Sheng et al., 2021b,a), language modeling (Lu et al., 2018;
Bordia and Bowman, 2019; Sheng et al., 2019;…",,,0,not_related
", 2021), dialogue systems (Cercas Curry et al., 2020; Dinan et al., 2020a; Liu et al., 2020a; Liu et al., 2020b; Sheng et al., 2021b; Sheng et al., 2021a), language modeling (Lu et al.",,,0,not_related
"CDA is a data-based debiasing strategy that is often used to mitigate gender bias (Zmigrod et al., 2019; Dinan et al., 2019; Webster et al., 2020; Barikeri et al., 2021).",,,0,not_related
"der biases (Dinan et al., 2020), and are capable of generating toxic or otherwise unsafe content (Weidinger et al.",,,0,not_related
"Different methods for avoiding inappropriate responses have been proposed (Dinan et al., 2020), especially by generating adversarial examples (Dinan et al.",,,0,not_related
"Different methods for avoiding inappropriate responses have been proposed (Dinan et al., 2020), especially by generating adversarial examples (Dinan et al., 2019; Xu et al., 2021a).",,,0,not_related
"Counterfactual data has been augmented to alter the training distribution to balance gender-based statistics [35,48,17].",,,0,not_related
"We use Reddit data through Pushshift (Baumgartner et al., 2020), an archive that has been widely used in NLP and related fields since its first release in 2015 (Hessel and Lee, 2019; Kennedy et al., 2020; Sap et al., 2020; Dinan et al., 2020, among many others).",,,1,related
"Dialog models are known to suffer from biases learnable from dialog training data, such as gender bias (Dinan et al., 2020).",,,0,not_related
"Past work has analyzed dialog models from the point of view of safety from toxic language (Xu et al., 2020; Dinan et al., 2019),
and gender biases (Dinan et al., 2020).",,,0,not_related
"Another way of mitigation is data augmentation (Zhao et al., 2018; Park et al., 2018; Dinan et al., 2020), for example by using gender swapping on the coreference resolution task Zhao et al. (2018).",,,1,related
"Another way of mitigation is data augmentation (Zhao et al., 2018; Park et al., 2018; Dinan et al., 2020), for example by using gender swapping on the coreference resolution task Zhao et al.",,,1,related
"Increasing the safety and alignment of pre-trained models remains a challenging problem (Dinan et al., 2020; Tamkin et al., 2021; Xu et al., 2020; Solaiman and Dennison, 2021; McGuffie and Newhouse, 2020).",,,0,not_related
"As in §4, we use the multi-dimensional gender bias classifier from Dinan et al. (2020b) to measure the amount of gender bias in conversation turns from Speaker A and B for 920,000 self-chats generated by each of our de-biased models.",,,1,related
"This is similar to the Counterfactual Data Augmentation techniques used by Maudslay et al. (2019); Dinan et al. (2020a); Liu et al. (2020a); Barikeri et al. (2021), and aims to ablate any association that the model may have between a certain name (or the gender associated with it) and the subject…",,,0,not_related
"For dialogue, injecting demographic information into personas (i.e., text character descriptions provided as context to the conversational agent) has proven useful in measuring the amount of gender bias agents express (Dinan et al., 2020a; Sheng et al., 2021).",,,0,not_related
", text character descriptions provided as context to the conversational agent) has proven useful in measuring the amount of gender bias agents express (Dinan et al., 2020a; Sheng et al., 2021).",,,0,not_related
"…standard training datasets for dialogue models in the AI literature contain social biases or demographic imbalances, and it has been established that models learn them (Dixon et al., 2018; Bordia and
Bowman, 2019; Lee et al., 2019; Dinan et al., 2020a,b; Liu et al., 2020a,b; Sheng et al., 2021).",,,0,not_related
"Second, we measure the amount of gender bias in BlenderBot3B self-chats using the multidimensional gender bias classifier from Dinan et al. (2020b), which predicts the genderedness of an utterance based on its context (SPEAKING-AS dimension for Speaker A lines and SPEAKING-TO dimension for Speaker…",,,1,related
", 2019) or methods for mitigating the bias of the dialogue model (Liu et al., 2020; Dinan et al., 2020) are recommended to be jointly used with our method when deploying our model in production.",,,1,related
"…methods for reducing the toxicity of the open-domain dialogue system (Xu et al., 2020; Dinan et al., 2019) or methods for mitigating the bias of the dialogue model (Liu et al., 2020; Dinan et al., 2020) are recommended to be jointly used with our method when deploying our model in production.",,,0,not_related
Dinan et al. (2020a) trains dialogue models with attribute conditioning to mitigate bias by producing genderneutral responses.,,,0,not_related
"The BST dataset contains 5K polite conversations between crowdworkers which aims to blend 3 conversational skills into one dataset 1) engaging personality (Zhang et al., 2018b; Dinan et al., 2020b), 2) empathetic dialogue (Rashkin et al., 2019) and 3) knowledge incorporation (Dinan et al., 2019b).",,,0,not_related
"Previous
1Our code and corpus are available at https:// github.com/abaheti95/ToxiChat
research has shown that dialogue models can produce utterances that are gender and racially biased (Wolf et al., 2017; Sheng et al., 2020; Dinan et al., 2020a).",,,1,related
"In the future, we head to apply and develop corresponding mitigation techniques (following works such as Dinan et al. (2020) and Liu et al. (2020)).",,,1,related
", 2019), dialogue generation (Dinan et al., 2020; Liu et al., 2020), and machine translation (Font & Costa-jussà, 2019).",,,0,not_related
"Data augmentation by controlling the gender attribute is an effective technique in mitigating gender bias in NLP processes (Dinan et al., 2020; Sun et al., 2019).",,,0,not_related
"Moreover, they are often vulnerable to biases and adversarial attacks [23,12,18].",,,0,not_related
Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning.,,,0,not_related
"Text Data Text Classiication [50, 119, 182, 209, 277, 386] Embedding [46, 56, 156, 246, 274, 397] Language Modeling [48, 148, 238, 320, 371] Machine Translation [34, 91, 157, 329, 345] Dialogue Generation [106, 117, 229] Audio Data Speech Recognition [72, 178, 294, 336]",,,0,not_related
Queens Are Powerful Too: Mitigating Gender Bias in Dialogue Generation.,,,0,not_related
"…language model objective on pushshift.io Reddit data (Baumgartner et al., 2020) and fine-tuned on several dialog safety classification tasks, including Wikipedia Toxic Comments (Wulczyn et al., 2017) as well as the standard and adversarial Build-it Break-it Fix-it tasks from Dinan et al. (2019b).",,,1,related
"These biases can result in toxicity being associated with certain words, such as profanities or identity terms (Dinan et al., 2019b; Dixon et al., 2018), or language varieties, such as African American English (AAE) (Liu et al., 2019; Sap et al., 2019).",,,0,not_related
"There exist a number of other issues related to the problem of safety for conversational AI, which we consider outside the scope of this work.",,,0,not_related
"In particular for conversation, Dinan et al. (2019c) apply retrieval over Wikipedia to aid in open-domain dialogs.",,,0,not_related
"The topic of when and how to release LLMs trained by research groups has been of increasing interest to the community (Solaiman et al., 2019; Crootof, 2019; Ovadya & Whittlestone, 2019; Partnership on AI, 2020; Partnership on AI , 2021).",,,0,not_related
"This might be because the safety classifier was trained to identify dialog utterances that are “not OK to send in a friendly conversation with someone you just met online”, which may encapsulate more than just toxic responses (Dinan et al., 2019b).",,,1,related
"Gathered from the literature on responsible AI, the topics of the framework are split out by concept for clarity and to allow for targeted mitigation measures, however the topics naturally support each other and are often not as clearly delineated for all applications.",,,0,not_related
"Dinan et al. (2019b) make a first attempt at this by building a dataset for offensive utterance detection within a multi-turn dialog context, but limited to human-human dialogs.",,,0,not_related
"Dinan et al. (2019a) find gender biases present in several conversational datasets, and evaluate three debiasing techniques: counterfactual data augmentation, targeted data collection, and bias controlled training.",,,0,not_related
"For the dialog domain, Xu et al. (2020) extend the strategy of Dinan et al. (2019b) for collecting and training on adversarial examples to the human-bot conversational setting, with crowdworkers attempting to elicit unsafe outputs from the system.",,,1,related
"While (to our knowledge) little work exists on this problem for conversational AI, Dinan et al. (2019b) highlight the risks of systems exhibiting the YEA-SAYER (ELIZA) EFFECT in such situations by potentially agreeing with user statements suggesting self-harm.",,,0,not_related
"To test how the model responds to toxic input, we select 180 examples from the Build-it Break-it Fix-it “Standard” dataset (Dinan et al., 2019b) which are labeled as unsafe.",,,1,related
"These two facts taken together can result in situations where the system generates inappropriate content (Dinan et al., 2019b), or responds inappropriately to offensive content (Cercas Curry & Rieser, 2018; Lee et al., 2019).",,,0,not_related
"For conversational AI, the language(s) the model was trained on, the demographic composition and size of the intended audience, and the intended audience’s familiarity with concepts and limitations of machine learning and NLP are all important considerations.",,,0,not_related
"• Dialog safety classifier: We use a dialog safety classifier from Dinan et al. (2019b), and report the percentage of model responses that are flagged as unsafe by this classifier.",,,1,related
"We call conversational models trained in this paradigm end-to-end
1We follow European Commission (2021)’s definition of AI, which includes Machine Learning, statistical, as well as logic- and knowledge-based approaches.
ar X
iv :2
10 7.",,,1,related
"Dinan et al. (2019b); Xu et al. (2020) augment training data for the task with adversarial examples elicited from crowd workers, and train Transformer-based models for these tasks.",,,0,not_related
"Evolving benchmarks, such as Dynabench (Kiela et al., 2021), or other adversarial iterative procedures (Dinan et al., 2019b; Nie et al., 2019; Xu et al., 2020) can provide the required adaptability: our societal standards and expectations change, and we would not tolerate models that do not reflect…",,,1,related
", 2020), data augmentation or collection (Dinan et al., 2020), and different objective functions (Qian et al.",,,0,not_related
"Existing approaches towards mitigating biases in generation currently require retraining the models through adversarial trigger prompts (Sheng et al., 2020), data augmentation or collection (Dinan et al., 2020), and different objective functions (Qian et al., 2019; Huang et al., 2020).",,,0,not_related
"One
line of work addresses safety based on the fairness of chatbots regarding gender and race (Liu et al., 2020; Dinan et al., 2020; Lee et al., 2019b).",,,0,not_related
"of chatbots regarding gender and race (Liu et al., 2020; Dinan et al., 2020; Lee et al., 2019b).",,,0,not_related
Dinan et al. (2020b) focus on multi-dimensional gender bias classification and controlled mitigation.,,,0,not_related
"Dinan et al. (2020a) analyze existing dialog data sets for gender bias and extend LIGHT (Urbanek et al., 2019), a resource for grounded dialog, with crowdsourced gender-balanced utterances.",,,0,not_related
"…of work that focuses on detecting and mitigating biases in conversational systems is surprisingly limited (Lee et al., 2019; Liu et al., 2020a,b; Dinan et al., 2020a,b), albeit some more research has recently emerged in the wider context of biases in generalpurpose language generation models…",,,0,not_related
"Dialog models are known to suffer from biases learnable from dialog training data, such as gender bias (Dinan et al., 2019).",,,0,not_related
"Other works have attempted to mitigate biases, such as gender bias (Dinan et al., 2020a) and racial bias (Sap et al.",,,0,not_related
"Other works have attempted to mitigate biases, such as gender bias (Dinan et al., 2020a) and racial bias (Sap et al., 2019).",,,0,not_related
"For example, recent workshop on safety for conversational AI (Dinan et al., 2020b) introduced
∗Equal contribution
an example of such risk: Bickmore et al. (2018) asked participants to query conversational agents for advice in situations where medical information is needed.",,,0,not_related
", replacing “he” with “they” following [14, 13]) and (ii) by substituting names with others that are statistically predictive [70] of another race or ethnicity (e.",,,0,not_related
"…2019)
There are also metrics for other bias evaluation setups in continuation generation tasks involving sentiment (Shwartz et al., 2020), the ratio of gendered words (Solaiman et al., 2019; Vig et al., 2020; Dinan et al., 2020a), and other novel metrics (Peng et al., 2020; Yeo and Chen, 2020).",,,0,not_related
"For autocomplete generation, Vig et al. (2020) analyze GPT-2 variants through a causal mediation analysis, finding that larger models contain more gender bias, and bias tends to be concentrated in a small number of neurons and attention heads.",,,0,not_related
", 2020a,b) and the percentage of gendered words (Dinan et al., 2020a).",,,0,not_related
"Other works on dialogue biases (Dinan et al., 2020a; Sheng et al., 2020, 2021b) focus",,,0,not_related
"There are also works that induce control by incorporating a bias control code through conditional training (Dinan et al., 2020a), by appending a target value to inputs during training (Ma et al., 2020), by using a normative classifier to produce reward values for backpropagation (Peng et al.,…",,,1,related
"For autocomplete generation, Sheng et al. (2019, 2020) and Groenwold et al. (2020) compare regard or sentiment scores across demographics, Shwartz et al. (2020) compare names across various intermediate metrics, Vig et al. (2020) measure proportional differences between the amount of bias under a gendered versus ambiguous reading, and Yeo and Chen (2020) compare occupations generated for different genders.",,,0,not_related
"Examples include: • Regard Ratio: negative-neutral-positive regard
score ratios of text generated from bias-inducing prompts (Sheng et al., 2019) • Sentiment Ratio: negative-neutral-positive sentiment score ratios of text generated from African American English (AAE) versus White-Aligned English (WAE) prompts (Groenwold et al., 2020) • Individual and Group Fairness through Sentiment: comparisons of the sentiment distributions of generated text across demographics and prompts (Huang et al., 2020) • Gendered Word Co-occurrence Score: mean and standard deviations of the absolute log ratio of probabilities: P(word|female terms) to P(word|male terms) across all words in generated text (Bordia and Bowman, 2019)
There are also metrics for other bias evaluation setups in continuation generation tasks involving sentiment (Shwartz et al., 2020), the ratio of gendered words (Solaiman et al., 2019; Vig et al., 2020; Dinan et al., 2020a), and other novel metrics (Peng et al., 2020; Yeo and Chen, 2020).",,,0,not_related
"CDA has been applied to datasets used for continued or fresh training in dialogue generation (Dinan et al., 2020a; Liu et al., 2020a) as well as machine translation (Saunders and Byrne, 2020; Costa-jussà and de Jorge, 2020; Stafanovičs et al., 2020).",,,0,not_related
"Other works on dialogue biases (Dinan et al., 2020a; Sheng et al., 2020, 2021b) focus on Transformer-based models such as DialoGPT (Zhang et al., 2020) and other custom architectures.",,,0,not_related
"CDA has been applied to datasets used for continued or fresh training in dialogue generation (Dinan et al., 2020a; Liu et al., 2020a) as well as machine translation (Saunders and Byrne, 2020; Costa-jussà and de Jorge, 2020; Stafanovičs et al.",,,0,not_related
"Bias studies in dialogue generation use relative scores by comparing sentiment and offensive language discrepancies (Henderson et al., 2018; Liu et al., 2020a,b) and the percentage of gendered words (Dinan et al., 2020a).",,,0,not_related
"Existing works analyzing biases in autocomplete generation have mostly examined Transformer-based models, including GPT (Shwartz et al., 2020), GPT2 (Solaiman et al., 2019; Sheng et al., 2019, 2020; Shwartz et al., 2020; Vig et al., 2020; Yeo and Chen, 2020; Huang et al., 2020; Dhamala et al., 2021; Schick et al., 2021), GPT-3 (Brown et al., 2020), CTRL (Dhamala et al., 2021), TransformerXL (Shwartz et al., 2020; Vig et al., 2020; Huang et al., 2020), and XLNet (Shwartz et al., 2020; Vig et al.,
2020; Yeo and Chen, 2020), though Bordia and Bowman (2019); Qian et al. (2019) also look at LSTM-based models.",,,0,not_related
"There are also works that induce control by incorporating a bias control code through conditional training (Dinan et al., 2020a), by ap-",,,0,not_related
"Biases can be towards people described in text, people who produce the text, or people to whom the text is addressed (Dinan et al., 2020b).",,,0,not_related
"Vig et al. (2020) also use prompts to investigate gender biases, though they do so in the context of a causal mediation analysis.",,,0,not_related
"Moreover, open dialogue chatbot models have been found to amplify gender bias that exist in training dialogues [7, 16].",,,0,not_related
"Previous existing works have taken different approaches to address the issue of gender bias by detecting the male/female ratio of images [19, 20], measuring fairness in dialogues systems [10, 23], language modeling [4], machine translation [7], and coreference resolution [42].",,,0,not_related
"Dialogue Personas Most similar to our work, Dinan et al. (2020) explore how different personas lead to different amounts of generated gendered words and pursue strategies for mitigation.",,,0,not_related
"…al., 2016; Hovy and Spruit, 2016; Caliskan et al., 2017; Rudinger et al., 2017; Garg et al., 2018; Garimella
et al., 2019; Gonen and Goldberg, 2019; Dinan et al., 2020a,b), we further observe in our results yet a higher order type of stereotyping that negatively affects women, namely androcentrism…",,,0,not_related
"…2017; Gonen and Goldberg, 2019; Dev and Phillips, 2019; Garg et al., 2018; Lauscher et al., 2020a) and even more models for removing or attenuating such biases have been developed (Zhao et al., 2019; Bordia and Bowman, 2019; Dinan et al., 2020; Webster et al., 2020; Qian et al., 2019, inter alia).",,,0,not_related
"Standard Data We consider the Wikipedia Toxic Comments dataset (WTC) (Wulczyn et al., 2017) designed to identify personal attacks online, consisting of ∼150k examples; we use the version that treats the data as a two-class problem (Khatri et al., 2018a; Dinan et al., 2019c).",,,1,related
"…dataset (Zhang et al., 2018) focuses on personality and engaging the other speaker, Empathetic Dialogues (Rashkin et al., 2019) focuses on empathy, and Wizard of Wikipedia (Dinan et al., 2019c) focuses
1https://github.com/huggingface/ tokenizers
2https://files.pushshift.io/reddit/
on knowledge.",,,0,not_related
"Firstly, we find our newly trained models superior to existing models from Dinan et al. (2019b) when using the same training sets, likely due to improved pushshift.io Reddit pre-training of our transformers compared to their BERT models.",,,1,related
"We consider Transformer-based classifiers, following the same structure as in Dinan et al. (2019b), with two sizes: 256M and 622M parameter models.",,,1,related
"Dinan et al. (2019a) measured gender bias in several conversational datasets and proposed three techniques to address it: counterfactual data augmentation, targeted data collection, and bias controlled training.",,,0,not_related
"In addition, we consider a dataset more specifically collected for safety in open-domain dialogue of (Dinan et al., 2019b), which consists of a further 8,000 offensive examples.",,,1,related
"The resulting conversational models were shown to use less gendered words, be less offensive, while being as engaging (Dinan et al., 2019a).",,,0,not_related
The classifiers we propose in this work can be seen as improvements over the variants introduced in Dinan et al. (2019b).,,,1,related
"We detailed previously how safety classifiers can be trained to be adversarially robust to human utterances, see Section 3.1.1 or Dinan et al. (2019b).",,,1,related
• Gender Bias Mitigation (§3.4): Using strategies from Dinan et al. (2019a) to force the model to respond with gender neutral language.,,,0,not_related
"We select a topic at random from 1087 topics judged as safe from the Wizard of Wikipedia conversational topic list (Dinan et al., 2019c).",,,1,related
"The work of
Dinan et al. (2019b) thus also explored an adversarial collection scheme to make classifiers more robust.",,,0,not_related
"Gender bias is exhibited across a wide range of conversational datasets, including Reddit (Dinan et al., 2019a).",,,0,not_related
Dinan et al. (2019b); Nie et al. (2019) are examples of such evolving benchmarks4.,,,0,not_related
"For example, the baseline BST 2.7B only provides OK responses 55% of the time on the adversarial test set, whereas our Safety classifier improves that to 87.5%, superior to the existing work of Dinan et al. (2019b) which yields 77.7%.",,,1,related
"…collection setup are given in Appendix A.
Figure 1 demonstrates how this adversarial setup differs from the “Build-it, Break-it, Fix-it” setup from Dinan et al. (2019b): namely, in the former, the “breaker” (or adversarial user) tries to break a classifier by submitting human-authored…",,,1,related
"(Some) (Most)
Two-stage models with classifiers
BST 2.7B + Multi-Turn Safety Classifier (Dinan et al., 2019b) 78.2 6.7 6.7 8.4 BST 2.7B + Safety Classifier 87.2 5.6 3.9 3.3 BST 2.7B + Safety Classifier (Semi-Sup.",,,1,related
"NLP papers on spurious associations have addressed social biases (Dixon et al., 2018; Zhao et al., 2018; Kiritchenko & Mohammad, 2018; Dinan et al., 2019; May et al., 2019), spurious signals learned due to annotation heuristics adopted by crowd workers (Gururangan et al., 2018; Poliak et al.,…",,,0,not_related
"NLP papers on spurious associations have addressed social biases (Dixon et al., 2018; Zhao et al., 2018; Kiritchenko & Mohammad, 2018; Dinan et al., 2019; May et al., 2019), spurious signals learned due to annotation heuristics adopted by crowd workers (Gururangan et al.",,,0,not_related
"…et al. (2020): ConvAI2 (Dinan et al., 2020b), EmpatheticDialogues (ED) (Rashkin et al., 2019), Wiz-
1Unlike in those works, the output of the encoder is then passed to a decoder, as in the late fusion case.
ard of Wikipedia (WoW) (Dinan et al., 2019c), and BlendedSkillTalk (Smith et al., 2020).",,,1,related
"Wizard of Wikipedia (WoW) The Wizard of Wikipedia dataset (Dinan et al., 2019c) involves
4https://pytorch.org/hub/ facebookresearch_WSL-Images_resnext/
5https://github.com/facebookresearch/ vilbert-multi-task
two speakers discussing a given topic in depth, comprising 194k utterances.",,,1,related
"…ways in which the MMB Style model could potentially display gen-
der bias: for instance, there is no safeguard against it misgendering a person in an image, and many common text datasets are known to contain gender bias (Dinan et al., 2019a, 2020a), which may lead to bias in models trained on them.",,,0,not_related
"ore with our best models is safety - that is, ensuring that our models are not offensive to their conversational partners. Dialogue safety is indeed a well-studied, but still unsolved, research area (Dinan et al., 2019b;Liu et al.,2019;Dinan et al.,2019a;Blodgett et al.,2020;Khatri et al.,2018;Schafer and¨ Burtenshaw,2019;Zhang et al.,2018a), yet we note that safety in the context of image-dialogue is relatively le",,,0,not_related
"To mitigate this problem, we first measure our models’ toxicity using an openly available blocklist3 and an offensive language classifier presented in Dinan et al. (2019b).",,,1,related
"Dialogue safety is indeed a well-studied, but still unsolved, research area (Dinan et al., 2019b; Liu et al., 2019; Dinan et al., 2019a; Blodgett et al., 2020; Khatri et al., 2018; Schäfer and Burtenshaw, 2019; Zhang et al., 2018a), yet we note that safety in the context of image-dialogue is…",,,0,not_related
"…this, we train a version of the MMB Style model in which we examine the label of each training example to determine whether it contains female or male words, and then a string representing that classification is appended to the example’s context string (Dinan et al., 2019a), for input to the model.",,,1,related
" to an unsafe response given a multi-modal context. To mitigate this problem, we ﬁrst measure our models’ toxicity using an openly available blocklist7 and an offensive language classiﬁer presented inDinan et al. (2019b). We deﬁne the term “toxicity” to mean the ratio between the number of offensive utterances and the total number of utterances generated by the model. We evaluate our model on the Image-Chat validat",,,1,related
"815 tem, enabling us to filter the possible outputs of the agent; and (2) dataset bias via curation through controlled crowdsourcing in the case of LIGHTQuests—the methods to debias the original LIGHT dataset can be found in Dinan et al. (2020) and crowdsourcing methods for the original ATOMIC work can be found in Sap et al.",,,1,related
…possible outputs of the agent; and (2) dataset bias via curation through controlled crowdsourcing in the case of LIGHTQuests—the methods to debias the original LIGHT dataset can be found in Dinan et al. (2020) and crowdsourcing methods for the original ATOMIC work can be found in Sap et al. (2019).,,,1,related
"This method tries to mitigate the gender bias in dialogue models by augmenting the training data (Liu et al., 2019a; Dinan et al., 2019).",,,0,not_related
Dinan et al. (2019) analyze gender bias in persona-based dialogue models and proposes a combination debiasing method.,,,0,not_related
"There are debiasing methods in NLP such as data augmentation (Dinan et al., 2019) and word embeddings regularization (Liu et al., 2019a).",,,1,related
"There are debiasing methods in NLP such as data augmentation (Dinan et al., 2019) and word embeddings regularization (Liu et al.",,,1,related
"Other work has focused on removing gender bias from language models (Bordia and Bowman, 2019; Dinan et al., 2020; Bolukbasi et al., 2016).",,,0,not_related
"ing set (Dinan et al., 2019a), and we make use of that publicly available data here as well.",,,1,related
"Additionally, gender bias concerns have been previously studied within the available LIGHT MTurk training set (Dinan et al., 2019a), and we make use of that publicly available data here as well.",,,1,related
"Game Safety We employ a safety classifier (Dinan et al., 2019b) on both human and model turns.",,,1,related
"A number of crowdsourced or scraped datasets have been developed to that end, including Daily Dialogue (Li et al., 2017), PersonaChat (Li et al., 2016a), Empathetic Dialogues (Rashkin et al., 2019) and Wizard of Wikipedia (Dinan et al., 2019c).",,,0,not_related
Vague/unstated Rudinger et al. (2018); Webster et al. (2018); Dinan et al. (2019); Florez (2019); Jumelet et al. (2019); Lauscher et al. (2019); Liang et al. (2019); Maudslay et al. (2019); May et al. (2019); Prates et al. (2019); Prost et al. (2019); Qian et al. (2019); Swinger et al. (2019); Zhao…,,,0,not_related
…et al. (2018); Shen et al. (2018); Bordia and Bowman (2019); Cao and Daumé (2019); Cho et al. (2019); Davidson et al. (2019); Dev et al. (2019); Dinan et al. (2019); Fisher (2019); Florez (2019); Font and Costa-jussà (2019); Garg et al. (2019); Huang et al. (2019); Liu et al. (2019); Nozza et…,,,0,not_related
"However, with the wide application of neural dialogue models, the ethical challenges they bring are attracting more and more attention (Henderson et al., 2017; Liu et al., 2019a; Dinan et al., 2019).",,,0,not_related
"Finally, many scholars have proposed a variety of computational techniques for mitigating gender norms and stereotypes in a wide range of languagebased applications (Dev and Phillips, 2019; Dinan et al., 2019; Ethayarajh et al., 2019; Hall Maudslay et al., 2019; Stanovsky et al., 2019; Tan and Celis, 2019; Zhou et al., 2019; Zmigrod et al., 2019).",,,0,not_related
"Control models have been used in a variety of settings to adjust length (Fan et al., 2018), bias (Dinan et al., 2019), and style (See et al., 2019).",,,0,not_related
", 2018), bias (Dinan et al., 2019), and style (See et al.",,,0,not_related
"In particular, NLP models often learn to replicate unwanted gender biases present in society (Bolukbasi et al., 2016; Hovy and Spruit, 2016; Caliskan et al., 2017; Rudinger et al., 2017; Garg et al., 2018; Gonen and Goldberg, 2019; Dinan et al., 2020).",,,0,not_related
"ited in coverage and applicability to a variety of domains (Dinan et al., 2020).",,,0,not_related
"have been found to be amplified in machine learning models (Lee et al., 2019; Dinan et al., 2020; Liu et al., 2019).",,,0,not_related
"Retriever We fine-tune the retrieval models on ConvAI2, Wizard of Wikipedia, Empathetic Dialogues, and Blended Skill Talk datasets (BST variants of each7) and automatically evaluate them by measuring hits@1/K on the validation sets of each of these datasets.",,,1,related
", 2019b) and mitigating gender bias in dialogue generation (Dinan et al., 2019a) but much work remains to be done.",,,0,not_related
"8We also compared adding a Wizard of Wikipedia-based topic vs. not to the context, and in that case saw no discernible difference in evaluation scores.",,,0,not_related
"We then assessed whether those generations were safe or not using two different methods: using an unsafe word list, or the safety classifier of Dinan et al. (2019b), both methods being available in ParlAI (Miller et al., 2017).",,,1,related
"We have also previously conducted studies into mitigating gender bias in dialogue through the use of conditional generation, controlling the amount of gendered words to be more neutral, with preliminary success (Dinan et al., 2019a).",,,0,not_related
"For example, the ConvAI2 dataset (Zhang et al., 2018) focuses on personality and engaging the other speaker, Empathetic Dialogues (Rashkin et al., 2019) focuses on empathy, and Wizard of Wikipedia (Dinan et al., 2019c) focuses on knowledge.",,,0,not_related
A retrieval system over Wikipedia was used from which the dialogues were grounded during the human-human crowdsourced conversations.,,,0,not_related
"This mirrors results found in some recent papers comparing generation and retrieval (Li et al., 2016; Dinan et al., 2019c).",,,0,not_related
"We use the same retrieval system as in that cited work, which uses a TF-IDF-based inverted index lookup over a Wikipedia dump2 to produce an initial set of knowledge candidates.",,,1,related
"We have previously investigated building better classifiers of toxic language by collecting adversarial toxic data that fools existing classifiers and is then used as additional data to make them more robust, in a series of rounds (Dinan et al., 2019b).",,,1,related
"We hence refer to this as a Wizard Generative model, as the supervised training signal of how to use knowledge in dialogue comes from the Wizard of Wikipedia task, even though we multi-task on other tasks as well.",,,1,related
"Wizard of Wikipedia (WoW): The Wizard of Wikipedia task involves discussing a given topic in depth, where the goal is to both engage the partner as well as display expert knowledge (Dinan et al., 2019c).",,,0,not_related
"7https://parl.ai/projects/bst
We also report perplexity both before and after fine-tuning each of these models on the ConvAI2, Wizard of Wikipedia, Empathetic Dialogues, and Blended Skill Talk datasets.",,,1,related
"We can then condition the generation on the retrieved knowledge, as done in models proposed for the Wizard of Wikipedia task (Dinan et al., 2019c).",,,1,related
"Offensive responses cause users discomfort and should be avoided (Henderson et al., 2018; Dinan et al., 2019b; Liu et al., 2019; Liu et al., 2020b).",,,0,not_related
"In the work (Dinan et al., 2019a), the authors examine gender bias in both dialogue datasets and generative dialogue models.",,,0,not_related
"In this measurement, we apply an offensive language detection model (Dinan et al., 2019b) to predict whether a response is offensive or not.",,,1,related
"…as related documents (Zhou et al., 2018; Dinan et al., 2019) and user-based features such as persona (Zhang et al., 2018; Majumder et al., 2020; Dinan et al., 2020b), emotion (Rashkin et al., 2019), social norms (Kim
4For a more comprehensive literature review, refer to survey papers on…",,,0,not_related
"ConvAI2 (Zhang et al., 2018; Dinan et al., 2020b): This dataset is designed for persona
5The purpose of this analysis is to find out if there are any notable patterns associated with the inclusion of situational statements rather than benchmarking response generation systems.",,,0,not_related
"Therefore, the data and system output should be closely monitored, either manually or through automatic methods such as debiasing techniques (Liu et al., 2020; Dinan et al., 2020a).",,,0,not_related
"Despite numerous bias mitigation approaches put forth (Cao and Daumé III, 2020; Dinan et al., 2020a; Hube and Fetahu, 2019; Webster et al., 2018; Zhao et al., 2018), many have limited efficacy, failing to address the complexity of biased language (Stańczak and Augenstein, 2021; Blodgett et al.,…",,,0,not_related
"Additionally, despite writing of four gender values (unknown, neutral, feminine, and masculine), the dataset and classifiers of Dinan et al. (2020b) are limited to “masculine and feminine classes” (317).",,,0,not_related
"Despite numerous bias mitigation approaches put forth (Cao and Daumé III, 2020; Dinan et al., 2020a; Hube and Fetahu, 2019; Webster et al., 2018; Zhao et al., 2018), many have limited efficacy, failing to address the complexity of biased language (Stańczak and Augenstein, 2021; Blodgett et al.",,,0,not_related
"…bias work, for example, often uses a binary gender framework either in its conceptualization (such as Webster et al. (2018)) or application (such as Dinan et al. (2020b)), and tends to focus on one variety of gender bias, stereotypes (Stańczak and Augenstein, 2021; Doughman et al., 2021;…",,,0,not_related
"Though Dinan et al. (2020b) also provide a framework for defining types of gender bias, their framework focuses on relationships between people in a conversation, identifying “bias when speaking ABOUT someone, bias when speaking TO someone, and bias from speaking AS someone” (316).",,,0,not_related
"…the natural language processing community, work has focused on combating gender bias in co-reference resolution (Zhao et al., 2018), dialogue (Dinan et al., 2019; Lee et al., 2019; Liu et al., 2020), detection of abusive language (Park et al., 2018), machine translation (Stanovsky et al.,…",,,0,not_related
", 2018), dialogue (Dinan et al., 2019; Lee et al., 2019; Liu et al., 2020), detection of abusive language (Park et al.",,,0,not_related
"Studies have indicated that the words used in biographies about women compared to biographies about men (Dinan et al., 2019) also differs, and is reflective of gendered terminology.",,,0,not_related
"Dinan et al. (2020b) point out that there are three types of gender bias in chat bots: the first one being due to the gender of the person that speakers are talking about, the
second being due to the gender of the speaker, and the last being due to the gender of the addressee.",,,0,not_related
"Similarly, Dinan et al. (2020a) propose to reduce gender bias via data augmentation, targeted data collection, and biascontrolled training.",,,0,not_related
"…on generative models (Perez et al., 2022; Xu et al., 2021b; Kirk et al., 2021a; Sheng et al., 2021b; Nozza et al., 2021; Renduchintala et al., 2021; Dinan et al., 2020a,b), which are well known to pose unique challenges for automatic evaluation (Lowe et al., 2017; Howcroft et al., 2020;…",,,0,not_related
"A popular set of techniques for measuring bias in generated text involves computing the frequency of different words on a word list, for example, those signifying gender (Dinan et al., 2020a); religion, race, gender, and orientation (Barikeri et al.",,,0,not_related
"A popular set of techniques for measuring bias in generated text involves computing the frequency of different words on a word list, for example, those signifying gender (Dinan et al., 2020a); religion, race, gender, and orientation (Barikeri et al., 2021); or occupations (Kirk et al., 2021b).",,,0,not_related
"There is existing and ongoing research on ways to mitigate these outputs (e.g. Xu et al., 2020; Dinan et al., 2019; Faal et al., 2022; Dinan et al., 2020), though Gonen and Goldberg (2019) argue that debiasing methods are insufficient and do not remove bias entirely.",,,0,not_related
"There is existing and ongoing research on ways to mitigate these outputs (e.g. Xu et al., 2020; Dinan et al., 2019; Faal et al., 2022; Dinan et al., 2020), though",,,0,not_related
"…investigate through analysis of the models, their training regimes, and the data that they rely on (Hall Maudslay et al., 2019; Zhao et al., 2019; Dinan et al., 2020a,b; Vargas and Cotterell, 2020; Smith and Williams, 2021; Talat et al., 2021), rather than seeking to imbue models with a sense of…",,,0,not_related
"Consequently, these biased word embeddings have effects on downstream applications (Dinan et al., 2020; Blodgett et al., 2020).",,,0,not_related
"(Dinan et al., 2019) present an example of a bias measure that uses a crowdsourced dataset (LIGHT from (Urbanek et al.",,,0,not_related
"(Dinan et al., 2019) present an example of a bias measure that uses a crowdsourced dataset (LIGHT from (Urbanek et al., 2019)) to evaluate gender biases—in this case, through the percentage of gendered words.",,,0,not_related
"[151] Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston.",,,0,not_related
"There has been extensive work studying gender and racial biases in word embeddings [70, 90, 329, 397, 489, 706], contextual embeddings such as ELMo [467], BERT [149, 324, 354, 407], and GPT [209, 358, 481, 571], and generative models [151, 252, 358, 479, 521].",,,0,not_related
"Dinan et al. (2020); Liu et al. (2020a,b) discuss gender bias in dialogue generation and Sheng et al. (2021b) investigates the ad hominems in dia-
logue responses regarding the race perspective.",,,0,not_related
"Its use case includes hate speech detection [11, 12], machine translation [35, 48] and dialogue generation [14].",,,0,not_related
"This method has proven to be successful within the field of natural language processing [11,12,14,35,36].",,,0,not_related
"Such an approach has proved promising for several use cases within the field of natural language processing [11, 12,14,35,36], recommendation systems [47] as well as visual question answering systems [41].",,,0,not_related
"Most prior work in bias mitigation has largely taken the “one-size-fits-all” approach, with most models being agnostic to the language of the speakers behind the language (Sun et al., 2019; Liang et al., 2020; Dinan et al., 2020; Garimella et al., 2021).",,,0,not_related
"At the training level, Dinan et al. (2020) adapted the training process and applied bias controlled training to generative dialogue models to make them generate an equal number of gendered words for both genders considered.",,,0,not_related
"…text systems, we propose re-purposing methods for con-
trolling text generation (Ghazvininejad et al., 2017; Holtzman et al., 2018; Tambwekar et al., 2018; Keskar et al., 2019; Sahar et al., 2020) which are being used to control ‘bias’ in text generation (Sheng et al., 2020; Dinan et al., 2020).",,,1,related
"We highlight: input processing methods such as ‘smart prompts’ (Sheng et al., 2020), model conditioning methods such as in Tambwekar et al. (2018); Keskar et al. (2019); Dinan et al. (2020), and output processing methods such as ‘guided decoding’ (Ghazvininejad et al., 2017; Holtzman et al., 2018).",,,0,not_related
", 2020) which are being used to control ‘bias’ in text generation (Sheng et al., 2020; Dinan et al., 2020).",,,0,not_related
"…generation (Ghazvininejad et al., 2017; Holtzman et al., 2018; Tambwekar et al., 2018; Keskar et al., 2019; Sahar et al., 2020; Sheng et al., 2020; Dinan et al., 2020) and the intersection of causal inference and language (Tan et al., 2014; Gligorić et al., 2019; Sridhar and Getoor, 2019;…",,,0,not_related
Dixon et al. (2018) analyze biases in a toxicity classification model through the Wikipedia Talk Pages dataset as well as through a templated test set.,,,0,not_related
"Dixon et al. (2018) analyze biases in a toxicity classification model through the Wikipedia Talk Pages dataset as well as through a templated test set. Jigsaw (Jigsaw, 2019) contains comments from the Civil Comments platform labeled with six types of toxicity (e.g., toxic, obscene, etc) and identity attributes (e.g., white, woman, etc). Along with this dataset, Jigsaw (2019) present a bias evaluation following that of Borkan et al.",,,0,not_related
Dinan et al. (2020) present an example of a bias measure that uses a crowdsourced,,,0,not_related
"Dinan et al. (2020) present an example of a bias measure that uses a crowdsourced
4Sheng et al. (2021) has a more comprehensive survey that goes beyond bias measures.
dataset (LIGHT from Urbanek et al. (2019)) to evaluate gender biases—in this case, through the percentage of gendered words.",,,0,not_related
"Additionally, gender bias concerns have been previously studied within the available LIGHT MTurk training set (Dinan et al., 2020), and we make use of that publicly available data here as well.",,,1,related
"…language modeling, coreference resolution, natural language inference, machine translation, and sentiment analysis (Sheng et al., 2019; Rudinger et al., 2018; Lu et al., 2018; Dinan et al., 2019; Rudinger et al., 2017; Kiritchenko and Mohammad, 2018); Blodgett et al. (2020) provide a review.",,,0,not_related
"Moreover, they are often vulnerable to biases, privacy violations, adversarial attacks, and safety concerns [1,10,14,21].",,,0,not_related
"Serious problems of representational bias, often related directly to undersampling of distinct geographic regions and/or demographic groups, have been uncovered in gold-standard genomic databases used throughout the biomedical sciences [40]–[42], in speech and facial/gesture recognition services [43], [44], in pop-",,,0,not_related
", 2019), but also gender identities (Font and Costa-jussà, 2019; Dinan et al., 2019; Dinan et al., 2020).",,,0,not_related
", 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al.",,,0,not_related
"…et al., 2018; Font and Costa-jussà, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanovičs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019).",,,0,not_related
"…at a great pace and raised expectation about the quality of results and especially their impact in a social context, including not only race (Merullo et al., 2019) and politics (Fan et al., 2019), but also gender identities (Font and Costa-jussà, 2019; Dinan et al., 2019; Dinan et al., 2020).",,,0,not_related
