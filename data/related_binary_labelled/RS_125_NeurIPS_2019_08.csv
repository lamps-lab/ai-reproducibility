text,label_score,label,target_predict,target_predict_label
"Such an alternative training of the potential function and the policy is similar to model-based reinforcement learning algorithms (Luo et al., 2018; Sun et al., 2018; Janner et al., 2019) for monotonic improvement of policies.",,,0,not_related
"For the pre-training objective, we use forward dynamics prediction, as it has been shown to be useful in model-based methods (Janner et al., 2019) and auxiliary loss literature (He et al.",,,1,related
"LatentDiffuser is incorporated into a research trajectory focused on model-based reinforcement learning (RL) (Sutton, 1990; Janner et al., 2019; Schrittwieser et al., 2020; Lu et al., 2021; Eysenbach et al., 2022; Suh et al., 2023), as it makes decisions by forecasting future outcomes.",,,0,not_related
• Official codes distributed from the paper [24]: to build PMT-G.,,,1,related
"[24] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
MBPO is the state-of-the-art model-based policy optimization [24].,,,0,not_related
"Existing methods for non-stationary environments can be grouped into three schools of thought: 1) shoehorning: directly using established frameworks for stationary MDPs, assuming no extra mechanisms are needed since non-stationarity already exists in standard RL due to policy updates; 2) model-based policy updates: updating models with new data, using short rollouts to prevent model exploitation [24, 29], online model updates, or through latent factor identification [4, 13–16]; and 3) anticipating future changes by forecasting policy gradients or value functions [7, 30, 20, 10, 31].",,,0,not_related
"Hence, a handful of Dyna-style methods proposed to simulate multi-step rollouts by using such ensemble models, such as SLBO [28] and MBPO [16].",,,0,not_related
[16] has been systematically explained that inaccuracies in learned models make long rollouts unreliable due to the compounding error.,,,0,not_related
"A bootstrapped ensemble of dynamics models is constructed to approximate the true transition dynamics of environment: f(st+1|st, at), which has been demonstrated in several studies [5, 20, 16, 36, 43, 44].",,,0,not_related
"A key issue in model learning is model bias, which refers to the error between the model and the real environment [19].",,,0,not_related
"Hence, we are inclined to propose a generic algorithm similar to [23, 19, 20] that can be plugged into many SOTA MFRL algorithms [14, 27], rather than just proposing for a specific policy optimization algorithm.",,,1,related
"In MFRL, methods such as TRPO [39] and CPI [21] choose to optimize the performance difference bound, whilst most of the previous work in MBRL [29, 19, 50, 36, 23] choose to optimize the difference of expected return under the model and that of the real environment, which is termed return discrepancy.",,,0,not_related
"By minimizing this optimization objective after the model update via maximum likelihood estimation (MLE) [6, 19], we can tune the model to adaptively find appropriate updates to get a performance improvement guarantee.",,,1,related
"On the contrary, model-based RL (MBRL) algorithms, using a world model to generate the imaginary rollouts and then taking them for policy optimization [29, 19], have high sample efficiency while achieving similar asymptotic performance, thus becoming a compelling alternative in practical cases [37, 15, 49].",,,0,not_related
"Phase 1 uses traditional MLE loss to train the model, which may impair the performance by excessive model updates due to only considering the impacts of model bias.",,,0,not_related
"In phase 1, the ensemble models are trained on shared but differently shuffled data, where the optimization objective is MLE [6, 19].",,,0,not_related
We follow the previous work [19] to use the combination of the ensemble model technique with short model rollouts to mitigate the compounding error.,,,1,related
"While having achieved comparable results, they only account for model bias in one iteration [19] but do not consider the impacts of model shift between two iterations [20], which can lead to performance deterioration due to excessive model updates.",,,0,not_related
"The performance difference bound can be decomposed into three terms, V π2|M2 − V π1|M1 = (V π2|M2 − V π2 M2)− (V π1|M1 − V π1 M1) + (V π2 M2 − V π1 M1) (5) Obviously, compared to directly optimizing the return discrepancy of each iteration [19], the performance difference bound chooses to optimize the return discrepancy of two adjacent iterations, namely V π2|M2 − V π2 M2 and V π1|M1 − V π1 M1 respectively, and the expected return variation between these two iterations, namely V π2 M2 − V π1 M1 , demonstrating better rigorousness.",,,0,not_related
"The uncertainty estimation techniques are used to adjust the rollout length [19, 28] or the transition weight [18, 36].",,,0,not_related
"Many prior methods [29, 19, 50, 36, 23] rely on return discrepancy to obtain model updates with a performance improvement guarantee.",,,0,not_related
An ensemble of probabilistic networks [97] was utilized by the MBPO method [98] to deal with the two sources of the dynamics model error.,,,0,not_related
"Overall, previous works [17], [19], [20] have suggested using deep neural networks to overcome the computational burden of probabilistic MBRL and left the following issues unaddressed: 1) the uncertainty propagation based on neural networks is unstable; 2) the fitting error of neural networks is less considered; 3) the aleatoric and epistemic uncertainties are not distinguished during propagation.",,,0,not_related
"We selected Deep Pilco [17], PETS [19], MBPO [20] as the MBRL baselines, and selected SAC [23], PPO [24], DDPG [25] as the model-free RL baselines2.",,,1,related
Approach MPC DNN Fitting Error Distinguished Uncertainties PILCO [10] × × N/A × GP-MPC [14] ⃝ × N/A × Deep Pilco [17] × ⃝ × × PETS [19] ⃝ ⃝ × × MBPO [20] × ⃝ × × DPETS (ours) ⃝ ⃝ ⃝ ⃝,,,0,not_related
propagation by generating enhanced data from the predictive model [20].,,,0,not_related
[23] employed a model ensemble approach to reduce biases introduced by probabilistic networks.,,,0,not_related
"This has been extensively studied in both online and offline model-based RL literature [37, 38, 39, 40].",,,0,not_related
"For both algorithms, we solve the inner problem using Model-Based Policy Optimization (MBPO; [39]) which uses Soft Actor-Critic (SAC; [26]) in a dynamics model ensemble.",,,1,related
"The MBRL baselines are MBPO: [33] with STL accuracy as the reward, PETS: [34] with a hand-crafted reward, and CEM: Cross Entropy Method [35] with STL robustness reward.",,,0,not_related
This reduces the interactions with the real environment and improves sample efficiency (Xu et al. 2018; Janner et al. 2019).,,,0,not_related
"Reinforcement learning provides an alternative way that allows robots to learn a robust policy through trial and error including either model-based method [24], modelfree method [25] or their combination [26], and the learned policy generalizes well under task uncertainties.",,,0,not_related
"Though model-based RL began with low-dimensional, compact state spaces [26, 37, 27, 57], advances in visual model-based reinforcement learning [17, 19, 18, 44, 42, 21] learn",,,0,not_related
"We adopt the baseline architecture from MBPO (Janner et al., 2019) and the implementation from Pineda et al. (2021), where the posterior MDP, denoted Γψ, is represented as an ensemble of n neural networks trained via supervised learning on the environment dataset D to predict the mean and variance…",,,1,related
"We adopt the baseline architecture from MBPO (Janner et al., 2019) and the implementation from Pineda et al.",,,1,related
"MBPO with slight modifications from Janner et al. (2019): (1) it only uses Dmodel to update the actor and critic, rather than mixing in data from D; (2) it uses a fixed rollout length k, instead of an adaptive scheme.",,,0,not_related
"Next, we use MOReL [12] as a representative of a general MBPO approach that covers both a classical (naïve) MBRL and a Pessimistic MDP-based MBRL.",,,1,related
3.2 MDP-based Learning A contrastive approach to offline RL is to derive an MDP from the data and solve it either optimally or approximately with a model-based policy optimization (MBPO) [10].,,,0,not_related
A contrastive approach to offline RL is to derive an MDP from the data and solve it either optimally or approximately with a model-based policy optimization (MBPO) [10].,,,0,not_related
"…Kumar et al., 2019b; Zhang et al., 2021; Kostrikov et al., 2021a); ii) enforcing conservative estimates of future rewards (Kumar et al., 2020; Yu et al., 2021; Cheng et al., 2022); and iii) model-based methods that estimate the uncertainty via ensembles (Janner et al., 2019; Kidambi et al., 2020).",,,0,not_related
Model-based policy optimisation (MBPO) [27] is a sample-efficient neural architecture for optimising policies in learned dynamics model.,,,0,not_related
"Furthermore, model-based approaches to safe RL have gained increasing traction, in part due to recent significant developments in model-based RL (MBRL) [18, 21, 22] and the superior sample complexity of model-based approaches [19, 27].",,,0,not_related
"Model-based RL as a paradigm for learning complex policies has become increasingly popular in recent years due to its superior sample efficiency [19, 27].",,,0,not_related
"However, training a policy using an inaccurately estimated model can be harmful (Janner et al., 2019).",,,0,not_related
"However, an inaccurately estimated model for unobserved state-action pairs in D can lead to poor performance of the learned policy (Janner et al., 2019).",,,0,not_related
"Following previous works (Janner et al., 2019; Yu et al., 2020; 2021b; Rigter et al., 2022), we train an ensemble of 7 such models that each contain the dynamics model and autoencoder and pick the best 5 models based on the validation prediction error on a held-out test set of 1000 transitions from…",,,1,related
"For example, model-based RL involves learning a model of the world (Racanière et al., 2017; Hafner et al., 2019; Janner et al., 2019; Schrittwieser et al., 2020) while most model-free policy gradient methods train a value or Q-network to control the variance of the gradient update (Mnih et al.,…",,,0,not_related
", 2012) Hopper implementation using the Model-Based Policy Optimization (MBPO) algorithm (Janner et al., 2019)10 that learns an ensemble of dynamics models, each predicting the parameters of a multivariate Gaussian distribution over the next-step observation and reward given the current state and performed action.",,,0,not_related
"We trained an agent in the MuJoCo (Todorov et al., 2012) Hopper implementation using the Model-Based Policy Optimization (MBPO) algorithm (Janner et al., 2019)10 that learns an ensemble of dynamics models, each predicting the parameters of a multivariate Gaussian distribution over the next-step…",,,1,related
"This is the statistical uncertainty representative of the inherent system stochasticity, i.e., the unknowns that differ each time the same experiment is run (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"We follow approaches in the model-based RL literature (Chua et al., 2018; Janner et al., 2019; Shyam et al., 2019; Pathak et al., 2019) where epistemic uncertainty is measured by estimating the level of disagreement between different predictive models, forming an ensemble, that are trained independently, usually by random sub-sampling of a common replay buffer.",,,1,related
"We follow approaches in the model-based RL literature (Chua et al., 2018; Janner et al., 2019; Shyam et al., 2019; Pathak et al., 2019) where epistemic uncertainty is measured by estimating the level of disagreement between different predictive models, forming an ensemble, that are trained…",,,1,related
", the unknowns that differ each time the same experiment is run (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"…or an action-value function, Q(s, a), asserting the value of executing some action given a state;
Model-based approaches (e.g., Chua et al., 2018; Janner et al., 2019) that learn a model of the environment’s dynamics, P(s′, r|s, a), i.e., a function mapping from observations and actions to…",,,0,not_related
"Model-based approaches (e.g., Chua et al., 2018; Janner et al., 2019) that learn a model of the environment’s dynamics, P(s′, r|s, a), i.",,,0,not_related
"The experimental results show that the proposed method can achieve higher sample efﬁciency than previous works such as vanilla HER [14], MBPO [11] and MHER [18].",,,0,not_related
"Based on the [10], the work in [11], [12] avoided the compounding error by generating short branched roll outs from real states, which is also used in our method.",,,1,related
"To answer the ﬁrst question, the following methods are compared with the GMRL. 1) MBRL, a standard model-based reinforcement learning which is similar to the MBPO [11] but uses a single model.",,,0,not_related
"1) MBRL, a standard modelbased reinforcement learning which is similar to the MBPO [11] but uses a single model.",,,0,not_related
"The experimental results show that the proposed method can achieve higher sample efficiency than previous works such as vanilla HER [14], MBPO [11] and MHER [18].",,,0,not_related
"In particular, we compare to the model-based approach MBPO [16] and the model-free approaches SAC [8] and PPO [9].",,,1,related
"The first source of error can arise if the model is used to simulate or ‘hallucinate’ trajectories for the system which are then added to the data set [16, 17, 18, 19].",,,0,not_related
"We will introduce a loss which learns an approximate model m̃, which can then be combined with the replay buffer D to use both experienced transitions and modelled transitions to learn π, as was done in e.g. Sutton (1991) or Janner et al. (2019).",,,1,related
"In addition, we use model-based RL strategies [43], [44], [61] to obtain the",,,1,related
"In the case of model-based RL, the stability issue with a large γ would be more obvious since there exist compounding model errors when a long rollout is sampled from the model [44], [60].",,,0,not_related
"In addition, various planning strategies can be used to derive a policy from the model [42], such as MPC [43] and policy optimization [44].",,,0,not_related
"1 Learned models are used to augment the reinforcement learning process in several ways [63, 9, 32, 28, 5], and depending on the way they are used and trained the behavior and guarantees of the RL agent change significantly.",,,0,not_related
"Note that this is a unique advantage of value-aware models, which is not true for observation-space or latent self-prediction losses such as those used in Dreamer [28] or MBPO [32].",,,0,not_related
"The resulting model errors can impact the learned policy [58, 35, 57, 65, 44, 32, 38] leading to worse performance.",,,0,not_related
"Among these, several focus on correcting models using information obtained during exploration [33, 65, 47, 56], or limiting interaction with wrong models [9, 32, 3].",,,0,not_related
This is as expected given that previous works have proved that the policy performance degradation is bounded by the difference in transition distributions between two systems [29].,,,0,not_related
"When the simulation dynamics are very close to the real-world dynamics, one can expect the trajectory rollouts in the simulator to be close to that in the real world as well; here, an optimal agent trained in the simulator would expect near-optimal performance in the real world [29].",,,0,not_related
"Moreover, such a model is not always available, and learning them is prone to errors that compound for longer horizons (Ross et al., 2011; Janner et al., 2019).",,,0,not_related
"[22] also proposes “irrecoverable state” with a similar meaning to dead-ends state, preventing dangerous situations from occurring through reward shaping and model-based rollout [23].",,,0,not_related
"We calculate the estimation error based on the difference between the Monte Carlo return value and the Q-estimates as in [33, 6, 14].",,,1,related
"We evaluate the policy return after each epoch by calculating the undiscounted sum of rewards when running the current learnt policy [6, 14].",,,1,related
"• Uncertainty Estimation [13], [40], [41]: This method is allowed to switch between conservative and naive offpolicy RL methods and conduct a proper estimation and usage of uncertainty.",,,0,not_related
The used model is an ensemble of 10 blackbox neural networks as [15] that jointly predicts the temperature transition and reward with a rollout length of 1.,,,0,not_related
■ BL4 (Model-based policy optimization) refers to a state-of-theart model-based method that uses short rollouts from the model to update the agent [15].,,,0,not_related
"In this example, the black-box model is an ensemble of black-box MLPs used in [15] trained with historical data.",,,0,not_related
"Though model-based methods have been applied to the humanoid task, prior works tend to keep the horizon intentionally short to prevent the accumulation of model errors (Janner et al., 2019; Amos et al., 2021).",,,0,not_related
"…been much work on learned latent spaces, with the usual spectrum ranging from those trained with reconstructive objectives (Hafner et al., 2021a; Janner et al., 2019) to those that contain only value-relevant information (Grimm et al., 2020) and options that interpolate between these two…",,,0,not_related
", 2017), MBPO (Janner et al., 2019), and MVE (Feinberg et al.",,,0,not_related
"There has been much work on learned latent spaces, with the usual spectrum ranging from those trained with reconstructive objectives (Hafner et al., 2021a; Janner et al., 2019) to those that contain only value-relevant information (Grimm et al.",,,0,not_related
"There is a natural tradeoff with γ-models: the higher γ is, the fewer model steps are needed to make long-horizon predictions, reducing model-based compounding prediction errors (Asadi et al., 2019; Janner et al., 2019).",,,0,not_related
"We compare this approach to the state-of-the-art in model-based and model-free methods, with representative algorithms consisting of SAC, PPO (Schulman et al., 2017), MBPO (Janner et al., 2019), and MVE (Feinberg et al., 2018).",,,1,related
"Empirically, dynamics models are found to be easier to train than value functions, allowing for better sample efficiency and generalization of learned models (Janner et al., 2019); this can be viewed as a consequence of either differences between the types of algorithms used to train value functions versus dynamics models (Kumar et al.",,,0,not_related
The rightmost plot depicts the value map produced by value iteration on a discretization of the same environment for reference. . . . . . . . . . . . . . . . . . . . . . . . 21 3.5 (γ-MVE control performance) Comparative performance of γ-MVE and four prior reinforcement learning algorithms on continuous control benchmark tasks. γMVE retains the asymptotic performance of SAC with sample-efficiency matching that of MBPO.,,,1,related
"…dynamics models are found to be easier to train than value functions, allowing for better sample efficiency and generalization of learned models (Janner et al., 2019); this can be viewed as a consequence of either differences between the types of algorithms used to train value functions versus…",,,0,not_related
"World Models Model-based RL algorithms use the experience gathered by an agent to learn a model of the environment [68, 10, 25].",,,0,not_related
"Our work focuses on training more robust world-models [18, 20, 21, 22, 25, 54, 62, 76] in the reward-free setting.",,,0,not_related
"Many recent MBRL methods (Janner et al., 2019; Clavera et al., 2020; Yu et al., 2020; Kidambi et al., 2020) employ the ensemble model used in PETS by default.",,,0,not_related
We also compare offline MBPO with HIPODE since it can also be seen as a method directly using dynamics-model-generated data as augmented data.,,,1,related
"The MB+αTD3BC results, together with those from MBPO, suggest that using dynamics-model-generated data as augmentation can harm the offline agent.",,,0,not_related
"Additionally, HIPODE is outperformed by vanilla model-based ORL methods (e.g., MBPO) on -random datasets because the value penalty is excessively strict on those datasets.",,,0,not_related
The difference between model based TD3BC and MBPO is that TD3BC has a behaviour cloning restrict on it’s critic [5] while MBPO [11] dose not.,,,0,not_related
"Indeed, many model-based RL algorithms explicitly rely on measures of model uncertainty, or alternatively on some measure of the distance to the previously observed training data, during policy synthesis [37, 38, 3, 39, 4, 40, 41, 42, 43, 44].",,,0,not_related
"Note that the proposed TDM is very different from the conventional dynamics models used in model-based RL (MBRL) methods [21, 50, 23, 42, 51].",,,0,not_related
"While these methods enables one-step forward prediction [38, 64, 103] and auto-regressive imaginary rollouts [18, 27, 68, 94, 113] that is a subset of the more general video prediction problem [14, 55, 63], model outputs usually degrade rapidly into the future [48, 115].",,,0,not_related
"Future work can improve sample efficiency through offline datasets [35], model-based reinforcement learning [36, 37], or better representation learning methods [38, 39].",,,0,not_related
"In contrast to previous methods that utilize SAC as policy optimization backbones [38, 46, 62, 39], our MB-BAC algorithm treats real and model-generated data differently.",,,0,not_related
"We evaluate the performance of MB-BAC, which integrates the BEE operator into the MBPO algorithm, against several model-based and model-free baselines.",,,1,related
"To facilitate a fair comparison, MB-BAC and MBPO are run with identical network architectures and training configurations as specified by MBRL-LIB.",,,0,not_related
"To ensure a fair comparison, we follow the same settings as our model-based baselines (MBPO [38], AutoMBPO [46], CMLO [39]), in which observations are truncated.",,,1,related
"As for model-based methods, we compare with four state-of-the-art model-based algorithms, MBPO [38], SLBO [52], CMLO [39], AutoMBPO [46].",,,1,related
"The implementation of SLBO is taken from an open-source MBRL benchmark [84], while MBPO is implemented based on the MBRL-LIB toolbox [65].",,,1,related
"Among the Dyna-style counterparts, MBPO [38], CMLO [39], and AutoMBPO [46] use SAC as the policy optimizer, while SLBO [52] employs TRPO [70].",,,0,not_related
"The practical implementation builds upon MBPO [38] by integrating the BAC as policy optimizer, with the pseudocode in Appendix B.",,,1,related
"Besides, in model-based RL, a series of works (Chua et al., 2018; Janner et al., 2019) adopt ensemble dynamic models for robust dynamics modeling.",,,0,not_related
"Before presenting the proof of Lemma A.3, we first introduce the assumption of concentration properties from (Auer et al., 2008; Kumar et al., 2020) and a modified lemma from (Janner et al., 2019).",,,1,related
"However, using single-step dynamics to autoregressively generate trajectories may suffer from the compounding rollout errors of long-term predictions (Janner et al., 2019), which would further lead to tremendous estimation error of value functions.",,,0,not_related
"Then we provide a lemma modified from Lemma B.3 in (Janner et al., 2019).",,,1,related
", 2020) and a modified lemma from (Janner et al., 2019).",,,1,related
"However, model-based methods encounter with model bias caused by the difference between the trained model and real environment [25], especially when the environment has high-dimensional states and complex dynamics.",,,0,not_related
"However, model-based methods are limited by model bias, as previous work [25] emphasized.",,,0,not_related
The hyper-parameters are kept the same with the MBPO baseline Janner et al. (2019) across all domains and are summarized as in Table 2.,,,1,related
"We compare MEX-MB with MBPO (Janner et al., 2019), where our method differs from MBPO only in the inclusion of the value gradient in (7.3) during model updates.",,,1,related
", 2018) and MBPO (Janner et al., 2019) to design practical versions of MEX in model-free and model-based fashion, respectively.",,,0,not_related
"We compare MEX-MB with MBPO (Janner et al., 2019), where our method differs from MBPO only in the inclusion of the value gradient in (7.",,,1,related
"Moving beyond theory and into practice, we adapt famous RL baselines TD3 (Fujimoto et al., 2018) and MBPO (Janner et al., 2019) to design practical versions of MEX in model-free and model-based fashion, respectively.",,,1,related
"Notably, in the sparse reward settings, MEX-MB excels at achieving the goal velocity and outperforms MBPO by a stable margin.",,,0,not_related
"Another line of research aiming at improving the sample efficiency in continuous control tasks sets their focus on learning a dynamics model of the environment [86, 9, 12, 44, 19, 57, 36, 97].",,,0,not_related
"This is similar in spirit to model-based methods (e.g., MBPO [44]) and REDQ [11] as they
usually employ a large update-to-data (UTD) ratio, i.e., update the critic multiple times by sampling with bootstrapping (the sampled batch is different each time).",,,0,not_related
"We note that 300K is a typical interaction step adopted widely in prior work [11, 44, 36] for examining sample efficiency.",,,1,related
"They achieve this by alleviating the overestimation bias in value estimate [29, 56, 50, 64], using high update-to-data (UTD) ratio [11, 41], adopting model-based methods [44, 51, 71, 102], etc.",,,1,related
"Though 300K or 500K (or even fewer) online interactions are widely adopted for examining sample efficiency in model-based methods [44, 71, 51, 101] and REDQ [11], one may wonder whether our method can consistently improve sample efficiency with longer online interactions.",,,0,not_related
The results even match the performance of MBPO [44] (around 73K).,,,0,not_related
"Inspired by [20], [10] and [25], we present an MBRL algorithm that uses an ensemble of probabilistic networks to learn a predictive model.",,,0,not_related
"Our proposal shares multiple aspects already mentioned in [20], [10]; we simplify these proposals in some aspects.",,,1,related
"For instance, the authors in [20] implemented branched rollouts with k-steps predictions.",,,0,not_related
"This could be a bottleneck deteriorating the performance of the policy, creating a limitation on MBRL methods to perform worse or converge to less optimal solutions than their model-free counterparts [48], [20].",,,0,not_related
"[27] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"The performance guarantee of a policy trained with imaginary transitions from an inaccurate dynamics model has been analyzed in prior Dynastyle [61, 62, 64] model-based RL algorithms [41, 27, 58].",,,0,not_related
"VGDF: We use a five-layer MLP with 200 units as the dynamics model using Swish activation following prior works [9, 27].",,,1,related
"However, the learned model can be inaccurate, which results in model exploitation and performance degradation [27, 28].",,,0,not_related
"The training of the dynamics model ensemble follows prior works [9, 27] with the MLE loss.",,,0,not_related
"Unlike prior works in model-based RL [27, 58] that utilize",,,0,not_related
"To provide rigorous interpretations for the results, we derive a performance guarantee for the dynamicsguided methods, which mainly build on the theories proposed in prior methods [27, 14].",,,1,related
"Reinforcement learning [Sutton and Barto, 2018] approaches are classified as model-free or model-based [Janner et al., 2019, Ha and Schmidhuber, 2018, Osband and Van Roy, 2014], dependent on if they attempt to explicitly try to learn the underlying transition dynamics an agent is subject to.",,,0,not_related
"[2013], including introductions of model-based variants [Janner et al., 2019].",,,0,not_related
"This was later extended to more tractable formulations and structured uncertainty sets in Tessler et al. [2019], Mankowitz et al. [2019], Pinto et al. [2017], Zhang et al. [2021], Tamar et al. [2013], including introductions of model-based variants [Janner et al., 2019].",,,0,not_related
(1) MBPO [19] is a modelbased RL method that learns a standard one-step dynamics model and uses actor-critic methods to plan in the model.,,,0,not_related
"Similar to MBPO, we pre-train dreamer’s dynamics branch with offline data before the online phase.",,,1,related
"We note that SF is performing reasonably well, likely because the method also reduces the compounding error compared to MBPO, and it has privileged information.",,,1,related
"Model-based RL arises as a natural fit for disentangling dynamics and rewards [33, 10, 17, 19, 20].",,,0,not_related
PETS adapts faster than MBPO but performs worse than our method as it suffers from compounding error.,,,0,not_related
"MBPO slowly catches up with our performance with more samples, since it still needs to learn the Q function from scratch even with the dynamics branch trained.",,,1,related
"11, when we curate the labeling process of the privileged dataset to satisfy the in-distribution assumption, CQL and SF receive a significant performance boost, while the performance
of our method and MBPO are unaffected as neither algorithm depends on the offline objectives.",,,1,related
MBPO adapts at a slower rate since higher dimensional observation and longer horizons increase the compounding error of model-based methods.,,,0,not_related
We pre-train the dynamics model for MBPO on the offline dataset.,,,1,related
We also would like to emphasize that the training time of our approach is much less than that of MBPO (4 hours v.s. 3 days).,,,1,related
", MBPO (Janner et al., 2019) on the phases criss-cross network environment (Fig.",,,0,not_related
"Dyna-type approaches have been successfully used in model-based online reinforcement learning for policy optimization, including several state-of-the-art algorithms, including ME-TRPO (Kurutach et al., 2018), SLBO (Luo et al., 2019), MB-MPO (Clavera et al., 2018), MBPO (Janner et al., 2019), MOPO (Yu et al., 2020).",,,0,not_related
"…have been successfully used in model-based online reinforcement learning for policy optimization, including several state-of-the-art algorithms, including ME-TRPO (Kurutach et al., 2018), SLBO (Luo et al., 2019), MB-MPO (Clavera et al., 2018), MBPO (Janner et al., 2019), MOPO (Yu et al., 2020).",,,0,not_related
"We also compare our algorithm SAC-ASG with state-of-art Dyna-type model-based approaches, i.e., MBPO (Janner et al., 2019) on the phases criss-cross network environment (Fig.",,,1,related
"We compare TOM within the widely used MBPO framework (Janner et al., 2019) to standard MLE model learning, and two representative recent approaches that target the MBRL objective mismatch problem.",,,1,related
"For example, in model-based RL [37,38], the state transition probability (and reward function) should be learned for planning the optimal action.",,,0,not_related
"Here, we consider the problem with the explicit definition of π as probability distribution, instead of Q-learning [36], which indirectly designs π fromQ(s, a), or model-based RL [37,38], which learns pe and obtains the optimal a through planning.",,,1,related
"This concept can be related to the recent strand of model-based policy optimization literature that simulates short trajectories in an estimated model of the considered domain to update the parameters of the policy (Janner et al., 2019; Nguyen et al., 2018; Bhatia et al., 2022).",,,0,not_related
"Forward dynamics models are an integral component of several model-based RL algorithms (Janner et al., 2019; Rajeswaran et al., 2020; Hafner et al., 2020).",,,0,not_related
"Because such a model can be used for planning (searching for a good policy without interacting with the environment), model-based methods have the potential to be substantially more sample efficient than model-free algorithms (Kaiser et al., 2019; Janner et al., 2019), which attempt to find good policies without building a model.",,,0,not_related
"…for planning (searching for a good policy without interacting with the environment), model-based methods have the potential to be substantially more sample efficient than model-free algorithms (Kaiser et al., 2019; Janner et al., 2019), which attempt to find good policies without building a model.",,,0,not_related
"This is an important development since planning has the potential to make model-based methods highly sample efficient (Kaiser et al., 2019; Janner et al., 2019).",,,0,not_related
"There are some typical methods such as Dyna [26], Model-based policy optimization (MBPO) [27], Model-based value expansion (MVE) [28], etc
Some efforts have been made to seek transition pathways via reinforcement learning in real-world applications.",,,0,not_related
"There are some typical methods such as Dyna [26], Model-based policy optimization (MBPO) [27], Model-based value expansion (MVE) [28], etc Some efforts have been made to seek transition pathways via reinforcement learning in real-world applications.",,,0,not_related
"However, most existing work either focuses on 2D environments [1, 4,12,19–24,28,30,32,43,48,57,59,63,64,66] or has to make strong assumptions about the accessible information of the underlying environment [2, 7, 26, 34, 35, 42, 46, 47, 53, 69] (e.",,,0,not_related
"This increase in interest can be attributed in part to exciting developments in MBRL [30, 31] and the superior sample complexity of modelbased approaches [28, 34].",,,0,not_related
"Building on these hypotheses, several mitigation strategies, such as model-based data augmentation (Janner et al., 2019), the use of ensembles (Chen et al., 2021), network regularizations (Hiraoka et al., 2021), and periodically reseting the RL agent from scratch while keeping the replay buffer…",,,0,not_related
"Devising such efficient RL algorithm has been an important thread of research in recent years (Janner et al., 2019; Chen et al., 2021; Hiraoka et al., 2021).",,,0,not_related
"However, when done naı̈vely, this can lead to worse performance (e.g., on DMC (Nikishin et al., 2022) and on MuJoCo gym (Janner et al., 2019)).",,,0,not_related
"Building on these hypotheses, several mitigation strategies, such as model-based data augmentation (Janner et al., 2019), the use of ensembles (Chen et al.",,,0,not_related
"MBRL algorithms (Sutton, 1991; Janner et al., 2019; Lee et al., 2020; Moerland et al., 2023) employ an explicit model trained to estimate the environment dynamics (i.e., state transition and reward functions) using self-supervised learning.",,,0,not_related
"MBRL algorithms (Sutton, 1991; Janner et al., 2019; Lee et al., 2020; Moerland et al., 2023) employ an explicit model trained to estimate the environment dynamics (i.",,,0,not_related
"Overall, we also observe that in easy tasks, it may be easier for MARL algorithms to learn from raw inputs rather than latent states generated by the model, which are subject to epistemic uncertainty (Janner et al., 2019).",,,1,related
It has been revealed that directly applying model-based online RL methods like MBPO [15] fails on offline datasets [44].,,,0,not_related
"However, MBPO fails to resolve the issue of extrapolation error in the offline setting.",,,0,not_related
"The practical implementation of TATU can be generally divided into three steps: Step 1: Training Dynamics Models: Following prior work [15], we train the dynamics model P̂ (·|s, a) with a neural network p̂ψ(s|s, a) parameterized by ψ that produces a Gaussian distribution over the next state, i.",,,1,related
MBPO improves the sample efficiency for online model-based RL by introducing the branch rollout method that queries the environmental dynamics model for short rollouts.,,,1,related
"2013) and MBPO (Janner et al. 2019) in the MuJoCo (Todorov, Erez, and Tassa 2012) benchmark, the overall improvement",,,0,not_related
"Nevertheless, compared to the performance of Dreamer V2 in Atari games (Bellemare et al. 2013) and MBPO (Janner et al. 2019) in the MuJoCo (Todorov, Erez, and Tassa 2012) benchmark, the overall improvement
of sample efficiency, as well as the asymptotic performances in some difficult tasks achieved by MAMBA are still relatively limited, which may be due to the high complexity of the dynamics of multi-agent systems.",,,0,not_related
It is worth noting that Theorem 1 is not simply a multiagent version of the results that have been derived in the single-agent setting (Luo et al. 2019; Janner et al. 2019).,,,1,related
"This work focuses on model learning and adopts the most common model usage, that is, generating pseudo samples to enrich the data buffer, so as to reduce the interaction with the environment and accelerate policy learning (Sutton 1990, 1991; Deisenroth et al. 2013; Kalweit and Boedecker 2017; Luo et al. 2019; Janner et al. 2019; Pan et al. 2020).",,,0,not_related
Most of previous works in MBRL train the model simply by minimizing each one-step prediction error for transitions available in the environment dataset (Kurutach et al. 2018; Chua et al. 2018; Janner et al. 2019).,,,0,not_related
", generating pseudo samples to enrich the dataset, so as to accelerate policy learning and reduce interactions with the true environment (Sutton 1991; Chua et al. 2018; Luo et al. 2019; Janner et al. 2019).",,,0,not_related
"[36] suffers from compounding errors as other learned single-step models do [20], but our non-parametric approach of composing previously seen transitions enables us to plan for longer horizons.",,,0,not_related
"This can be expected, as learned models suffer from compounding errors when rolled out (Janner et al., 2019) and prior methods that use MPC for object-centric methods only roll out for very short horizons (Veerapaneni et al., 2020).",,,0,not_related
"But planning with object-centric methods that do infer entities (Veerapaneni et al., 2020) is also not easy because the difficulties of long-horizon planning with learned parametric models (Janner et al., 2019) are exacerbated in combinatorial spaces.",,,0,not_related
"…observe that it is indeed difficult to perform shooting-based planning with an entity-centric world model trained to predict a single step forward (Janner et al., 2019): the MPC baseline performs poorly because its rollouts are
poor, and it is significantly more computationally expensive to run…",,,0,not_related
"Veerapaneni et al. (2020) also considers control tasks, but their shooting-based planning method suffers from compounding errors as other learned single-step models do (Janner et al., 2019), while our hierarchical non-parametric approach enables us to plan for longer horizons.",,,0,not_related
"Model-based learning methods from standard RL can be used to learn from demonstrations [150, 23].",,,0,not_related
IRL [67] 2008 Sensors GPS Data IRL N/A Matching Path Following Mobile Robot MBPO [150] 2019 N/A N/A Policy Learning (OPE) Regression Acc.,,,0,not_related
"A theoretical analysis is present in [150], where they formulate the bounds on the error between the learned policy and the policy in the data set, due to distributional shifts in the policy and model.",,,0,not_related
"Baselines: For all cases we compare with: model-base RL (MBPO (Janner et al., 2019)), model-free RL(SAC (Haarnoja et al.",,,1,related
"Baselines: For all cases we compare with: model-base RL (MBPO (Janner et al., 2019)), model-free RL(SAC (Haarnoja et al., 2018), PPO (Schulman et al., 2017)
and DDPG (Lillicrap et al., 2015)) and model predictive control (MPC).",,,1,related
"Modelbased methods, such as MBPO (Janner et al. 2019), are most suitable for such adaptations.",,,0,not_related
"We note the parallels between synthetic data generation and model-based reinforcement learning [34, 47, 75]; methods that generate synthetic samples by rolling out from observed states.",,,1,related
"Our algorithm builds on MBPO (Janner et al., 2019), which is a Dyna-style approach that learns policy with real data and simulated data.",,,1,related
"We choose MBPO, a widely-used MBRL algorithm with asymptotic performance rivaling the best modelfree algorithms, as the baseline.",,,1,related
"One widely used approach to modeling the transition is to directly minimize the distance between the predictions and the ground truth data, where the distance can be a mean square error or likelihood probability with various re-weighting methods (Janner et al., 2019; Yu et al., 2020).",,,0,not_related
"This model is then used for data generation (Sutton, 1990; Janner et al., 2019; Cowen-Rivers et al., 2022), planning (Chua et al., 2018; Hafner et al., 2019; Lutter et al., 2021a;b; Schneider et al., 2022) or stochastic optimization (Deisenroth & Rasmussen, 2011; Heess et al., 2015; Clavera et al.,…",,,0,not_related
"This model is then used for data generation (Sutton, 1990; Janner et al., 2019; Cowen-Rivers et al., 2022), planning (Chua et al.",,,0,not_related
"In practice, rollout horizons are often kept short to avoid significant compounding model error build-up (Janner et al., 2019).",,,0,not_related
"Recently, various improvements have been proposed to the original DynaQ algorithm, such as using ensemble neural network models and short rollout horizons (Janner et al., 2019; Lai et al., 2020), and improving the synthetic data generation with model predictive control (Morgan et al.",,,0,not_related
"Recently, various improvements have been proposed to the original DynaQ algorithm, such as using ensemble neural network models and short rollout horizons (Janner et al., 2019; Lai et al., 2020), and improving the synthetic data generation with model predictive control (Morgan et al., 2021).",,,0,not_related
The learned dynamics model is a reimplementation of the one introduced in Janner et al. (2019).,,,0,not_related
"In model-based RL, a model of the system dynamics is usually learned from data, which is subsequently used for planning (Chua et al., 2018; Hafner et al., 2019) or for policy learning (Sutton, 1990; Janner et al., 2019).",,,0,not_related
"[20, 21] use bootstrap ensembles of predictive models, which are able to capture aleatoric uncertainty and epistemic uncertainty.",,,0,not_related
"This is in accordance with many previous works (Thm. 1 in (Xu et al., 2019), Thm. 4.1 in (Janner et al., 2019) and Thm. 1 in (Schulman et al., 2015)), which include (1 − γ)2 in the denominator when it comes to differences of the cumulative return, given the difference in the action distribution.",,,0,not_related
"This is a well-studied phenomenon in model-based RL [e.g. see Janner et al., 2019].",,,0,not_related
"For instance, this is discussed extensively in the context of model-based RL in Janner et al. [2019]. Often, a discount factor is used when computing returns to alleviate these issues.",,,0,not_related
"Among the various model-based RL approaches [24], [25], [26], Dyna stands as a fundamental architecture which combines the model-free and model-based algorithms flexibly [27].",,,0,not_related
"training stability with a smaller gradient update number, which could attribute to that the model-generated data can effectively reduce the overfitting risk [26].",,,0,not_related
"These imaginary transitions can be used as extra training samples for TD methods (e.g. Sutton, 1990; Gu et al., 2016; Feinberg et al., 2018; Janner et al., 2019; D’Oro & Jaśkowski, 2020; Buckman et al., 2018).",,,0,not_related
"(2, top row) shows TaTD3 performs at least as well, if not batter, than the baseline algorithms in all four benchmark tasks: note the much poorer performance of MAGE on
Walker2d-v2, of MBPO on Humanoid-v2 relative to TaTD3.",,,0,not_related
"Dyna-TD3 is conceptually similar to MBPO, with the main difference of MBPO relying on SAC instead of TD3.",,,0,not_related
Plotted performance of MBPO was directly taken from the official algorithm repository on GitHub.,,,1,related
"The first model-based algorithm is Model-based Policy Optimization (MBPO) (Janner et al., 2019), which employs the soft actor-critic algorithm (SAC) (Haarnoja et al.",,,0,not_related
"…transitions can be used to provide better TD targets for existing data points (e.g. Feinberg et al., 2018) or to train the actor and/or critic by generating short-horizon trajectories starting at existing state-action pairs (e.g. Janner et al., 2019; Clavera et al., 2020; Buckman et al., 2018).",,,0,not_related
"The first model-based algorithm is Model-based Policy Optimization (MBPO) (Janner et al., 2019), which employs the soft actor-critic algorithm (SAC) (Haarnoja et al., 2018) within a model-based Dyna setting.",,,0,not_related
"These imaginary transitions can be used as extra training samples for TD methods (e.g. Sutton, 1990; Gu et al., 2016; Feinberg et al., 2018; Janner et al., 2019; D’Oro & Jaśkowski, 2020; Buckman et al., 2018).",,,0,not_related
"Lastly, we optimize πφ as in MBPO via SGD on the SAC policy loss, but also adding the uncertainty term from (9).",,,1,related
"The performance of deep MBRL algorithms was historically lower than that of model-free methods, but the gap has been closing in recent years (Janner et al., 2019).",,,0,not_related
We adopt as a baseline architecture MBPO by Janner et al. (2019) and the implementation from Pineda et al. (2021).,,,1,related
"The optimistic approach on top of MBPO (Janner et al., 2019) is presented in Algorithm 2.",,,1,related
The original MBPO only executes the former to fill up Dmodel.,,,1,related
"D.1 Implementation Details
The optimistic approach on top of MBPO (Janner et al., 2019) is presented in Algorithm 2.",,,1,related
The original MBPO trains Q-functions represented as neural networks via TD-learning on data generated via modelrandomized k-step rollouts from initial states that are sampled from Dt.,,,0,not_related
"We propose a new UBE and integrate it within a model-based soft actor-critic (Haarnoja et al., 2018) architecture similar to Janner et al. (2019); Froehlich et al. (2022).",,,1,related
"Algorithm 2 MBPO-style optimistic learning
1: Initialize policy πφ, predictive model pθ, critic ensemble {Qi}Ni=1, uncertainty net Uψ (optional), environment dataset Dt, model datasets Dmodel and { Dimodel }N i=1
.",,,1,related
MBPO maximizes the minimum of the twin critics (as in SAC).,,,0,not_related
Algorithm 1 requires a few modifications from the MBPO methodology.,,,1,related
MBPO trains twin critics (as in SAC) on mini-batches from Dmodel.,,,0,not_related
"…Lillicrap et al., 2015) and improves learning efficiency in high-dimensional continuous control tasks (Fujimoto et al., 2018; Haarnoja et al., 2018), it was later shown in Ha & Schmidhuber (2018); Janner et al. (2019) that model-based methods have much higher sample efficiency once properly tuned.",,,0,not_related
", 2020), or data augmentation (Fan et al., 2021; Janner et al., 2019; Hansen et al., 2021) have been adopted to account for overfitting.",,,0,not_related
"Several bounds have been introduced in MBPO (Janner et al., 2019) for the return bound analysis, which however are not sufficient in decentralized learning.",,,0,not_related
"Dyna-style methods (Sutton, 1990; Feinberg et al., 2018; Janner et al., 2019) use both data collected in the real environment and data generated by the learned model to update the policy.",,,0,not_related
"Moreover, when using the learned latent variable model to train an agent, we adopt k-step branched model rollout in MBPO (Janner et al., 2019) to avoid compounding model error due to long-horizon rollout.",,,1,related
"Popular off-policy temporal difference algorithms spanning both imitation learning [39, 59] and RL [27, 20, 28, 69, 34] exemplify this class.",,,0,not_related
"of the agent even after updating the behavior more than once, we collect a large number of artificial samples Nmodel in each iteration (in [24], for example, 400 model rollouts are performed for each sample of the environment).",,,1,related
Adopting the soft-actor critic [37] as our off-policy method (as in MBPO [24]) the samples our model synthesizes in each iteration can be kept in dataset Dmodel for a number of repetitions.,,,1,related
", [24]), we make use of the elite mechanism for the ensemble.",,,1,related
", [24]) are performed per environment interaction.",,,0,not_related
what extent capable function approximators can overcome modeling bias and how to avoid for errors to propagate into control strategies [21]–[24].,,,0,not_related
"today in state-of-the-art algorithms for model-based control [24], [29].",,,0,not_related
"We use short model-based rollouts of policy πω for artificial data collection early on, and increase their prediction horizon once more data is available [24].",,,1,related
"In offline RL, the model is often used to augment data (Yu et al., 2020; 2021) or act as a surrogate of real environment to interact with agent (Kidambi et al., 2020), which would easily introduce bootstrapped errors along the long horizon (Janner et al., 2019).",,,1,related
"Following previous work (Janner et al., 2019; Yu et al., 2020; 2021), we implement the probabilistic dynamics model using an ensemble of deep neural networks {pθ1, . . . , pθB}.",,,1,related
"Following previous work (Janner et al., 2019; Yu et al., 2020; 2021), we implement the probabilistic dynamics model using an ensemble of deep neural networks {pθ(1), .",,,1,related
", 2020), which would easily introduce bootstrapped errors along the long horizon (Janner et al., 2019).",,,0,not_related
"Thereafter it updates the policy with augmented data in the ensemble model, using TRPO model-free algorithm [64]; Model-Based Policy Optimization (MBPO) [81] samples the branched rollouts with the policy and the learned model, and utilizes SAC [70] to further learn the optimal policy with augmented data.",,,0,not_related
"Thereafter it updates the policy with augmented data in the ensemble model, using TRPO model-free algorithm [64]; Model-Based Policy Optimization (MBPO) [81] samples the branched rollouts with the policy and the learned model, and utilizes SAC [70] to further learn the optimal policy with",,,0,not_related
"Previous works generally try to resolve this by restricting model usage (Buckman et al., 2018; Janner et al., 2019) or better estimating uncertainty (Chua et al., 2018).",,,0,not_related
"Previous works generally try to resolve this by restricting model usage (Buckman et al., 2018; Janner et al., 2019) or better estimating uncertainty (Chua et al.",,,0,not_related
", 2018) or only using the model for short rollouts (Buckman et al., 2018; Janner et al., 2019).",,,0,not_related
"Erroneous predictive models can yield deleterious effects on policy learning, which is known as the model exploitation problem (Ross & Bagnell, 2012; Janner et al., 2019; Kidambi et al., 2020; Kang et al., 2022).",,,0,not_related
"MBRL Performance Bound
We first present the performance bound of a policy π in the original MDPM = (S,A, µ, p, r) and its model-based MDP (Janner et al., 2019).",,,1,related
"MBPO Training Procedure
MBPO (Janner et al., 2019) is a Dyna-style (Sutton, 1991) model-based RL algorithm, which trains a model-free RL method on top of truncated model-based rollouts starting from intermediate environment states.",,,0,not_related
"…et al., 2018; Hafner et al., 2019; Nagabandi et al., 2019), reinforcement learning (Heess et al., 2015; Feinberg et al., 2018; Buckman et al., 2018; Janner et al., 2019; Hafner et al., 2020; Nguyen et al., 2021), or both (Argenson & Dulac-Arnold, 2021; Sikchi et al., 2022; Hansen et al., 2022).",,,0,not_related
"We first present the performance bound of classic MBRL (Janner et al., 2019): Theorem 6.",,,1,related
"To avoid making suboptimal decisions based on incorrect models, prior works either restrict the horizon length of model rollouts (Janner et al., 2019) or employ various uncertainty estimation techniques, such as Gaussian processes (Rasmussen & Kuss, 2003; Deisenroth & Rasmussen, 2011) or model…",,,0,not_related
", 2019), reinforcement learning (Heess et al., 2015; Feinberg et al., 2018; Buckman et al., 2018; Janner et al., 2019; Hafner et al., 2020; Nguyen et al., 2021), or both (Argenson & Dulac-Arnold, 2021; Sikchi et al.",,,0,not_related
"Previous approaches often try to address model exploitation by estimating model uncertainty (Chua et al., 2018) or only using the model for short rollouts (Buckman et al., 2018; Janner et al., 2019).",,,0,not_related
"The first is based on MBPO (Janner et al., 2019), which we describe in Section 5.1.",,,1,related
"MBPO (Janner et al., 2019) is a Dyna-style model-based RL algorithm that trains a model-free RL method on top of truncated model-based rollouts starting from intermediate environment states.",,,0,not_related
"We first present the performance bound of classic MBRL (Janner et al., 2019):
Theorem 6.2.",,,1,related
"(38)
Equation (23) is the same bound as Lemma B.3 in Janner et al. (2019), but we use a milder assumption in Equation (22), which only assumes that the expectation (not the maximum) of the total variation distance between the policies is bounded.",,,1,related
"The first is based on MBPO (Janner et al., 2019), which we describe in Section 5.",,,1,related
"To avoid making suboptimal decisions based on incorrect models, prior works either restrict the horizon length of model rollouts (Janner et al., 2019) or employ various uncertainty estimation techniques, such as Gaussian processes (Rasmussen & Kuss, 2003; Deisenroth & Rasmussen, 2011) or model ensembles (Rajeswaran et al.",,,0,not_related
"The model is trained using negative log likelihood loss (Janner et al.,
2019): L(θk) = ∑N t=1[µθk(st, at) − st+1]
⊤Σ−1θk (st, at)[µθk(st, at) − st+1] + log detΣθk(st, at) During model rollouts, the probabilistic dynamics model ensemble first randomly selects a network from the ensemble and then…",,,1,related
"For the probabilistic dynamics model ensemble, we set the ensemble size to 7 which is the setting used in the original paper of MBPO (Janner et al., 2019).",,,1,related
"We first conduct an experiment on the MuJoCo Humanoid environment with MBPO (Janner et al., 2019), the SOTA model-based algorithm using Soft Actor-Critic (SAC) (Haarnoja et al.",,,1,related
"We first conduct an experiment on the MuJoCo Humanoid environment with MBPO (Janner et al., 2019), the SOTA model-based algorithm using Soft Actor-Critic (SAC) (Haarnoja et al., 2018) as the backbone algorithm for policy and value optimization.",,,1,related
"Among these approaches, the most popular and common approach is to use an ensemble of probabilistic dynamics models (Buckman et al., 2018; Janner et al., 2019; Lai et al., 2020; Clavera et al., 2020; Froehlich et al., 2022; Li et al., 2022).",,,0,not_related
"The model is trained using negative log likelihood loss (Janner et al., 2019): L(θk) = ∑N t=1[μθk(st, at) − st+1] ⊤Σ−1 θk (st, at)[μθk(st, at) − st+1] + log detΣθk(st, at) During model rollouts, the probabilistic dynamics model ensemble first randomly selects a network from the ensemble and then samples the next state from the predicted Gaussian distribution.",,,1,related
"We compare the two training mechanisms with MBPO (Janner et al., 2019) using an ensemble of probabilistic transition models.",,,1,related
"[23] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
[18] ∥ρ − ρ∥1 ≤ 2tD TV (π||β) ≤ 2t √ Dmax KL (π||β) (79) which can be adopted from [34] [24] [35] [23].,,,0,not_related
"Similar to previous work (Janner et al., 2019), we modelled state transition dynamics as a multivariate normal distribution with a diagonal covariance matrix, where the vector of means and log-standard deviations were outputted from a single feed-forward neural network with two hidden layers of 200 units each.",,,1,related
"While guarantees of many OPE methods require similar assumptions (Janner et al., 2019; Le et al., 2019; Xie et al., 2019), extending the MSBE to have better guarantees in the face of partial coverage (Uehara et al.",,,0,not_related
", 2021) and learned transition models (Chua et al., 2018; Kurutach et al., 2018; Janner et al., 2019; Rajeswaran et al., 2020; As et al., 2022), and has been applied to promote both exploration and safety.",,,0,not_related
"We compare CAROL with the following methods: (1) MBPO [17], our base RL algorithm.",,,1,related
We implement CAROL on top of the MBPO [17] model-based RL algorithm using the implementation from [27].,,,1,related
"During exploration, our algorithm learns a model of the environment using an existing model-based reinforcement learning algorithm [17].",,,1,related
"3: for N epochs do 4: Train model Eθ on Denv via maximum likelihood 5: Unroll M trajectories int he model under πψ; add to Dmodel 6: Take action in environment according to πψ; add to Denv 7: for G gradient updates do 8: Calculate normal policy loss L(πψ,Dmodel) as in MBPO [17] 9: Sample ⟨st, at, st+1, rt⟩ uniformly from Dmodel 10: Rollout π starting from st under Eθ for Ttrain steps and compute the total reward R 11: Compute the worst-case reward Rmin using Algorithm 2 over horizon Ttrain.",,,1,related
(2019) [41] proposed model-based Policy Optimization (MBPO) algorithm to boost PETS through monotonic improvement at each step to get the best performance.,,,0,not_related
"This may be explained by the relatively small scale of policy shift with respect to model error, as observed in [Janner et al., 2019].",,,0,not_related
"The model-based baselines include MBPO [Janner et al., 2019], which uses short-horizon rollouts branched from the state distribution of previous policies, SLBO [Luo et al.",,,0,not_related
"In Walker2d, the best rollout length is one step in our implementation and MBPO, thus resulting in similar performance between these two methods.",,,1,related
"In our experiments, we plug the model learning process of P2P into MBPO since it is a widely accepted strong baseline in MBRL.",,,1,related
"The model-based baselines include MBPO [Janner et al., 2019], which uses short-horizon rollouts branched from the state distribution of previous policies, SLBO [Luo et al., 2019], which enjoys theoretical performance guarantee and uses model rollouts from the initial state distribution, and STEVE [Buckman et al., 2018], which also generates short rollouts but uses model data for estimating target values instead of policy learning.",,,0,not_related
", on Prioritized Experience Replay buffers [19, 26, 39], which are a form of (limited) non-parametric models [45].",,,0,not_related
"Model based reinforcement learning (Clavera et al., 2020, 2018; Janner et al., 2019; Wang et al., 2019) builds an additional internal model of the environment to predict next states given the current state and action being taken.",,,0,not_related
"RAC-SAC RAC-TD3 REDQ MBPO TQC20 TD3 SAC TQC
Humanoid 11,107 ± 475 9,321 ± 1,126 5,504 ± 120 5,162 ± 350 7,053 ± 857 7,014 ± 643 7,681 ± 1,118 10,731 ± 1,296
Ant 6,283 ± 549 6,470 ± 165 5,475 ± 890 5,281 ± 699 4,722 ± 567 6,796 ± 277 6,433± 332 6,402± 1,371
Walker 5,860 ± 440 5,114 ± 489 5,034 ± 711 4,864 ± 488 5,109 ± 696 4,419 ± 1,682 5,249 ± 554 5,821 ± 457
Hopper 3,421 ± 483 3,495 ± 672 3,563 ± 94 3,280 ± 455 3,208 ± 538 3,433 ± 321 2,815 ± 585 3,011 ± 866
HalfCheetah 15,717 ± 1,063 15,083 ± 1,113 10,802 ± 1,179 13,477 ± 443 12,123 ± 2,600 14,462 ± 1,982 16,330 ± 323 17,245 ± 293
Swimmer 143 ± 6.8 71 ± 83 98 ± 31 - 143 ± 9.6 53 ± 8.8 51 ± 4.2 65 ± 5.8
Themaximum value for each task is bolded.",,,0,not_related
", 2021), MBPO (Janner et al., 2019), SAC (Haarnoja et al.",,,0,not_related
"For MBPO (https://github.com/JannerM/ mbpo), REDQ (https://github.com/watchernyu/REDQ), TD3 (https://github.com/sfujim/TD3), and TQC (https://github. com/SamsungLabs/tqc_pytorch), we use the authors’ code.",,,1,related
"RAC outperforms the current state-of-the-art algorithms (MBPO Janner et al., 2019, REDQ Chen et al., 2021, and TQC Kuznetsov et al., 2020), achieving state-of-the-art sample efficiency on the Humanoid benchmark.",,,0,not_related
"The baseline algorithms are REDQ (Chen et al., 2021), MBPO (Janner et al., 2019), SAC (Haarnoja et al., 2018), TD3 (Fujimoto et al., 2018), and TQC (Kuznetsov et al., 2020).",,,0,not_related
"RACSAC
REDQ MBPO TQC TQC20 REDQ/RACSAC MBPO/RACSAC TQC/RACSAC TQC20/RACSAC
Humanoid at 2,000 63 K 109 K 154 K 145 K 147 K 1.73 2.44 2.30 2.33
Humanoid at 5,000 134 K 250 K 295 K 445 K 258 K 1.87 2.20 3.32 1.93
Humanoid at 10,000 552 K - - 3,260 K - - - 5.91 -
Ant at 1,000 21 K 28 K 62 K 185 K 42 K 1.33 2.95 8.81 2.00
Ant at 3,000 56 K 56 K 152 K 940 K 79K 1.00 2.71 16.79 1.41
Ant at 6,000 248 K - - 3,055 K - - - 12.31 -
Walker at 1,000 27 K 42 K 54 K 110 K 50 K 1.56 2.00 4.07 1.85
Walker at 3,000 53 K 79 K 86 K 270 K 89K 1.49 1.62 10.75 1.68
Walker at 5,000 147 K 272 K - 960 K 270 K 1.85 - 6.53 1.84
Sample efficiency (Chen et al., 2021; Dorner, 2021) is measured by the ratio of the number of samples collected when RAC and some algorithms reach the specified performance.",,,1,related
"Results of MBPO are obtained at 3 × 105 time steps for Ant, Humanoid, and Walker2d, 4 × 105 for HalfCheetah and 1.25× 105 for Hopper.",,,1,related
"In fact, although model-based methods are data-efficient, they suffer from the compounding prediction error increasing with model rollout length, which greatly affects the performance and limits model rollout length [Janner et al., 2019].",,,0,not_related
"Popular model-based RL methods include Guestrin et al. (2002); Janner et al. (2019); Lai et al. (2020); Li et al. (2020), to name a few.",,,0,not_related
"In the field of deep reinforcement learning, new algorithms using various optimization theory, information theory, and control theory are being actively developed [41], [42], [43], [44], [45], [46], [47], [48], [49].",,,0,not_related
"Taking the cumulative reward subjected to a policy in the actual environment as η and its counterpart in the constructed virtual environment model as ηM, we can achieve the relationship between η and ηM within k iteration steps as [37]:",,,1,related
"Function approximation: Stochastic policies Focusing on the most recent works on model-based RL (Janner et al. 2019; Yu et al. 2020), stochastic Gaussian networks can be used as the function approximator to learn the policy f ≈ T , such that:",,,0,not_related
"Function approximation: Stochastic policies Focusing on the most recent works on model-based RL (Janner et al. 2019; Yu et al. 2020), stochastic Gaussian networks can be used as the function approximator to learn the policy f ≈ T , such that:
f = ∆ŝ
dt ∼ N
( µs,a, σ 2 s,a ) (9)
This allows to…",,,0,not_related
"For example, MBPO [35], MOPO [41] and MBOP [24] use a neural network that outputs the parameters of a Gaussian distribution, to predict the next state and reward.",,,0,not_related
"This modelling assumption is fairly common in applications involving continuous state spaces [22, 35, 40, 41].",,,0,not_related
"2 Model-based methods Model-based algorithms rely on an approximation of the environment’s dynamics [34, 35], that is probability distributions where the next state and reward are predicted from a current state and action.",,,0,not_related
"Model-based RL approaches typically use such dynamics’ models conditioned on the action as well as the state to make predictions [24, 35, 40, 41].",,,0,not_related
"Some model-based RL algorithms use the model just to generate additional data and update the policy using a model-free algorithm (Sutton, 1991; Janner et al., 2019).",,,0,not_related
"[9] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"Model-based offline RL methods [9, 11, 26, 27] train a model of the environment using state-action transitions from the logged data.",,,1,related
A recent comprehensive review of the model-based RL is presented by Janner et al. (2019).,,,0,not_related
"Furthermore, t ose methods reflect a higher sample efficiency, s r flected through empirical [62,63] and theoretical [64] studies.",,,0,not_related
"Many recent works focus on improving MBRL in line with the advancements in deep-RL [13], [14], [8], [15], [9].",,,0,not_related
"In [15], a theoretical analysis is formulated to guarantee a monotonic policy improvement in MBRL and demonstrates that a simple procedure of using short model-generated rollouts branched from real data could improve the performance of MBRL.",,,0,not_related
Dashed red line indicates the standard Q-values from running MBPO [32].,,,1,related
"MBPO performs standard off-policy RL using an augmented dataset D ∪ D̂, where D̂ is synthetic data generated by simulating short rollouts in the learnt model.",,,1,related
"Following previous works [34, 74, 75], our approach utilises model-based policy optimisation (MBPO) [32].",,,0,not_related
"However, sampling full length trajectories is not desirable in model-based methods due to compounding modelling error [32].",,,0,not_related
"Thus, naive policy optimisation on a learnt model in the offline setting can result in model exploitation [32, 40, 54], i.",,,1,related
"MOPO extends MBPO [13], which utilizes learned environment models to generate short roll-outs of length h that are employed to update the learning policy using the soft-actor critic (SAC) policy gradient algorithm [11, 12].",,,0,not_related
"Errors made by the learned environment models, such as those arising from poor generalization performance under distributional shifts, can result in model exploitation: the policy being trained learns to take advantage of model errors when optimizing the reward, leading to poor performance in the real environment [5, 20, 13, 7, 17].",,,0,not_related
"MOPO extends MBPO [12], which combines the soft-actor critic (SAC) policy gradient67 algorithm [10, 11] with using learned dynamics models to generate short roll-outs for training.68
3 Methods69 Our method stems from the observation that different demonstrators in an offline RL setting cor-70 respond to different domains in a domain generalization setting.",,,1,related
"For instance, learning to predict the continuous control parameters in dynamic models has been built on a Multivariate Gaussian Distribution (MGD) where the covariance matrix is diagonal [3].",,,0,not_related
"Model-based RL approaches typically use such dynamics’ models conditioned on the action as well as the state to make predictions [30, 63, 36, 2].",,,0,not_related
"In the online setting, they tend to improve sample efficiency [33, 30, 15, 7, 12].",,,0,not_related
"Modelling the environment dynamics as a Gaussian distribution is common for continuous state-space applications [30, 63, 36, 62].",,,0,not_related
"Model-based algorithms rely on an approximation of the environment’s dynamics [58, 30].",,,0,not_related
"[30] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"…in the model-based approaches to combat compounding error and model exploitation (Kurutach et al., 2018; Chua et al., 2018; Lai et al., 2020; Janner et al., 2019), in model-free to greatly increase sample efficiency (Chen et al., 2021; Hiraoka et al., 2021; Liang et al., 2022) and in…",,,0,not_related
"They are employed in the model-based approaches to combat compounding error and model exploitation (Kurutach et al., 2018; Chua et al., 2018; Lai et al., 2020; Janner et al., 2019), in model-free to greatly increase sample efficiency (Chen et al.",,,0,not_related
"Finally, the value-function effect of replacing P with P̃ , is an instance of sensitivity analysis for MDPs; see e.g. Mastin and Jaillet (2012); Ross et al. (2009) and, in model-based reinforcement learning, Janner et al. (2019); Sun et al. (2018).",,,1,related
"There is a large body of empirical research showing the potential of learned models to improve sample efficiency (Deisenroth & Rasmussen, 2011; Buckman et al., 2018; Kaiser et al., 2019; Janner et al., 2019; Curi et al., 2020; Hafner et al., 2021).",,,0,not_related
"There is a large body of empirical research showing the potential of learned models to improve sample efficiency (Deisenroth & Rasmussen, 2011; Buckman et al., 2018; Kaiser et al., 2019; Janner et al., 2019; Curi et al., 2020; Hafner et al., 2020).",,,0,not_related
"Comparisons to other methods such as MFRL, for instance adapting the methods of [6] to include demonstrations, could help establish how well our policies perform relative to policies trained with other methods.",,,0,not_related
"We choose this class of policy optimizer over others possible, such as model-free reinforcement learning (MFRL), primarily because MBRL has been shown by past studies to be more data-efficient than MFRL [30], [32]–[34].",,,1,related
"We are not the first to use RL [5], [16], [29], [30] nor RL+IL [1]–[3] for robot locomotion, and as such our methods use concepts from these prior methods.",,,1,related
"Our algorithm is inspired in particular by the algorithms of [5] and [30], and similarly iterate between collecting data, training the model, and training the policy.",,,1,related
Ensemble methods are widely used for better performance in RL [37]–[40].,,,0,not_related
"In model-based RL, PETS [39] and MBPO [40] use probabilistic ensembles to effectively model the dynamics of the environment.",,,0,not_related
", 2019; Ciosek & Whiteson, 2020); and achieving better quality gradients through simulating additional transitions via dynamics model in model-based SPG (MB-SPG) (Janner et al., 2019).",,,0,not_related
"…SPG algorithms: achieving better quality gradients through MA via Q-network (QMA) (Asadi et al., 2017; Petit et al., 2019; Ciosek & Whiteson, 2020); and achieving better quality gradients through simulating additional transitions via dynamics model in model-based SPG (MB-SPG) (Janner et al., 2019).",,,0,not_related
"From the perspective of model-based on-policy SPG, MBMA builds upon on-policy Model-Based Policy Optimization (MPBO) (Janner et al., 2019) but introduces the distinction between two roles for simulated transitions: whereas MBPO calculates gradient at simulated states, we propose to use information from the dynamics model by backpropagating from real states with simulated actions (i.",,,1,related
"By comparing MBPO-PPO and MBMA-PPO we compare variance reduction of many-actions (MBMA) as opposed to extending the trajectory length (MBPO) in the MB-SPG context and validate our theoretical contribution
2.",,,1,related
"Similarly, neither MBPO nor MBMA uses an ensemble of dynamics models (Buckman et al., 2018; Kurutach et al., 2018; Janner et al., 2019).",,,0,not_related
"MBPO PPO that leverages dynamics model to perform finite horizon rollouts branching from the on-policy data (Janner et al., 2019).",,,0,not_related
"From the perspective of model-based on-policy SPG, MBMA builds upon on-policy Model-Based Policy Optimization (MPBO) (Janner et al., 2019) but introduces the distinction between two roles for simulated transitions: whereas MBPO calculates gradient at simulated states, we propose to use information from the dynamics model by backpropagating from real states with simulated actions (i.e. simulating Q-values of those actions).",,,1,related
"From the perspective of model-based on-policy SPG, MBMA builds upon on-policy Model-Based Policy Optimization (MPBO) (Janner et al., 2019) but introduces the distinction between two roles for simulated transitions: whereas MBPO calculates gradient at simulated states, we propose to use information…",,,1,related
"MBMA yields a favorable bias/variance structure as compared to learning from states simulated in the dynamics model rollout (Janner et al., 2019; Kaiser et al., 2019; Hafner et al., 2019) in the context of onpolicy SPG.",,,0,not_related
"Given a fixed amount of interactions with the environment, our theoretical analysis is related to two notions in on-policy SPG algorithms: achieving better quality gradients through MA via Q-network (QMA) (Asadi et al., 2017; Petit et al., 2019; Ciosek & Whiteson, 2020); and achieving better quality gradients through simulating additional transitions via dynamics model in model-based SPG (MB-SPG) (Janner et al., 2019).",,,0,not_related
"In both simulated experiments, we compare against three baselines: PETS [20], SAC [39], and MBPO [10].",,,0,not_related
"We chose these three methods, because they are popular examples of each of the three main categories that most modern RL algorithms fall into: SAC is completely model-free, PETS is fully model-based and MBPO is a hybrid approach where the model is used to generate additional data for an underlying model-free agent.",,,1,related
"4: Cumulative reward in three different experiments for both versions of our agent (MI and LI) in comparison to three baselines: PETS [20], SAC [39], and MBPO [10].",,,0,not_related
"MoPAC [11] improved over MBPO by employing modelpredictive rollouts in the approximate MDP that is learned through the model, incentivizing the agent to explore areas of the state-space where model predictions are inefficient.",,,1,related
"Note that SAC and MBPO with their maximum entropy exploration strategy did not manage to learn the tasks, revealing that hard exploration problems require directed information seeking strategies, as our method does.",,,0,not_related
"Following methods attempt to couple model-free exploration with model learning, as in model-based policy optimization (MBPO) [10], but the purpose is to accelerate policy learning by utilizing model-based approximate samples.",,,0,not_related
"However, advantages of these methods compared to MBPO, MoPAC and other model-based methods without explicit exploration bonus [12] are not wellestablished.",,,0,not_related
"Following methods attempt to couple model-free exploration with model learning, as in model-based policy optimization (MBPO) [10], but",,,0,not_related
Training our dynamics models on existing state transitions is efficient and circumvents the challenges associated with learning dynamics in the context of a long-horizon task [53].,,,0,not_related
"Our work is also related to modelbased RL methods which jointly learn dynamics and reward models to guide planning [50–52], policy search [53, 54], or combine both [55, 56].",,,1,related
"Alongside a similar vein of research would be to explore the application of model-based RL approaches [56], [57] on real cells directly and infer the model “on-the-go” to apply RL on it.",,,0,not_related
"More recent works showed that it is possible to exploit expressive neural-networks models to learn complex dynamics in robotics systems [41], and use them for planning [17] or policy learning [30].",,,0,not_related
"[20] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"The model is “rolled out” to generate “imagined” trajectories, which are used either for direct planning [11, 8], or as training data for the agent’s policy and value functions [56, 20].",,,0,not_related
"If we restrict ourselves to states and actions in the empirical dataset (EMP) or short-horizon rollouts that start in the empirical state-action distribution (DYNA), as is typical in Dyna-style approaches [57, 20], we limit ourselves to a small neighborhood of the empirical stateaction distribution.",,,1,related
"By contrast, model-driven agent introduces an analytic environment model, which is then used for efficient simulation [24]–[26] or gradient computing by backpropogation through time [27]–[29].",,,0,not_related
"Another line of work focuses on RL with a learned model, which is promising for sample efficient learning [8, 10, 11, 12, 18, 27, 29, 48, 14].",,,0,not_related
"While many approaches rely on explicit epistemic uncertainty to tackle this issue (Chua et al., 2018; Janner et al., 2019), RSSMs succeed without capturing epistemic uncertainty.",,,0,not_related
"…needed for model-based RL.
Epistemic Uncertainty for Model-Based RL. Ample work emphasises the importance of modeling epistemic uncertainty for model-based RL (Deisenroth & Rasmussen, 2011; Chua et al., 2018; Janner et al., 2019) and several authors equipped RSSMs with epistemic uncertainty.",,,0,not_related
"Many modelbased RL approaches (Chua et al., 2018; Janner et al., 2019) handle this issue by explicitly modeling the epistemic uncertainty of the model, which is not required by the RSSM.",,,0,not_related
"Ample work emphasises the importance of modeling epistemic uncertainty for model-based RL (Deisenroth & Rasmussen, 2011; Chua et al., 2018; Janner et al., 2019) and several authors equipped RSSMs with epistemic uncertainty.",,,0,not_related
"[12]), with the exception of no environment interactions.",,,0,not_related
"For the model rollouts, we use the variance scaling action selection strategy for the first action only and use increasing rollout lengths for all domains, similar to [12].",,,1,related
"Consistently, we find that our policy enjoys higher performance, with an average return lead of about 1855.29 over MBPO in the first 300k steps.",,,1,related
"For practical implementation, the probabilistic models { ˆfφ1 , ˆfφ2 , . . . , ˆfφK} are fitted on shared but differently shuffled replay buffer De, and the target is to optimize the Negative Log Likelihood (NLL).",,,1,related
"(6) AutoMBPO [28], a variant of MBPO, that uses an automatic hyperparameter controller to tune the model-training frequency but suffers from high pre-training cost and lacks theoretical analysis on parameters rationality.",,,0,not_related
"To ensure a fair comparison, we run CMLO and MBPO with the same network architectures and training configurations based on MBRLLIB.",,,1,related
"Various attempts have been proposed to improve model accuracy by investigating high-capacity models (the model ensemble technique [27, 8] and better function approximators [15, 38]) or amending the policy optimization stage based on model bias [24, 39, 28, 20, 7, 57].",,,0,not_related
"Among then, we truncate some redundant observations for Hopper, Ant and Humanoid as our model-based baselines (MBPO[20], AutoMBPO[28]) do.",,,1,related
"The probabilistic models are fitted on shared but differently shuffled replay buffer De, and the target is to optimize the Negative Log Likelihood (NLL).",,,0,not_related
We find that CMLO achieves a more accurate model than the state-of-the-art baseline MBPO.,,,1,related
"Our method adopts an ensemble of probabilistic networks similarly as in [8, 20].",,,0,not_related
"The main difference from the general rollouts mechanism is that we restrict our rollouts to be generated from fresh models, rather than using outdated models to generate rollout data as MBPO [20] and AutoMBPO [28] do in their implementations.",,,1,related
"As for model-based methods, we compare with several algorithms including PETS [8], SLBO [34], MBPO [20] and AutoMBPO [28].",,,1,related
"In HalfCheetah, we find that our policy achieves higher coverage especially in first 4 stages than MBPO.",,,1,related
"While constructing such a bound for performance gap is straightforward, it has not been explored in previous MBRL theorectical analyses, instead they [49, 34, 20, 12, 28] turn to bound the discrepancy between returns under a model and those in the real environment.",,,0,not_related
"(5) MBPO [20], that employs a similar design of model ensemble technique (ensemble of probabilistic dynamics networks) and policy optimization oracle (SAC) as we do.",,,0,not_related
"Although there has been interest in non-decreasing performance guarantee, previous works [34, 20] commonly derive under a ""discrepancy bound"" scheme disregarding the model shifts (i.",,,0,not_related
"To reduce model bias, we chose to use NLL as a loss function in our implementation, which has been shown an effective way to learn model dynamics.",,,1,related
"And training objectives vary from Mean Square Error (MSE) [38, 34], Negative Log Likelihood (NLL) [8, 20], etc.",,,0,not_related
"Here, we present the numerical comparison to MBPO in Table 5.",,,1,related
"Besides, we notice that the performance is comparable to other MBRL baselines (MBPO etc.) when fixing our model training interval at 250.",,,1,related
"Although there has been recent interest in related subjects, most of the theoretical works seek to characterize the monotonicity in terms of a fixed model of interest [49, 34, 20, 12, 28], which does not naturally fit our case when the model is dynamically shifted.",,,0,not_related
"Model-Based Policy Optimization (MBPO) [17] uses short model-generated rollouts branched from real data to update the policy, and does
not consider safety constraints.",,,0,not_related
"Among all tasks, model-based algorithms (DRPO, MBPO and SMBPO) have fewer CTVs due to their high sample efficiency.",,,0,not_related
"Due to the probabilistic model (either posterior sampling in [19] or gaussian ensembles in [17]), the state trajectories vary a lot, leading to a distribution of Qh(s, a).",,,0,not_related
"Although [15], [17] proposed probabilistic model ensembles and clipped rollouts to cover the true dynamics within the support of the ensembles and mitigate the error, the stochasticity of the subsequent states s′ generated by the uncertain model P̂ will still lead to a deviated or even wrong estimation about the cost value or reachability certificate in safe RL if not addressed properly.",,,0,not_related
"Model-Based Policy Optimization (MBPO) [17] uses short model-generated rollouts branched from real data to update the policy, and does",,,0,not_related
Our DRPO relies on MBPO as well but leverages DRC and the shield policy to reduce violations.,,,1,related
"Model-based RL [1], [15], [17], [24] replaces the unknown transition dynamics with a learned model P̂ which is trained by minimizing E(s,a,s′)∼B[D(P, P̂ )], where D is a certain distance metric and B is either an offline dataset of stateaction pairs or a replay buffer storing historical interactions.",,,0,not_related
"1) Model Learning and Usage: Same as prior MBRL work [15], [17], [18], we adopt an ensemble of diagonal Gaussian dynamics model parameterized by φ as the world model approximators, denoted as {P̂φi}i=1, where P̂φi = N (μφi(s, a), σ(2) φi(s, a)).",,,1,related
This type of truncated rollout method leads to a smaller error in terms of the value function [17].,,,0,not_related
", performing policy updates with model-generated virtual data [1], [15]–[17].",,,0,not_related
Safe MBPO (SMBPO) [18] builds on MBPO and heavily penalizes unsafe trajectories to avoid safety violations.,,,0,not_related
"3 and Figure 4 of Kurutach et al. [2018]), but increase in number of models also leads to increase in space complexity.",,,0,not_related
"In order to tackle this problem, most of the model-based RL approaches [Deisenroth and Rasmussen, 2011, Kurutach et al., 2018, Janner et al., 2019] use shorter (or truncated) horizon during the policy optimization phase and achieve similar performance as Model-Free RL approaches.",,,0,not_related
"For both experience thresholds, SSPG obtains the best average performance in 5/6 tasks, and still lags very close the model-based MBPO [61] algorithm for the remaining task (HalfCheetah-v2).",,,0,not_related
We consider REDQ [60] and MBPO [61] for state-of-the-art algorithms based on the traditional model-free and model-based RL frameworks.,,,1,related
"Following REDQ [60] and MBPO [61], we employ a critic ensemble of 10 models and use the suggested task-specific target entropy values for automatic tuning of the MaxEnt coefficient, α [39].",,,1,related
"SSPG converges much earlier than other algorithms, even while performing many less optimization steps (REDQ, REDQ-FLOW, MBPO, and SAC-20 all employ a UTD of 20, while we train SSPG with a UTD of 10, see Section 4).",,,1,related
"[43] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"However, directly optimizing policy based on an offline learned model is vulnerable to model exploitation [22, 43].",,,0,not_related
"(c) To alleviate the compounding errors (Janner et al., 2019), MuZero Unplugged unrolls the dynamics for multiple steps (5) and learns the policy, value, and reward predictions on the recurrently imagined latent state to match the real trajectory’s improvement targets.",,,1,related
Most of the existing offline MBRL works focus on policy learning under a given model trained by MLE.,,,0,not_related
(11); train P̂ and r̂ by weighted MLE (Eq.,,,1,related
"In online (off-policy) MBRL, Lambert et al. [23] identify the mismatched objectives between the MLE model-training and the model’s usage of improving the control performance.",,,1,related
"Most of the prior offline model-based RL (MBRL) methods [e.g., 16, 18–21], however, first pretrain a one-step forward dynamic model via maximum likelihood estimation (MLE) on the offline dataset, and then use the learned model to train the policy, without further improving the dynamic model during the policy learning process.",,,0,not_related
"With the offline dataset Denv, P̂ is trained via the MLE [15, 16, 18] as
arg maxP̂∈P E(s,a,s′)∼Denv [ log P̂ (s′ | s, a) ] .",,,1,related
The reward function r̂ is still estimated by the weighted-MLE objective.,,,1,related
"This lower bound, leading to a weighted MLE objective for the dynamic-model training, is relaxed to a tractable regularized objective for the policy learning.",,,0,not_related
"As in prior work using Gaussian probabilistic ensemble on model-based RL [83, 15, 16, 21, 18], we use a double-head architecture for our dynamic model, where the two output heads represent the mean and log-standard-deviation of the normal distribution of the predicted output, respectively.",,,1,related
"We initialize the dynamic model by standard MLE training, and periodically update the model by minimizing Eq.",,,1,related
"To verify the effectiveness of our MIW-weighted model (re)training scheme, we compare our AMPL with its variant of training the model only at the beginning using MLE, i.e., No Weights (dubbed as NW).",,,1,related
"Thus, given the MIW ω, we can optimize P̂ by minimizing the following loss
`(P̂ ) , −E(s,a,s′)∼dP∗πb,γ [ ω(s, a) log P̂ (s′ | s, a) ] , (6)
which is an MLE objective weighted by ω(s, a).",,,1,related
"[15] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"Most similar to our work, Hishinuma and Senda [47] also use a MIW-weighted MLE objective for model training.",,,1,related
"As a result, the objective function used for model training (e.g., MLE) and the objective of model utilization are unrelated with each other.",,,0,not_related
"Initialize: Dynamic model P̂ and r̂, policy πφ, critics Qθ1 and Qθ2 , discriminator Dψ , MIW ω. Initialize P̂ and r̂ via the MLE (Eq.",,,1,related
"Rather than using a fixed MLE-trained model, we derive an objective that trains both the policy and the dynamic model toward maximizing a lower bound of true expected return (simultaneously minimizing the policy evaluation error |J(π, P ∗)− J(π, P̂ )|).",,,1,related
"We follow the literature [83, 15, 16, 47] to assume no prior knowledge about the reward function and thus use neural network to approximate transition dynamic and the reward function.",,,1,related
The model is initialized by the MLE loss.,,,1,related
"With the offline dataset Denv, P̂ is trained via the MLE [15, 16, 18] as arg maxP̂∈P E(s,a,s′)∼Denv [ log P̂ (s′ | s, a) ] .",,,1,related
"The differentiable world model bridges the gap between the simulation and control policy, allowing the objectives defined in the motion domain to supervise the policy directly, thus achieving efficient and stable training [Deisenroth and Rasmussen 2011; Janner et al. 2021].",,,0,not_related
"…to pass through the barrier of the simulation [Chiappa et al. 2017; Heess et al. 2015; Schmidhuber 1990], thus enabling policy optimization to be solved efficiently using gradient-based techniques [Deisenroth and Rasmussen 2011; Heess et al. 2015; Janner et al. 2021; Nagabandi et al. 2018].",,,0,not_related
"Approximate models, or the World Models [Ha and Schmidhuber 2018], are typically formulated as Gaussian Process [Deisenroth and Rasmussen 2011] or neural networks [Janner et al. 2021; Nagabandi et al. 2018].",,,0,not_related
"Typically, MB algorithms — e.g., MOPO (Yu et al., 2020), MOReL (Kidambi et al., 2020), and COMBO (Yu et al., 2021) — adopt the Dyna-style policy optimization approach developed in online RL (Janner et al., 2019; Sutton, 1990).",,,0,not_related
"Assume we have a bootstrapped dynamics ensemble model f̂ consisting of K different models (f̂1, . . . , f̂K) trained with different sequences of mini-batches of D (Chua et al., 2018; Janner et al., 2019).",,,1,related
"Model-based value expansion Unlike Dyna-style methods that augment the dataset with modelgenerated rollouts (Sutton, 1990; Janner et al., 2019), MVE (Feinberg et al., 2018) uses them for better estimating TD targets during policy evaluation.",,,0,not_related
"The approach for training the dynamics ensemble closely follows previous work on Bayesian ensemble estimation (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
", f̂K) trained with different sequences of mini-batches of D (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"We follow the common configurations used in the literature, e.g., MBPO (Janner et al., 2019) and MOPO (Yu et al., 2020).",,,1,related
"Model-based value expansion Unlike Dyna-style methods that augment the dataset with modelgenerated rollouts (Sutton, 1990; Janner et al., 2019), MVE (Feinberg et al.",,,0,not_related
"These methods follow the Dyna-style policy learning where model rollouts are used to augment the offline dataset (Sutton, 1990; Janner et al., 2019).",,,0,not_related
", 2021) — adopt the Dyna-style policy optimization approach developed in online RL (Janner et al., 2019; Sutton, 1990).",,,0,not_related
"[32] use an actor-critic method trained via rollouts in the model alongside the data collected to find a policy, and Chua et al.",,,0,not_related
"Recent work indicates that state-of-the-art models suffer from sever policy drift after a few predictions [2, 8, 10], and CostNet is no exception.",,,0,not_related
"In [10], the authors analyze previous methods and their capability to generalize well for longer time horizons.",,,0,not_related
"Model-based reinforcement learning (MBRL) leverages a learned dynamic model of the environment to plan a sequence of actions in advance which augment the data (Sutton, 1991; Janner et al., 2019; Pan et al., 2020; Mu et al., 2020; Peng et al., 2021) or obtain the desired behavior through planning (Chua et al.",,,0,not_related
"…learning (MBRL) leverages a learned dynamic model of the environment to plan a sequence of actions in advance which augment the data (Sutton, 1991; Janner et al., 2019; Pan et al., 2020; Mu et al., 2020; Peng et al., 2021) or obtain the desired behavior through planning (Chua et al., 2018; Hafner…",,,0,not_related
"In state-based RL, model-based algorithms [2, 1, 28, 16] which learn a dynamics model from the pre-recorded dataset and augment the dataset with generated state transitions have emerged as a promising paradigm.",,,0,not_related
"Our learning algorithm is based on MBPO (Janner et al., 2019) using Soft Actor-Critic (Haarnoja et al., 2018a) as the underlying model-free learning algorithm.",,,1,related
"To do this, we build on top of model-based policy optimization (MBPO) (Janner et al., 2019).",,,1,related
"As mentioned in Section 6, our tool is built on top of MBPO (Janner et al., 2019) using SAC (Haarnoja et al., 2018a) as the underlying learning algorithm.",,,1,related
"Our learning algorithm is based on MBPO (Janner et al., 2019) using Soft Actor-Critic (Haarnoja et al.",,,1,related
"(Janner et al. (2019), Lemma B.3) Let the expected KL-divergence between two transition distributions be bounded by maxt Ex∼pt1(x)DKL(p1(x
′u | x)‖p2(x′,u | x)) ≤ m and maxxDTV (π1(u | x)‖π2(u | x)) < π .",,,1,related
"Since the method proposed in this article can be used in conjunction with the system identification methods, our method in principle can be used as a model-based policy optimization method, which is similar in spirit to the modelbased RL approaches [7].",,,1,related
We then analyze the monotonic improvement for the joint policy under the world model based on Janner et al. (2019).,,,1,related
"See (Janner et al., 2019) (Lemma B.2).",,,0,not_related
"See (Janner et al., 2019) (Lemma B.1).",,,0,not_related
"Dynamics models combined with powerful search methods have led to impressive results on a wide variety of tasks such as Atari (Schrittwieser et al., 2020) and continuous control (Hafner et al., 2019a; Janner et al., 2019; Sikchi et al., 2021; Lowrey et al., 2018).",,,0,not_related
"There has been recent work focused on unifying modelfree and model-based approaches (Janner et al., 2019; Du and Narasimhan, 2019).",,,0,not_related
"4.2 Practical Algorithm
Algorithm 1 Practical CDPO Algorithm Input: Prior ϕ, model-based policy optimization solver MBPO(π, f,J ).",,,0,not_related
"For instance, Dyna agents [61, 20, 17] optimize policies using model-free learners with model-generated data.",,,0,not_related
"1: for iteration t = 1, ..., T do 2: qt ← MBPO(·, f̂LSt , (4.1)) 3: Sample N models {ft,n}Nn=1 4: πt ← MBPO(qt, {ft,n}Nn=1, (4.2)) 5: Execute πt in the real MDP 6: UpdateHt+1 = Ht ∪ {sh,t, ah,t, sh+1,t}h 7: Update f̂LSt+1 and ϕ 8: end for 9: return policy πT
The pseudocode of CDPO is in Alg.",,,1,related
"It will also be interesting to explore different choices of the MBPO solvers, which we would like to leave as future work.",,,1,related
"We also examine a broader range of MBRL algorithms, including MBPO [20], SLBO [35], and ME-TRPO [30].",,,1,related
"The model-based solver MBPO(π, f,J ) outputs the policy (qt or πt) that optimizes the objective J with access to model f .",,,1,related
Ablation on different choices of MBPO solver (Dyna and POPLIN-P [63]) shows the generalizability of CDPO.,,,1,related
*Equal contribution model for the MuJoCo Humanoid task [11].,,,1,related
"For example, MBPO [12] proposes the branched rollouts scheme with a gradually growing branch length to truncate imaginary model rollouts, avoiding the participation of unreliable fake samples in policy optimization.",,,0,not_related
"1 Model Learning Like MBPO [12], our dynamics model is an ensemble neural network that takes state-action pair as input and outputs Gaussian distribution of the next state and reward.",,,0,not_related
"For model-based methods, we compare MPPVE with MBPO [12], the most representative model-based method so far, and BMPO [15], which proposes a bidirectional dynamics model to generate model data with less compounding error than MBPO.",,,1,related
"We also utilize model rollouts to generate fake transitions, as proposed by MBPO [12].",,,1,related
"Moreover, MBPO [12] builds on SAC [11], which is an off-policy RL algorithm, and updates the policy with a mixture of the data from the real environment and imaginary branched rollouts.",,,0,not_related
"Some model-based methods estimate the model’s uncertainty and penalize the actions whose consequences are highly uncertain (Janner et al., 2019; Kidambi et al., 2020).",,,0,not_related
"One class trains “on-policy” model-free algorithms virtually inside the environment model [Kurutach et al., 2018, Luo et al., 2019] while the other trains “off-policy” model-free algorithms virtually [Janner et al., 2019].",,,0,not_related
", 2019] while the other trains “off-policy” model-free algorithms virtually [Janner et al., 2019].",,,0,not_related
", model-based policy optimization (MBPO) [18] and masked model-based actor-critic (M2AC) [19], where the agents are trained with samples drawn from the approximated environment model.",,,0,not_related
"CRPM can directly serve as approximated environment model as in modern model-based reinforcement learning algorithms, e.g., model-based policy optimization (MBPO) [18] and masked model-based actor-critic (M2AC) [19], where the agents are trained with samples drawn from the approximated environment model.",,,0,not_related
"For example, model-based RL algorithms generate extra experiences by interacting with a learned model [18], [19].",,,0,not_related
MBPO [13] generates truncated model rollouts branched from real states to cripple the influence of model error and provides the condition for the return improvement in the true dynamics.,,,1,related
"According to the definition of the return in terms of the occupancy measure and the total variation distance, the return
discrepancy bound can be derived as:
|η[π]− η[πe]| = | ∑ s,a kb∑ t=0 γtr(s, a)(pt(s, a)− pet (s, a))|
≤ 2rmax ∑ s,a kb∑ t=0 γt 1 2 |pt(s, a)− pet (s, a)|
≤ 2rmax kb∑ t=0 γtDTV (pt(s, a)||pet (s, a)),
(18) Next, according to Lemma B.1 in MBPO [13], we convert
joint distribution to marginal distribution, thus we have:
DTV (pt(s, a)||pet (s, a)) ≤ DTV (pt(s)||pet (s))+ max t Es∼pt(s)[DTV (πt(a|s)||πet (a|s))]
(19)
Then, let ξt = DTV (pt(s)||pet (s)), and inspired by Lemma B.1 in BMPO [6], we have:
ξt ≤E(s′,a)∼pt+1(s′,a)[DTV (p(s|s′, a)||pe(s|s′, a))] +DTV (pt+1(s ′, a)||pet+1(s′, a)) (20)
Here, we make the assumption without loss of generality:
m ≥ max t E(s′,a)∼pt(s′,a)[DTV (p(s|s′, a)||pe(s|s′, a))]
(21) After that, we can iteratively do the above decomposition to obtain:
ξt ≤ ( m + π) + ξt+1 ≤ ( m + π)(kb − t) + ξkb ≤ kb( m + π) (22)
where ξkb = 0, because the real states sampled from D b hs are used as the starting states for the backward rollouts.",,,1,related
Other hyperparameters not listed here are the same as those in MBPO [6].,,,0,not_related
"To evaluate the importance of BI, we compare the performance among three models: 1)
MBPO that optimizes the policy on forward rollout samples via RL algorithm (Baseline); 2) BMPO that adds the backward rollout samples to the baseline model (Baseline+BR); 3) the model that employs backward imitation (Baseline+BI).",,,1,related
The strategy of increasing rollout length linearly has shown the effectiveness in MBPO and BMPO.,,,0,not_related
"On the one hand, prior works [13], [6] have studied the discrepancy between the expected return in the actual environment and that in the branched rollouts, which is applicable for the analysis on the forward rollout return in BIFRL.",,,0,not_related
"To be specific, for model-based methods, we compare against MBPO [13] that is the backbone model of our method, and BMPO [6] that treats the samples from backward rollouts in the same way as those from forward rollouts.",,,1,related
"Besides, BIFRL is built on the Model-based Policy Optimization (MBPO) [13] algorithm that is one of stateof-the-art Dyna-style model-based methods and investigates forward truncated model rollouts.",,,0,not_related
"More qualitative results are shown in our supplementary materials, which provides the qualitative comparisons among BIFRL, BMPO, MBPO and SAC.",,,1,related
"1 in MBPO [13], we convert joint distribution to marginal distribution, thus we have:",,,1,related
"We observe that BMPO [6] ignores the variation distance between the forward policy and the backward policy (corresponding to π in BIFRL), and consequently set kb = kf to obtain the tighter return discrepancy bound, compared to MBPO [13].",,,1,related
"In practice, we use kb = 23kf in most domains except Walker2D and Walker2D-NT where kb is set to the same as kf due to kf = 1 in the original MBPO paper.",,,1,related
"However, Effective MBRL may be cumbersome because the ease of data generation must be weighed against the bias of model-generated data [40].",,,0,not_related
"For modelbased methods, we choose MBPO (Janner et al., 2019), AMPO (Shen et al., 2020), and VaGraM (Voelcker et al., 2022).",,,1,related
"For Humanoid, we use the modified version introduced by MBPO (Janner et al., 2019).",,,1,related
"We conduct an experiment using a state-of-the-art modelbased RL method called MBPO (Janner et al., 2019) on four MuJoCo (Todorov et al.",,,1,related
"For better model usage, Janner et al. (2019) proved that short model rollouts could avoid the model error and improve the quality of model samples.",,,0,not_related
"Most of the model-based RL algorithms first use supervised learning techniques to learn a dynamics model based on the samples obtained from the real environment, and then use this learned dynamics model to generate massive samples to derive a policy (Luo et al., 2018; Janner et al., 2019).",,,0,not_related
"As introduced in MBPO (Janner et al., 2019), the rollout horizon should start at a short horizon and increase linearly with the interaction epoch.",,,0,not_related
"We conduct an experiment using a state-of-the-art modelbased RL method called MBPO (Janner et al., 2019) on four MuJoCo (Todorov et al., 2012) environments HalfCheetah, Hopper, Walker2d, and Ant.",,,1,related
"For modelbased methods, we choose MBPO (Janner et al., 2019), AMPO (Shen et al.",,,1,related
", model-based RL [9, 10], which generates extra data through interacting with a learned model.",,,0,not_related
"Dyna-style algorithms learn world models by interacting with the real environment, which is the most investigated category of model-based algorithms, and its typical algorithms include Deep Dyna-Q, SLBO, MBPO, LatCo and Dreamer,etc [3] [24] [25] [26] [27].",,,0,not_related
"By learning a forward dynamics model to approximate the state transition dynamics of the environment, model-based reinforcement learning (MBRL) methods can achieve better sampling efficiency than model-free reinforcement learning
(MFRL) methods.",,,0,not_related
"The learned dynamics model can be used as a simulator of MFRL [12], [13], provide priors for the algorithm [14], [15], or be utilized to predict future trajectory in an MPC fashion [16], [17].",,,0,not_related
", deterministic models [1], [6], stochastic models [7], and ensemble models [8].",,,0,not_related
"However, due to the accumulation of prediction error at each 70 step [5] and the large search space for long-horizon planning, the search for an optimal plan on 71 long-horizon tasks using model-based RL is inaccurate and computationally expensive [6, 7, 8].",,,0,not_related
"However, such model-based RL approaches have 32 shown only limited success in long-horizon tasks due to low long-horizon prediction accuracy [5] 33 and computationally expensive search [6, 7, 8].",,,0,not_related
"While the success of model-based RL (MB-RL) has been witnessed in many single agent RL tasks [10], [11], [12], [13], the understanding of its MARL counterpart is still limited.",,,0,not_related
"To reduce the negative effect of model error, we adopt a branched rollout scheme proposed in [12], [18].",,,1,related
"Due to low data efficiency, model-based methods are widely studied as a promising approach for improving sample efficiency [24], [8], [11], [12].",,,0,not_related
"To alleviate the issue of compounding model error, we adopt a branching strategy [18], [12] by replacing few long-horizon rollouts with many short-horizon rollouts to reduce compounding error in model-generated rollouts.",,,1,related
"Besides, the computational cost is a critical factor when applying Adv USR to more complex dynamics with millions of parameters (common in current model-based RL research (Janner et al., 2021)).",,,0,not_related
"We predict the difference of states rather than the next states as it has been shown in past studies [21, 22] to yield better dynamics predictions.",,,0,not_related
"The sample efficiency requirement for offline IL methods reminds us of the success of model-based approaches in the online and offline RL domains [21, 22, 23, 24, 25].",,,0,not_related
"Model-based offline RL, in part, has shown to better generalize to out-ofdistribution states because the agent’s internal world model allows for offline exploration, branched from real data (Janner et al. 2019).",,,1,related
"Model-based reinforcement learning (RL) holds the promise of sample-efficient robot learning by learning a world model and leveraging it for planning [1, 2, 3] or generating imaginary states for behavior learning [4, 5].",,,0,not_related
", 2018), providing synthetic data (Kurutach et al., 2018; Janner et al., 2019), or improving Q-value estimates (Feinberg et al.",,,0,not_related
"Then the dynamics model and reward predictor are used for planning (Williams et al., 2017; Chua et al., 2018; Nagabandi et al., 2018), providing synthetic data (Kurutach et al., 2018; Janner et al., 2019), or improving Q-value estimates (Feinberg et al., 2018; Amos et al., 2021).",,,0,not_related
"Second, in contrast with model-based dynamics (Janner et al., 2019) and curiosity (Pathak et al.",,,0,not_related
"Second, in contrast with model-based dynamics (Janner et al., 2019) and curiosity (Pathak et al., 2017) approaches, which require either the next state or the next action to compute the intrinsic signal, we need only the current state-action pair to obtain a plausible action direction.",,,0,not_related
"The learned model could also be used to extract a task policy later using model-based policy optimization [50] or to perform offline RL on the data generated during free play, which we demonstrate in Sec.",,,1,related
"To solve this problem some researchers use RNN to encode the interactions for constructing the state in the model-based RL method [2, 10, 19, 54, 60].",,,0,not_related
"Theorem 7 (Theorem 4.3 in [Janner et al., 2019]).",,,0,not_related
"The coefficient means a quadratic compounding error of learning in the model, which is the reason that studies such as [Janner et al., 2019] only adopt short rollouts, say, less than 10 steps, in the learned model.",,,0,not_related
MBPO also gives a monotonic improvement theorem with model bias and k-branch rollouts.,,,0,not_related
"With a sufficiently accurate model, it is intuitive that MBRL yields higher sample efficiency than MFRL, as shown in recent studies from both theoretical [Sun et al., 2019] and empirical [Janner et al., 2019, Wang et al., 2019] perspectives.",,,0,not_related
"A straightforward approach to model learning fits one-step transitions, which has been widely employed [Kurutach et al., 2018a, Feinberg et al., 2018, Luo et al., 2018, Janner et al., 2019, Rajeswaran et al., 2020].",,,0,not_related
"On the contrary, the performance of MBPO drops rapidly as the rollouts become longer.",,,0,not_related
"Motivated by such an analysis, they proposed the AutoMBPO framework to automatically schedule the key hyperparameters of the MBPO [Janner et al., 2019] algorithm.",,,0,not_related
"(5)
The probabilistic transition model is often instantiated as a Gaussian distribution [Chua et al., 2018, Luo et al., 2018, Janner et al., 2019], i.e., Mθ(·|s, a) = N (µθ(s, a),Σθ(s, a)) with parameterized models of µθ and Σθ.",,,0,not_related
"In many cases, the reward function is also explicitly defined, thus the major task of the model learning is to learn the state transition dynamics [Luo et al., 2018, Janner et al., 2019].",,,0,not_related
"Note that the bound of MBPO with the same rollout length to BMPO is C( m, π, m′ , k1 + k2).",,,1,related
MBPO begins a rollout from a state sampled in the real environment and runs k steps according to policy π and the learned model pθ.,,,1,related
"Typically, MBPO uses a short rollout length to avoid large compounding errors, which may limit the model usage.",,,0,not_related
"Model-based policy optimization (MBPO) [Janner et al., 2019], on the other hand, samples the branched rollout in the model.",,,0,not_related
"Moreover, MBPO also adopts Soft Actor-Critic [Haarnoja et al., 2018], which is an off-policy RL algorithm, to update the policy with the mixed data from the real environment and learned model.",,,0,not_related
"In the recent analysis [Luo et al., 2018, Janner et al., 2019, Xu et al., 2020], the error of the learned transition model Mθ is measured over the data distribution, and thus directly connects with the learning loss Eq.",,,0,not_related
"Another way is to train an additional model learning environment dynamics [18, 20, 30, 53].",,,0,not_related
"Following prior works [23, 73, 26], we train an ensemble of seven such probability neural networks for both the forward and backward model.",,,0,not_related
"Model-based online RL methods achieve superior sample efficiency [59, 25, 4, 23] by learning a dynamics model of the environment and planning with the model [60, 66, 69].",,,0,not_related
"Model-based reinforcement learning (RL) learns either forward dynamics or reverse dynamics of the environment [60, 20], and can produce imaginary transitions for training, which has been widely demonstrated to be effective in improving the sample efficiency of RL in the online setting [4, 23].",,,0,not_related
"Following prior work [73, 26], we train an ensemble of bootstrapped probabilistic dynamics models, which has been widely demonstrated to be effective in model-based RL [8, 23].",,,1,related
"Perform h-step rollouts using the learned model P̂ and the current policy πφ by branching from the offline datasetDenv, and adding the generated data to a separate replay bufferDmodel, as in Janner et al. (2019) and Yu et al. (2020).",,,1,related
"With the offline dataset Denv, P̂ is typically trained using MLE (Janner et al., 2019; Yu et al., 2020; 2021) as
arg maxP̂∈P E(s,a,s′)∼Denv [ log P̂ (s′ | s, a) ] .",,,1,related
"…and subsequently using that for policy learning, MBRL can provide a sample-efficient solution to answer the counterfactual question pertaining to action-value estimation in the online setting (Janner et al., 2019; Rajeswaran et al., 2020), and to provide an augmentation to the
ar X
iv :2
20 6.",,,1,related
"We follow prior work (Chua et al., 2018; Janner et al., 2019; Yu et al., 2020) to construct our model as an ensemble of Gaussian probabilistic networks.",,,1,related
"(5) can be minimized via the MLE for P̂ , i.e., Eq.",,,1,related
"Combine Theorems 3.1 and 3.2 and Proposition 3.3, and suppose the dynamic model P̂ is trained by the MLE objective (Eq.",,,1,related
"With the learned model P̂ , we follow prior model-based RL work (Janner et al., 2019; Yu et al., 2020) to augment Denv with the replay buffer Dmodel consisting of h-horizon rollouts of the current policy πφ on the learned model P̂ , by branching from states in the offline dataset Denv.",,,1,related
"As in prior model-based RL using Gaussian probabilistic ensemble (Chua et al., 2018; Janner et al., 2019; Yu et al., 2020; Cang et al., 2021; Yu et al., 2021), the output of our model is double-headed, respectively outputting the mean and log-standard-deviation of the normal distribution of the…",,,0,not_related
"To implement this bound, we (1) train a dynamic model in a sufficiently rich function class via the maximum likelihood estimation (MLE); and (2) add a tractable regularizer into the policy optimization objective.",,,1,related
"Combining Theorem 3.1, 3.2 and Proposition 3.3, and suppose the dynamics model P̂ is trained by the MLE objective (Eq.",,,1,related
"Different from the goal of estimating the performance of some policies using offline data, our goal here is to construct a tractable regularization for policy optimization, which involves a learned transition model trained by MLE.",,,1,related
"Informally, this proposition coincides with the intuition that estimating the environmental dynamic using MLE can be helpful for matching dP̂π with d P∗
π .",,,1,related
"Similar to Janner et al. (2019) and Yu et al. (2020) we consider the rollout horizon h ∈ {1, 3, 5}.",,,1,related
"If the function class P for the estimated dynamic P̂ is rich enough and we have sufficiently many empirical samples from dP ∗
πb , under the classical statistical regularity condi-
tion, we can achieve a small model error Emodel by MLE (Ferguson, 1996; Casella & Berger, 2001).",,,1,related
The first step is to minimize the model error Emodel by pre-training the dynamic model under a sufficiently rich function class via MLE.,,,1,related
"(6) contains a constraint on the policy update size, and directly solving 1The above terms and assumptions have been widely used in RL in the literature such as [9, 15, 11, 7].",,,1,related
"For the latter, we use an ensemble of dynamics models of size 7, following [7].",,,1,related
It has been proved that the true return can be improved by interacting with the learned dynamics model when the model error is bounded [7].,,,0,not_related
"Compared to model-free methods, model-based RL [1, 14, 8, 7, 13] learns a dynamics model that mimics the transitions in the true environment, and then the policy can feel free to interact with the learned dynamics instead of the true environment.",,,0,not_related
We also involve a state-of-the-art model-based method MBPO [7] as one of the baseline methods.,,,1,related
"This is possible when P (the dynamics in source environment) is trainable, as considered in physical dynamics modeling [6, 4, 23] and model-based RL methods [7].",,,0,not_related
"[7] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"Model We train the model as similar to [18, 24], where we train an ensemble of 7 neural networks and predict with a random sample from the ensemble predictions.",,,1,related
"In particular, [18, 17] builds on the work of [24] that uses an ensemble of model to estimate epistemic uncertainty.",,,0,not_related
"Indeed, offline estimation and usage of a model comes with its own set of difficulties[23, 18, 24], as the model only has access to data with limited support and cannot improve its accuracy using additional experience.",,,0,not_related
"[24] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"It is also well known that a policy optimizing in such a model could learn to exploit where it is inaccurate, which degrades the evaluated performance[24, 23].",,,0,not_related
"As opposed to standard model based RL[24], in the imitation learning setting we provide some additional justifications as to why a longer H (up to a certain point) might be desirable in section B.",,,1,related
"Meanwhile too short of a rollout from Tl such as in[18, 24] increases the sampling bias from not sampling from dπ̂Tl , and the resulting algorithm strays too much from our analysis.",,,1,related
"However, with infinite state action spaces, naively rolling out the entire trajectory for 1 1−γ steps in Tl might not be desirable[24] as the model’s error compounds as shown in section 4[34].",,,0,not_related
"Following works in the model based offline reinforcement learning literature[24], we train Tl as an ensemble of K probabilistic neural networks that each outputs a mean and a covariance matrix Σ to estimate the transition from the behavior dataset, and use the discrepancies in their predictions to estimate uncertainty.",,,1,related
"Model-based Reinforcement Learning Recent research in model-based reinforcement learning (MBRL) has shown a significant advantage [12, 16, 13, 35, 40] in sample efficiency over model-free reinforcement learning.",,,0,not_related
"Due to the accumulation of the prediction error at each step, these future estimations can quickly diverge from reality [16, 43].",,,0,not_related
World models help agents understand environment dynamics and improve policies in sample efficiency [16] and performance [35].,,,0,not_related
"For example, Dynastyle methods like [24, 16, 28, 42] use world models to augment the training data.",,,0,not_related
"Dynamics Generalization in MBRL Several meta-learning-based MBRL methods are proposed Nagabandi et al. (2018a;b); Sæmundsson et al. (2018); Huang et al. (2021) to adapt the MBRL into environments with unseen dynamics by updating model parameters via a small number of gradient updates Finn et al. (2017) or hidden representations of a recurrent model Doshi-Velez & Konidaris (2016), and then Wang & van Hoof (2021) proposes a graph-structured model to improve dynamics forecasting.",,,0,not_related
"…model f̂ to improve its generalization ability on different dynamics by optimizing the objective function following (Lee et al., 2020; Seo et al., 2020; Janner et al., 2019):
Lpredθ,φ = − 1
N N∑ i=1 log f̂(sit+1|sit, ait, g(τ it−k:t−1;φ); θ), (1)
where k is the length of transition segments, t is…",,,0,not_related
"Therefore, a slight change of environmental dynamics may cause a significant performance decline of MBRL methods (Lee et al., 2020; Nagabandi et al., 2018a; Seo et al., 2020).",,,0,not_related
"This problem is called the dynamics generalization problem in MBRL, where the training environments and test environments share the same state S and action space A but the transition dynamics between states p(st+1|st, at) varies across different environments.",,,0,not_related
The vulnerability of MBRL to the change of environmental dynamics makes them unreliable in real world applications.,,,0,not_related
"Therefore, model-based reinforcement learning (MBRL) (Janner et al., 2019; Kaiser et al., 2019; Schrittwieser et al., 2020; Zhang et al., 2019b; van Hasselt et al., 2019; Hafner et al., 2019b;a; Lenz et al., 2015), which explicitly builds a predictive model to generate samples for learning RL…",,,0,not_related
"After obtaining environment-specified ẑt−k:t−1 at timestep t, we incorporate it into the dynamics prediction model f̂ to improve its generalization ability on different dynamics by optimizing the objective function following (Lee et al., 2020; Seo et al., 2020; Janner et al., 2019):",,,1,related
"However, the performance of MBRL methods highly relies on the prediction accuracy of the learned environmental model (Janner et al., 2019).",,,0,not_related
"Therefore, model-based reinforcement learning (MBRL) (Janner et al., 2019; Kaiser et al., 2019; Schrittwieser et al., 2020; Zhang et al., 2019b; van Hasselt et al., 2019; Hafner et al., 2019b;a; Lenz et al., 2015), which explicitly builds a predictive model to generate samples for learning RL policy, has been widely applied to a variety of limited data sequential decision-making problems.",,,0,not_related
"Taking the robotic control as an example (Nagabandi et al., 2018a; Yang et al., 2020; Rakelly et al., 2019; Gu et al., 2017; Bousmalis et al., 2018; Raileanu et al., 2020; Yang et al., 2019), dynamics change caused by parts damages could easily lead to the failure of MBRL algorithms.",,,0,not_related
", poor transitions can be generated, especially in complex high-dimensional environments [22].",,,0,not_related
"Recent state-of-the-art approaches in model-based deep reinforcement learning have extended this architecture to perform multi-step rollouts where each transition in the synthesized trajectory is treated as an experience for model-free reinforcement learning (Holland et al., 2018; Janner et al., 2019; Kaiser et al., 2020).",,,0,not_related
"Our MBRL architecture is outlined in algorithm 1, which is similar to the off-policy Dyna-style architecture presented by Holland et al. (2018), Janner et al. (2019) and Kaiser et al. (2020).",,,1,related
"Janner et al. (2019) derive bounds on policy improvement using the model, based on the choice of the rollout length and the model’s ability to generalize beyond its training distribution.",,,1,related
"…in model-based deep reinforcement learning have extended this architecture to perform multi-step rollouts where each transition in the synthesized trajectory is treated as an experience for model-free reinforcement learning (Holland et al., 2018; Janner et al., 2019; Kaiser et al., 2020).",,,0,not_related
"Following Janner et al. (2019), the transition subnetwork first predicts the difference between the next state and the current state, and then reconstructs the next state using the difference.",,,0,not_related
"(2) Model-Based (MB): MB trains dynamics and reward models by maximizing the log likelihood of the next state and reward from the offline dataset as recent successful model-based RL algorithms [48, 17].",,,0,not_related
"The model learning process is similar to MBPO [17], which learns an ensemble of probabilistic dynamic models and uses them for short rollouts.",,,0,not_related
"Such errors will accumulate along the trajectory and lead to the so-called compounding error [17, 18], which significantly degrades the performance of model-based value estimation.",,,0,not_related
"Recent model-based RL algorithms [17, 20, 21] use a bootstrap ensemble of dynamic models.",,,0,not_related
"There are other popular model based methods in literature such as MBPO (Janner et al. 2019) and PETS (Chua et al. 2018) but MPC-PSRL is already shown to outperform them in (Fan and Ming 2021, Fig.",,,0,not_related
There are other popular model based methods in literature such as MBPO (Janner et al. 2019) and PETS (Chua et al.,,,0,not_related
"The proposed method is model-free (see [67]), as compared with model-based methods (see [73]).",,,0,not_related
Janner et al. (2019) [Janner et al.(2019)] propose model-based Policy Optimization (MBPO) algorithm to boost PETS through monotonic improvement at each step and get state-of-the-art performance.,,,0,not_related
"Due to a certain deviation between the established environmental model and the real model, Janner and Fu et al. (2019) [Janner et al.(2019)] proposed a novel modelbased algorithm MBPO to improve PETs.",,,0,not_related
"DynaQ [Peng et al.(2018)], PETS [Chua et al.(2018)], MBPO [Janner et al.(2019)] are powerful model-based reinforcement algorithms in Gym environment.",,,0,not_related
"On the policy gradient side, Kang et al. (2018) [Kang et al.(2018)] utilize the state-of-art Asynchronous Advantage Actor-Critic algorithm (A3C [Mnih et al.(2016)]) to solve the portfolio management problem and design a standalone deep RL model. Moreover, Li et al. [Li et al.(2019a)] propose an adaptive deep deterministic RL method (Adaptive DDPG) for some portfolio allocation task, which incorporates optimistic or pessimistic deep RL algorithm based on the influence of prediction error. Through analyzing data from Dow Jones 30 component stocks, the trading strategy outperforms traditional DDPG method [Lillicrap et al.(2015)]. Sutta [Sornmayura(2019)] compares the performance of an AI agent to the results of the buy-and-hold strategy and the expert trader by testing 15 years’ forex data market with a paired t-Test. The findings show that AI can beat the buy & hold strategy and commodity trading advisor in FOREX for both EURUSD and USDJPY currency pairs. However, there are still complex and challenging issues with reinforcement learning for financial environment. First of all, financial marketing data, in most scenarios appear to have complex non-linearity, even complex chaotic behaviors and uncertainties including non-Gaussian noise, which causes finance time series data has distribution shift [Cai and Wei(2020)] over time.",,,0,not_related
"On the policy gradient side, Kang et al. (2018) [Kang et al.(2018)] utilize the state-of-art Asynchronous Advantage Actor-Critic algorithm (A3C [Mnih et al.(2016)]) to solve the portfolio management problem and design a standalone deep RL model. Moreover, Li et al. [Li et al.(2019a)] propose an adaptive deep deterministic RL method (Adaptive DDPG) for some portfolio allocation task, which incorporates optimistic or pessimistic deep RL algorithm based on the influence of prediction error. Through analyzing data from Dow Jones 30 component stocks, the trading strategy outperforms traditional DDPG method [Lillicrap et al.(2015)]. Sutta [Sornmayura(2019)] compares the performance of an AI agent to the results of the buy-and-hold strategy and the expert trader by testing 15 years’ forex data market with a paired t-Test.",,,0,not_related
"On the policy gradient side, Kang et al. (2018) [Kang et al.(2018)] utilize the state-of-art Asynchronous Advantage Actor-Critic algorithm (A3C [Mnih et al.(2016)]) to solve the portfolio management problem and design a standalone deep RL model.",,,0,not_related
"On the policy gradient side, Kang et al. (2018) [Kang et al.",,,0,not_related
"On the policy gradient side, Kang et al. (2018) [Kang et al.(2018)] utilize the state-of-art Asynchronous Advantage Actor-Critic algorithm (A3C [Mnih et al.",,,0,not_related
"On the policy gradient side, Kang et al. (2018) [Kang et al.(2018)] utilize the state-of-art Asynchronous Advantage Actor-Critic algorithm (A3C [Mnih et al.(2016)]) to solve the portfolio management problem and design a standalone deep RL model. Moreover, Li et al. [Li et al.(2019a)] propose an adaptive deep deterministic RL method (Adaptive DDPG) for some portfolio allocation task, which incorporates optimistic or pessimistic deep RL algorithm based on the influence of prediction error.",,,0,not_related
"On the policy gradient side, Kang et al. (2018) [Kang et al.(2018)] utilize the state-of-art Asynchronous Advantage Actor-Critic algorithm (A3C [Mnih et al.(2016)]) to solve the portfolio management problem and design a standalone deep RL model. Moreover, Li et al. [Li et al.(2019a)] propose an adaptive deep deterministic RL method (Adaptive DDPG) for some portfolio allocation task, which incorporates optimistic or pessimistic deep RL algorithm based on the influence of prediction error. Through analyzing data from Dow Jones 30 component stocks, the trading strategy outperforms traditional DDPG method [Lillicrap et al.(2015)]. Sutta [Sornmayura(2019)] compares the performance of an AI agent to the results of the buy-and-hold strategy and the expert trader by testing 15 years’ forex data market with a paired t-Test. The findings show that AI can beat the buy & hold strategy and commodity trading advisor in FOREX for both EURUSD and USDJPY currency pairs. However, there are still complex and challenging issues with reinforcement learning for financial environment. First of all, financial marketing data, in most scenarios appear to have complex non-linearity, even complex chaotic behaviors and uncertainties including non-Gaussian noise, which causes finance time series data has distribution shift [Cai and Wei(2020)] over time. Besides, hidden interactions among different agencies can be unpredictably difficult to understand. As it is shown from lots of stock marketing analysis, agencies with large capital investment can sometimes cause dramatic price fluctuation, leading to marketing panic and instability among the public. Therefore, scholars are actively seeking intrinsic mechanics to solve the above problems. On one hand, various data denoise methods are utilized to preprocess the financial time series data. For example, Bao W, et al. (2017) [Bao et al.",,,0,not_related
"It is common practice to use the disagreement of the predictions over an ensemble of neural networks as the epistemic uncertainty to guide exploration [27, 15, 36, 19].",,,0,not_related
rollouts to H = 15 to account for increased complexity of the multiagent environments and possible compounding error [31] of the world model.,,,0,not_related
Value function estimation tends to be accurate when predicting at points in the neighborhood of the present observation [25].,,,0,not_related
"There are also uncertainty-based and model-based methods that regularize the value function or policy with epistemic uncertainty estimated from model or value function (Janner et al., 2019; Yu et al., 2020; Uehara & Sun, 2021; Wu et al., 2021; Zhan et al., 2022; Bai et al., 2021).",,,0,not_related
"Examples from online RL include [7, 38], which learn one-step observation-based dynamics along with extensions to ensembles [10, 6, 29, 50].",,,0,not_related
via measures of ensemble-member disagreement [29].,,,0,not_related
"In this work, we use the same FF base-model architecture and training details as MBPO [29].",,,1,related
"We also tried MBPO [35], but we found that this method takes too much memory and could not finish any test.",,,1,related
As such we need consider the aleatoric uncertainty and the epistemic uncertainty [43].,,,1,related
"To generate the synthetic data, MBPO performs k-step rollouts in M̂ starting from states s ∈ D, and adds this data to D
M̂ .",,,1,related
"We train the policy with an actor-critic algorithm using synthetic data generated from the model in addition to data sampled from the dataset, similar to Dyna [55] and a number of recent methods [18, 68, 21, 67].",,,1,related
"Following previous approaches [68, 67, 18] we store data in D T̂φ in a first in, first out manner so that only data generated from recent iterations is stored in D T̂φ .",,,1,related
MBPO utilises a standard actor-critic RL algorithm.,,,0,not_related
"In line with existing works [68, 67, 5], we use the model-based policy optimisation (MBPO) [18]
approach to learn the optimal policy for M̂ .",,,1,related
"Thus, naive policy optimisation on a learned model in the offline setting can result in model exploitation [18, 27, 48].",,,0,not_related
"In line with existing works [68, 67, 5], we use the model-based policy optimisation (MBPO) [18] approach to learn the optimal policy for M̂ .",,,1,related
"(2021) propose to improve a model-based RL method MBPO (Janner et al., 2019) by augmenting the rollout samples using data augmentation that relies on the Jacobian from the differentiable simulator.",,,0,not_related
"The core idea in DYNA was extended by STEVE (Buckman et al., 2018) and MBPO (Janner et al., 2019), which use interpolation between different horizon predictions compared to a single step roll-out in DYNA.",,,0,not_related
"Notably, as noted by Janner et al. (2019), empirically the one-step rollout is a very strong baseline to beat in part because error in model can undermine the advantage from model-based data-augmentation.",,,0,not_related
"On the other side, model-based RL methods (Kurutach et al., 2018; Janner et al., 2019) have been proposed to learn an approximated dynamics model from little experience and then fully exploit the learned dynamics model during policy learning.",,,0,not_related
"SE-MBPO: Qiao et al. (2021) propose to improve a model-based RL method MBPO (Janner et al., 2019) by augmenting the rollout samples using data augmentation that relies on the Jacobian from the differentiable simulator.",,,0,not_related
"To further illustrate the advantage of our method over model-based RL, we conduct the experiments to compare to Model-Based Policy Optimization (MBPO) (Janner et al., 2019).",,,1,related
", 2018) and MBPO (Janner et al., 2019), which use interpolation between different horizon predictions compared to a single step roll-out in DYNA.",,,0,not_related
"For instance, the exploitation of model inaccuracies by policy optimizers has been investigated in the literature [25], [50], [51].",,,0,not_related
"Suggested strategies are the use of probabilistic ensembles [25], [27], [52]–[54], shorter task horizons [51] and denoising autoencoders [37].",,,0,not_related
Janner et al. (2019) use fully connected neural networks with four hidden layers and 200 neurons per layer.,,,0,not_related
"…models and losses for future work but highlight that significant performance might be gained by finding better tradeoffs than those discussed in Janner et al. (2019), especially when comparing deterministic and probabilistic models (compare Appendix F) as well as value-aware and value-agnostic…",,,0,not_related
"On the Ant-v2, we see small performance improvements above the results reported by Janner et al. (2019) and Pineda et al. (2021).",,,1,related
To assure a fair comparison we used the hyperparameters provided by Janner et al. (2019) for all experiments with our approach and the NLL loss function used for the baseline.,,,1,related
"As a deep learning based Dyna algorithm, we chose Model-based Policy Optimization (MBPO) (Janner et al., 2019) and ran all of our experiments using the implementation provided by Pineda et al.",,,1,related
"As a deep learning based Dyna algorithm, we chose Model-based Policy Optimization (MBPO) (Janner et al., 2019) and ran all of our experiments using the implementation provided by Pineda et al. (2021).",,,1,related
"One of the core problems of model-based policy learning methods however is that the accuracy of the model directly influences the quality of the learned policy or plan (Schneider, 1997; Kearns & Singh, 2002; Ross & Bagnell, 2012; Talvitie, 2017; Luo et al., 2019; Janner et al., 2019).",,,0,not_related
"F.1 ABLATION EXPERIMENT WITH DETERMINISTIC MODELS
In our experiments in the Hopper domain, we used probabilistic models following Janner et al. (2019).",,,1,related
[8] is to use shorter rollout horizons with learned models.,,,0,not_related
"The learned models have been used for data augmentation [8, 9, 11], improving the value targets [2, 4, 12, 13], improving the policy gradient [7] or any combination thereof.",,,0,not_related
"More recent Model-Based RL algorithms such as MBPO [46] or PETS [47] may offer a better solution to solving our control problem, with their CMA-ES ensemble maximum likelihood [48] based model learning procedure.",,,0,not_related
MAMBPO follows the principle that only the generated data which is close to the real data can be leveraged for policy improvement.,,,0,not_related
"MAMBPO [Willemsen et al., 2021] investigates model-based methods in Centralized Training Decentralized Execution (CTDE) paradigm where agents only observe local observations when making decisions, and global information is accessible when agents improving their policies.",,,0,not_related
"MAMBPO can indeed improve the sample efficiency empirically, and alleviate the partial observability and non-stationarity problems by adopting the CTDE paradigm.",,,0,not_related
"Dyna-style methods in single-agent scenarios show their sample efficiency both practically and theoretically [Janner et al., 2019].",,,0,not_related
"Similar to MBPO [Janner et al., 2019], the model rollouts begin from the experienced states, and the data generated from the model within k steps is used to improve the policies.",,,0,not_related
"For example, the Model-based Policy Optimization (MBPO) algorithm uses short horizons to avoid the compounding error problem (Janner et al., 2019), but there-in loses out on a lot of the potential for having a learned model and anticipating the future.",,,0,not_related
"The field of model-based reinforcement learning (MBRL) showcases this process and has been used to solve many robotic tasks by iteratively learning a black-box model (Deisenroth and Rasmussen, 2011; Chua et al., 2018; Nagabandi et al., 2019; Janner et al., 2019).",,,0,not_related
"…successes in model-based reinforcement learning are one-step dynamics models; where they have been used for online model predictive control (MPC) (Chua et al., 2018; Nagabandi et al., 2019; Lambert et al., 2019) or value-estimation and offline rollouts of imagined policies (Janner et al., 2019).",,,0,not_related
"Different model parametrizations, particularly ensembles and probabilistic loss functions, have been shown to improve the peak performance of MBRL algorithms (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"LOOP has been shown to outperform a number of model-based methods, e.g., MBPO (Janner et al., 2019) and POLO (Lowrey et al., 2019)) on select MuJoCo tasks.",,,0,not_related
"A common paradigm is to learn a model of the environment that can be used for planning (Ebert et al., 2018; Zhang et al., 2018; Janner et al., 2019; Hafner et al., 2019; Lowrey et al., 2019; Kaiser et al., 2020; Bhardwaj et al., 2020; Yu et al., 2020; Schrittwieser et al., 2020; Nguyen et al.,…",,,0,not_related
"A common paradigm is to learn a model of the environment that can be used for planning (Ebert et al., 2018; Zhang et al., 2018; Janner et al., 2019; Hafner et al., 2019; Lowrey et al., 2019; Kaiser et al., 2020; Bhardwaj et al., 2020; Yu et al., 2020; Schrittwieser et al., 2020; Nguyen et al., 2021) or for training a model-free algorithm with generated data (Pong et al.",,,0,not_related
"Unlike prior work (Ha & Schmidhuber, 2018; Janner et al., 2019; Hafner et al., 2019; 2020; Sikchi et al., 2020), we find it sufficient to implement all components of TOLD as purely deterministic MLPs, i.e., without RNN gating mechanisms nor probabilistic models.",,,1,related
"To provide a rich learning signal for model learning, prior work on model-based RL commonly learn to directly predict future states or pixels (Ha & Schmidhuber, 2018; Janner et al., 2019; Lowrey et al., 2019; Kaiser et al., 2020; Sikchi et al., 2020).",,,0,not_related
"Planning is a powerful approach to such sequential decision making problems, and has achieved tremendous success in application areas such as game-playing (Kaiser et al., 2020; Schrittwieser et al., 2020) and continuous control (Tassa et al., 2012; Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"This ablation makes our method more similar to prior work on model-based RL from states (Janner et al., 2019; Lowrey et al., 2019; Sikchi et al., 2020; Argenson & DulacArnold, 2021).",,,1,related
"Concretely, prior work on model-based methods can largely be subdivided into two directions, each exploiting key advantages of model-based learning: (i) planning, which is advantageous over a learned policy, but it can be prohibitively expensive to plan over long horizons (Janner et al., 2019; Lowrey et al., 2019; Hafner et al., 2019; Argenson & DulacArnold, 2021); and (ii) using a learned model to improve †Equal advising.",,,0,not_related
"…model-based learning: (i) planning, which is advantageous over a learned policy, but it can be prohibitively expensive to plan over long horizons (Janner et al., 2019; Lowrey et al., 2019; Hafner et al., 2019; Argenson & DulacArnold, 2021); and (ii) using a learned model to improve
†Equal…",,,0,not_related
"Unlike prior work (Ha & Schmidhuber, 2018; Janner et al., 2019; Hafner et al., 2019; 2020; Sikchi et al., 2020), we find it sufficient to implement all components of TOLD as purely deterministic MLPs, i.",,,1,related
"and favor decisions that are consistent across the models [19],",,,0,not_related
"With these modified reward functions in hand, we can then use classic model-based approaches to tackle offline RL problems, such as Dyna-based methods that sample transitions from the model to train a model-free algorithm [19], [34].",,,1,related
"offline [4], [14] and repeat the data collection and learning loop until the task is solved [9], [11].",,,0,not_related
"However, such model-based methods may suffer from additional computation costs and may perform suboptimally in complex environments (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"Model-based RL for fully observable MDPs [5] is better understood and include “world models” [17, 18] and Dyna-Q based methods [23, 50].",,,0,not_related
"[29] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"Model-based methods (to list a few, [29, 5, 42, 100, 40, 10, 67, 13, 38]) can be used together with VRL3.",,,0,not_related
"Experiments indicate that the practical instantiation of our algorithm, Safe Model-Based Policy Optimization (SMBPO), effectively reduces the number of safety violations on several continuous control tasks, achieving a comparable performance with far fewer safety violations compared to several model-free safe RL algorithms.",,,0,not_related
"We build on practices established in previous deep model-based algorithms, particularly MBPO [Janner et al., 2019] a state-of-the-art model-based algorithm (which does not emphasize safety).",,,1,related
"The algorithm, dubbed Safe Model-Based Policy Optimization (SMBPO), is described in Algorithm 1.",,,0,not_related
"3https://github.com/abalakrishna123/recovery-rl
MBPO is competitive in terms of sample efficiency but incurs more safety violations because it isn’t designed explicitly to avoid them.",,,1,related
"In the experimental evaluation, we compare our algorithm to several model-free safe RL algorithms, as well as MBPO, on various continuous control tasks based on the MuJoCo simulator [Todorov et al., 2012].",,,1,related
"All of the above algorithms except for MBPO are as implemented in the Recovery RL paper [Thananjeyan et al., 2020] and its publicly available codebase3.",,,1,related
"In MBPO, this takes the form of short model-based rollouts, starting from states in D, to reduce the risk of compounding error.",,,0,not_related
"With a larger C, SMBPO incurs substantially fewer safety violations, although the total rewards are learned slower.",,,0,not_related
"Here are some additional details regarding the (S)MBPO implementation:
• All neural networks are implemented in PyTorch [Paszke et al., 2019] and optimized using the Adam optimizer [Kingma and Ba, 2014] and batch size 256.",,,1,related
"Algorithm 1 Safe Model-Based Policy Optimization (SMBPO) Require: Horizon H 1: Initialize empty buffers D and D̂, an ensemble of probabilistic dynamics {T̂θi} N i=1, policy πφ, critic Qψ .",,,1,related
A single run of (S)MBPO takes as long as 72 hours on a single GPU.,,,0,not_related
"We compare against the following algorithms:
• MBPO: Corresponds to SMBPO with C = 0.",,,1,related
"MBPO is based on the soft actor-critic (SAC) algorithm, a widely used off-policy maximum-entropy actor-critic algorithm [Haarnoja et al., 2018a].",,,0,not_related
"Following prior work [Chua et al., 2018, Janner et al., 2019], we employ an ensemble of (diagonal) Gaussian dynamics models {T̂θi}Ni=1, where T̂i(s, a) = N (µθi(s, a),diag(σ2θi(s, a))), in an attempt to capture both aleatoric and epistemic uncertainties.",,,1,related
Code is made available at https://github.com/gwthomas/Safe-MBPO.,,,1,related
"• MBPO+bonus: The same as MBPO, except adding back in the alive bonus which was subtracted
out of the reward.",,,1,related
1The term ‘model-based’ does not mean ‘model-based reinforcement learning’ [1].,,,0,not_related
"Furthermore, we can used sauteed environments in the model-based RL setting (MBPO and PETS) as well.",,,1,related
", 2021) as the core library, which has PyTorch implementation of MBPO (Janner et al., 2019), and PETS (Chua et al.",,,0,not_related
"While we mostly test with model-free approaches (PPO, TRPO, SAC), the model-based methods are also “sauteable”, which we illustrate on MBPO and PETS.",,,1,related
"For our model-based implementations we used (Pineda et al., 2021) as the core library, which has PyTorch implementation of MBPO (Janner et al., 2019), and PETS (Chua et al., 2018).",,,1,related
We proceed by “sauteing” MBRL methods: MBPO and PETS.,,,1,related
", 2018), model-based policy optimization (MBPO) (Janner et al., 2019), probabilistic ensembles with trajectory sampling (PETS) (Chua et al.",,,0,not_related
"We discuss in detail these state augmentation methods in Appendix B, but note that (Calvo-Fullana et al., 2021), (Chow et al., 2017) have not extended their methods to modern model-free and model-based RL methods such as trust region policy optimization (TRPO) (Schulman et al., 2015), proximal policy optimization (PPO) (Schulman et al., 2017), soft actor critic (SAC) (Haarnoja et al., 2018), model-based policy optimization (MBPO) (Janner et al., 2019), probabilistic ensembles with trajectory sampling (PETS) (Chua et al., 2018).",,,1,related
"…policy optimization (TRPO) (Schulman et al., 2015), proximal policy optimization (PPO) (Schulman et al., 2017), soft actor critic (SAC) (Haarnoja et al., 2018), model-based policy optimization (MBPO) (Janner et al., 2019), probabilistic ensembles with trajectory sampling (PETS) (Chua et al., 2018).",,,0,not_related
"Our model-based algorithm can be further enhanced by using synthetic one-step transitions similarly to Janner et al. (2019), which would improve sample efficiency.",,,1,related
"…et al., 2016), we would expect analogous improvements to be possible in MBRL. Algorithms for MBRL are infamously sensitive to choice of prediction horizon, and one possible explanation is poor generalization caused by weak inductive biases (Janner et al., 2019; Pan et al., 2020; Amos et al., 2021).",,,1,related
"Algorithms for MBRL are infamously sensitive to choice of prediction horizon, and one possible explanation is poor generalization caused by weak inductive biases (Janner et al., 2019; Pan et al., 2020; Amos et al., 2021).",,,0,not_related
"Following the same line as in (Janner et al., 2019; Yu et al., 2020; 2021b), the dynamics model for each task is represented as a probabilistic neural network that takes the current state-action as input and outputs a Gaussian distribution over the next state and reward:
T̂θ(st+1, r|s, a) = N…",,,0,not_related
"2 in (Janner et al., 2019), we can obtain that Dtv(ρ πo t (s, a)||ρ πc t (s, a)) ≤Dtv(ρ t (s)||ρ πc t (s)) + max s Dtv(πo(a|s)||πc(a|s)) ≤tmax s Dtv(πo(a|s)||πc(a|s)) + max s Dtv(πo(a|s)||πc(a|s))",,,1,related
", 2021b), have demonstrated promising performance on a single offline RL task by combining model-based policy optimization (Janner et al., 2019) and conservative policy evaluation (CQL (Kumar et al.",,,0,not_related
"To remove the need of uncertainty quantification, COMBO (Yu et al., 2021b) is proposed by combining model-based policy optimization (Janner et al., 2019) and conservative policy evaluation (Kumar et al., 2020).",,,1,related
"Following the same line as in (Janner et al., 2019; Yu et al., 2020; 2021b), the dynamics model for each task is represented as a probabilistic neural network that takes the current state-action as input and outputs a Gaussian distribution over the next state and reward: T̂θ(st+1, r|s, a) = N (μθ(st, at),Σθ(st, at)).",,,0,not_related
"Since both state-action marginals here correspond to rolling out πo and πc in the same MDP M̂, based on Lemma B.1 and B.2 in (Janner et al., 2019), we can obtain that
Dtv(ρ πo t (s, a)||ρ πc t (s, a))
≤Dtv(ρπot (s)||ρ πc t (s)) + max s Dtv(πo(a|s)||πc(a|s))
≤tmax s Dtv(πo(a|s)||πc(a|s)) + max s…",,,1,related
"Recent model-based offline RL algorithms, e.g., COMBO (Yu et al., 2021b), have demonstrated promising performance on a single offline RL task by combining model-based policy optimization (Janner et al., 2019) and conservative policy evaluation (CQL (Kumar et al., 2020)).",,,0,not_related
", 2021b) is proposed by combining model-based policy optimization (Janner et al., 2019) and conservative policy evaluation (Kumar et al.",,,0,not_related
"…reinforcement learning, simulators are often learned, but accumulate errors over long time horizons and often struggle to generalize beyond their training data (Janner et al., 2019; Talvitie, 2014; Venkatraman et al., 2015), making them unsuitable for design optimization without further finetuning.",,,0,not_related
"In robotics and reinforcement learning, simulators are often learned, but accumulate errors over long time horizons and often struggle to generalize beyond their training data (Janner et al., 2019; Talvitie, 2014; Venkatraman et al., 2015), making them unsuitable for design optimization without further finetuning.",,,0,not_related
"To verify the superiority of the system model designed in this paper, the model is compared with the MLP model [37] and CNN‐LSTM model [38], which are commonly used in data‐driven system modeling.",,,0,not_related
"To verify the superiority of the system mo el desig ed in this paper, t e o el is co pared with the MLP model [37] and CNN-LSTM model [38], which are co only used in data-driven syste modeling.",,,1,related
"We compare our algorithm with multiple state-of-the-art deep RL methods including model-free algorithms such as Tpprl (Upadhyay, De, and Gomez-Rodriguez 2018), SAC (Haarnoja et al. 2018), TD3 (Fujimoto, Hoof, and Meger 2018), DDQN (Van Hasselt, Guez, and Silver 2016) and model-based ones such as Dreamer (Hafner et al. 2019a) and MBPO (Janner et al. 2019).",,,1,related
"…RL methods including model-free algorithms such as Tpprl (Upadhyay, De, and Gomez-Rodriguez 2018), SAC (Haarnoja et al. 2018), TD3 (Fujimoto, Hoof, and Meger 2018), DDQN (Van Hasselt, Guez, and Silver 2016) and model-based ones such as Dreamer (Hafner et al. 2019a) and MBPO (Janner et al. 2019).",,,0,not_related
We replace the feed-forward neural network in MBPO by LSTM. 7) LSTM+SAC: The sequence encoding is obtained by the LSTM.,,,1,related
6) MBPO: MBPO applies the model ensemble in the dynamic model learning to enhance the performance.,,,0,not_related
"In (a), our method SEDRL has the best result followed by Tpprl, Transformer+SAC, LSTM+ SAC, MBPO, Transformer+TD3, Dreamer and then Transformer+DDQN (Trans+Q).",,,1,related
The above hyper-parameter configurations in the online setting are adapted from MBPO.,,,0,not_related
", 2021] are existing offline-to-online algorithms, and MBPO [Janner et al., 2019] is a pure online RL algorithm.",,,0,not_related
"Therefore, the online training process quickly turns into a MBPO-like training process, an efficient model-based online RL algorithm [Janner et al., 2019].",,,0,not_related
"We compare our algorithm with three baselines, where balance replay [Lee et al., 2021] and AWAC [Nair et al., 2021] are existing offline-to-online algorithms, and MBPO [Janner et al., 2019] is a pure online RL algorithm.",,,1,related
"A promising alternative to improve sample efficiency is to use model-based reinforcement learning (MBRL) approaches
(Deisenroth & Rasmussen, 2011; Chua et al., 2018; Hafner et al., 2019a; Janner et al., 2019).",,,0,not_related
"Lemma 3.1 (Janner et al., 2019): Let the expected total variation distance (TV-distance)6 Es∼πD,k[DTV(p(s′, r|s, a)‖pw(s′, r|s, a))] between two transition distributions be bounded at each time step by m and the policy divergence DTV [π‖πD] be bounded by π .",,,1,related
Janner et al. (2019) provided conditions under which model usage can facilitate policy optimisation.,,,0,not_related
"Dyna-style approaches are popular since they facilitate faster inference at deployment time (forward-pass of the policy rather than an optimization loop) and have been shown to be highly sample-efficient both from proprioceptive states (Janner et al., 2019) and pixels (Hafner et al.",,,0,not_related
"…RL. Dyna-style approaches are popular since they facilitate faster inference at deployment time (forward-pass of the policy rather than an optimization loop) and have been shown to be highly sample-efficient both from proprioceptive states (Janner et al., 2019) and pixels (Hafner et al., 2020).",,,0,not_related
A contrastive approach to constraining the RL to the dataset is to derive an MDP from the data and either solve it optimally or use model-based policy optimization (MBPO).,,,0,not_related
"Next, we use MOReL and MOPO as two representatives of the general MBPO [17] approach that covers both classical (näıve) MBRL and Pessimistic MDP-based MBRL.",,,1,related
A contrastive approach to constraining the RL to the dataset is to derive an MDP from the data and either solve it optimally or use model-based policy optimization (MBPO) [17].,,,0,not_related
"Next, we use MOReL [10] as a representative of a general MBPO approach that covers both a classical (näıve) MBRL and a Pessimistic MDPbased MBRL.",,,1,related
"Other than applying the IC-INVASE algorithm to solve Equation (6), another way of leveraging IC-INVASE in action space pruning is to combine it with the model-based methods [11, 16, 13, 14], where a dynamic model P : S ×A 7→ S is learned through regression: P = arg min P E(s,a,s′)∼π,T (s′ − P(s, a))(2) (10)",,,0,not_related
"The model can be used in variousways, such as execution-time planning [5, 21], generating imaginary experiences for training the control policy [12, 32]), etc.",,,0,not_related
"However, efficiently learning accurate models remains challenging, especially in complex and noisy environments (Janner et al. 2019; Wang et al. 2019; Pan et al. 2020), which creates a hindrance in further improving the sample efficiency of model-based approaches.",,,0,not_related
", Model-Based Policy Optimization (Janner et al. 2019).",,,0,not_related
", the dyna-style algorithm, which has recently shown the potential to achieve high sample efficiency (Janner et al. 2019).",,,0,not_related
"Contribution of each component The path from MBPO (Janner et al. 2019) to CMBAC comprises three modifica-
tions: Q-network size increase (Big), learning multiple estimates of the Q-value (LMEQ), and conservative policy optimization (CPO).",,,0,not_related
"In this section, we present the notation and provide a brief introduction to the state-of-the-art model-based algorithm, i.e., Model-Based Policy Optimization (Janner et al. 2019).",,,1,related
"…fall into three categories according to the way of model usage: (1) dynastyle methods (Sutton 1990; Luo et al. 2019; Zhou, Li, and Wang 2020; Janner et al. 2019), which use the model to generate imaginary samples as additional training data; (2) shooting algorithms (de Boer et al. 2005;…",,,0,not_related
"Our work falls into the first category, i.e., the dyna-style algorithm, which has recently shown the potential to achieve high sample efficiency (Janner et al. 2019).",,,1,related
"However, many of these results are achieved by model-free algorithms and generally require a massive number of samples, which significantly hinders the applications of modelfree methods in real-world tasks (Kurutach et al. 2018; Janner et al. 2019).",,,0,not_related
Contribution of each component The path from MBPO (Janner et al. 2019) to CMBAC comprises three modifica-,,,0,not_related
Model-based policy optimization (MBPO) is a state-of-theart model-based algorithm that has achieved impressive performance (Janner et al. 2019).,,,0,not_related
"Roughly speaking, model-based approaches fall into three categories according to the way of model usage: (1) dynastyle methods (Sutton 1990; Luo et al. 2019; Zhou, Li, and Wang 2020; Janner et al. 2019), which use the model to generate imaginary samples as additional training data; (2) shooting algorithms (de Boer et al.",,,0,not_related
Model-Based Policy Optimization Model-based policy optimization (MBPO) is a state-of-theart model-based algorithm that has achieved impressive performance (Janner et al. 2019).,,,0,not_related
"Off-Policy Model-Based: In particular, our proposed approaches share similarities with off-policy model-based approaches such as Dyna (Sutton, 1991) and more modern variants for deep RL like MBPO (Janner et al., 2019).",,,0,not_related
", 2020) (Algorithm 2) and we also combine ED2 with MBPO (Janner et al., 2019) in the appendix.",,,1,related
"Baselines & Benchmarks: For a comparative study, we take two state-of-the-art MBRL methods: MBPO (Janner et al., 2019) and Dreamer (Hafner et al., 2020) as the baselines, and extend them with ED2 as ED2-MBPO, ED2-Dreamer.",,,1,related
"Here we provide the practical combination implementation of ED2 with Dreamer(Hafner et al., 2020) (Algorithm 2) and we also combine ED2 with MBPO (Janner et al., 2019) in the appendix.",,,1,related
"Model ensemble is also widely used in model construction for uncertainty estimation, which provides a more reliable prediction (Janner et al., 2019; Pan et al., 2020).",,,0,not_related
"World models are usually formulated with latent dynamics (Janner et al., 2019; Hafner et al., 2020), and the general form of the latent dynamics model can be summarized as follows:
Latent transition kernel: ht = f(s≤t−1, a≤t−1) Stochastic state function: p(st|ht) Reward function: p(rt|ht)
The…",,,0,not_related
"World models are usually formulated with latent dynamics (Janner et al., 2019; Hafner et al., 2020), and the general form of the latent dynamics model can be summarized as follows: Latent transition kernel: ht = f(s≤t−1, a≤t−1) Stochastic state function: p(st|ht) Reward function: p(rt|ht) The latent transition kernel (shorthand as kernel) predicts the latent state ht with input s≤t−1 and a≤t−1.",,,0,not_related
"Baselines & Benchmarks: For a comparative study, we take two state-of-the-art MBRL methods: MBPO (Janner et al., 2019) and Dreamer (Hafner et al.",,,1,related
"On the other side of things stands the hands off approach taken in the RL community, where general and unstructured neural networks are used for both transition models [9, 52, 22] as well as policies and value functions [17].",,,0,not_related
"State of the art model based approaches on Mujoco tend to use an ensemble of small MLPs that predict the state transitions [9, 52, 22, 2], without exploiting any structure of the state space.",,,0,not_related
"Model-free approaches tend to require many more interactions with the environment [22], hence our choice of a model-based approach for this work.",,,0,not_related
"Lemma C.3 (Theorem 2 of (Syed, Bowling, and Schapire 2008)). if ρ ∈ D, then ρ is the occupancy measure for πρ (a|s) , ρ(s,a)∑
a′ ρ(s,a ′) , and πρ is the only policy whose oc-
cupancy measure is ρ. Lemma C.4 (Lemma 3.2 of (Ho and Ermon 2016)) Let H̄ (ρ) = − ∑ s,a ρ (s, a) log (ρ(s, a)/ ∑ a′ ρ (s,…",,,1,related
"Compared to Soft-Actor-Critic (SAC), which is model-free and uses a UTD of 1, MBPO achieves much higher sample efficiency in the OpenAI MuJoCo benchmark (Todorov et al., 2012; Brockman et al., 2016).",,,0,not_related
"In particular, Model-Based Policy Optimization (MBPO) (Janner et al., 2019) uses a large UTD ratio of 20-40.",,,0,not_related
"Although some theoretical analysis [17, 12, 22] and thorough empirical evaluation [30] have been conducted in the previous literature, it remains unclear how to appropriately schedule the hyperparameters to achieve optimum performance when training a Dyna-style MBRL algorithm.",,,0,not_related
"On all these tasks, the MBPO instance trained with hyperparameters scheduled by AutoMBPO can significantly surpass the one with original configuration [12].",,,0,not_related
[12] manually design a schedule that linearly increases the rollout length across epochs.,,,0,not_related
"Although real data ratio is an essential factor empirically [13, 12], it has not yet been studied thoroughly in theory.",,,0,not_related
"MBRL methods can be roughly categorized into four types according to different model usage: (i) Dyna-style algorithms [28, 17, 4, 14, 12, 15] leverage the model to generate imaginary data and adopt some off-the-shelf MFRL algorithms to train a policy using both real data and imaginary data; (ii) shooting algorithms [19, 3] use model predictive control (MPC) to plan directly without explicit policy; (iii) analytic-gradient algorithms [7, 16, 5] search policies with back-propagation through time by exploiting the model derivatives; (iv) model-augmented value expansion algorithms [9, 2] utilize model rollouts to improve the target value for temporal difference (TD) updates.",,,0,not_related
"When optimizing the policy, we can use merely imaginary data [17, 14] or a mixture of real data and imaginary one in a fixed ratio [12].",,,1,related
"is also critical [12] since too short rollout length fails to sufficiently leverage the model to plan forward, while too long rollout length may bring disastrous compounding error [1].",,,0,not_related
"Since Dyna-style algorithms can seamlessly take advantage of innovations in MFRL literature and have recently shown impressive performance [12, 30], this paper mainly focuses on Dyna-style algorithms.",,,0,not_related
"[12] fix the ratio of real data as 5%, while Kalweit and Boedecker [13] use the uncertainty to adaptively choose the ratio, which tends to use more imaginary samples initially and gradually use more real samples afterward.",,,0,not_related
"As for imaginary data generation, it is suggested to generate short rollouts to reduce compounding model error [12], and the model’s uncertainty is further incorporated to choose reliable imaginary data dynamically [20].",,,0,not_related
"The idea of integrating model-based and model-free RL has also been studied independently of planning (Pong et al., 2018; Janner et al., 2019).",,,0,not_related
"In general, we cannot guarantee that the reward model will generalize well to regions of the state-action space that are underexplored by the logging policy (Janner et al., 2019).",,,1,related
"In this work, STEVE is used as the base learning algorithm, but our framework is generally applicable to other model-based algorithms with uncertainty quantification, such as MBPO, M2AC, etc.",,,0,not_related
ME-TRPO [26] and MBPO [27] are directly trained on imaginary data to accelerate policy learning.,,,0,not_related
"The learned dynamics model can be used for planning [4, 23], value expansion [24, 25], or imaginary training [26, 27, 28].",,,0,not_related
"63 In model-based reinforcement learning (MBRL), there are also many algorithms that constrain the 64 exploitation in the environment with effective uncertainty estimation methods [23, 24, 25, 26, 27, 10].",,,0,not_related
"As the baseline of our framework is built upon MBPO implementation, we derive the “same hyperparameters” for our experiments and all the baseline algorithms.",,,1,related
"Further, the number of interactions with the true environment for outer loop policy were kept constant to 1000 for each epoch, same as MoPAC and MBPO.",,,0,not_related
Our rewards in Ant-v2 were comparable with MoPAC but still significantly better than MBPO.,,,0,not_related
This will be compared with the existing approaches MoPAC [12] and MBPO [10] on the benchmark MuJoCo control environments.,,,0,not_related
Model Based Policy Optimisation (MBPO) [10] introduced the outer loop policy to collect transition to train approximate model and sample over it to train the policy.,,,0,not_related
"We would like to emphasize that our final rewards are eventually the same as achieved by MoPAC and MBPO, however the progress rate is faster for all our experiments with lesser true environment interactions.",,,1,related
Several experiments were conducted on the MuJoCo [18] continuous control tasks with the OpenAI-Gym benchmark and the performance was compared with recent related works MoPAC [12] and MBPO [10].,,,0,not_related
"2 FACTORS OF GENERALIZATION Planning Model-based RL is an active area of research [28, 46] with the majority of work focusing on gains in data efficiency [26, 27, 34, 37, 61], though it is often motivated by a desire for better zero- or few-shot generalization as well [18, 29, 48, 64, 70].",,,0,not_related
"This can be alleviated in practice by rolling out the state predictions on short horizons, similarly as in Janner et al. (2019).",,,0,not_related
", 2020), due to the higher sample-efficiency compared to model-free algorithms (Chua et al., 2018; Gal et al., 2016; Janner et al., 2019; Nagabandi et al., 2018; Wang et al., 2019).",,,0,not_related
", 2016; Gal & Ghahramani, 2016) or ensembles of probabilistic NNs (Janner et al., 2019; Chua et al., 2018).",,,0,not_related
"…are a popular alternative with several implementation variants, e.g. deterministic NNs (Nagabandi et al., 2018), Bayesian NNs relying on the Monte Carlo (MC) Dropout approximation (Gal et al., 2016; Gal & Ghahramani, 2016) or ensembles of probabilistic NNs (Janner et al., 2019; Chua et al., 2018).",,,0,not_related
"…attention as this approach is expected to close the gap between simulated and real world tasks (Nagabandi et al., 2020), due to the higher sample-efficiency compared to model-free algorithms (Chua et al., 2018; Gal et al., 2016; Janner et al., 2019; Nagabandi et al., 2018; Wang et al., 2019).",,,0,not_related
"The automatic learning and use of the agent-environment interaction model have shown its effectiveness in game [13, 14] and robotic [15, 16] environments.",,,0,not_related
Simulation results show that the proposed methodology is better than or at least as good as MoPAC [15] and MBPO [8] in terms of sample-efficiency.,,,0,not_related
"In the context of robotics, this model has proven to be very beneficial in developing robust control strategies based on predictive simulations [8].",,,0,not_related
We will compare the DeMo RL with the existing approaches MoPAC [15] and MBPO [8] on the benchmark MuJoCo control environments.,,,1,related
"This shows that our formulation is more generic and some of the existing approaches could be derived with suitable choice: [15, 1] and [8].",,,0,not_related
1 DeMo RL: Results and Comparison Several experiments were conducted on the MuJoCo [25] continuous control tasks with the OpenAI-Gym benchmark and the performance was compared with recent related works MoPAC [15] and MBPO [8].,,,0,not_related
"to learn a model from offline data, with minimal modification to the policy learning (Kidambi et al., 2020; Yu et al., 2020; Janner et al., 2019).",,,0,not_related
"Model-based methods attempt
to learn a model from offline data, with minimal modification to the policy learning (Kidambi et al., 2020; Yu et al., 2020; Janner et al., 2019).",,,0,not_related
"The baseline algorithms are REDQ [3], MBPO [19], SAC [16], TD3 [6] and TQC [9].",,,0,not_related
"For MBPO1, REDQ2, TD33 and TQC4, we use the authors’s code.",,,1,related
"L G
] 1
0 N
ov 2
02 1
2 0.0 1.0 2.0 3.0 Time Steps 1e5 0.0 2.0 4.0 6.0 Av er ag e R et ur n 1e3 Ant 0.0 0.2 0.4 0.6 0.8 1.0 Time Steps 1e6 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1e4 Humanmoid 0.0 1.0 2.0 3.0 Time Steps 1e5 0.0 1.0 2.0 3.0 4.0 1e3 Hopper
RAC-SAC RAC-TD3 REDQ SAC TQC TQC20 MBPO
0.0 1.0 2.0 3.0 Time Steps 1e5
0.0
1.0
2.0
3.0
4.0
5.0
6.0
1e3 Walker2d
Fig.",,,1,related
"We compare to state-of-the-art algorithms: SAC [16], TD3 [6], MBPO [19], REDQ [3] and TQC [9] on 4 challenging continuous control tasks (Walker2d, HalfCheetah, Ant and Humanoid) from MuJoCo environments [18] in the OpenAI gym benchmark [17].",,,1,related
"the-art algorithms (MBPO [19], REDQ [3] and TQC [9]), achieving state-of-the-art sample efficiency on the Humanoid benchmark.",,,0,not_related
"The shaded areas denote one standard deviation.
the-art algorithms (MBPO [19], REDQ [3] and TQC [9]), achieving state-of-the-art sample efficiency on the Humanoid benchmark.",,,1,related
"An important line of research focuses on correctly balancing real-world experience with data generated from the internal model of the agent [25, 11].",,,0,not_related
"Note that in contrast to the result by Janner et al. (2019), Eq. (5) is a bound on the policy improvement instead of a lower bound on ηn+1. The first error term compares ηn+1 and η̃n+1, the performance estimation gap under the optimized policy πn+1 that we obtain in Line 4 of Algorithm 1. Since at this point we have only collected data with πn in Line 2, this term depends on the generalization properties of our model to new data; what we call the off-policy model error. For our data-based model p̃ that just replays data under πn independently of the action, this term can be bounded for stochastic policies. For example, Schulman et al. (2015) bound it by the average KL-divergence between πn and πn+1.",,,1,related
"In the following section, we investigate the impact of OPC on a range of continuous control tasks and compare it to the current state-of-the-art model-based and model-free RL algorithms MBPO (Janner et al., 2019) and SAC (Haarnoja et al.",,,1,related
"Note that in contrast to the result by Janner et al. (2019), Eq.",,,1,related
The original data for MBPO and the other baselines were provided by Janner et al. (2019).,,,1,related
"Our implementation is based on the code from MBPO (Janner et al., 2019), which is open-sourced under the MIT license.",,,1,related
"If the reward function is known, this term only depends on the model quality of p̃ relative to p. Note that in contrast to the result by Janner et al. (2019), Eq.",,,0,not_related
"To alleviate this issue, one could extend Theorem 1 similarly to the results in Janner et al. (2019), such that the rollouts are only of length H T .",,,1,related
"For H →∞, we obtain the original result form Janner et al. (2019) with ∑ t≥1 tγ t = γ/(1−γ)2.",,,1,related
Janner et al. (2019) use the learned model only for short simulated rollouts starting from real observed states to mitigate the issue of compounding errors for long-term predictions.,,,0,not_related
"The comparison includes the following methods:
• MBPO (Janner et al., 2019), • PETS (Chua et al., 2018), • SAC (Haarnoja et al., 2018), • PPO (Schulman et al., 2017), • STEVE (Buckman et al., 2018), • SLBO (Luo et al., 2018).",,,0,not_related
"F IMPLEMENTATION AND COMPUTATIONAL RESOURCES
Our implementation is based on the code from MBPO (Janner et al., 2019), which is open-sourced under the MIT license.",,,1,related
"…choice of initial state distribution ρ̃(s0) depends on the optimization method: For model predictive control, Chua et al. (2018) use the current state to optimize the next action based on a rollout, while Janner et al. (2019) use the empirical state distribution observed from the true environment.",,,1,related
"…Schulman et al. (2015) to provide guarantees for MBRL. Luo et al. (2018) provide a general framework to show monotonic improvement towards a local optimum of the value function, while Janner et al. (2019) present a lower-bound on performance for different rollout schemes and horizon lengths.",,,1,related
Note that this result contradicts the findings by Janner et al. (2019) that one-step predictions are oftentimes sufficient for MBPO.,,,0,not_related
Our implementation is based upon the code from Janner et al. (2019).,,,1,related
"In the following section, we investigate the impact of OPC on a range of continuous control tasks and compare it to the current state-of-the-art model-based and model-free RL algorithms MBPO (Janner et al., 2019) and SAC (Haarnoja et al., 2018).",,,1,related
", 2019), robotics (Kalashnikov et al., 2018) and neural architecture search (Zoph & Le, 2016). All of these model-free approaches rely on large numbers of interactions with the environment to ensure successful learning. While this issue is less severe for environments that can easily be simulated, it limits the applicability of model-free RL to (real-world) domains where data is scarce. Model-based RL (MBRL) aims to reduce the amount of data required for policy optimization by learning a model that approximates the true environment, which we can use to generate cheap, simulated state transitions (Sutton, 1990; Racanière et al., 2017; Moerland et al., 2020). While early approaches on low-dimensional tasks by Schneider (1997); Deisenroth & Rasmussen (2011) used probabilistic models with closed-form posteriors, recent methods rely on neural networks to scale to more complex tasks on discrete (Kaiser et al.",,,0,not_related
"Consequently, for prediction horizons shorter than the effective horizon encoded in the discount factor, H   γ/(1 − γ), the on-policy error term grows as O(min(H/(1 − γ), H2)), c.f., (Janner et al., 2019) and Appendix A.3.",,,1,related
"For the dynamics model p̃model, we follow Chua et al. (2018); Janner et al. (2019) and use a probabilistic ensemble of neural networks, where each head predicts a Gaussian distribution over the next state and reward.",,,1,related
The results for all baselines were obtained from Janner et al. (2019) via personal communication.,,,0,not_related
"For H →∞, we obtain the original result form Janner et al. (2019) with ∑ t≥1 tγ
t = γ/(1−γ)2.",,,1,related
"SAC-RCBF, we leverage the partially learned dynamics, the reward function and the RCBF constraints to generate shorthorizon rollouts as in [35].",,,1,related
"At present the number of models used has not been discussed since MBPO, which trains seven probabilistic dynamics models of the same architecture (with different initializations), using only the top five models based on validation accuracy (referred to as “Elites” in the Evolutionary community, e.g. Mouret & Clune (2015)).",,,0,not_related
", 2020) follows the successful online RL algorithm MBPO (Janner et al., 2019) but trains inside a conservative MDP, penalizing the reward based on the maximum aleatoric uncertainty over the ensemble members.",,,0,not_related
"D), but conversely can improve performance when errors are properly managed (Janner et al., 2019; Pan et al., 2020).",,,0,not_related
"MOPO (Yu et al., 2020) follows the successful online RL algorithm MBPO (Janner et al., 2019) but trains inside a conservative MDP, penalizing the reward based on the maximum aleatoric uncertainty over the ensemble members.",,,0,not_related
"MBPO (Janner et al. 2019), state-of-the-art, model-based algorithm on OpenAI Gym.",,,0,not_related
2021) and model-based algorithms (Janner et al. 2019) advanced RL’s sample-efficiency on several benchmark tasks.,,,0,not_related
"2016), GPL-SAC outperforms both model-based (Janner et al. 2019) and model-free (Chen et al.",,,0,not_related
"Inline with the other considered state-of-the-art baselines (Chen et al. 2021; Janner et al. 2019), we use an increased ensemble size and update-to-data (UTD) ratio for the critic.",,,1,related
"Within model-based RL, recent works have achieved remarkable sample efficiency by learning large ensembles of dynamic models for better predictions (Chua et al. 2018; Wang and Ba 2019; Janner et al. 2019).",,,0,not_related
", 2020) and it is possible to train policies on model rollouts to improve data efficiency (Sutton, 1991; Janner et al., 2019).",,,0,not_related
"…two of these approaches: value gradients can be backpropagated through dynamics models to improve credit assignment (Nguyen & Widrow, 1990; Heess et al., 2015; Amos et al., 2020) and it is possible to train policies on model rollouts to improve data efficiency (Sutton, 1991; Janner et al., 2019).",,,0,not_related
"Most model-based RL methods use maximum likelihood to fit the dynamics model and then use RL to maximize the expected return under samples from that model [10, 11, 23, 24, 48].",,,0,not_related
"(Left) On the OpenAI gym benchmark [7], MnM-approx performs on par with a prior state-of-the-art method (MBPO [24]), while consistently outperforming a recent method that addresses objective mismatch (VMBPO [9]).",,,1,related
", using maximum likelihood), but apply these models by using data sampled from the learned dynamics [11, 23, 24, 48].",,,0,not_related
"Following prior work [24], we learn a neural network to predict the true environment rewards.",,,1,related
We use MBPO [24] as a baseline for model-based RL because it achieves state-of-the-art results and is a prototypical example of model-based RL algorithms that use maximum likelihood models.,,,1,related
"Figure 6: Alternative model learning objectives: Using the DClawScrewFixed-v0 task, we compare MnMapprox and MBPO [24] to two additional model learning objectives suggested in the literature, VAML [17] and value-weighted maximum likelihood [29].",,,1,related
"Encouraged by the success of MBPO, many RL methods with high UTD ratios have been proposed (Shen et al., 2020; Lai et al., 2020).",,,0,not_related
"Following Chen et al. (2021b); Janner et al. (2019), we prepared the following environments: Hopper, Walker2d, Ant, and Humanoid.",,,1,related
"Ensemble transition models: Ensembles of transition (and reward) models have been introduced to model-based RL, e.g., (Chua et al., 2018; Kurutach et al., 2018; Janner et al., 2019; Shen et al., 2020; Yu et al., 2020; Lee et al., 2020; Hiraoka et al., 2021; Abraham et al., 2020).",,,0,not_related
Chen et al. (2021b) demonstrated that the sample efficiency of REDQ is equal to or even better than that of MBPO.,,,0,not_related
"Model-Based Policy Optimization (MBPO) (Janner et al., 2019) is a seminal RL method that uses a high UTD ratio of 20–40 and achieves significantly higher sample efficiency than SAC, which uses a UTD ratio of 1.",,,0,not_related
"REDQ runs 1.1 to 1.4 times faster than MBPO (Chen et al., 2021b) but is still less computationally efficient than non-ensemble-based RL methods (e.g., SAC) due to the use of large ensembles.",,,0,not_related
"For this reason, instead of dropout approaches, ensemble approaches have been used in RL with high UTD ratio settings (Chen et al., 2021b; Janner et al., 2019; Shen et al., 2020; Hiraoka et al., 2021; Lai et al., 2020).",,,0,not_related
"MBPO [5] optimises the length of model-generated rollouts to avoid compounding model errors, while Ha and Schmidhuber [28] optimise stochasticity in the imagined environment to prevent exploitation of a deterministic model.",,,0,not_related
"Indeed, Yang et al. [33] find that MBPO and MVE [20] perform poorly in goal-based sparse-reward tasks.",,,0,not_related
"Examples of these neural network dynamic models (NNDM) include virtual world models in video 34 games or dynamic models of a complicated integrated robot, etc [7, 8].",,,0,not_related
"Recent work has shown that model-based RL algorithms can be a magnitude more dataefficient on some problems [5, 6, 7, 8, 9, 10].",,,0,not_related
", 2020) that follows MBPO (Janner et al., 2019) with additional reward penalties and MBOP (Argenson and Dulac-Arnold, 2020) that learns an offline model to perform online planning.",,,0,not_related
"MOPO (Yu et al., 2020) follows MBPO Janner et al. (2019) with additional reward penalty on unreliable model-generated transitions.",,,0,not_related
"We also compare against model-based approaches including MOPO (Yu et al., 2020) that follows MBPO (Janner et al., 2019) with additional reward penalties and MBOP (Argenson and Dulac-Arnold, 2020) that learns an offline model to perform online planning.",,,1,related
"Thus, another important direction for future work would be to reduce sample complexity to increase the feasibility of real robot training; perhaps achievable via a model-based reinforcement learning approach [18,33].",,,0,not_related
"Model-based RL methods, which explicitly learn a model of their environment, have been proposed to further improve sample complexity [17,18,19], and have seen success in real robot settings (e.",,,0,not_related
"Prior work either learns forward models using random policies [14] or update and refine the dynamics model incrementally as the policy is being optimized for the task [10], [15], [16], [17], [18].",,,0,not_related
"using imagined rollouts from the model’s predictions [18], [16], [38], and ii) model-based planning methods which generally use online planning algorithms with the model to choose optimal actions in a model predictive control (MPC) loop [10], [15].",,,0,not_related
We hypothesize that this is because the complexity of the physical system exceeds the expressive power of the model network in MBPO.,,,1,related
"We observed that around the 100th epoch, the losses for the Q function of the MBPO method increased a lot.",,,1,related
"For example, in MBPO (Janner et al., 2019), the policy network is updated using the gradients of the critic:",,,0,not_related
"5 that shows our MuJoCo Ant experiment results, we observe an even faster convergence than MBPO, while MBPO already outperforms most of the RL algorithms.",,,1,related
"MBPO works well in easy scenarios with up to 3 links, but its performance degrades starting at 4 links.",,,0,not_related
"As mentioned in the main text, the performance of MBPO degrades while ours does not.",,,0,not_related
"Other than that, everything is the same as in MBPO.",,,0,not_related
"For example, in MBPO (Janner et al., 2019), the policy network is updated using the gradients of the critic:
Lµ = −Q(s, µ(s)) + Z, (12)
where Lµ is the loss for the policy network µ, Q is the value function for state-action pairs, and Z is the regularization term.",,,1,related
"We use the model-based MBPO optimizer as the main baseline (Janner et al., 2019).",,,1,related
MBPO does not attain satisfactory results for 6- and 7-link systems.,,,0,not_related
"Based on previous works [11], [19], [38], we have the following Lemma to build the lower bound of the discrepancy of the total returns from the true model and the learned model in conventional model-based RL:",,,1,related
"From the theoretical perspective, previous works [11], [19]–[21] have provided general frameworks for analysing model-based RL, which include monotonic improvement guarantees.",,,0,not_related
"The dependency of two policies π and πD is measured by the TVD π = DTV(π(a|s)|πD(a|s)), and is bounded by a constant δπ , where DTV(π(a|s)|πD(a|s)) ≤ δπ .",,,0,not_related
"Related research topics References Support federated training Support decision-making Sample efficiency Federated reinforcement learning [7], [9], [10], [15], [16] X X low Model-based reinforcement learning [11], [12], [17]–[21] × X high Federated ensemble distillation [22], [23] X × high Federated ensemble model-based reinforcement learning our work X X high",,,0,not_related
"As long as we improve the returns under the learned model by more than B, we can guarantee improvement under the environment [11].",,,1,related
"One assumes that the dynamics model is a complex probability distribution, and measures the distance using Total Variation Distance (TVD) [11].",,,0,not_related
"Since TVD requires weaker assumptions and is typically more practical than 1-Wasserstein distance, we use TVD in our analysis.",,,1,related
"The generalisation error is measured by the TVD, defined as m := DTV(T̂ (·|s, a)|T (·|s, a)) =
Algorithm 1 FEMRL running on K clients (indexed by k) for E epochs, each consisting of Tc rounds of federated communication and G steps of policy update.",,,1,related
"Compared to model-free methods, model-based RL [11]– [13] is much more sample efficient.",,,0,not_related
"There are three key factors affecting the maximal value of generalisation error m: the virtual global empirical error Ŝk(hŜk), the number of training samples m, and the sum of TVDs between the clients’ sample policies and the virtual global sample policy, Γ.
Note that, The virtual global empirical error can in principle be estimated and optimised approximately by the training loss.",,,0,not_related
"In addition, [38] uses a general measurement, Integral Probability Metric, where TVD and 1-Wasserstein distance are two special cases.",,,0,not_related
"We adapt Model-based Policy Optimization (MBPO) [29], one of the most popular model-based RL (MBRL) algorithm as our offline RL baseline.",,,1,related
Lemma B.1 of Janner et al. (2019) allows us to bound difference in the joint distribution of states and actions at a time step t by the sum of the individual errors.,,,1,related
We can leverage proofs from Janner et al. (2019) to simplify the analysis.,,,1,related
"Then, we have:
DTV (p̂(a1:N,t|st)||p(a1:N,t|st)) ≤ c(N − 1)
We can then apply Lemma B.2 of Janner et al. (2019) to bound the overall state distribution at time step t as:
DTV (p̂(st, a1:N,t)||p(st, a1:N,t)) ≤ tc(N − 1)
Next, we let p(s, a1:N ) = (1− γ) ∑T t=0 γ tp(st, a1:N,t) denote the discounted…",,,1,related
"Model-based RL methods attempt to learn an MDP and often use it for planning online [9, 14, 15, 21, 22].",,,0,not_related
"For larger problems it can be a parameterized estimate instead (Janner et al., 2019).",,,0,not_related
"However, the computation cost of rollout increases exponentially with the planning horizon to get an accurate estimate of ao∗, while in practice the compounding error of the environmental model also increases with the planning horizon [15].",,,0,not_related
"[15] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"In practice, we heuristically approximate a calibrated dynamics model by learning an ensemble of probabilistic dynamics models, following common practice in RL [Yu et al., 2020, Janner et al., 2019, Chua et al., 2018].",,,1,related
"(Interestingly, prior work shows that an ensemble of probabilistic models can still capture the error of estimating a deterministic ground-truth dynamics model [Janner et al., 2019, Chua et al., 2018].)",,,0,not_related
"To further enhance the interpretability of the predictive models and improve the robustness of the learned policies [12, 13], ensemble-based methods [14, 15] train an ensemble of models to comprehensively capture the uncertainty in the environment and have been empirically shown to obtain significant improvements in sample efficiency [5, 12, 16].",,,0,not_related
"For model-based methods, we compare against MBPO [16], which uses short-horizon model-based rollouts started from samples in the real environment; STEVE [30], which dynamically incorporates data from rollouts into value estimation rather than policy learning; and SLBO [31], a model-based algorithm with performance guarantees.",,,1,related
significantly influence the performance of later planning [13].,,,0,not_related
"…include baselines with moderate explo-
ration power: b) SLBO (Luo et al., 2018) enforces entropy regularization during TRPO updates and adding OrnsteinUhlunbeck noise while collecting samples. c) MBPO (Janner et al., 2019) uses SAC (Haarnoja et al., 2018) as the planner to encourage exploration.",,,1,related
Top Right: PC-MLP can explore strategically and thus learns much faster than other MBRL (SLBO and MBPO) baselines which rely on random exploration.,,,0,not_related
"We also include baselines with moderate explo-
ration power: b) SLBO (Luo et al., 2018) enforces entropy regularization during TRPO updates and adding OrnsteinUhlunbeck noise while collecting samples. c) MBPO (Janner et al., 2019) uses SAC (Haarnoja et al., 2018) as the planner to encourage exploration.",,,1,related
"Here “ET” in the environment name denotes that the planner has access to the termination function, which is a common assumption in current MBRL algorithms (Janner et al., 2019; Rajeswaran et al., 2020).",,,1,related
"Recently, neural networks have been used widely for model-based RL (Gal et al., 2016; Depeweg et al., 2016; Nagabandi et al., 2018; Chua et al., 2018; Janner et al., 2019).",,,0,not_related
Theorem 1 [20] indicates that as long as we improve the returns under the model RT [π] by more than,,,1,related
"The inherent model bias in the simulation can be exploited by the optimization algorithms, leading to large reality gap between the actual and simulated policy performance [20].",,,0,not_related
"Recently, model-based reinforcement learning (RL) has achieved a lot of progress in terms of data efficiency and model estimation accuracy [22, 23, 24].",,,0,not_related
"We also make fair comparison with model-based RL baselines in Figure 3 (c) and (d), including Model-based Value Expansion (MVE) [10] and Model-based Policy Optimization (MBPO) [15].",,,1,related
Implementation details of MVE and MBPO are provided in Appendix D.,,,1,related
Model-Based Policy Optimization (MBPO) [15] proves a monotonic improvement with limited use of a predictive model.,,,0,not_related
"Existing methods [21, 15, 14, 28] have apply neural models to greatly facilitate predicting physical dynamics and the consequences of actions, and provide a strong inductive bias for generalization to novel environment situations.",,,0,not_related
"Different computational approaches have been proposed covering Gradient Descent [9], Model-based Reinforcement Learning (RL) [4], [5], explanations with recommender systems [15], etc.",,,0,not_related
"Advance in Reinforcement Learning methods have proven efficiency to solve a large scale of decision making problems [4] [5] [1], [6], [7].",,,0,not_related
"More sophisticated approaches use function approximators and minimize various statistical distances – e.g. KL (Ross & Bagnell, 2012), total-variation (Janner et al., 2019) or Wasserstein metrics (Wu et al., 2019).",,,0,not_related
", 2018) and MBPO (Janner et al., 2019) as a foundation for evaluating value-aware approaches in continuous control.",,,0,not_related
"…optimizing for auxiliary objectives (Lee et al., 2020; Nair et al., 2020; Tomar et al., 2021), augmenting model-learning with exploration strategies (Janner et al., 2019; Kidambi et al., 2020), meta-learning to closely intertwine the two objectives (Nagabandi et al., 2018) and introducing…",,,0,not_related
"KL (Ross & Bagnell, 2012), total-variation (Janner et al., 2019) or Wasserstein metrics (Wu et al.",,,0,not_related
"We select two commonly adopted dyna-style MBRL algorithms – SLBO (Luo et al., 2018) and MBPO (Janner et al., 2019) as a foundation for evaluating value-aware approaches in continuous control.",,,1,related
"Figure 3: Evaluation on continuous control environments for value aware methods and baselines with MBPO (Janner et al., 2019), without tuning existing parameters, over 5 random seeds.",,,0,not_related
", 2018) on Swimmer-v1, Hopper-v1 and Ant-v1 environments and in conjunction with the MBPO algorithm (Janner et al., 2019) on Walker-v2 and Hopper-v2 environments.",,,0,not_related
"Next, we describe our algorithm with which we find positive results in conjunction with the SLBO algorithm (Luo et al., 2018) on Swimmer-v1, Hopper-v1 and Ant-v1 environments and in conjunction with the MBPO algorithm (Janner et al., 2019) on Walker-v2 and Hopper-v2 environments.",,,1,related
"Second, we evaluate Algorithm 2 together with our proposed and a prior value aware objective on several continuous control tasks, with two recent dyna-style MBRL algorithms – SLBO (Luo et al., 2018) and MBPO (Janner et al., 2019).",,,1,related
"In contrast, we focus on the class of MBRL methods that explicitly make predictions in the state space, allowing for simple adaptations on top of of well-known MBRL frameworks e.g. Dyna-style algorithms (Sutton, 1990) such as SLBO (Luo et al., 2018) and MBPO (Janner et al., 2019).",,,1,related
"We empirically test our proposed algorithm and novel upper bound on two recent dyna-style MBRL algorithms – SLBO (Luo et al., 2018) and MBPO (Janner et al., 2019).",,,1,related
", 2021), augmenting model-learning with exploration strategies (Janner et al., 2019; Kidambi et al., 2020), meta-learning to closely intertwine the two objectives (Nagabandi et al.",,,0,not_related
"(12) Taking the cumulative reward subjected to a policy in the actual environment as and its counterpart in the constructed virtual environment model as , we can achieve the relationship between and within rollout steps as [28]:",,,1,related
"Some representative examples of Dyna-style algorithms include MBPO [31], METRPO [44], PAL/MAL [30], and Dreamer [32].",,,0,not_related
"We learn the policy and Q-function using MBPO [31] (which itself uses SAC [38] internally), similar to MOPO.",,,1,related
"We learn a policy to solve this optimization using SAC [38], resulting in an algorithm that is similar to a behavior regularized version of Dyna [39] and MBPO [31].",,,1,related
"Recently, MBRL algorithms have demonstrated strong results in a variety of RL tasks [30, 31, 32, 33], including offline RL [20, 21, 22].",,,0,not_related
"For example, there are online RL algorithms that alternate between data collection steps using a fixed policy, and policy improvement steps by learning on the collected dataset (Janner et al., 2019).",,,0,not_related
"Our contributions can be summarized as follows:
• We propose OMD, an end-to-end MBRL method that optimizes expected returns directly.",,,1,related
"In the previous section, we have empirically demonstrated that OMD outperforms Dyna-style (Sutton, 1991) MBRL agents when the model capacity is limited.",,,1,related
"Overall, these findings suggest that the OMD agent achieves near-optimal returns, performs better than the MLE-based MBRL agent as well as VEP under model misspecification, and learns a model that is useful for control despite having low likelihood.",,,0,not_related
"A standard MBRL agent first estimates the transition parameters and the reward function of a Markov Decision Process and then uses the approximate model for planning (Theil, 1957; Kurano, 1972; Mandl, 1974; Georgin, 1978; Borkar & Varaiya, 1979; Hernández-Lerma & Marcus, 1985; Manfred, 1987; Sato et al., 1988).",,,0,not_related
"The experiments reflect the challenges an MBRL agent will face in complex domains such as (Bellemare et al., 2013; Beattie et al., 2016; Kalashnikov et al., 2021): accurately predicting the next observations can be infeasible because the underlying dynamics can be too involved and there might be few components that are important for taking action.",,,0,not_related
"We use the principle of value equivalence for MBRL (Grimm et al., 2020) and argue that value equivalent models are optimal solutions to (2) and (6).",,,1,related
Grimm et al. (2020) introduce the principle of value equivalence for MBRL defining two models to be equivalent if they induce the same Bellman operator.,,,0,not_related
"…separation between model learning and policy optimization is the basis for much of the work on modelbased reinforcement learning (MBRL) (Grefenstette et al., 1990; Sutton, 1991; Lin, 1992; Boots et al., 2011; Chua et al., 2018; Hafner et al., 2019; Janner et al., 2019; Kaiser et al., 2019).",,,0,not_related
The result suggests that likelihood optimization may be an unnecessary step for MBRL algorithms.,,,0,not_related
• We demonstrate that OMD outperforms likelihoodbased MBRL agents under the model misspecification in both tabular and non-tabular settings.,,,1,related
"The paper proposes optimal model design (OMD), a method for learning control-oriented models that addresses the short-
comings of likelihood-based MBRL approaches.",,,0,not_related
OMD optimizes the expected returns in an end-to-end manner and alleviates the objective mismatch of standard MBRL methods that train models using a proxy of the true RL objective.,,,1,related
This finding suggests that likelihood optimization might be an unnecessary step for MBRL.,,,0,not_related
"The conceptual separation between model learning and policy optimization is the basis for much of the work on modelbased reinforcement learning (MBRL) (Grefenstette et al., 1990; Sutton, 1991; Lin, 1992; Boots et al., 2011; Chua et al., 2018; Hafner et al., 2019; Janner et al., 2019; Kaiser et al., 2019).",,,0,not_related
"The MLE agent trains the model with MSE effectively becoming the MBPO algorithm (Janner et al., 2019) without having an ensemble of models and learning
the variance of the predictions.",,,0,not_related
"Several works (Skelton, 1989; Joseph et al., 2013; Lambert et al., 2020) have pointed out on the objective mismatch in MBRL and demonstrated that optimization of model likelihood might be unrelated to optimization of the returns achieved by the agent that uses the model.",,,0,not_related
"…impose a great amount of inductive bias (fundamental assumptions about the output function), but as we know incorrect bias can be detrimental to decision-making agents and fully modeling every contributing process can be costly and laborious (Dullerud and Paganini (2013); Janner et al. (2019)).",,,0,not_related
"Though model-based methods have been applied to the humanoid task, prior works tend to keep the horizon
intentionally short to prevent the accumulation of model errors (Janner et al., 2019; Amos et al., 2021).",,,0,not_related
"intentionally short to prevent the accumulation of model errors (Janner et al., 2019; Amos et al., 2021).",,,0,not_related
"It is healthy to point out the limitations of this work as well as some interesting future research directions: • This paper does not solve the “planning horizon dilemma”, a fundamental issue of error accumulation of tree search expansion using imperfect models [23].",,,0,not_related
"Figure 1 illustrates the last idea, which has received much attention recently (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"The follow-up work (Janner et al., 2019) extended PETS with policy learning.",,,0,not_related
"…et al., 2017; Ruiz et al., 2019), iii) randomizing trajectory synthesis (Andrychowicz et al., 2017; Lu et al., 2019), iv) selecting more diverse data contributors (Stasaski et al., 2020), and v) sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"Different from (Janner et al., 2019), our method is designed for dialogue agents’ discrete action space.",,,0,not_related
"In our method, each training trajectory has an overlap much larger than (Janner et al., 2019) has with the expert trajectory.",,,0,not_related
", 2020), and sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"…parameters (Tobin et al., 2017; Ruiz et al., 2019), randomizing trajectory synthesis (Andrychowicz et al., 2017; Lu et al., 2019), selecting more diverse data contributors (Stasaski et al., 2020), and sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
", 2020), and v) sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"Furthermore, TD-learning is the dominant paradigm in RL for sample efficiency, and also features prominently as a sub-routine in many model-based RL algorithms (Sutton, 1990; Janner et al., 2019).",,,0,not_related
"TRPO [9] and MBPO [11] utilize Bmodel, the compounding error inheriting from model-bias would propagate during agent training.",,,0,not_related
"For example, Dyna-style methods [9], [11], [15] use the models to generate augmented samples.",,,0,not_related
"In our comparisons, we compare to SAC [20] and MBPO [11], which represent the state-of-the-art in both model-free and model-based RL.",,,1,related
"In our experiments, MEEE consistently improves the performance of state-of-the-art RL methods and outperforms baselines, including SAC [20] and MBPO [11].",,,1,related
This error propagation is shown to cause convergence and divergence between the optimal policies under the true dynamic P and the model-ensemble Pφ [11].,,,0,not_related
"deep neural networks [12], [15], [16]) or even non-parametric models (e.",,,0,not_related
"deep neural networks ([12], [15], [16])).",,,0,not_related
"Finally, the recently used probabilistic model ensembles [15], [16] allowed model-based methods to achieve the same asymptotic performance as state-of-the-art model-free methods, with higher sample efficiency.",,,0,not_related
"In probabilistic model-based RL algorithms, this corresponds to the policy improvement step, where the agent seeks to find the best control given a probabilistic model π , using a policy search method [10], [12], [16] or MPC [13], [15] or other methods.",,,0,not_related
"In BRL algorithms [10], [12], [13], [15], [16], [14], the probability distribution, which is built upon the data collected while exploring the partially unknown environment, is updated when new experience is gained.",,,0,not_related
"We emphasize that the performance difference below measures the difference in returns in the true MDPM, rather than, as is commonly seen in model-based RL [23], the difference in the latent MDP defined byRZ ,PZ .",,,1,related
"However, as the agent encounters contexts with structural similarities with respect to previously-encounters ones (around time step 160k), MBCD’s performance becomes near-optimal: it rapidly identifies whenever a context change has occurred and deploys an appropriate policy.3 MBPO and SAC, on the other hand, suffer from negative transfer due to learning average policies or dynamics models.",,,0,not_related
2 Notice that this is similar to the procedure used by the Model-Based Policy Optimization (MBPO) algorithm [11].,,,0,not_related
"In particular, following recent work on model-based RL [4, 11], MBCD models the dynamics of a given environmentMz , pθz (St+1, Rt |St , At ), via a bootstrap ensemble of probabilistic neural networks whose outputs parameterize a multivariate Gaussian distribution with diagonal covariance matrix.",,,0,not_related
"All rollouts are stored in a buffer, 𝐷𝑚𝑜𝑑𝑒𝑙 .2 Notice that this is similar to the procedure used by the Model-Based Policy Optimization (MBPO) algorithm [11].",,,1,related
"5 compares MBCD, MBPO, and SAC in the fully-online setting—no pre-training phase is allowed.",,,0,not_related
"To do this, we compare MBCD, MBPO, SAC, ReBAL, and GrBAL, in the nonstationary continuous particle maze domain, where the sources of non-stationarity are as discussed earlier.",,,1,related
"In particular, both MBCD and MBPO’s performances temporarily drop when a novel context is encountered for the first time.",,,0,not_related
"MBCD’s performance drops because it instantiates a new dynamics model for the newly encountered context, while MBPO’s performance drops because it undergoes negative transfer.",,,0,not_related
"3 shows the cumulative sum of rewards achieved by MBCD, MBPO, SAC, and by an Oracle algorithm that is initialized with optimal policies for all contexts and that detects context changes with zero delay.",,,0,not_related
"In our setting, MBPO can be seen as a particular case of our algorithm, where a single dynamics model and policy are tasked with optimizing behavior under changing
0 40 k 80 k 120 k 160 k 200 k 240 k 280 k 320 k 360 k 400 k 440 k Step
−2000
−1500
−1000
−500
0
E pi
so de
T ot
al R
ew ar
d
MBCD (ours) MBPO SAC
(a) Total reward achieved by different methods (MBCD, MBPO, and SAC) as contexts change.",,,1,related
"2a shows the total reward achieved by different methods (ours, MBPO, SAC) as contexts change.",,,0,not_related
We first evaluate ourmethod on the non-stationaryHalf-Cheetah domain and compare it with two state-of-the-art RL algorithms: MBPO [11] and SAC [8].,,,1,related
Notice that our method and MBPO have similar performances when interacting for the first time with the first three random contexts.,,,0,not_related
SAC works similarly to MBPO but does not perform Dynastyle planning steps using a learned dynamics model.,,,0,not_related
"In this paper, we expand model-based policy optimization [Janner et al., 2019] into the goal-based setting and propose Universal Model-based Policy Optimization (UMPO)
for efficient goal-based policy learning.",,,1,related
"A.3 Details of Implementation The code of dynamics model is based on the realization of [Janner et al., 2019] and we modified it slightly to fix the bug that using an increasing number of video memory.",,,1,related
"Besides, the learned dynamics models are further used to generate branched short rollouts for policy optimization in a Dyna-style manner [Janner et al., 2019].",,,0,not_related
"In this paper, we expand model-based policy optimization [Janner et al., 2019] into the goal-based setting and propose Universal Model-based Policy Optimization (UMPO)",,,1,related
"[Janner et al., 2019] generate (truncated) short trajectories with a probabilistic ensemble to train the policy of a MFRL, thus improving significantly its sampling efficiency.",,,0,not_related
"Here we first investigate a k-step rollout scheme, which is a natural extension of MBPO [Janner et al., 2019] to multi-agent scenario.",,,1,related
"We use neural nets to parameterize both distributions since they are powerful function approximators that have been effective for model-based RL (Chua et al., 2018; Nagabandi et al., 2018; Janner et al., 2019).",,,1,related
"Similar to prior work (Janner et al., 2019), our baseline feedforward model outputs the mean and log variance of all state dimensions and reward simultaneously, as follows: pθ(st+1, rt+1 | st, at) = N ( μ(st, at),Diag(exp{l(st, at)}) ) , (3) where μ(st, at) ∈ R denotes the mean for the concatenation of the next state and reward, l(st, at) ∈ R denotes the log variance, and Diag(v) is an operator that creates a diagonal matrix with the main diagonal specified by the vector v.",,,1,related
"The field of model-based RL has matured in recent years to yield impressive results for both online (Nagabandi et al., 2018; Chua et al., 2018; Kurutach et al., 2018; Janner et al., 2019) and offline (Matsushima et al.",,,0,not_related
"As proof of concept for MBRL-Lib, we provide implementations for two state-of-the-art MBRL algorithms, namely, PETS [Chua et al., 2018] and MBPO [Janner et al., 2019].",,,1,related
"In MBRL, the code landscape consists mostly of a relatively limited number of specific algorithm implementations that are publicly available [Chua et al., 2018, Janner et al., 2019, Wang and Ba, 2019].",,,0,not_related
"• Use a model-free policy: In this case a model-free learner, such as SAC [Haarnoja et al., 2018], is used over the predicted dynamics, often by populating a populating a replay buffer with “imagined” trajectories obtained from the model [Gu et al., 2016, Kurutach et al., 2018, Janner et al., 2019].",,,0,not_related
"[21] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"Likewise, [27] gives some hints when to trust",,,0,not_related
"namely Trust-Region Policy Optimization (TRPO) [26], and Model-Based Policy Optimization (MBPO) [23].",,,0,not_related
"Our aim is to better understand the practical merits and limitations of the proposed approach by assessing the results in light of the following questions:
1) Can CMBPO maintain safety constraints throughout training in high-dimensional state- and action-spaces?",,,1,related
CMBPO reaches model-free asymptotic performances on all tested experiments with an increase in sample efficiency by 1-2 orders of magnitude relative to model-free methods.,,,0,not_related
"Finally, a particularly exciting avenue is the application and evaluation of CMBPO for safe control on physical robots.",,,0,not_related
"1d, [8]) rewards an agent for running along a circle with the constraint of staying within a safe corridor.
a) Comparative Evaluation: We compare CMBPO to three safe exploration algorithms, namely Constrained Policy Optimization (CPO) [8], Lagrangian Trust Region Policy Optimization (TRPO-L), Lagrangian Proximal Policy Optimization (PPO-L) [7], and two unconstrained algorithms,
namely Trust-Region Policy Optimization (TRPO) [26], and Model-Based Policy Optimization (MBPO) [23].",,,1,related
"Lastly, we point out that preliminary experiments on the recently published benchmark suite safety-gym [7] showed poor performances of CMBPO due to partial observability and strong covariances between state dimensions.",,,1,related
"We observe that the unconstrained algorithms TRPO and MBPO exceed cost constraints on all experiments, highlighting the trade-off between greedy return maximization and constraint satisfaction in our environments.",,,1,related
"The model-based baseline MBPO exceeds CMBPO’s sample efficiency considerably, an observation we mainly attribute to its high reliance on modelgenerated and off-policy samples.",,,1,related
"Compared to the safe baseline algorithms, CMBPO exhibits slightly less adherence to constant constraint satisfaction but succeeds in reaching safe policies asymptotically.",,,0,not_related
A comprehensive list of the hyperparameters for CMBPO in our experiments is given in Table II.,,,1,related
propose a branched rollout scheme where short model- trajectories are started from previously observed off-policy states [23].,,,1,related
"[23], where starting states are sampled from an off-policy buffer, thus allowing coverage of the whole task",,,0,not_related
"3 shows training curves for CMBPO compared to variants with a constant real-tomodel sample ratio α, and a rollout schedule with fixed horizons.",,,0,not_related
"To our surprise, we find that CMBPO in some cases exceeds the asymptotic performance of model-free optimizers, which we suspect may be caused by exploration through temporary constraint violation or lower-variance gradients due to only performing expected state-transitions in model-trajectories.
b) Ablation on Sample Mixing and Adaptive Rollouts: Our ablation studies are aimed to better our understanding of the influence of sample mixing and adaptive rollouts on CMBPO’s performance.",,,1,related
"We evaluate the efficacy of our algorithm, labeled Constrained Model-Based Policy Optimization (CMBPO), on several simulated high-dimensional robot locomotion tasks with continuous state- and action spaces.",,,1,related
"In model-based reinforcement learning an unrolled one-step model would struggle with compounding errors (Janner et al., 2019).",,,0,not_related
"…et al. 2018a,b)1, TD3 (Fujimoto, Hoof, and Meger 2018), ME-TRPO (Kurutach et al. 2018), MB-MPO(Clavera et al. 2018), PETS (Chua et al. 2018), MBPO (Janner et al. 2019)
1We select the PyTorch implement of soft actor-critic in https://github.com/pranz24/pytorch-soft-actor-critic to evaluate the…",,,0,not_related
"Compared with MBPO, ReW-PE-SAC is better on four environments and is slightly weaker in the tasks of HalfCheetah and Hopper.",,,0,not_related
We reproduce results from (Wang et al. 2019; Janner et al. 2019) and additionally run MBPO on the tasks of Slimhumanoid and Swimmer as the according experimental results are absent.,,,1,related
"As shown in Table 1, ReW-PE-SAC achieves better performance compared with all other state-of-the-art algorithms except MBPO running with 200, 000 time-steps in all the environments.",,,0,not_related
(Janner et al. 2019) replaces model-generated rollouts begin from the initial state distribution with short model-generated rollouts branched from the real data.,,,0,not_related
", 2018), MBPO (Janner et al., 2019) and POPLIN (Wang and Ba, 2019).",,,0,not_related
"We reproduce results from (Wang et al., 2019; Janner et al., 2019) and additionally run MBPO on the tasks of Slimhumanoid and Swimmer as the according experimental results are absent.",,,1,related
"(Janner et al., 2019) replaces model-generated rollouts begin from the initial state distribution with short model-generated rollouts branched from the real data.",,,0,not_related
"We compare ReW-PE-SAC with state-of-the-art model-free and model-based RL methods, including SAC (Haarnoja et al. 2018a,b)1, TD3 (Fujimoto, Hoof, and Meger 2018), ME-TRPO (Kurutach et al. 2018), MB-MPO(Clavera et al. 2018), PETS (Chua et al. 2018), MBPO (Janner et al. 2019)
1We select the PyTorch implement of soft actor-critic in https://github.com/pranz24/pytorch-soft-actor-critic to evaluate the performance.",,,1,related
"Long horizon prediction is a commonly used task to test the quality of learned dynamical models (Sanchez-Gonzalez et al., 2018; Lutter et al., 2019; Greydanus et al., 2019; Miles et al., 2020; Janner et al., 2019).",,,0,not_related
"Feed-forward networks are commonly used model classes to parametrize these forward models (Janner et al., 2019).",,,0,not_related
"Our models are deep neural networks trained to maximize the log likelihood of the next state and reward given the current state and action, similar to models from successful model-based RL algorithms (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"[1], [2], [3], [4]), robotic performance remains decidedly sub-human in almost all scenarios.",,,0,not_related
"RNN’s are worse than MLP’s on Hard model rollouts, in spite of their superior performance on single-step predictions.",,,0,not_related
"6: We plot the performance of our optimized MLP networks on long-term prediction
of position and orientation with 95% confidence intervals.",,,1,related
"5: The structure of our RNN predictors. φ is a recurrent unit (GRU), while φdec is an
MLP decoder.",,,1,related
"To make a fair comparison with our RNN’s of history-length h = 16, our MLP rollout experiments also start from the 16th time-step.",,,1,related
"The first, and most elementary, is to pick h = 1 and map xt to xt+1 with a simple multilayer perceptron (MLP) ([2], [4]).",,,1,related
"For each MLP and RNN architecture, we tried different target variables and sweep over different values of learning-rate, hidden-layer size, and weight-decay, centered around hand-tuned values.",,,1,related
"Each model is an MLP with input żt , two hidden layers of width 128, and output żt+1.",,,0,not_related
"[1], [2], [3], [4], [7]) follow the same fundamental approach: fitting a neural network approximation of the system dynamics (3) directly to data.",,,0,not_related
"While the history-length is 1 for MLPs, for RNNs we also tried different history-lengths.",,,1,related
"APPENDIX III LEARNING DETAILS
Our MLPs consist of 4 hidden fully-connected layers with ReLU activations, plus a final linear layer.",,,0,not_related
"While MLP’s and RNN’s had similar training error and generalization error trends, the long-term prediction error was noticeably different (see Fig.",,,0,not_related
"Another specific model-based off-policy subclass recovers MBPO (Janner et al., 2019) which uses SAC (Haarnoja et al., 2019) internally, but SAC can be replaced by either DDPG (Lillicrap et al., 2016) or TD3 (Fujimoto et al., 2018), as all of those model-free off-policy algorithms are provided by…",,,0,not_related
"Another specific model-based off-policy subclass recovers MBPO (Janner et al., 2019) which uses SAC (Haarnoja et al.",,,1,related
"Another specific model-based off-policy subclass recovers MBPO (Janner et al., 2019) which uses SAC (Haarnoja et al., 2019) internally, but SAC can be replaced by either DDPG (Lillicrap et al., 2016) or TD3 (Fujimoto et al., 2018), as all of those model-free off-policy algorithms are provided by TF-Agents.",,,0,not_related
"This enables e.g. to systematically compare Bellman’s PETS, ME-TRPO, MBPO and TRPO implementations against PPO, SAC, DDPG and TD3 from TF-Agents.",,,0,not_related
"One approach to prevent such problems is to not rely on the learned models entirely [1], [2], rather use them as close approximations of the system’s behaviour, and at the same time design/learn a controller that is both robust to the model",,,0,not_related
"In all environments MoPAC and MBPO outperform both SAC, evincing the advantage of using model rollouts, and MBRL, that strongly suffers from poor exploration in the use of the learned model.",,,0,not_related
MBPO [14] uses branched rollouts in an actor-critic setting to exploit the learned dynamics for modelbased policy optimization.,,,0,not_related
"known regions of the learned dynamics without exploring outwards to new, unrevealed states [12]–[14].",,,0,not_related
"interactions per epoch, against the baselines SAC [25], MBPO [14], and MBRL [10].",,,0,not_related
"We compare the average return of MoPAC over 5 trials, consisting of 1, 000 true environment interactions per epoch, against the baselines SAC [25], MBPO [14], and MBRL [10].",,,1,related
"Here, SAC and MBPO are trained according to the hyperparameter settings provided by their respective works, while for MBRL we use the same settings as MoPAC, namely using horizons 5− 15 with linear annealing for all tasks.",,,1,related
"The Ant-v2 and Hopper-v2 are more challenging tasks, as they require more interactions with the environment to learn
their dynamics; in Hopper-v2 we observe a speedup in learning and convergence, while in Ant-v2 MoPAC learns faster in the first epochs and ends up with slightly better performance than MBPO.",,,0,not_related
"A batch size of 10, 000 is chosen for the model rollouts in both MoPAC and MBPO.",,,0,not_related
"[33] proposes an extension to MBPO, by performing model rollouts of specific horizons, while optimizing the policy objective with back-propagation through time.",,,0,not_related
"Penta-Valve Round-Valve
Finger Gaiting
(a)
(b)
time
MoPAC (ours)
MBPO
known regions of the learned dynamics without exploring outwards to new, unrevealed states [12]–[14].",,,0,not_related
"Notably, MoPAC learns faster than MBPO.",,,0,not_related
"We further underscore the efficacy of MoPAC by comparing our algorithm with SAC and MBPO on a Yale Openhand Model Q [41], [42] through two different manipulation tasks– valve rotation and finger gaiting.",,,1,related
"Model-based monotonic improvement, as proven by [14], can be achieved when learning the dynamics model together with the policy.",,,0,not_related
"Concurrently, some recent work has integrated policy networks
with deep models (Janner et al., 2019; Wang & Ba, 2019) in spirit of Dyna (Sutton, 1990).",,,0,not_related
"Recent techniques attempt to avoid compounding model error by restricting the number of model unrolling steps (Janner et al., 2019; Feinberg et al., 2018), but this creates a trade-off between planning performance and sample efficiency.",,,0,not_related
"173 Concurrently, some recent work has integrated policy networks with deep models [16, 37] in spirit 174 of Dyna [34].",,,0,not_related
"18 Recent techniques attempt to avoid compounding model error by restricting the number of model 19 unrolling steps [16, 11], but this creates a trade-off between planning performance and sample 20 efficiency.",,,0,not_related
"…use of neural network function approximators, spurring new algorithmic developments in both model-free (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Gu et al., 2016b; 2017; Haarnoja et al., 2018) and model-based (Chua et al., 2018; Janner et al., 2019; Hafner et al., 2020a) RL.",,,0,not_related
"As for the cases with 1,000 dimensions (Figure 12), RAMCO (Random) shows performance similar to PETS, which are still better than the model-based methodMBMF andMBPO.",,,0,not_related
"MBPO uses its official implementation and PETS uses the implementation from (Vuong, 2020).",,,0,not_related
"• Model-Based Policy Optimization (MBPO) (Nagabandi et al., 2018): MBPO uses a probabilistic dynamics model to generate additional data to a replay buffer for training a SAC-based model-free agent.",,,0,not_related
"Recently proposed Dyna-style methods including Model-Based Acceleration (MBA) (Gu et al., 2016), Model-Based Value Expansion (MVE) (Feinberg et al., 2018), Model-Based Policy Optimization (MBPO) (Janner et al., 2019) and so on.",,,0,not_related
"As we can see, while the dimensionality of the environment increases, model-based methods (including PETS and MBMF, and hybrid methods like MBPO) begin to suffer from model bias and hence produce much worse results than model-free methods.",,,1,related
"However, RAMCO based on PPO-generated training data still shows a jump in its training curve after the warm-up phase, which makes it outperforms other state-of-theart model-free methods including PPO, DDPG and SAC and
model-based methods including PETS, MBMF and MBPO.",,,0,not_related
"RAMCO (Random) shows performance similar to PETS, which are still better than the model-based method MBMF and hybrid method MBPO.",,,0,not_related
The experiment of the MBPO method shows that it can obtain better sample efficiency than prior model-based methods and asymptotic performance of the state-of-the-art model-free algorithms.,,,0,not_related
"MBPO can be regarded as an SAC algorithm with a dynamics model added to it, and hence an evident advantage over SAC and other model-free methods.",,,0,not_related
"However, RAMCO based on PPO-generated training data still outperforms other state-of-the-art model-free methods including PPO, DDPG and SAC andmodel-based methods including PETS, MBMF and MBPO after the warm-up phase.",,,0,not_related
", 2018), Model-Based Policy Optimization (MBPO) (Janner et al., 2019) and so on.",,,0,not_related
"[26]): "" # ∞ Õ � ∗ = argmax E� � � � (�� , �� ) (1)",,,1,related
"RL algorithm: Multi-Agent Model-Based Policy Optimization (MAMBPO), which is a multi-agent adaptation of the Model-Based Policy Optimization (MBPO) algorithm [2].",,,0,not_related
"The degree of the improvement varies per domain, as is also the case for the single-agent MBPO [2].",,,0,not_related
"data, similar to the original MBPO implementation [2].",,,0,not_related
"a) Model-Based Learning through MBPO: To learn a world model and generate experience to train the policy on, we adapt the single-agent MBPO algorithm [2] to be suitable for multi-agent domains.",,,1,related
"Such models have resulted in algorithms that are comparable to state-of-the-art model-free algorithms in terms of asymptotic performance, but can learn with up to an order of magnitude greater sample efficiency [2].",,,0,not_related
"However, model-based methods encounter with model bias caused by the difference between the trained model and real environment [9], especially when the environment has high-dimensional states and complex dynamics.",,,0,not_related
"However, model-based methods are limited by model bias, as previous work [9] addressed.",,,0,not_related
We also compare against model-based offline RL algorithms including MOPO (Yu et al. 2020) that follows MBPO (Janner et al. 2019) with additional reward penalties.,,,1,related
MOPO (Yu et al. 2020) extends MBPO (Janner et al. 2019) with an additional reward penalty on generated transitions with large variance from the learned dynamic model.,,,0,not_related
2020) that follows MBPO (Janner et al. 2019) with additional reward penalties.,,,0,not_related
2020) extends MBPO (Janner et al. 2019) with an additional reward penalty on generated transitions with large variance from the learned dynamic model.,,,0,not_related
"Similarly, Janner et al. (2019) used short model-generated rollouts branched from real data for both value and policy learning, matching the asymptotic performance of the best model-free methods [14].",,,0,not_related
"(2019) used short model-generated rollouts branched from real data for both value and policy learning, matching the asymptotic performance of the best model-free methods [14].",,,0,not_related
Related research works are Chua et al. (2018); Janner et al. (2019); Luo et al. (2019); Munos & Szepesvári (2008).,,,0,not_related
", 2020] and MBPO [Janner et al., 2019], as well as the standard baselines of SAC [Haarnoja et al.",,,0,not_related
"We compared GELATO to contemporary model-based offline RL approaches; namely, MOPO [Yu et al., 2020] and MBPO [Janner et al., 2019], as well as the standard baselines of SAC [Haarnoja et al., 2018] and imitation (behavioral cloning, Bain and Sammut [1995], Ross et al. [2011]).",,,0,not_related
"ed RL has seen several advances (Sutton,1990;Li and Todorov,2004;Deisenroth and Rasmussen,2011) including ones based on deep learning (e.g.,Lampe and Riedmiller(2014);Gu et al.(2016);Luo et al.(2018);Janner et al. (2019);Lowrey et al.(2019);Wang et al.(2019)). Given MobILE’s modularity, these advances in model-based RL can be used to design improved algorithms for the ILFO problem. MobILE bears parallels to provably ",,,0,not_related
"[24] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"We compare the difference in performance to SACSVG when the horizon length is varied (see MBPO environments in Table 1) and then compare the performance of our method against multiple model based methods including PETS (Chua et al., 2018), POPLIN (Wang & Ba, 2019), METRPO (Kurutach et al., 2018), and the model free SAC (Haarnoja et al., 2018) algorithm (see POPLIN environments in Table 1).",,,1,related
"…2017; Chua et al., 2018; Nagabandi et al., 2018), 2) to improve estimates of the Q value by rolling out the model for a small number of steps (Feinberg et al., 2018; Amos et al., 2020) and 3) to provide synthetic data samples for a model-free learner (Janner et al., 2019; Kurutach et al., 2018).",,,0,not_related
"On the other hand, the MBPO based environments refer to the ones used by the paper (Janner et al., 2019) and largely correspond to the ‘-v2’ versions from OpenAI Gym.",,,1,related
"Invariant MBRL performance on four MuJoCo based domains from POPLIN (Wang & Ba, 2019) (left) and five MuJoCo based domains from MBPO (Janner et al., 2019) (right).",,,0,not_related
", 2020) and 3) to provide synthetic data samples for a model-free learner (Janner et al., 2019; Kurutach et al., 2018).",,,0,not_related
"While any RL or planning algorithm can be used to learn the optimal policy for M̂, we focus specifically on MBPO [20, 57] which was used in MOPO.",,,1,related
"Subsequently, it employs an actor-critic method where the value function is learned using both the offline dataset as well as synthetically generated data from the model, similar to Dyna [57] and a number of recent methods [20, 67, 7, 48].",,,0,not_related
"In high-dimensional image-based domains, which we use to answer question (3), we compare to LOMPO [48], which is a latent space offline model-based RL method that handles image inputs, latent space MBPO (denoted LMBPO), similar to Janner et al. [20] which uses the model to generate additional synthetic data, the fully offline version of SLAC [32] (denoted SLAC-off), which only uses a variational model for state representation purposes, and CQL from image inputs.",,,1,related
"Specifically, at each iteration, MBPO performs k-step rollouts using T̂ starting from state s ∈ D with a particular rollout policy µ(a|s), adds the model-generated data to Dmodel, and optimizes the policy with a batch of data sampled from D ∪Dmodel where each datapoint in the batch is drawn from D with probability f ∈ [0, 1] and Dmodel with probability 1− f .",,,1,related
"[20] which uses the model to generate additional synthetic data, the fully offline version of SLAC [32] (denoted SLAC-off), which only uses a variational model for state representation purposes, and CQL from image inputs.",,,0,not_related
"To highlight the distinction between COMBO and a naïve combination of CQL and MBPO, we perform such a comparison in Table 8 in Appendix C.",,,1,related
"MBPO follows the standard structure of actor-critic algorithms, but in each iteration uses an augmented dataset D ∪Dmodel for policy evaluation.",,,0,not_related
"Furthermore, one can use the transition model alongside a reward model to generate offline data to improve value function learning (Sutton, 1991; Janner et al., 2019).",,,0,not_related
"These approaches fall into two camps: using models to extract a policy in a Dynastyle approach (Sutton, 1991; Janner et al., 2019; Sutton et al., 2008; Yao et al., 2009; Kaiser et al., 2020), or incorporating the model in a planning loop, i.",,,0,not_related
"Offline model-based reinforcement learning Modelbased methods have shown promise by facilitating better generalization (Janner et al., 2019).",,,0,not_related
"In model-based RL methods (Wang et al., 2019; Schrittwieser et al., 2020; Janner et al., 2019; Wang & Ba, 2019; Kaiser et al., 2019; Luo et al., 2018; Deisenroth & Rasmussen, 2011), the transition dynamics or simulator is learnt and subsequently utilized for policy learning.",,,0,not_related
"It has been exhibited (Janner et al., 2019; Yu et al., 2020; Levine et al., 2020) that certain model-based methods that were originally targeted for the online setting can potentially still deliver reasonable performance with offline data and minimal algorithmic change.",,,0,not_related
"Further, there has been some work showing the success of online model-based RL approaches with offline data, with minimal change in the algorithm (Janner et al., 2019; Yu et al., 2020).",,,0,not_related
", 2016; 2015; 2017; Ge, Ma, 2020; Lee et al., 2016)). In RL, local maxima can often be global as well for many cases (Agarwal et al., 2020b).2 Zero-order optimization or policy gradient algorithms can converge to local maxima and become natural potential competitors. They are widely believed to be less sampleefficient than the model-based approach because the latter can leverage the extrapolation power of the parameterized models. Theoretically, our formulation aims to characterize this phenomenon with results showing that the model-based approach’s sample complexity mostly depends (polynomially) on the complexity of the model class, whereas policy gradient algorithms’ sample complexity polynomially depend on the dimensionality of policy parameters (in RL) or actions (in bandit). Our technical goal is to answer the following question: Can we design algorithms that converge to approximate local maxima with sample complexities that depend only and polynomially on the complexity measure of the dynamics/reward class? We note that this question is open even if the dynamics hypothesis class is finite, and the complexity measure is the logarithm of its size. The question is also open even for nonlinear bandit problems (where dynamics class is replaced by reward function class), with which we start our research. We consider first nonlinear bandit with deterministic reward where the reward function is given by η(θ, a) for action a ∈ A under instance θ ∈ Θ. We use sequential Rademacher complexity (Rakhlin et al., 2015a;b) to capture the complexity of the reward function η. Our main result for nonlinear bandit is stated as follows. Theorem 1.1 (Informal version of Theorem 3.1). Suppose the sequential Rademacher complexity of a loss function class (defined later) induced by the reward function class {η(θ, ·) : θ ∈ Θ} is bounded by √ R(Θ)Tpolylog(T ). Then, there exists an algorithm (ViOlin, Alg. 1) that finds an -approximate local maximum with Õ(R(Θ) −8) samples. In contrast to zero-order optimization, which does not use the parameterization of η and has a sample complexity depending on the action dimension, our bound only depends on the complexity of the reward function class. This suggests that our algorithm exploits the extrapolation power of the reward function class. To the best of our knowledge, this is the first action-dimension-free result for both linear and nonlinear bandit problems. More concretely, we instantiate our theorem to the following settings and get new results (2)The all-local-maxima-are-global condition only needs to hold to the ground-truth total expected reward function. This potentially can allow disentangled assumptions on the ground-truth instance and the hypothesis class. that leverage the model complexity (more in Section A.1). 1. Linear bandit with finite parameter space Θ. Because η is concave in action a, our result leads to a sample complexity O(poly(log|Θ|, 1/ )) for finding an approximate optimal action. In this case both zero-order optimization and the SquareCB algorithm in Foster, Rakhlin (2020) have sample complexity/regret that depend on the dimension of action space dA. 2. Linear bandit with s-sparse or structured instance parameters. Our algorithm ViOlin achieves an O(poly(s, 1/ )) sample complexity when the instance/model parameter is s-sparse and the reward is deterministic. The sample complexity of zero-order optimization depends polynomially on dA. Carpentier, Munos (2012) achieve a stronger Õ(s √ T ) regret bound for s-sparse linear bandits with actions set A = Sd−1.",,,0,not_related
", 2016; 2015; 2017; Ge, Ma, 2020; Lee et al., 2016)). In RL, local maxima can often be global as well for many cases (Agarwal et al., 2020b).2 Zero-order optimization or policy gradient algorithms can converge to local maxima and become natural potential competitors. They are widely believed to be less sampleefficient than the model-based approach because the latter can leverage the extrapolation power of the parameterized models. Theoretically, our formulation aims to characterize this phenomenon with results showing that the model-based approach’s sample complexity mostly depends (polynomially) on the complexity of the model class, whereas policy gradient algorithms’ sample complexity polynomially depend on the dimensionality of policy parameters (in RL) or actions (in bandit). Our technical goal is to answer the following question: Can we design algorithms that converge to approximate local maxima with sample complexities that depend only and polynomially on the complexity measure of the dynamics/reward class? We note that this question is open even if the dynamics hypothesis class is finite, and the complexity measure is the logarithm of its size. The question is also open even for nonlinear bandit problems (where dynamics class is replaced by reward function class), with which we start our research. We consider first nonlinear bandit with deterministic reward where the reward function is given by η(θ, a) for action a ∈ A under instance θ ∈ Θ. We use sequential Rademacher complexity (Rakhlin et al., 2015a;b) to capture the complexity of the reward function η. Our main result for nonlinear bandit is stated as follows. Theorem 1.1 (Informal version of Theorem 3.1). Suppose the sequential Rademacher complexity of a loss function class (defined later) induced by the reward function class {η(θ, ·) : θ ∈ Θ} is bounded by √ R(Θ)Tpolylog(T ). Then, there exists an algorithm (ViOlin, Alg. 1) that finds an -approximate local maximum with Õ(R(Θ) −8) samples. In contrast to zero-order optimization, which does not use the parameterization of η and has a sample complexity depending on the action dimension, our bound only depends on the complexity of the reward function class. This suggests that our algorithm exploits the extrapolation power of the reward function class. To the best of our knowledge, this is the first action-dimension-free result for both linear and nonlinear bandit problems. More concretely, we instantiate our theorem to the following settings and get new results (2)The all-local-maxima-are-global condition only needs to hold to the ground-truth total expected reward function. This potentially can allow disentangled assumptions on the ground-truth instance and the hypothesis class. that leverage the model complexity (more in Section A.1). 1. Linear bandit with finite parameter space Θ. Because η is concave in action a, our result leads to a sample complexity O(poly(log|Θ|, 1/ )) for finding an approximate optimal action. In this case both zero-order optimization and the SquareCB algorithm in Foster, Rakhlin (2020) have sample complexity/regret that depend on the dimension of action space dA.",,,0,not_related
"Although there exists various methods (Kurutach et al., 2018; Luo et al., 2018; Clavera et al., 2018; Janner et al., 2019; Rajeswaran et al., 2020) for MBRL, the common strategy is to first learn the environment model and use it to generate fictitious experiences for learning an agent’s policy.",,,0,not_related
"The solid line and shaded regions represent the mean and standard deviation, respectively, across five runs with random seeds.
original MBPO in Figure F.2.",,,0,not_related
"One can observe that the learning termination function is significantly harmful to the sample efficiency compared to the
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 TimeSteps(M)
0
500
1000
1500
2000
2500
3000
3500
4000
Ac cu
m ul
at ed
R ew
ar ds
MBPO with Termination Ftn. MBPO without Termination Ftn.
Hopper-v3
Figure F.2: Learning curves of MBPO on Hopper-v3 belonging to MuJoCo environments.",,,1,related
One can observe that learning terminal functions is harmful to sample efficiency in MBPO.,,,0,not_related
"Many prior works (Janner et al., 2019; Nagabandi et al., 2018) have proposed to combine the
Preprint.",,,0,not_related
"Many prior works (Janner et al., 2019; Nagabandi et al., 2018) have proposed to combine the",,,0,not_related
"Moreover, as we mentioned in Section D, to show that prior knowledge is crucial to MBRL, we provide an experimental result about one of the state-of-the-art MBRL methods, i.e., MBPO (Janner et al., 2019) with and without learning a termination function.",,,1,related
"This is evidenced by the plethora of literature, both model-free and model-based, that chooses SAC as the standard [3, 4, 5, 6, 7], often showing it as the best performing model-free approach.",,,0,not_related
"For MBPO, we use the default network architectures for the Q networks, policy network, and model ensembles (Janner et al., 2019).",,,1,related
"Since MBPO builds on top of a SAC agent, to make our comparisons fair, meaningful, and consistent with previous work, we make all SAC related hyperparameters exactly the same as used in the MBPO paper (Janner et al., 2019).",,,1,related
"Sampling a number of random datapoints at the start of training is also a common technique that has been used in previous model-free as well as model-based works (Haarnoja et al., 2018a; Fujimoto et al., 2018; Janner et al., 2019).",,,0,not_related
", 2019), recent methods such as MBPO combine a model ensemble with a carefully controlled rollout horizon to obtain better performance (Janner et al., 2019; Buckman et al., 2018).",,,0,not_related
"To address some of the critical issues in model-based learning (Langlois et al., 2019), recent methods such as MBPO combine a model ensemble with a carefully controlled rollout horizon to obtain better performance (Janner et al., 2019; Buckman et al., 2018).",,,0,not_related
"For example, Model-Based Policy Optimization (MBPO) (Janner et al., 2019), is a state-of-the-art model-based algorithm which updates the agent with a mix of real data from the environment and “fake” data from its model, and uses a large UTD ratio of 20-40.",,,0,not_related
"Janner et al. (2019) proposed Model-Based Policy Optimization (MBPO), which was shown to be much more sample efficient than popular model-free algorithms such as SAC and PPO for the MuJoCo environments.",,,0,not_related
"As in the MBPO paper, we train for 125K for Hopper, and 300K for the other three environments (Janner et al., 2019).",,,1,related
"…McAllister & Rasmussen, 2016; Chua et al., 2018; Amos et al., 2018; Hafner et al., 2019b; Nagabandi et al., 2018; Kahn et al., 2020; Dong et al., 2020) or policy optimization (Sutton, 1991; Weber et al., 2017; Ha & Schmidhuber, 2018; Janner et al., 2019; Wang & Ba, 2019; Hafner et al., 2019a).",,,0,not_related
"One class of algorithms trains “on-policy” model-free algorithms virtually inside the environment model [Kurutach et al., 2018, Luo et al., 2019] while the other trains “off-policy” model-free algorithms virtually [Janner et al., 2019].",,,0,not_related
", 2019] while the other trains “off-policy” model-free algorithms virtually [Janner et al., 2019].",,,0,not_related
"We evaluate an MBPO based model (Janner et al. (2019)), which also carries out policy rollouts in latent space similar to LOMPO, but does not apply an uncertainty penalty.",,,1,related
"Model-based RL algorithms have demonstrated impressive sample efficiency in interactive RL (Janner et al., 2019; Rajeswaran et al., 2020; Hafner et al., 2020).",,,0,not_related
"We decided to use a short horizon to diminish the impact of the compound error [53], the accumulation error following a wrong model, as well known in DYNA-style approaches.",,,1,related
"Alternatives as [46, 47, 53] also take these uncertainties into consideration using probabilistic ensembles.",,,0,not_related
"MBRL has shown impressive performance and sample efficiency on many robotics tasks [13], [1], [2], [14].",,,0,not_related
"One-step models will likely remain useful for other algorithms less focused on the long-term future, such as MBPO [14], or in situations where data is non-episodic, so applying the timedependant structure could be forced.",,,0,not_related
"One-step models will likely remain useful for other algorithms less focused on the long-term future, such as MBPO [14], or in",,,0,not_related
"Furthermore, as described in Figure 1, we found that SAC and MBPO fail in reset-free settings when denied access to resets if they continue gradient updates, which we attributed to gradient-based instability.",,,1,related
"In particular, it is imperative that the model not only be used
for sample-efficient policy optimization (as in MBPO), but also that the model be used to safely act in the environment.",,,0,not_related
"To illustrate this issue, we first pre-train agents to convergence in the episodic Hopper environment (Brockman et al., 2016) with state-of-the-art model-free and model-based RL algorithms: Soft Actor Critic (SAC) (Haarnoja et al., 2018) and Model-Based Policy Optimization (MBPO) (Janner et al., 2019), respectively.",,,1,related
"To actually learn the policy, we use the model to generate short rollouts, optimizing πθ with SAC, similar to model-based policy learning works that find long rollouts to destabilize learning due to compounding model errors (Janner et al., 2019).",,,1,related
"…issue, we first pre-train agents to convergence in the episodic Hopper environment (Brockman et al., 2016) with state-of-the-art model-free and model-based RL algorithms: Soft Actor Critic (SAC) (Haarnoja et al., 2018) and Model-Based Policy Optimization (MBPO) (Janner et al., 2019), respectively.",,,1,related
"In model-free RL, short-range model-generated rollouts branched from the real-world data were demonstrated to avoid the model pitfalls [34].",,,0,not_related
"In contrast, (Janner et al., 2019; Hafner et al., 2019) fully amortize learned policies over the entire training experience, which is fast even for high-dimensional action spaces, but cannot directly transfer to new environments with different dynamics and reward functions.",,,0,not_related
"…form of learned transition dynamics, and reward models, which are used to learn either parametric policies by amortizing over the entire training experience (Hafner et al., 2019; Janner et al., 2019), or perform online planning as done in Planet (Hafner et al., 2018), and PETS (Chua et al., 2018).",,,0,not_related
"World models summarize an agent’s experience in the form of learned transition dynamics, and reward models, which are used to learn either parametric policies by amortizing over the entire training experience (Hafner et al., 2019; Janner et al., 2019), or perform online planning as done in Planet (Hafner et al.",,,0,not_related
"In contrast, (Janner et al., 2019; Hafner et al., 2019) fully amortize learned policies over the entire training experience, which is fast even for high-dimensional action spaces, but cannot directly transfer to new environments with different dynamics and reward functions.",,,0,not_related
"…form of learned transition dynamics, and reward models, which are used to learn either parametric policies by amortizing over the entire training experience (Hafner et al., 2019; Janner et al., 2019), or perform online planning as done in Planet (Hafner et al., 2018), and PETS (Chua et al., 2018).",,,0,not_related
"World models summarize an agent’s experience in the form of learned transition dynamics, and reward models, which are used to learn either parametric policies by amortizing over the entire training experience (Hafner et al., 2019; Janner et al., 2019), or perform online planning as done in Planet (Hafner et al.",,,0,not_related
"When combined with modern techniques (Kurutach et al., 2018; Luo et al., 2018; Nagabandi et al., 2018; Ha & Schmidhuber, 2018; Hafner et al., 2019; Wang & Ba, 2019; Janner et al., 2019), MBRL is able to achieve some level of success.",,,0,not_related
"In our experiment, we have shown that our model-based algorithm outperforms Chua et al. (2018) and Janner et al. (2019) in given environments.",,,1,related
"However, in our method, there is a limitation of using MPC, which might fail in even higher-dimensional tasks as shown in Janner et al. (2019). Incorporating policy gradient techniques for action-selection might further improve the performance and we leave it for future work.",,,1,related
"Model-Based Policy
Optimization (MBPO) from Janner et al. (2019) uses the same bootstrap ensemble techniques as PETS in modeling, but differs from PETS in policy optimization with a large amount of short model-generated rollouts, and can cope with environments with no oracle rewards provided.",,,0,not_related
"However, in our method, there is a limitation of using MPC, which might fail in even higher-dimensional tasks as shown in Janner et al. (2019).",,,1,related
"Notice that Chua et al. (2018), Janner et al. (2019) have already greatly outperformed stateof-the-art model-free methods in sample efficiency as shown in their papers.",,,1,related
"[25] uses the same bootstrap ensemble techniques as PETS in modeling, but differs from PETS in policy optimization with a large amount of short model-generated rollouts, and can cope with environments with no oracle rewards provided.",,,0,not_related
"However, [33] proves the fact that even a small model error will cause great approximation bias of the value function obtained by simulating k steps in the incorrect predictive dynamics shown as (11).",,,0,not_related
"Similarly, we chose the MPO update for its ease of implementation, but it is likely other forms of regularized policy gradient (e.g. TRPO [59], or more generally natural or mirror policy optimization [72, 2]) would result in quantitively similar findings.",,,1,related
"Recent work by Grill et al. [20] showed that the MuZero policy update approximates TRPO, making the learning algorithm implemented by these Dyna-style algorithms quite similar to that implemented by MuZero.",,,0,not_related
"[34] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"For example, the “Learn” variant of MuZero is similar to Dyna-style methods that perform policy updates with TRPO [34, 39, 46, 52].",,,0,not_related
"There is also an interesting connection to recent Dyna-style MBRL algorithms via regularized policy optimization: specifically, these methods simulate data from the model and then update the policy on this data using TRPO [34, 39, 46, 52].",,,0,not_related
"In contrast, Kaiser et al. [17] use the MBPO [22] framework where policies are directly trained on the deep prediction model instead of the real world.",,,0,not_related
[17] use the MBPO [22] framework where policies are directly trained on the deep prediction,,,0,not_related
"Modelbased RL algorithms have demonstrated significantly better sample-efficiency on the simulated Ant benchmark [24], [25].",,,0,not_related
"Model-based RL algorithms have demonstrated significantly better sample-efficiency on the simulated Ant benchmark [25], [26].",,,0,not_related
"For MBRL, black-box models have been widely adopted due to their generic applicability and simplicity [28]–[30].",,,0,not_related
"There is a natural tradeoff with -models: the higher is, the fewer model steps are needed to make long-horizon predictions, reducing model-based compounding prediction errors (Asadi et al., 2019; Janner et al., 2019).",,,0,not_related
", 2017), MBPO (Janner et al., 2019), and MVE (Feinberg et al.",,,0,not_related
"Longer horizons carry more information, but present a more difficult prediction problem, as errors accumulate rapidly when a model is applied to its own previous outputs (Janner et al., 2019).",,,0,not_related
"We compare this approach to the state-of-the-art in model-based and model-free methods, with representative algorithms consisting of SAC, PPO (Schulman et al., 2017), MBPO (Janner et al., 2019), and MVE (Feinberg et al., 2018).",,,1,related
"Overall, we believe our approach would further strengthen the understanding of dynamics generalization and could be useful to other relevant topics such as model-based policy optimization methods [15, 16].",,,0,not_related
"Such a learned dynamics model can be used as a simulator for model-free RL methods [16, 18, 40], providing a prior or additional features to a policy [9, 47], or planning ahead to select actions by predicting the future consequences of actions [1, 22, 42].",,,0,not_related
"Many previous work has focused on building better models to fit the data [9, 11, 13, 18, 20, 21], but ignoring the acquisition of diverse data, resulting in the learned model can only make good predictions for the limited state, lacking the ability to generalize the new state.",,,0,not_related
"As they neither require accurate dynamics models nor try to explicitly fit them, the disadvantages of identifying models and quantifying their trustworthiness [4] are alleviated.",,,0,not_related
"ilar results have appeared in [26], [27].",,,0,not_related
"Despite recent empirical advances in MBRL [26], [27], [38], [39], the performance of policy trained by MBRL significantly relies on the accuracy of an empirical model.",,,0,not_related
"the model bias, which is an improvement over the quadratic dependency in prior studies [26], [27].",,,0,not_related
"In the literature of MBRL [26], [27], we could assess the quality of the learned transition model Mu by the evaluation error of an arbitrary policy p, i.",,,1,related
"Many studies [26], [27], [31] have shown that if the empirical environment is trained with behavioral cloning (i.",,,0,not_related
"This environment imitation method suggests a promising application of GAIL for model-based reinforcement learning (MBRL) [3], [27].",,,0,not_related
"methods [26], [27], [30] learns the environment simply via fitting one-step transitions shown in Eq.",,,0,not_related
"13 to provide target values for the Q-networks, as in Janner et al. (2019).",,,0,not_related
"As model-based RL remains an active research area (Janner et al., 2019), we provide a proof-of-concept in this setting, using a learned deterministic model on HalfCheetah-v2 (see Appendix A.5).",,,1,related
"Besides, in modelbased reinforcement learning, model deficiencies are typically handled by considering only short-term rollouts (Janner et al 2019) or by model predictive control (Nagabandi et al 2018).",,,0,not_related
Model-based policy optimization (MBPO) [43] uses a probabilistic model ensemble and performs a large amount of short model rollouts that start from a state distribution with states from the real environment dynamics.,,,0,not_related
"Generating data that are not so far away from ones in the batch prevents the accumulation of model error, but this theoretical aspect, even if already mentioned in (Janner et al. 2019), should not affect the bound that aims to be valid on any uncertainty penalized MDP independently of other factors.",,,0,not_related
[27] The authors empirically show that MBPO performs significantly better in continuous control tasks compared to previous methods.,,,0,not_related
"Furthermore, the authors show that as long as they can improve the C, the performance will increase monotonically [27].",,,0,not_related
MBPO (Janner et al. 2019) as well as BMPO (Lai et al.,,,0,not_related
MBPO (Janner et al. 2019) as well as BMPO (Lai et al. 2020) combines model ensembles with short model rollouts for sufficient policy optimization.,,,0,not_related
"(7) is for stochastic transitions, but the experiments in Janner et al. (2019) used deterministic transitions.",,,0,not_related
"Such a suggestion on branched length (or equivalently, the branched discount factor β) supports the experimental success of Janner et al. (2019) and their choice of hyperparameter, as mentioned in the last paragraph of § 4.2.",,,0,not_related
"3 in (Janner et al., 2019), branched rollouts of length k satisfy",,,0,not_related
"To mitigate the cumulative reward error, Janner et al. (2019) experimentally shows that branched rollouts (short model rollouts initialized by previous real rollouts) help reduce this error and improve experimental results.",,,0,not_related
"Most prior error analyses impose a strong assumption in their proofs; e.g., Lipschitz value function (Luo et al., 2019; Xiao et al., 2019; Yu et al., 2020) or maximum model error (Janner et al., 2019).",,,0,not_related
"A major contribution of Janner et al. (2019) is the use of branched rollouts generated by (ρπDT , π, T̂ ).",,,0,not_related
"Also, this result is for deterministic transitions, so this resolves an open issue in Janner et al. (2019), as they proved for stochastic transitions but experimented with deterministic transitions.",,,0,not_related
"In addition, we enhance the results of Janner et al. (2019), by showing their constants “in maxima” can be replaced by constants “in expectation”.",,,1,related
"By Theorem 4.3 in (Janner et al., 2019), branched rollouts of length k satisfy
R(π, T )−Rbranch(π) ≥−2rmax [ γk+1 π
(1− γ)2 + γk π 1− γ + k m 1− γ
] , (7)
with the same constants as Eq.",,,0,not_related
"Such closeness of policies is commonly used in the literature (Luo et al., 2019; Janner et al., 2019; Yu et al., 2020).",,,0,not_related
"Prior work has done extensive experiments on branched rollouts (Janner et al., 2019), Generative Adversarial Imitation Learning (Ho and Ermon, 2016) and the Ensemble Method (Kurutach et al., 2018).",,,0,not_related
"Prior work has done extensive experiments on branched rollouts (Janner et al., 2019), Generative Adversarial Imitation Learning (Ho and Ermon, 2016) and the Ensemble Method (Kurutach et al.",,,0,not_related
"However, the effectiveness of branched rollouts remains unclear since the experiments of Janner et al. (2019) use deterministic transitions (MuJoCo (Todorov et al., 2012)).",,,0,not_related
"Model-based reinforcement learning (MBRL) has emerged as a functional candidate for robotic control in a data-efficient manner [1]–[3], [29] – this letter extends the functionality of MBRL to nonholonomic planning for flying robots.",,,0,not_related
"Dyna-Q (Sutton, 1990) is an early method which uses the model exclusively for 3, followed more recently by ME-TRPO (Kurutach et al., 2018) and MBPO (Janner et al., 2019).",,,0,not_related
"Three of the works we compared to in the previous section, Chua et al. (2018), Wang and Ba (2019), and Janner et al. (2019), all relied on very similar implementations of bootstrapped ensembles of fully-connected networks to model environment dynamics.",,,0,not_related
"Of particular interest is the first combination, since it bears close resemblance to a similar ablation performed in Janner et al. (2019).",,,0,not_related
"We evaluate SAC-SVG(H) on all of the MuJoCo (Todorov et al., 2012) locomotion experiments considered by POPLIN, MBPO, and STEVE, which are the most recent state-of-the-art related approaches that use model-based rollouts.",,,1,related
"Interestingly, even though the single recurrent model overfits much more heavily to the training data, the asymptotic reward of our humanoid agent is significantly higher and qualitatively different than that reported in Janner et al. (2019).",,,0,not_related
The approaches in MBPO and STEVE are complimentary to ours as MBPO augments the replay buffer with imagined model rollouts and STEVE would improve the critic target.,,,1,related
"Model-based RL theoretical results often start with an assumed bound on the model error and proceed to establish a bound on the resulting error in the infinite-horizon model value expansion (Luo et al., 2018; Janner et al., 2019).",,,0,not_related
"In contrast, MBPO periodically trained the dynamics model to convergence on the full replay buffer, generated a large batch of fictional transitions, and proceeded to repeatedly update the actor/critic models on those stored transitions.",,,0,not_related
", 2015), and show that with the addition of an entropy term to encourage exploration they yield competitive policies in comparison to more recent model-based agents (Buckman et al., 2018; Wang and Ba, 2019; Janner et al., 2019).",,,0,not_related
"…expert domain knowledge (Todorov et al., 2012), and often find higher reward policies than their model-free counterparts when only a small amount of ground-truth data can be collected (Deisenroth and Rasmussen, 2011; Chua et al., 2018; Wang and Ba, 2019; Janner et al., 2019; Kaiser et al., 2019).",,,0,not_related
", 2012), and often find higher reward policies than their model-free counterparts when only a small amount of ground-truth data can be collected (Deisenroth and Rasmussen, 2011; Chua et al., 2018; Wang and Ba, 2019; Janner et al., 2019; Kaiser et al., 2019).",,,0,not_related
"…known family of methods, stochastic value gradients (SVG) (Heess et al., 2015), and show that with the addition of an entropy term to encourage exploration they yield competitive policies in comparison to more recent model-based agents (Buckman et al., 2018; Wang and Ba, 2019; Janner et al., 2019).",,,0,not_related
", 2018) G MF+rollout data MF+rollout data Det NN Yes Proprio MBPO (Janner et al., 2019) G MF+rollout data MF+rollout data Prob NN Yes Proprio SAC (Haarnoja et al.",,,1,related
We found that running the public MBPO code produces an agent that simply collects the keep-alive bonus by standing stock-still.,,,1,related
"Videos of our trained agents are available at sites.google.com/view/2020-svg.
Figure 2 shows our results in comparison to MBPO and STEVE, which evaluate on the MuJoCo tasks in the OpenAI gym (Brockman et al., 2016) that are mostly the standard v2 tasks with early termination and alive bonuses, and with a truncated observation space for the humanoid and ant that discards the inertial measurement units.",,,1,related
"In spite of the recent developments in machine learning to deal with complex problems [1–9], RL algorithms often have a hard time learning simple tasks that efficiently learnt by animals [10].",,,0,not_related
"This is similar to approaches used in MBPO (Janner et al., 2019) and DREAMER (Hafner et al.",,,0,not_related
", 2020) and MBPO (Janner et al., 2019), with values taken from the MOPO paper (Yu et al.",,,1,related
", 2018] Pix2pix MCTS + - Cheetah Policy Optim [Janner et al., 2019] Ensemble Short rollouts + - Cheetah Video-prediction [Oh et al.",,,1,related
"…et al., 2018, Chua et al., 2018, Clavera et al., 2018, Feinberg et al., 2018, Gu et al., 2016, Hafner et al., 2018, Heess et al., 2015, Janner et al., 2019] and other robotics tasks [Finn and Levine, 2017, Hafner et al., 2019, Levine and Abbeel, 2014, Sekar et al., 2020, Tassa et al.,…",,,0,not_related
"…Policy [Clavera et al., 2018] Meta-ensembles Short rollouts + - Cheetah GATS [Azizzadenesheli et al., 2018] Pix2pix MCTS + - Cheetah Policy Optim [Janner et al., 2019] Ensemble Short rollouts + - Cheetah Video-prediction [Oh et al., 2015] CNN/LSTM Action + + Atari VPN [Oh et al., 2017] CNN…",,,0,not_related
"propose in Modelbased Policy Optimization (MBPO) a new approach to short rollouts with ensembles [Janner et al., 2019].",,,0,not_related
"The results reported indicate that meta-learning a policy over an ensemble of learned models approaches the level of performance of model-free methods with
10
Approach Learning Planning Reinforcement Learning Application Local Model [Gu et al., 2016] Quadratic Non-linear Short rollouts Q-learning Cheetah MVE [Feinberg et al., 2018] Samples Short rollouts Actor-critic Cheetah Meta Policy [Clavera et al., 2018] Meta-ensembles Short rollouts Policy optimization Cheetah GATS [Azizzadenesheli et al., 2018] Pix2pix MCTS Deep Q Network Cheetah Policy Optim [Janner et al., 2019] Ensemble Short rollouts Soft-Actor-Critic Cheetah Video predict [Oh et al., 2015] CNN/LSTM Action Curriculum Atari VPN [Oh et al., 2017] CNN encoder d-step k-step Mazes, Atari SimPLe [Kaiser et al., 2019] VAE, LSTM MPC PPO Atari
TABLE 4 Overview of Hybrid Model-Free/Model-based Imagination Methods
substantially better sample complexity.",,,0,not_related
"…Meta-ensembles Short rollouts Policy optimization Cheetah GATS [Azizzadenesheli et al., 2018] Pix2pix MCTS Deep Q Network Cheetah Policy Optim [Janner et al., 2019] Ensemble Short rollouts Soft-Actor-Critic Cheetah Video predict [Oh et al., 2015] CNN/LSTM Action Curriculum Atari VPN [Oh et…",,,1,related
"Janner et al. propose in Modelbased Policy Optimization (MBPO) a new approach to short rollouts with ensembles [Janner et al., 2019].",,,0,not_related
"3.2) GPS [Levine and Abbeel, 2014] iLQG Trajectory - - Swimmer
SVG [Heess et al., 2015] Value Gradients Trajectory - - Swimmer PETS [Chua et al., 2018] Uncertainty Ensemble MPC - - Cheetah Visual Foresight [Finn and Levine, 2017] Video Prediction MPC - - Manipulation Local Model [Gu et al., 2016] Quadratic Non-linear Short rollouts + - Cheetah MVE [Feinberg et al., 2018] Samples Short rollouts + - Cheetah Meta Policy [Clavera et al., 2018] Meta-ensembles Short rollouts + - Cheetah GATS [Azizzadenesheli et al., 2018] Pix2pix MCTS + - Cheetah Policy Optim [Janner et al., 2019] Ensemble Short rollouts + - Cheetah Video-prediction [Oh et al., 2015] CNN/LSTM Action + + Atari VPN [Oh et al., 2017] CNN encoder d-step + + Atari SimPLe [Kaiser et al., 2019] VAE, LSTM MPC + + Atari PlaNet [Hafner et al., 2018] RSSM (VAE/RNN) CEM - + Cheetah Dreamer [Hafner et al., 2019] RSSM+CNN Imagine - + Hopper Plan2Explore [Sekar et al., 2020] RSSM Planning - + Hopper
End-to-End Learning VIN [Tamar et al., 2016] CNN Rollout in network + - Mazes Planning & Transitions VProp [Nardelli et al., 2018] CNN Hierarch Rollouts + - Maze, nav (Sect.",,,0,not_related
", 2018] Pix2pix MCTS Deep Q Network Cheetah Policy Optim [Janner et al., 2019] Ensemble Short rollouts Soft-Actor-Critic Cheetah Video predict [Oh et al.",,,0,not_related
" with zero mean and w = diag(4 10 4;0:1;1 10 8), v = diag(4 10 5;0:01;1 10 9). The same uncertainty for the initial conditions is assumed. Bayesian frameworks have been used in reinforcement learning [45, 46] as they can model both the epistemic and aleatoric uncertainty (in Gaussian processes, usually homoscedastic aleatoric uncertainty is considered). One of the fundamental steps here is the propagation",,,0,not_related
"Model-based RL algorithms build world model(s) to predict the future conditioned on the past and actions, and then act through planning [6, 14, 17, 18, 23, 25, 41].",,,0,not_related
"The choice of pairing states with on-policy actions to form hypothetical experiences has been reported to be beneficial [Gu et al., 2016, Pan et al., 2018, Janner et al., 2019].",,,0,not_related
"Existing works show that smart search-control strategies can further improve sample efficiency of a Dyna agent [Sutton et al., 2008, Gu et al., 2016, Goyal et al., 2019, Holland et al., 2018, Pan et al., 2018, Corneil et al., 2018, Janner et al., 2019, Chelu et al., 2020].",,,0,not_related
", 2018), or to modulate the distance of such states from real experience (Janner et al., 2019).",,,0,not_related
"…abundant existing works (Moore & Atkeson, 1993; Sutton et al., 2008; Gu et al., 2016; Pan et al., 2018; Corneil et al., 2018; Goyal et al., 2019; Janner et al., 2019; Pan et al., 2019) report different level of sample efficiency improvements by using different way of generating hypothetical…",,,0,not_related
"Another strategy has been to generate a more diverse set of states from which to sample (Gu et al., 2016; Holland et al., 2018), or to modulate the distance of such states from real experience (Janner et al., 2019).",,,0,not_related
"Similarly, Janner et al. (2019) explore only leveraging the learned model over finite horizons where it has accurate predictions and Levine et al. (2016) use local models.",,,0,not_related
"…et al., 2018a; Hafner
et al., 2019b; Nagabandi et al., 2019) or optimizing a policy in the model (Racanière et al., 2017; Ha and Schmidhuber, 2018; Łukasz Kaiser et al., 2020; Lee et al., 2019; Janner et al., 2019; Wang and Ba, 2019; Hafner et al., 2019a; Gregor et al., 2019; Byravan et al., 2019).",,,0,not_related
", 2019) or optimizing a policy in the model (Racanière et al., 2017; Ha and Schmidhuber, 2018; Łukasz Kaiser et al., 2020; Lee et al., 2019; Janner et al., 2019; Wang and Ba, 2019; Hafner et al., 2019a; Gregor et al., 2019; Byravan et al., 2019).",,,0,not_related
"Theoretical analyses similar to the bounds we propose have also been presented in a model-based RL context (Sun et al., 2018; Janner et al., 2019).",,,1,related
"Since our transformer model performed poorly when used as a dynamics model, our Dyna baseline for batch RL adopts a state-of-the-art architecture [34] that employs a 7-model ensemble (MBPO).",,,1,related
"We also consider combining CoDA with MBPO, by first expanding the dataset with MBPO and then applying CoDA to the result.",,,1,related
"Third: Dyna [82], including MBPO [34], augments real states with new actions and resamples the next state using a learned dynamics model.",,,0,not_related
"For each dataset, we train both mask and reward functions (and in case of MBPO, the dynamics model) on the provided data and use them to generate different amounts of counterfactual data.",,,1,related
"Besides the above innovations in model learning, Nguyen et al. and Xiao et al. (2019) developed methods of using adaptive rollout horizon according to the estimated compounding error, and Janner et al. (2019) proposed to use truncated short rollouts branched from real states.",,,0,not_related
"The original MBPO (Janner et al., 2019) algorithm iterates between three stages: data collection, model learning, and policy optimization.",,,0,not_related
"We evaluate our BMPO and previous state-of-the-art algorithms (Haarnoja et al., 2018; Janner et al., 2019) on a range of continuous control benchmark tasks.",,,1,related
"With this insight, we combine bidirectional models with recent MBPO method (Janner et al., 2019) and propose a practical MBRL algorithm called Bidirectional Modelbased Policy Optimization (BMPO).",,,1,related
"Prior works (Chua et al., 2018; Janner et al., 2019) have demonstrated that the ensemble of probabilistic models is quite effective in MBRL, even when the ground truth dynamics are deterministic.",,,0,not_related
"Janner et al. (2019), instead, derived a bound of discrepancy between returns in real environment and those under the branched rollout scheme of MBPO in terms of rollout length, model error, and policy shift divergence.",,,0,not_related
"Specifically, for model-based methods, we compare against MBPO (Janner et al., 2019), as our method builds on top of it; SLBO (Luo et al., 2018) and PETS (Chua et al., 2018), both performing well in the model-based benchmarking test (Langlois et al., 2019).",,,1,related
"Specifically, for model-based methods, we compare against MBPO (Janner et al., 2019), as our method builds on top of it; SLBO (Luo et al.",,,1,related
"We notice that in MBPO (Janner et al., 2019), the authors derived a similar return discrepancy bound (refer to Theorem 4.3 therein) with only one forward dynamics model.",,,1,related
"Although bidirectional models can be incorporated into almost any Dyna-style model-based algorithms (Sutton, 1991), we choose the Model-based Policy Optimization (MBPO) (Janner et al., 2019) algorithm as the framework backbone since it is the state-of-the-art MBRL method and is sufficiently general.",,,1,related
"Though it has been discovered that linearly increasing rollout length achieves excellent performance (Janner et al., 2019), it remains a problem that how to choose the backward rollout length k1 according to the forward length k2.",,,0,not_related
"Model ensembles have shown to be effective in preventing a policy or a controller from exploiting the inaccuracies of any single model (Rajeswaran et al., 2016; Chua et al., 2018; Kurutach et al., 2018; Janner et al., 2019).",,,0,not_related
"We notice that in MBPO (Janner et al., 2019), the authors derived a similar return discrepancy bound (refer to Theorem 4.",,,1,related
"The MBPO algorithm (Janner et al., 2019) avoided the compounding error by generating short branched rollouts from real states.",,,0,not_related
We fix k2 to be the same as Janner et al. (2019) and vary k1 from 0 to 2k2.,,,1,related
", 2018) used a bootstrap ensemble to account for uncertainty, and scales up to a 7 degrees-offreedom (DOF) action space, while model-based policy optimization (MBPO) (Janner et al., 2019), using a similar bootstrap ensemble for model estimation, even scales up to a 22 DOF humanoid robot (in simulation).",,,0,not_related
"On the other hand, model-based RL algorithms, such as PILCO [Deisenroth and Rasmussen, 2011], MBPO [Janner et al., 2019], and Visual Foresight [Ebert et al., 2018], despite their success, still have many issues in handling the difficulties of learning a model (dynamics) in a high-dimensional…",,,0,not_related
"On the other hand, model-based RL algorithms, such as PILCO [Deisenroth and Rasmussen, 2011], MBPO [Janner et al., 2019], and Visual Foresight [Ebert et al.",,,0,not_related
"While CARL is compatible with most PI-style (actor-critic) RL algorithms, following a recent work, MBRL [Janner et al., 2019], we choose SAC as the RL algorithm in CARL.",,,1,related
"To incorporate this extra piece of information in the representation learning process, we utilize results from variational model-based policy optimization (VMBPO) work by Chow et al. [2020].",,,1,related
"On the other hand, model-based RL algorithms, such as PILCO [Deisenroth and Rasmussen, 2011], MBPO [Janner et al., 2019], and Visual Foresight [Ebert et al., 2018], despite their success, still have many issues in handling the difficulties of learning a model (dynamics) in a high-dimensional (pixel) space.",,,0,not_related
"3 we compare NARL against the publicly released data from MBPO [Janner et al., 2019] on the InvertedPendulum, Hopper and HalfCheetah environments.",,,1,related
"We implement our algorithm in by using an ensemble, as is common in existing state-of-the-art methods [Janner et al., 2019, Clavera et al., 2018, Kurutach et al., 2018, Chua et al., 2018, Ball et al., 2020].",,,1,related
"Again, we are able to perform favorably vs. MBPO, demonstrating the potential for our approach to scale to larger environments.",,,1,related
This performance comes despite using over 50% fewer models than MBPO (3 models vs. 7).,,,0,not_related
"To prevent this, state-of-theart methods such as MBPO randomly sample models from the ensemble to prevent the policy exploiting an individual (potentially biased) model.",,,0,not_related
"Interestingly, an undocumented feature in MBPO [Janner et al., 2019] that mirrors this is the idea of maintaining a subset of “elite"" models.",,,0,not_related
"Since the latter technique is used in many prominent state-of-the-art deep MBRL algorithms [Kurutach et al., 2018, Janner et al., 2019, Chua et al., 2018, Ball et al., 2020], we have all the ingredients we need to scale to that paradigm.",,,0,not_related
"3 we compare NARL against the publicly released data from [34], setting (M = 3, M = 1) and (M = 5, M = 0.",,,1,related
"1, we must focus on term II by mitigating model errors [34].",,,1,related
"This architecture was extended with an ensemble of probabilistic neural networks (from [46]) alongside MPC [17], and used alongside the Soft Actor Critic [30, 31] and shortened horizon rollouts (eschewing the Dyna-approach due to SAC being off-policy) to achieve the current state-of-the-art [34].",,,0,not_related
"For our implementation, we focus on [34], using probabilistic dynamics models [46] and a Soft Actor Critic (SAC, [30, 31]) agent learning inside the model.",,,1,related
"We implement our algorithm in deep RL by using an ensemble, as is common in existing state-ofthe-art methods [34, 19, 42, 17, 12].",,,1,related
"Since bootstrapped ensembles are commonly used in deep reinforcement learning [42, 34, 17, 12], we have all the ingredients we need to scale NARL to that paradigm.",,,0,not_related
"This way reduces to the work (Luo et al., 2018; Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"A straightforward way is to optimize Q, V and π using the imaginary data from the rollout, which reduces to Luo et al. (2018); Janner et al. (2019) and many others.",,,1,related
"Follow the notion in (Janner et al., 2019), we call it k-step branched rollout.",,,1,related
"Several state-of-the-art MBRL algorithms generate hundreds of thousands imaginary data from the model and a few real samples (Luo et al., 2018; Janner et al., 2019).",,,0,not_related
Janner et al. (2019) provide an error bound on the long term return of the k-step rollout given that the total variation of model bias and policy distribution are bounded by .,,,1,related
", 2018), MBPO (Janner et al., 2019) and POPLIN (Wang and Ba, 2019).",,,0,not_related
"Comparing with the total variation used in (Janner et al., 2019), the Wasserstein distance has better representation in the sense of how close p̂ approximate p (Asadi et al.",,,0,not_related
"Four model-based reinforcement learning baselines are SVG (Heess et al., 2015),SLBO (Luo et al., 2018), MBPO (Janner et al., 2019) and POPLIN (Wang and Ba, 2019).",,,0,not_related
MBPO has the similar spirit but with SAC as the learning algorithm on the imaginary data.,,,0,not_related
"We assume W (p(s′|s, a), p̂(s′|s, a)) ≤ m, ∀s, a and W (π(a|s), πD(a|s)) ≤ π, ∀s. Comparing with the total variation used in (Janner et al., 2019), the Wasserstein distance has better representation in the sense of how close p̂ approximate p (Asadi et al., 2018).",,,1,related
Luo et al. (2018); Chua et al. (2018); Kurutach et al. (2018); Janner et al. (2019) use the current policy to gather the data from the interaction with the environment and then learn the dynamics model.,,,0,not_related
"2019), PILCO (Deisenroth and Rasmussen 2011), MBPO (Janner et al. 2019)) sheds light on the importance of our acquisition functions for safe exploration.",,,0,not_related
"We use PlaNet and MBPO as generic model-based deep reinforcement learning algorithm that use neural network dynamic models, i.e., we do not anticipate a considerably higher sample efficiency from other methods with neural network dynamic models.",,,1,related
"Learner Safety gym car Safety gym point
Learning Evaluation Learning Evaluation
Samples TC TV TC Samples TC TV TC
TRPO 1000 202 3.7 5.6 1000 26.8 8.6 13 PPO 1000 205 7.4 11 1000 9 6.1 9.2 PlaNet 100 3.5 3.2 4.9 100 2.6 7.7 16 MBPO 90 3.4 4.3 5.6 100 3.2 6.9 21 PPILCO 2 0.47 7.4 11 8 0.31 5.4 8.9 PlaNet w RS 100 3.4 3.1 3.9 100 2.5 7.9 13 CPO 1000 49 4.7 4.4 1000 14 1.1 5 STRPO 1000 64 2.0 2.1 1000 17 2.1 2.7 SPPO 1000 66 1.7 2.4 1000 10 1 1.8 SAMBA (w/o
active learning)
0.8 0.13 1.4 4.8 0.7 0.56 1.8 2.5
SAMBA 0.6 0.01 1.3 2.2 0.5 0.04 1.3 1.5
1 3
Safety Gym Car and Safety Gym Point environments, respectively.",,,0,not_related
"2019), PILCO (Deisenroth and Rasmussen 2011), MBPO (Janner et al.",,,0,not_related
", 2018), and model-based (Janner et al., 2019; Williams et al., 2015) algorithms.",,,0,not_related
"Our method is applicable to any maximum entropy RL algorithm, including on-policy (Song et al., 2019), off-policy (Abdolmaleki et al., 2018; Haarnoja et al., 2018), and model-based (Janner et al., 2019; Williams et al., 2015) algorithms.",,,1,related
"Finally, we compared against two model-based RL methods: MBPO (Janner et al., 2019) and PETS (Chua et al.",,,1,related
"Algorithms for model-based RL (e.g., (Polydoros & Nalpantidis, 2017; Sutton, 1991; Janner et al., 2019; Deisenroth & Rasmussen, 2011; Wang et al., 2019; Williams et al., 2015; Hafner et al., 2018; Chua et al., 2018; Finn & Levine, 2017) and off-policy RL (e.g., (Munos et al., 2016; Fujimoto et al.,…",,,0,not_related
"Finally, we compared against two model-based RL methods: MBPO (Janner et al., 2019) and PETS (Chua et al., 2018).",,,1,related
"This resembles methods that combine model learning with model-free RL in single-tasks settings [32, 12].",,,0,not_related
"end end When using data generated from a learned model to train a policy, the model’s predicted trajectory often diverges from the real dynamics after a large number of time steps, due to accumulated error [12].",,,0,not_related
"Using the same control tasks, we also compare VMBPO with several state-of-the-art model-based and model-free RL algorithms, including model-based policy optimization (MBPO) [Janner et al., 2019] and maximum a posteriori policy optimization (MPO) [Abdolmaleki et al.",,,1,related
"Model-based RL algorithms address the data efficiency issue of the model-free methods by learning a model, and combining model-generated data with those collected from interaction with the real system [Sutton, 1990; Janner et al., 2019].",,,0,not_related
"We compare VMBPO with five baselines, two popular model-free algorithms: MPO [Abdolmaleki et al., 2018] and SAC [Haarnoja et al., 2018], and three recent model-based algorithms: MBPO [Janner et al., 2019], PETS [Chua et al., 2019], and STEVE [Buckman et al., 2018].",,,0,not_related
"Model-based RL algorithms address the data efficiency issue of the model-free methods by learning a model, and com-
bining model-generated data with those collected from interaction with the real system [Sutton, 1990; Janner et al., 2019].",,,0,not_related
"…tasks, we also compare VMBPO with several state-of-the-art model-based and model-free RL algorithms, including model-based policy optimization (MBPO) [Janner et al., 2019] and maximum a posteriori policy optimization (MPO) [Abdolmaleki et al., 2018], and show its sample efficiency and performance.",,,1,related
", 2018], and three recent model-based algorithms: MBPO [Janner et al., 2019], PETS [Chua et al.",,,0,not_related
"Model-based reinforcement learning (MBRL) (Janner et al., 2019; Buckman et al., 2018; Xu et al., 2018; Chua et al., 2018) shows competitive performance compared with best model-free reinforcement learning (MFRL) algorithms (Schulman et al.",,,0,not_related
"…benchmarks (Todorov et al., 2012) and the experimental results show that MEMR matches asymptotic performance and sample efficiency with MBPO (Janner et al., 2019) while significantly reduces the number of policy updates and model rollouts, which leads to faster learning speed. ar X iv :2 00…",,,1,related
"We compare our method with the state-of-the-art model-based method, MBPO (Janner et al., 2019).",,,1,related
"It indicates that much computation power is wasted in (Janner et al., 2019) on less informative model rollouts that barely help the learning of the value functions in SAC.",,,0,not_related
"…2018) develops a theoretical framework that provides monotonic improvement of the to a local maximum of the expected reward for MBRL. Model-based policy optimization (MBPO) (Janner et al., 2019) achieves state-of-the-art sample efficiency and matches the asymptotic performance of MFRL approaches.",,,0,not_related
"In MBPO, learned dynamics model is used to generate branched model rollouts with short horizons (Janner et al., 2019).",,,0,not_related
"Recently, (Janner et al., 2019) proposed Model-based Policy Optimization (MBPO), including a theoretical framework that encourages short-horizon model usage based on an optimistic assumption of a bounded model generalization error given policy shift.",,,0,not_related
"Model-based policy optimization (MBPO) (Janner et al., 2019) achieves state-of-the-art sample efficiency and matches the asymptotic performance of MFRL approaches.",,,0,not_related
"…(MBRL) learns a dynamics model and directly perform model predictive control (MPC) (Nagabandi et al., 2017; Chua et al., 2018; Deisenroth & Rasmussen, 2011) or derives the policy using model generated rollouts (Kurutach et al., 2018; Janner et al., 2019; Buckman et al., 2018; Xu et al., 2018).",,,0,not_related
"Uniform sampling of true states 1 to generate model rollouts is adopted in MBPO (Janner et al., 2019).",,,1,related
"Our approach combines (Janner et al., 2019) and (Sutton, 1991) by proposing an non-trivial sampling approach to significantly reduce the number of policy updates and model rollouts that obtain asymptotic performance.",,,1,related
"Although (Janner et al., 2019) presented theoretical analysis to bound the policy performance trained using model generate rollouts, the over exploitation of model generalization can’t be eliminated.",,,0,not_related
", 2012) and the experimental results show that MEMR matches asymptotic performance and sample efficiency with MBPO (Janner et al., 2019) while significantly reduces the number of policy updates and model rollouts, which leads to faster learning speed.",,,0,not_related
"Model-based reinforcement learning (MBRL) (Janner et al., 2019; Buckman et al., 2018; Xu et al., 2018; Chua et al., 2018) shows competitive performance compared with best model-free reinforcement learning (MFRL) algorithms (Schulman et al., 2017; 2015; Mnih et al., 2013; Haarnoja et al., 2018a;b)…",,,0,not_related
"(5) This bound guarantees the improvement under the true returns as long as the improvement under the model returns increases by more than the slack in the bound due to m, π [22, 30].",,,1,related
"[22], we denote the generalization error of a dynamics model on the state distribution under the true behavior policy as m = maxt Es∼dπb t DTV (p(st+1|st, at)||pφ(st+1|st, at)), where DTV represents the total variation distance between true dynamics p and learned model pφ.",,,1,related
"[22] as, η[π] ≥ η̂[π]− [ 2γrmax( m + 2 π) (1− γ)2 + 4rmax π (1− γ) ] .",,,0,not_related
"A variety of remedies have been proposed to relieve the problem of model bias, such as the use of multiple dynamics models as an ensemble [6, 28, 22], meta-learning [7], energy-based model regularizer [2], and explicit reward penalty for unknown state [25, 57].",,,0,not_related
"Then, in Section 5, we conduct a theoretical analysis to justify the use of branched rollouts, which is a promising model-based rollout method proposed by Janner et al. (2019), in the meta-RL setting.",,,1,related
"We refine analyses in Janner et al. (2019) by considering multiple-model-based rollout factors (Appendix A.1), and extend our analyses results into a meta-RL setting (Section 5).",,,1,related
"Branched rollouts (Janner et al., 2019) Branched rollouts are Dyna-style rollouts (Sutton, 1991), in which model-based rollouts are run as being branched from real trajectories.",,,0,not_related
"Existing works have focused on the performance bound of model-based RL (Feinberg et al., 2018; Henaff, 2019; Janner et al., 2019; Luo et al., 2018; Rajeswaran et al., 2020), while ignoring model-based meta-RL.",,,0,not_related
"Specifically, we extend the branched rollout defined originally in the state-action space (Janner et al., 2019) to a branched rollout defined in the history-action space (Figure 3).",,,1,related
"To provide our theorem, we extend the notion and theorem of the branched rollout proposed in Janner et al. (2019) into the meta-RL setting.",,,1,related
"In existing Dyna-style RL methods (e.g. Janner et al. (2019); Shen et al. (2020); Yu et al. (2020)), this mixture ratio is fixed during the training phase.",,,0,not_related
"The branched rollout (Janner et al., 2019) is a kind of Dyna-style rollouts (Sutton, 1991), in which k-step model-based rollouts are run as being branched from real trajectories.",,,0,not_related
"The proof of Theorem 1 is given in Appendix A.2, where we extend the result of Janner et al. (2019) to the meta-RL setting.",,,1,related
A theorem for the performance guarantee of the branched rollout in MDPs are provided as Theorem 4.2 in Janner et al. (2019).,,,1,related
"As a proof-of-concept experiment, we evaluate two state-of-the-art off-policy model-based and model-free algorithms, MBPO [28] and SAC [26], in Figure 1.",,,1,related
"While prior approaches have used these models to select actions using planning [66, 16, 53, 50, 58], we choose to build upon Dyna-style approaches that optimize for a policy [63, 65, 71, 31, 25, 27, 43], specifically MBPO [28].",,,1,related
"To approach this question, we first hypothesize that model-based RL methods [63, 11, 41, 37, 28, 43] make a natural choice for enabling generalization, for a number of reasons.",,,1,related
Figure 1: Comparison between vanilla model-based RL (MBPO [28]) with or without model ensembles and vanilla model-free RL (SAC [26]) on two offline RL tasks: one from the D4RL benchmark [17] and one that demands out-of-distribution generalization.,,,0,not_related
"We now summarize model-based policy optimization (MBPO) [28], which we build on in this work.",,,1,related
"Complementary work (Janner et al., 2019) shows that simulating one-step transitions provides a strong baseline with respect to partial or complete policy rollouts with a learned model, and PPO mantains its monotonic improvement property.",,,0,not_related
"Such a dynamics model can then be used for control by planning (Atkeson & Santamaria, 1997; Lenz et al., 2015; Finn & Levine, 2017), or for improving the data-effciency of model-free RL methods (Sutton, 1990; Gu et al., 2016; Janner et al., 2019).",,,0,not_related
", 2015; Finn & Levine, 2017), or for improving the data-effciency of model-free RL methods (Sutton, 1990; Gu et al., 2016; Janner et al., 2019).",,,0,not_related
"As a result, MBRL algorithms have been highly sample efficient for online RL [28, 29].",,,0,not_related
"As a result, planning using a learned model without any safeguards against model inaccuracy can result in “model exploitation” [30, 31, 29, 28], yielding poor results [32].",,,0,not_related
"Following Janner et al. (2019), if we assume that the total variation distance (TVD) between the learned model Tψ and true model T is bounded by m = maxt Edπt DTV (Tψ(st+1|st, at)‖T (st+1|st, at)), and the TVD between π and πβ is likewise bounded on sampled states by π, then the true policy value…",,,1,related
"…low-dimensional MDPs, such uncertainty estimates can be produced by means of Bayesian models such as Gaussian processes (Deisenroth and Rasmussen, 2011), while for higher-dimensional problems, Bayesian neural networks and bootstrap ensembles can be utilized (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"Janner et al. (2019) also argue that a modified model-based RL procedure that resembles Dyna Sutton (1991), where only short-horizon rollouts from the model are generated by “branching” off of states seen in the data, can mitigate this accumulation of error.",,,0,not_related
"…Q-learning and one-step predictions via the model from previously seen states (Sutton, 1991), while a variety of recently proposed algorithms employ synthetic model-based rollouts with policy gradients (Parmas et al., 2019; Kaiser et al., 2019a) and actor-critic algorithms (Janner et al., 2019).",,,0,not_related
", 2019a) and actor-critic algorithms (Janner et al., 2019).",,,0,not_related
"Hybrid methods that combine model-based and model-free learning, for example by utilizing short rollouts (Sutton, 1991; Janner et al., 2019) or avoiding prediction of full observations (Dosovitskiy and Koltun, 2016; Oh et al.",,,0,not_related
"Many such methods have been known to exhibit excellent performance in conventional off-policy settings, where additional data collection is allowed, but prior data is also utilized (Sutton, 1991; Watter et al., 2015; Zhang et al., 2018; Hafner et al., 2018; Janner et al., 2019).",,,0,not_related
"Hybrid methods that combine model-based and model-free learning, for example by utilizing short rollouts (Sutton, 1991; Janner et al., 2019) or avoiding prediction of full observations (Dosovitskiy and Koltun, 2016; Oh et al., 2017; Kahn et al., 2020) offer some promise in this area.",,,0,not_related
"Theoretical analysis of model-based policy learning can provide bounds on the error incurred from the distributional shift due to the divergence between the learned policy π(a|s) and the behavior policy πβ(a|s) (Sun et al., 2018b; Luo et al., 2018; Janner et al., 2019).",,,0,not_related
"In low-dimensional MDPs, such uncertainty estimates can be produced by means of Bayesian models such as Gaussian processes (Deisenroth and Rasmussen, 2011), while for higher-dimensional problems, Bayesian neural networks and bootstrap ensembles can be utilized (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"In this case, the policy is prone to exploit regions where the model is inaccurate (Janner et al., 2019).",,,0,not_related
"…work on model-based RL for sample efficient control (Deisenroth & Rasmussen, 2011; Kurutach et al., 2018; Peng et al., 2018; Kaiser et al., 2019; Janner et al., 2019), with the key difference that the state transition function is known and the reward function is unknown in our work, whereas…",,,0,not_related
"DyNA PPO is related to existing work on model-based RL for sample efficient control (Deisenroth & Rasmussen, 2011; Kurutach et al., 2018; Peng et al., 2018; Kaiser et al., 2019; Janner et al., 2019), with the key difference that the state transition function is known and the reward function is unknown in our work, whereas most existing model-based RL approaches seek to model the state-transition function and consider the reward function as known.",,,0,not_related
Janner et al. (2019) investigate conditions in which an estimate of model generalization (their analysis uses validation accuracy) could justify model usage in such model-based policy optimization settings.,,,0,not_related
"By ignoring the model if it is inaccurate, we aim to prevent the policy from exploiting deficiencies of the model (Janner et al., 2019).",,,1,related
"[9] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",,,0,not_related
"Specifically, we compare against the modelfree soft actor-critic (SAC) [6] as well as two state-of-the-art model-based baselines: model-based policy-optimization (MBPO) [9] and stochastic ensemble value expansion (STEVE) [1].",,,1,related
"Then, model-based methods use the model to derive controllers from it, either parametric [14, 1, 9] or non-parametric [17, 2].",,,0,not_related
"We train both Q functions by minimizing the Bellman error (Section 2): JQ(ψ) = E[(Qψ(st, at)− (r(st, at) + γQψ(st+1, at+1)))(2)] Similar to [9], we minimize the Bellman residual on states previously visited and imagined states obtained from unrolling the learned model.",,,1,related
"The main contribution of this work is a model-based method that significantly reduces the sample complexity compared to state-of-the-art model-based algorithms [9, 1].",,,0,not_related
"in a differentiable form, it needs to be substituted with an approximate model p̂, as commonly done in model-based RL [7, 9, 20].",,,0,not_related
"While it is often hard to determine under which circumstances the addition of an approximate learned model to a model-free algorithm is beneficial [20], we have shown that model-based techniques such as MAGE’s gradient-learning procedure, can unlock novel learning modalities, inaccessible for model-free algorithms.",,,0,not_related
"It resembles 1-step horizon Model-based Policy Optimization (MBPO [20]), but uses a deterministic policy optimized by TD3.",,,0,not_related
"Our algorithm, which learns a Q-function from model-generated data but only optimizes the policy by using real data, is related to the approaches that compute the policy gradient by using a model-based value function together with trajectories sampled in the environment [1, 10, 19, 20].",,,1,related
"In practice, we leverage an ensemble of models, which has been shown to improve performance in a variety of contexts [7, 20, 23].",,,0,not_related
One may compare their methods to MBPO method (Janner et al. 2019) that also uses offline data while our method focuses on online learning.,,,1,related
"State-of-the-art algorithms do not include steps to filter data in search of generalization and maximum sample efficiency [15], [16] – but such a direct training process can result in numerical instability in prediction and low effectiveness in terms of maximizing",,,0,not_related
"While state-of-the-art MBRL algorithms showcase strong asymptotic performance [15], [16], the computational requirements include substantial data storage to form the model and low-frequency control – even on a graphics processing unit.",,,0,not_related
"efﬁciency. Using these imaginary rollouts improves model-free reinforcement learning effectively if the model’s predictions are accurate, and deteriorates the performance when the model is inaccurate [26, 11]. To address this problem, the model-based value expansion method [12] controls the depth of imagination to improve the performance of the model-based method by keeping the model-generated data accura",,,0,not_related
"PAL is also twice as efficient as MBPO (Janner et al., 2019), a state of the art hybrid model-based and model-free algorithm.",,,0,not_related
"For baselines, we consider MBPO (Janner et al., 2019), PETS (Chua et al., 2018), STEVE (Buckman et al., 2018), SLBO (Xu et al., 2018), and SAC (Haarnoja et al., 2018).",,,1,related
"Dyna (Sutton, 1990) and MBPO (Janner et al., 2019) use a learned model to provide additional learning targets for an actor-critic algorithm through short-horizon synthetic trajectories.",,,0,not_related
", [34], [35] for recent developments in the field.",,,0,not_related
"However, model-based reinforcement learning has been shown to be more sample efficient in other domains [11], so it may be a viable direction to take in future work, especially in situations where the reward function is very expensive to evaluate.",,,0,not_related
"We chose two function approximators for the learned residual dynamics to account for model learning approaches that use global function approximators such as neural networks (NN) [14], and local function approximators such as K-nearest neighbor regression (KNN) [25, 16].",,,1,related
[12] propose a monotonic model-based policy optimization (MBPO) to provide a performance guarantee.,,,0,not_related
", 2018) and data-efficient reinforcement learning algorithms (Nagabandi et al., 2018; Janner et al., 2019).",,,0,not_related
"Accurate transition models for macroscopic physical systems are critical components in control systems (Lenz et al., 2015; Kamthe and Deisenroth, 2017; Chua et al., 2018) and data-efficient reinforcement learning algorithms (Nagabandi et al., 2018; Janner et al., 2019).",,,0,not_related
"• Model-based RL: Neural-network parameterized policy and ensemble of dynamic models trained using the algorithm,
MBPO [34].",,,0,not_related
"For model-based baselines, we consider model-based policy optimization (MBPO) [34] and the demonstrator MPC.",,,1,related
Suboptimality was analysed in [45] for MPC and in [34] for policies.,,,0,not_related
"2For all our experiments, training datapoints: PPO: 4×106, SAC: 4×106, MBPO: 2.4× 105, NLMPC: 104 (random) + 104 (demonstrations).",,,1,related
"• Model-based RL: Neural-network parameterized policy and ensemble of dynamic models trained using the algorithm, MBPO [34].",,,0,not_related
"This sharing can be potentially approached by Hindsight Experience Replay [6] or model-based RL [18, 32, 54].",,,0,not_related
"Within MBRL, commonly explored methods include actionconditional, next-step models (Oh et al., 2015; Ha & Schmidhuber, 2018; Chiappa et al., 2017; Schmidhuber, 2010; Xie et al., 2016; Deisenroth & Rasmussen, 2011; Lin & Mitchell, 1992; Li et al., 2015; Diuk et al., 2008; Igl et al., 2018; Ebert et al., 2018; Kaiser et al., 2019; Janner et al., 2019).",,,0,not_related
"…next-step models (Oh et al., 2015; Ha & Schmidhuber, 2018; Chiappa et al., 2017; Schmidhuber, 2010; Xie et al., 2016; Deisenroth & Rasmussen, 2011; Lin & Mitchell, 1992; Li et al., 2015; Diuk et al., 2008; Igl et al., 2018; Ebert et al., 2018; Kaiser et al., 2019; Janner et al., 2019).",,,0,not_related
"Additionally, these effects should be quantified in other MBRL algorithms such as MBPO (Janner et al., 2019) and POPLIN (Wang and Ba, 2019).",,,0,not_related
"For example, how to efficiently leverage imperfect models [34], and how to maximize the joint benefit by combining policy learning and motion planning (trajectory optimization) [31, 35], where a policy has the advantage of execution coherence and fast deployment while the trajectory planning has the competence of adaption to unseen or future situations.",,,0,not_related
"While several existing actor-critic algorithms either use model-based estimates [Janner et al., 2019] or use IS corrections and truncations [Wang et al., 2017], we propose a novel approach towards extending doubly robust estimators, based on a combination of direct model-based approach and…",,,1,related
"While several existing actor-critic algorithms either use model-based estimates [Janner et al., 2019] or use IS corrections and truncations [Wang et al.",,,0,not_related
"However, prior works have learned such predictive models from interaction data alone [24,23,28,16,68].",,,0,not_related
"Model-based algorithms presented in [2, 3, 4, 5] achieve the same asymptotic performance as model-free algorithms while requiring an order of magnitude less data.",,,0,not_related
"Current model-based RL algorithms generally fall into one of three categories: Dyna-style algorithms, where the model is used to create imaginary experience for a model-free algorithm [10, 4, 7, 2, 11, 12, 5]; model predictive control (MPC) algorithms, where the model is used for planning at each time-step [13, 3]; and policy search with backpropagation-through-time approaches, which exploit the model derivatives [14, 15, 16, 17].",,,0,not_related
"model-based policy optimization algorithms which learn policies orQ-functions, parameterized by neural networks, on the estimated dynamics, using off-the-shelf model-free algorithms or their variants (Luo et al., 2019; Janner et al., 2019; Kaiser et al., 2019; Kurutach et al., 2018; Feinberg et al., 2018; Buckman et al., 2018), and 2.",,,0,not_related
", 2019), and MBPO (Janner et al., 2019) with various architecture size.",,,0,not_related
", 2019), which is called MBSAC, see Appendix A for details; (c) MBPO (Janner et al., 2019), the previous state-of-the-art model-based RL algorithm.",,,0,not_related
"…model-free RL algorithm; (b) a computationally efficient variant of MBPO (Janner et al., 2019) that we developed using ideas from SLBO (Luo et al., 2019), which is called MBSAC, see Appendix A for details; (c) MBPO (Janner et al., 2019), the previous state-of-the-art model-based RL algorithm.",,,1,related
"These algorithms include both model-free algorithms such as DQN (Mnih et al., 2015) and SAC (Haarnoja et al., 2018), and model-based policy optimization algorithms such as SLBO (Luo et al., 2019) and MBPO (Janner et al., 2019).",,,0,not_related
", 2018), the state-of-the-art model-free RL algorithm; (b) a computationally efficient variant of MBPO (Janner et al., 2019) that we developed using ideas from SLBO (Luo et al.",,,1,related
"We verify this claim on the randomly generated MDPs discussed in Section 5.1, by running DQN (Mnih et al., 2015), SLBO (Luo et al., 2019), and MBPO (Janner et al., 2019) with various architecture size.",,,1,related
"…parameterized by neural networks, on the estimated dynamics, using off-the-shelf model-free algorithms or their variants (Luo et al., 2019; Janner et al., 2019; Kaiser et al., 2019; Kurutach et al., 2018; Feinberg et al., 2018; Buckman et al., 2018), and 2. model-based planning…",,,0,not_related
"…of three algorithms: (a)
SAC (Haarnoja et al., 2018), the state-of-the-art model-free RL algorithm; (b) a computationally efficient variant of MBPO (Janner et al., 2019) that we developed using ideas from SLBO (Luo et al., 2019), which is called MBSAC, see Appendix A for details; (c) MBPO (Janner…",,,1,related
"MBPO (Janner et al., 2019), STEVE (Buckman et al., 2018), and MVE (Feinberg et al., 2018) are model-based Q-learning-based policy optimization algorithms, which can be viewed as modern extensions and improvements of the early model-based Qlearning framework, Dyna (Sutton, 1990).",,,0,not_related
"Another direction is to use multi-step greedy in model-based RL
(e.g., Kumar et al., 2016; Talvitie, 2017; Luo et al., 2018; Janner et al., 2019) and solve the surrogate decision problem with an approximate model.",,,0,not_related
"Another direction is to use multi-step greedy in model-based RL (e.g., Kumar et al., 2016; Talvitie, 2017; Luo et al., 2018; Janner et al., 2019) and solve the surrogate decision problem with an approximate model.",,,0,not_related
"Another direction the RL and control has been pursuing is on the combination of modelbased and model-free methods (Bansal et al., 2017; Okada et al., 2017; Jonschkowski et al., 2018; Pereira et al., 2018; Amos et al., 2018; Okada & Taniguchi, 2019; Janner et al., 2019; Pong et al., 2018).",,,0,not_related
"For example, we employ the the ensemble of policies trick which could also improve other off-policy policy-gradient based methods such as MBPO, SAC, and PPO.",,,1,related
", 2018) and model-based (Janner et al., 2019) baselines on continuous control benchmarks Hopper-v3 and HalfCheetah-v3 Each environments is evaluated with the canonical 1000-step variant of the task.",,,0,not_related
"We evaluate Neural-PSRL on the popular and widely studied MuJoCo continuous control tasks (Todorov et al., 2012) of HalfCheetahv3 and Hopper-v3 (Erez et al., 2012), comparing our method to Model Based Policy Optimization (MBPO) (Janner et al., 2019).",,,1,related
"For each run we averaged the last 50 (for Neural-PSRL and MBPO) and last 100 (for MBPO) returns, and take the average and standard deviation of these five values to report both mean and 95% confidence intervals.",,,1,related
In Janner et al. (2019) this issue is addressed and a method for optimal Trajectory length is developed.,,,0,not_related
"Like in MBPO, we use the canonical 1000-step horizon with early termination versions of both tasks, and assume knowledge of the termination criteria, and for the sake of simplicity also assume knowledge of the reward distribution instead of learning it.",,,1,related
"For the results in table 1, we ran Neural-PSRL and MBPO for 400 epochs (so 400K steps), and used the 3000 epoch (so 3M step) runs discussed above for SAC.",,,1,related
"Further, in keeping with Janner et al. (2019) we assume knowledge of the terminal conditions.",,,1,related
", 2012), comparing our method to Model Based Policy Optimization (MBPO) (Janner et al., 2019).",,,1,related
"Since MBPO clearly outperforms by a wide margin other reinforcement learning methods such as PPO (Schulman et al., 2017) and PETS (Chua et al., 2018), we compare only to the model-based MBPO and the model-free SAC.",,,1,related
"To the best of our knowledge, MBPO is the current state-of-the-art
on these tasks when the number of environment evaluations allowed is restricted (≤ 500K steps).",,,1,related
"0 50000 100000 150000 Number of datapoints
−0.2
−0.1
0.0
T as
k R
ew ar
d
Baoding Balls
MBPO PETS Nagabandi et. al SAC NPG PDDM (Ours)
Figure 8: PDDM outperforms prior model-based and modelfree methods.",,,1,related
"al [20] learns a deterministic neural network model, combined with a random shooting MPC controller; PETS [27] combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation; NPG [33] is a model-free natural policy gradient method, and has been used in prior work on learning manipulation skills [4]; SAC [34] is an off-policy model-free RL algorithm; MBPO [35] is a recent hybrid approach that uses data from its model to accelerate policy learning.",,,0,not_related
"In this section, we compare our method to the following state-of-the-art model-based and model-free RL algorithms: Nagabandi et. al [20] learns a deterministic neural network model, combined with a random shooting MPC controller; PETS [27] combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation; NPG [33] is a model-free natural policy gradient method, and has been used in prior work on learning manipulation skills [4]; SAC [34] is an off-policy model-free RL algorithm; MBPO [35] is a recent hybrid approach that uses data from its model to accelerate policy learning.",,,1,related
"Most previous works of MBRL adopt supervised learning with `2-based errors [Clavera et al., 2018; Kurutach et al., 2018; Luo et al., 2019] or maximum likelihood [Janner et al., 2019], to obtain an environment model that synthesizes real transitions.",,,0,not_related
", 2019] or maximum likelihood [Janner et al., 2019], to obtain an environment model that synthesizes real transitions.",,,0,not_related
"L G
] 1
4 O
ct 2
01 9
In this work, we derive and empirically validate model-free deep RL (DRL) implementations of κ-PI and κ-VI.",,,1,related
"Furthermore, and although in this work we focused on model-free DRL, it is arguably more natural to use multi-step DP in model-based DRL (e.g.,Kumar et al., 2016; Talvitie, 2017; Luo et al., 2018; Janner et al., 2019).",,,1,related
"1), we use standard policy evaluation deep RL (DRL) algorithms.",,,1,related
"[7, 15, 18] utilize model ensembles to reduce model overfitting for policy learning.",,,0,not_related
"Several methods incorporate the model uncertainty into policy updates, by using Gaussian processes and moment matching approximations (Deisenroth and Rasmussen 2011), Bayesian neural networks (Gal, McAllister, and Rasmussen 2016) or ensembles of forward models (Chua et al. 2018; Kurutach et al. 2018; Janner et al. 2019; Buckman et al. 2018).",,,0,not_related
"…into policy updates, by using Gaussian processes and moment matching approximations (Deisenroth and Rasmussen 2011), Bayesian neural networks (Gal, McAllister, and Rasmussen 2016) or ensembles of forward models (Chua et al. 2018; Kurutach et al. 2018; Janner et al. 2019; Buckman et al. 2018).",,,0,not_related
"In this paper, we propose a model-based learning [13, 14, 15] framework that significantly improves sample efficiency and task generalization compared to model-free methods.",,,1,related
"In this work, our predictive model serves to accelerate task learning by separately addressing representation learning, in contrast to existing model-based RL approaches, which use predictive models either for generating cheap synthetic experience [51, 22, 32] or for planning into the future [11, 13, 46, 9, 55, 26].",,,0,not_related
[32] M.,,,0,not_related
"3: for N epochs do 4: Train model Eθ on Denv via maximum likelihood 5: Unroll M trajectories int he model under πψ; add to Dmodel 6: Take action in environment according to πψ; add to Denv 7: for G gradient updates do 8: Calculate normal policy loss L(πψ,Dmodel) as in MBPO (Janner et al., 2019) 9: Sample 〈st, at, st+1, rt〉 uniformly from Dmodel 10: Rollout π starting from st under Eθ for Ttrain steps and compute the total reward R 11: Compute the worst-case reward Rmin using Algorithm 2 over horizon Ttrain.",,,1,related
"We implement CAROL on top of the MBPO (Janner et al., 2019) model-based RL algorithm using the implementation from Pineda et al. (2021).",,,1,related
"We compare CAROL with the following methods: (1) MBPO (Janner et al., 2019), our base algorithm for policy optimization.",,,1,related
"During exploration, our algorithm learns a model of the environment using an existing model-based policy optimization algorithm (Janner et al., 2019).",,,1,related
"…6: Take action in environment according to πψ; add to Denv 7: for G gradient updates do 8: Calculate normal policy loss Lnormal(πψ,Dmodel) as in MBPO (Janner et al., 2019) 9: Sample 〈st, at, st+1, rt〉 uniformly from Dmodel 10: Rollout π starting from st under Eθ for Ttrain steps and compute…",,,1,related
"We implement CAROL on top of the MBPO (Janner et al., 2019) model-based RL algorithm using the implementation from Pineda et al.",,,1,related
"We train
an ensemble of seven such neural networks by following prior work [Janner et al., 2019].",,,1,related
"It has been revealed that directly applying model-based online RL methods like MBPO [Janner et al., 2019] fails on offline datasets [Yu et al.",,,0,not_related
"The practical implementation of TATU can be generally divided into three steps: Training Dynamics Models: Following prior work [Janner et al., 2019], we train the dynamics model P̂ (·|s, a) with a neural network p̂ψ(s′|s, a) parameterized by ψ that produces a Gaussian distribution over the next…",,,1,related
"It has been revealed that directly applying model-based online RL methods like MBPO [Janner et al., 2019] fails on offline datasets [Yu et al., 2020].",,,0,not_related
"The practical implementation of TATU can be generally divided into three steps: Training Dynamics Models: Following prior work [Janner et al., 2019], we train the dynamics model P̂ (·|s, a) with a neural network p̂ψ(s|s, a) parameterized by ψ that produces a Gaussian distribution over the next state, i.",,,1,related
"…gradient targets in temporal difference learning by backpropagating through the learned dynamics; (v) Model-Based Policy Optimization (MBPO) method (Janner et al., 2019b), which shows that using short model-generated rollouts branched from real data could benefit model-based algorithms; (vi)…",,,0,not_related
"All network structure, including model, critic, and policy are the same as MAAC (Clavera et al., 2019) and MBPO (Janner et al., 2019b).",,,0,not_related
"With several key innovations (Janner et al., 2019a; Clavera et al., 2019), model-based RL algorithms have shown outstanding data efficiency and performance compared to their model-free counterparts, which make it possible to be applied in real-world physical systems when data collection is arduous…",,,0,not_related
"In our method, for a fair comparison, except the D3P planning, we keep the model learning , policy learning, and Q-function learning to be the same as prior work (Janner et al., 2019b; Clavera et al., 2019).",,,1,related
"Therefore, the sample efficiency of our method is comparable with MBPO and MAAC which also used the same state augmentation strategy.",,,1,related
"One is to serve the learned model as a black-box simulator to generate the data (Janner et al., 2019b; Yu et al., 2020; Lee et al., 2020).",,,0,not_related
"Most of the model-based RL methods make the decision by using the learned policy solely (Janner et al., 2019b; Yu et al., 2020; Clavera et al., 2019; Hafner et al., 2021).",,,1,related
Janner et al. (2019b) is a representing work of this line.,,,0,not_related
"When doing planning and rollout with the learned model to generate fake data, we follow the method used by Janner et al. (2019a); Clavera et al. (2019) to truncate the trajectory and use Q-function to approximate the return after the truncation.",,,1,related
Noting that our planner is built upon the framework of MBPO and MAAC.,,,1,related
"The detailed hyper-parameters are summarized in Table 1, and refer to Janner et al. (2019b); Clavera et al. (2019) for more details.",,,1,related
"Model-based reinforcement learning (RL) (Janner et al., 2019a; Yu et al., 2020; Schrittwieser et al., 2020; Hafner et al., 2021) has shown its promise to be a general-purpose tool for solving sequential decision-making problems.",,,0,not_related
"Following Janner et al. (2019a) and Clavera et al. (2019), we use parametric neural networks to approximate the transition function, reward function, policy function and Q-value function with the following objective function to be optimized Jf (ψ) = E [ log f(xt+1|xt, at) ] , Jr(ω) = E [ log…",,,1,related
"To show the effectiveness of our algorithm, we compare our method on six classical continuous control tasks against the following state-of-the-art model-free and model-based RL algorithms: (i) Soft Actor-Critic (SAC) (Haarnoja et al., 2018), a popular off-policy actor-critic RL algorithm based on maximum entropy RL framework; (ii) SVG(1) (Heess et al., 2015a), which first uses dynamics derivatives in model-based RL; (iii) STochastic Ensemble Value Expansion (STEVE) method (Buckman et al., 2018), which utilizes the learned models only when the uncertainty of the learned model is not too high; (iv) Model-based Action-Gradient-Estimator policy optimization (MAGE) method (D’Oro & Jaśkowski, 2020), which computes gradient targets in temporal difference learning by backpropagating through the learned dynamics; (v) Model-Based Policy Optimization (MBPO) method (Janner et al., 2019b), which shows that using short model-generated rollouts branched from real data could benefit model-based algorithms; (vi) Model-Augmented Actor-Critic (MAAC) (Clavera et al., 2019) method, which exploits the learned model by computing the analytic gradient of the returns with respect to the policy.",,,1,related
"In the first class, the models play an auxiliary role to only affect the decision-making by helping the policy learning (Janner et al., 2019b; Clavera et al., 2019).",,,0,not_related
The results are consistent with prior work Janner et al. (2019b); Clavera et al. (2019).,,,1,related
"Despite this, prevalent MBRL algorithms (Kurutach et al., 2018; Chua et al., 2018; Janner et al., 2019; Eysenbach et al., 2022) often struggle to balance exploration and exploitation, leading to poor performance when
*Work done in Google Brain (now Google DeepMind).",,,0,not_related
"For the baseline methods, we consider a range of modelbased methods including SLBO (Luo et al., 2019), PETS (Chua et al., 2018), and MBPO (Janner et al., 2019), as well as a model-free approach, SAC (Haarnoja et al., 2018).",,,1,related
"These tasks are taken from the official Github repository of MBPO (Janner et al., 2019), https://github.com/jannerm/mbpo.",,,1,related
"To put this method into practice, we amalgamate deep ensembles (Lakshminarayanan et al., 2017) and Model-based Policy Optimization (MBPO) (Janner et al., 2019).",,,1,related
"In terms of the hyperparameters, our choice of them are mostly the same as the ones adopted in MBPO (Janner et al., 2019) and Pineda et al. (2021) for Ant, Halfcheetah, Hopper, Walker2D and Cartpole-swingup, and Eysenbach et al. (2022) for Window-open-v2, which are sufficiently optimized by the…",,,1,related
"Subsequently, we present a pragmatic version of the algorithm based deep ensembles (Lakshminarayanan et al., 2017) and MBPO (Janner et al., 2019).",,,1,related
"As a mitigation strategy, Kurutach et al. (2018); Luo et al. (2019); Janner et al. (2019) propose training a policy on top of the model to amortize the planning cost.",,,0,not_related
"…PETS and ME-TRPO
Popular model-based reinforcement learning algorithms such as ME-TRPO (Kurutach et al., 2018), PETs (Chua et al., 2018) and MBPO (Janner et al., 2019) typically repeat the following three steps: 1) train a dynamics model (or an ensemble of models) q(M|DE); 2) train/extract a…",,,0,not_related
"This approach aligns with ME-TRPO (Kurutach et al., 2018), PETS (Chua et al., 2018), and MBPO (Janner et al., 2019), with the additional facet of modeling the uncertainty over policies, i.e., q(π|M,DE), as well as dynamics, i.e., q(M|DE).",,,0,not_related
"The most widespread technique for model learning involves the use of the L2 loss for one-step transitions (Kurutach et al., 2018; Luo et al., 2019; Chua et al., 2018; Janner et al., 2019; Rajeswaran et al., 2020).",,,0,not_related
"Following MBPO, we learn an ensemble of N dynamics models {T̂ iθ = N (µiθ,Σiθ)}Ni=1, each of which is a neural network that outputs a Gaussian distribution over the next state and reward and is trained independently via maximum likelihood.",,,1,related
"MBPO utilizes a standard actorcritic RL algorithm but uses an augmented dataset D ∪ Dmodel to train the policy, where Dmodel is synthetic data generated by performing h-step rollouts in M̂ starting from states in D.",,,1,related
"We are now ready to present our overall approach in Algorithm 1, which is built upon an off-the-shelf model-based off-policy online RL algorithm, model-based policy optimization (MBPO) (Janner et al., 2019).",,,1,related
"Similarly, model-based offline RL also needs to incorporate conservatism due to inevitable model errors (Xu et al., 2020; Janner et al., 2019; Luo et al., 2019).",,,0,not_related
"In line with some existing work, we choose model-based policy optimization (MBPO) (Janner et al., 2019) to learn the optimal policy for M̂.",,,1,related
"We train an ensemble of 7 such dynamics models following (Janner et al., 2019; Yu et al., 2020) and pick the best 5 models based on the validation prediction error on a held-out set that contains 1000 transitions in the offline dataset D.",,,1,related
"We focus on Dyna-style modelbased RL (Janner et al., 2019; Lin et al., 2022).",,,1,related
"To this end, we propose our AMPO (Adaptation augmented Model-based Policy Optimization) framework upon the existing MBPO (Janner et al., 2019) method with two variants, dubbed as FAMPO and IAMPO, respectively.",,,1,related
"Under this taxonomy, our approaches are Dyna-style methods that are inspired by the recent MBPO (Janner et al., 2019) algorithm.",,,1,related
"We demonstrate the detailed process of IAMPO upon the MBPO (Janner et al., 2019) backbone in Algorithm 2.",,,1,related
"Therefore, it is necessary in MBRL to derive an upper bound of the discrepancy between the expected return in the real environment ηT (π) and the expected return in the model ηT̂ (π) with the same policy π in the following form (Luo et al., 2018; Janner et al., 2019): ηT̂ (π)− ηT (π) ≤ C.",,,1,related
"In practice, following MBPO (Janner et al., 2019), we use an ensemble of probabilistic networks to represent the model and train the model ensemble via maximum likelihood.",,,1,related
"For example, MBPO (Janner et al., 2019) used the model to generate short",,,0,not_related
"3 Model-based Policy Optimization We briefly summarize the model-based policy optimization (MBPO) (Janner et al., 2019) algorithm, on top of which we build our algorithm.",,,1,related
"For model-based methods, we compare to MBPO (Janner et al., 2019), PETS (Chua et al.",,,1,related
"On the other hand, for model usage, delicate rollout schemes (Janner et al., 2019; Pan et al., 2020) have been adopted to stop the model rollouts before the simulated data deviate too much from the real distribution.",,,0,not_related
"Thereafter, in this paper, we adopt MBPO (Janner et al., 2019) as our baseline backbone framework due to its remarkable success in practice.",,,1,related
"Model-free deep RL algorithms such as SAC (Haarnoja et al., 2018), TD3 (Fujimoto et al., 2018), DDPG (Lillicrap et al., 2015) and model-based deep RL algorithms such as Dreamer (Hafner et al., 2023), TD-MPC (Hansen et al., 2022), LOOP (Sikchi et al., 2022b), MuZero (Schrittwieser et al., 2020) and MBPO (Janner et al., 2019) exemplify this class of successful algorithms.",,,0,not_related
", 2020) and MBPO (Janner et al., 2019) exemplify this class of successful algorithms.",,,0,not_related
"…et al., 2018), DDPG (Lillicrap et al., 2015) and model-based deep RL algorithms such as Dreamer (Hafner et al., 2023), TD-MPC (Hansen et al., 2022), LOOP (Sikchi et al., 2022b), MuZero (Schrittwieser et al., 2020) and MBPO (Janner et al., 2019) exemplify this class of successful algorithms.",,,0,not_related
", 2021) or can be used to generate additional, fictitious data for the agent to train on (Kurutach et al., 2018; Janner et al., 2019).",,,0,not_related
"These models can either be used for better estimates of the value function (Feinberg et al., 2018; Amos et al., 2021) or can be used to generate additional, fictitious data for the agent to train on (Kurutach et al., 2018; Janner et al., 2019).",,,0,not_related
"In fact, we also tried two different MBRL methods, namely SVG(H)-SAC [Amos et al. (2020)] and MBPO [Janner et al. (2019)] and found that their performance were not on par with our modified Dreamer.",,,1,related
"These approaches consider an MBPO-style approach (Janner et al., 2019) in the latent space of a variational model.",,,0,not_related
"A line of prior works (Yu et al., 2020; 2021c; Cang et al., 2021) use MBPO-style optimization (Janner et al., 2019), which mixes real and model-generated data in a replay buffer used for policy training.",,,0,not_related
", 2021) use MBPO-style optimization (Janner et al., 2019), which mixes real and model-generated data in a replay buffer used for policy training.",,,0,not_related
"Although learning P has many advantages since this model is very general and can be used for other problems with the same dynamics, it is a difficult problem since inaccuracies can propagate during the search making long sampled trajectories unusable [42].",,,0,not_related
(which has been increasingly studied in the context of deep RL [11]) is to use a model-based approach to accelerate learning.,,,0,not_related
"To better train fM , we use D similar to [12, 22] to collect data as the training set during the interaction of agents with the environment.",,,1,related
"To train an accurate Message Estimator, we seek inspiration from model-based reinforcement learning methods [12, 22].",,,1,related
"…Kumar et al., 2019b; Zhang et al., 2021; Kostrikov et al., 2021a); ii) enforcing conservative estimates of future rewards (Kumar et al., 2020; Yu et al., 2021; Cheng et al., 2022); and iii) model-based methods that estimate the uncertainty via ensembles (Janner et al., 2019; Kidambi et al., 2020).",,,0,not_related
", 2022); and iii) model-based methods that estimate the uncertainty via ensembles (Janner et al., 2019; Kidambi et al., 2020).",,,0,not_related
"One key challenge to RL is the distribution shift due to the difference between the learned policy and the behavior policy (Lagoudakis & Parr, 2003; Lange et al., 2012; Schulman et al., 2015; Sun et al., 2018; Janner et al., 2019).",,,0,not_related
"Related Work One key challenge to RL is the distribution shift due to the difference between the learned policy and the behavior policy (Lagoudakis & Parr, 2003; Lange et al., 2012; Schulman et al., 2015; Sun et al., 2018; Janner et al., 2019).",,,0,not_related
"Our work is also related to the model-based RL literature, where a predictive model is learned from data for policy optimization (Janner et al., 2019; Feinberg et al., 2018; Zhang, 2022; Amos et al., 2021) or planning (Wang & Ba, 2019; Schrittwieser et al.",,,1,related
"For MBPO (Janner et al., 2019), we use neural network (NN) models that are trained by minimizing the mean squared error.",,,1,related
"Future work could investigate improving sample efficiency through offline datasets [36], model-based reinforcement learning [37, 38], or better representation learning methods [39, 40].",,,0,not_related
"Model-based RL and RL with Representation Learning Model-based RL and RL with representation learning are two active research areas but without a clear general state-of-the-art (Bharadhwaj et al., 2022; Chebotar et al., 2017a; Deisenroth & Rasmussen, 2011; Eysenbach et al., 2021; Hafner et al., 2019; Janner et al., 2019; Kim et al., 2019; Laskin et al., 2021; Lee et al., 2020; Nagabandi et al., 2018; Trabucco et al., 2022; Watter et al., 2015).",,,0,not_related
", 2015) and leverage discrete-time models (Deisenroth & Rasmussen, 2011; Gu et al., 2016; Janner et al., 2019; Mhammedi et al., 2020; Nagabandi et al., 2018).",,,0,not_related
"Such a setting is designed to implement the DA-level policy which is commonly optimized by using RL algorithms [29, 49, 68, 86].",,,0,not_related
"Model-based RL [17, 49] has been developed and employed in policy optimization in dialogue act level where the dialogue policy is optimized to conduct a planning step.",,,0,not_related
"The code for building Ant and Humanoid environment is provided by Janner et al. (2019).1
B.2.2 HYPERPARAMETER SETTINGS
0 25k 50k 75k 100k Steps
0
500
1000
1500
2000
2500
3000
3500
Av er
ag e
Re tu
rn
Hopper
ours(H=1) ours(H=2) ours(H",,,1,related
Use the learned model to argument data: Janner et al. (2019) use the learned model to rollout the trajectories and use both the generated and the true trajectories to train the SAC algorithm.,,,1,related
"The learned model can be viewed as a black-box simulator and then used for training a model-free policy (Nagabandi et al., 2018; Luo et al., 2018; Kurutach et al., 2018; Wang et al., 2019; Janner et al., 2019; Pan et al., 2020).",,,0,not_related
"For MBPO, we directly use the reported number given by Janner et al. (2019)2; For SAC we use the codes and hyperparameters available3; For MAGE, we use the codes available4 and hyperparameters from the author.",,,1,related
"Recent work on model-based RL (Nagabandi et al., 2018; Luo et al., 2018; Kurutach et al., 2018; Wang et al., 2019; Janner et al., 2019; Pan et al., 2020), has shown the power of first learning the environment model and then using it to do the policy optimization.",,,0,not_related
"The third step is to perform k step model rollout on the prediction model M̃pθp with the current policy, where k is increased over time which proposed by Janner et al. (2019) to achieve better performance.",,,1,related
"Training of the prediction model is commonly through supervised learning, e.g., maximum likelihood with early stopping on a validation set (Janner et al., 2019; Clavera et al., 2019).",,,0,not_related
"The prediction model M̃p is trained following Janner et al. (2019) via maximum likelihood (Equation 2) To improve the ability of models to portray complex environment, we use a bootstrap ensemble of models {M̃θ1 , . . . , M̃θB} which is consistent with Janner et al. (2019); Clavera et al. (2019).",,,1,related
"Note that the Ant and Humanoid environments are truncated observations which is consistent with MBPO (Janner et al., 2019).",,,0,not_related
"For model-based methods, we select MBPO (Janner et al., 2019) and STEVE (Buckman et al., 2019) as our baseline which both use short-horizon model-based rollouts.",,,1,related
", maximum likelihood with early stopping on a validation set (Janner et al., 2019; Clavera et al., 2019).",,,1,related
"To address this problem, a popular approach is to use interpolation between different horizon predictions (Buckman et al., 2019; Janner et al., 2019) and interpolating between model and real data (Kalweit & Boedecker, 2017).",,,0,not_related
"Recent work on model-based RL (Nagabandi et al., 2018; Luo et al., 2018; Kurutach et al., 2018; Wang et al., 2019; Janner et al., 2019; Pan et al., 2020), has shown the power of first learning the environment model and then use it to do the policy optimization.",,,0,not_related
"…as a general-purpose tool for learning complex policies (Mnih et al., 2015; Lillicrap et al., 2016; Haarnoja et al., 2018), has the problem of low-efficiency (Janner et al., 2019), which limits the application in real-world physical systems where data collection can be an arduous process.",,,0,not_related
"For model-based methods, we select MBPO (Janner et al., 2019) and STEVE (Buckman et al.",,,1,related
"The policy being trained learns to take advantage of model errors when optimising the reward, leading to poor performance in the true environment (Cang et al., 2021; Clavera et al., 2018; Janner et al., 2019; Levine et al., 2020; Rajeswaran et al., 2016).",,,0,not_related
"MBPO and MOPO use short rollouts from ID starting locations in an explicit attempt to limit distributional shift, therefore improved in-distribution performance may be beneficial for finding locally optimal policies.",,,0,not_related
"Appendix B
Additional Experiments
B.1 Individual Demonstrator MBPO Policies
Table B.1 shows the results for training policies on individual demonstrators using rollout length h = 5.0 and MOPO penalty coefficient λ = 0.",,,1,related
B.1 Individual Demonstrator MBPO Policies . . . . . . . . . . . . . . . . . . . 72,,,1,related
"This may inhibit learning entirely, or may be exhibited as model exploitation, whereby the learned policy obtains higher reward using the dynamics model than it does when deployed to the real environment (Cang et al., 2021; Clavera et al., 2018; Janner et al., 2019).",,,0,not_related
Yu et al. (2020) subsequently adapted MBPO into the offline method MOPO.,,,0,not_related
"When the MOPO penalty coefficient is removed, we are left with the MBPO algorithm (Janner et al., 2019).",,,1,related
"When compared to a range of SOTA algorithms, MOPO outperforms model-free methods and MBPO on all but the medium dataset.",,,0,not_related
"In an attempt to limit distributional shift, previous works (Janner et al., 2019; Yu et al., 2020) sample rollout starting locations from the same dataset used to train the dynamics model, and use horizons of at most five steps.",,,0,not_related
"Methods are grouped based on how they utilise the collected rollout data: learning a dynamics model, learning a trajectory distribution, or using the data directly in model-free approaches, which learn direct mappings from states to actions (Janner et al., 2019).",,,0,not_related
"The dynamics model training process from PETS is further used in the online method MBPO (Janner et al., 2019) to train policies.",,,0,not_related
Janner et al. (2019) use fully connected neural networks with four hidden layers and 200 neurons per layer.,,,0,not_related
"…models and losses for future work but highlight that significant performance might be gained by finding better tradeoffs than those discussed in Janner et al. (2019), especially when comparing deterministic and probabilistic models (compare Appendix F) as well as value-aware and value-agnostic…",,,0,not_related
"On the Ant-v2, we see small performance improvements above the results reported by Janner et al. (2019) and Pineda et al. (2021).",,,1,related
To assure a fair comparison we used the hyperparameters provided by Janner et al. (2019) for all experiments with our approach and the NLL loss function used for the baseline.,,,1,related
"As a deep learning based Dyna algorithm, we chose Model-based Policy Optimization (MBPO) (Janner et al., 2019) and ran all of our experiments using the implementation provided by Pineda et al.",,,1,related
"As a deep learning based Dyna algorithm, we chose Model-based Policy Optimization (MBPO) (Janner et al., 2019) and ran all of our experiments using the implementation provided by Pineda et al. (2021).",,,1,related
"One of the core problems of model-based policy learning methods however is that the accuracy of the model directly influences the quality of the learned policy or plan (Schneider, 1997; Kearns & Singh, 2002; Ross & Bagnell, 2012; Talvitie, 2017; Luo et al., 2019; Janner et al., 2019).",,,0,not_related
"F.1 ABLATION EXPERIMENT WITH DETERMINISTIC MODELS
In our experiments in the Hopper domain, we used probabilistic models following Janner et al. (2019).",,,1,related
"4 in [1], setting ε π ≤ επ and all other errors set to 0.",,,0,not_related
"4 in [1], setting ε m ≤ εm and all other errors set to 0.",,,1,related
"5 Proof for Returns Estimation Error Upper Bound In this section, we prove the upper bound of the value estimation error for MBRL under the IDM framework based on Janner’s work [1].",,,1,related
"Next, we analyze the IDM framework based on Janner’s work [1].",,,1,related
"They are also utilized as a component of MbRL (Janner et al., 2019).",,,0,not_related
"First, MaPER dramatically increases sample efficiency of state-of-the-art algorithms: SAC, TD3, Rainbow, and MBPO since it largely alleviates the underestimation or overestimation of the value with the conventional Q-learning.",,,0,not_related
"To this end, we consider components in Model-based RL (MbRL).",,,1,related
MbRL. Figure 3 shows the learning curves of the MBPO with and without MaPER on the MuJoCo environments.,,,1,related
"Due to its ability to generate transitions, MbRL’s sample efficiency is remarkable on certain tasks although much larger computing costs are generally needed.",,,0,not_related
"MBPO (Janner et al., 2019)) methods with negligible memory or computational overhead.",,,0,not_related
"We validate the effectiveness of MaPER with the following algorithms: Soft Actor-Critic (SAC) (Haarnoja et al., 2018a), Twin Delayed Deep Deterministic policy gradient (TD3), Rainbow (Hessel et al., 2018), and Model-based Policy Optimization (MBPO) (Janner et al., 2019).",,,1,related
"Although our approach is fundamentally different from Model-based RL (MbRL) approaches which generate virtual experiences to train agents by planning, we briefly introduce some of them because we want to verify our method in MbRL.",,,1,related
"Thus far, various MbRL methods (Kurutach et al., 2018; Luo et al., 2018; Clavera et al., 2018; Janner et al., 2019; Rajeswaran et al., 2020; Clavera et al., 2020; Silver et al., 2017; Schrittwieser et al., 2020; Hafner et al., 2020a; Kaiser et al., 2019) have been proposed, but the common strategy…",,,0,not_related
"• MaPER can be seamlessly integrated into any modern off-policy RL frameworks with critic networks, including both MfRL (e.g. SAC (Haarnoja et al., 2018a), TD3 (Fujimoto et al., 2018)
and Rainbow (van Hasselt et al., 2019)) and MbRL (e.g. MBPO (Janner et al., 2019)) methods with negligible memory or computational overhead.",,,0,not_related
We observe that MBPO with MaPER consistently outperforms the vanilla MBPO.,,,1,related
", 2018), and Model-based Policy Optimization (MBPO) (Janner et al., 2019).",,,0,not_related
"Thus far, various MbRL methods (Kurutach et al., 2018; Luo et al., 2018; Clavera et al., 2018; Janner et al., 2019; Rajeswaran et al., 2020; Clavera et al., 2020; Silver et al., 2017; Schrittwieser et al., 2020; Hafner et al., 2020a; Kaiser et al., 2019) have been proposed, but the common strategy across them is to first learn the environment models and use them to generate fictitious experiences for learning a policy.",,,0,not_related
"…integrated into any modern off-policy RL frameworks with critic networks, including both MfRL (e.g. SAC (Haarnoja et al., 2018a), TD3 (Fujimoto et al., 2018)
and Rainbow (van Hasselt et al., 2019)) and MbRL (e.g. MBPO (Janner et al., 2019)) methods with negligible memory or computational overhead.",,,0,not_related
These impressive results with MBPO further show MaPER’s versatility and effectiveness.,,,0,not_related
"via measures of ensemble-member disagreement (Janner et al., 2019).",,,0,not_related
"Model-based approaches to ORL have commonly used ensembles as an attempt to quantify uncertainty, e.g. via measures of ensemble-member disagreement (Janner et al., 2019).",,,0,not_related
"…systems to support model-based RL. Examples from online RL include Clavera et al. (2018); Kurutach et al. (2018), which learn one-step observation-based dynamics along with extensions to ensembles Deisenroth & Rasmussen (2011); Chua et al. (2018); Janner et al. (2019); Nagabandi et al. (2020).",,,0,not_related
"In this work, we use the same FF base-model architecture and training details as MBPO (Janner et al., 2019).",,,1,related
"Thus, naive policy optimisation on a learnt model in the offline setting can result in model exploitation (Janner et al., 2019; Kurutach et al., 2018; Rajeswaran et al., 2020), i.",,,1,related
"Following previous works (Kidambi et al., 2020; Yu et al., 2021; 2020), we use model-based policy optimisation (MBPO) (Janner et al., 2019) to learn the optimal policy for M̂ .",,,1,related
"Thus, naive policy optimisation on a learnt model in the offline setting can result in model exploitation (Janner et al., 2019; Kurutach et al., 2018; Rajeswaran et al., 2020), i.e. the policy chooses actions that the model erroneously predicts will lead to high reward.",,,0,not_related
"Our approach is based on Model-Based Policy Optimisation (Janner et al., 2019).",,,1,related
"To generate the synthetic data, MBPO performs k-step rollouts in M̂ starting from states s ∈ D, and adds this data to D̂.",,,1,related
MBPO utilises a standard off-policy actor-critic RL algorithm.,,,1,related
", 2021; 2020), we use model-based policy optimisation (MBPO) (Janner et al., 2019) to learn the optimal policy for M̂ .",,,1,related
"The dashed red line indicates the Q-values from running MBPO (Janner et al., 2019).",,,1,related
"However, sampling full length trajectories is not desirable in model-based methods due to compounding modelling error (Janner et al., 2019).",,,0,not_related
"If we restrict ourselves to states and actions in the empirical dataset (EMP) or short-horizon rollouts that start in the empirical state-action distribution (DYNA), as is typical in Dyna-style approaches (Sutton and Barto, 2018; Janner et al., 2019), we limit ourselves to a small neighborhood of the empirical state-action distribution.",,,1,related
", 2008), or as training data for the agent’s policy and value functions (Sutton, 1991; Janner et al., 2019).",,,0,not_related
"…actions in the empirical dataset (EMP) or short-horizon rollouts that start in the empirical state-action distribution (DYNA), as is typical in Dyna-style approaches (Sutton and Barto, 2018; Janner et al., 2019), we limit ourselves to a small neighborhood of the empirical state-action distribution.",,,1,related
"The model is “rolled out” to generate “imagined” trajectories, which are used either for direct planning (De Boer et al., 2005; Chaslot et al., 2008), or as training data for the agent’s policy and value functions (Sutton, 1991; Janner et al., 2019).",,,0,not_related
"To alleviate the issue of compounding model error, we adopt a branching strategy [16], [12] by replacing few long-horizon rollouts with many short-horizon rollouts to reduce compounding error in model-generated rollouts.",,,1,related
"While the success of model-based RL (MB-RL) has been witnessed in many single agent RL tasks [10], [11], [12], the understanding of its MARL counterpart is still limited.",,,0,not_related
"To reduce the negative effect of model error, we adopt a branched rollout scheme proposed in [12], [16].",,,1,related
"Due to low data efficiency, model-based methods are widely studied as a promising approach for improving sample efficiency [22], [8], [11], [12].",,,0,not_related
"In order to estimate the model uncertainty accurately and alleviate the model exploitation problem, we follow previous works (Janner et al., 2019; Yu et al., 2020; Kidambi et al., 2020; Rafailov et al., 2021; Yu et al., 2021) and construct a bootstrap ensemble of K environment models {M̂}i=1.",,,1,related
"However, RL algorithms developed for the online/interactive setting usually perform poorly in the offline setting (Fujimoto et al., 2019; Janner et al., 2019) due to the data distribution shift caused by (1) the difference between the policy-in-training and the behavior policies used to collect the data; and (2) the difference between the realistic environment in which we will deploy the policy and the environments used to collect the data.",,,0,not_related
"However, RL algorithms developed for the online/interactive setting usually perform poorly in the offline setting (Fujimoto et al., 2019; Janner et al., 2019) due to the data distribution shift caused by (1) the difference between the policy-in-training and the behavior policies used to collect the…",,,0,not_related
"Prior work (Ha & Schmidhuber, 2018; Janner et al., 2019; Yu et al., 2020) demonstrated that the method effectively alleviates the model exploitation issue (Levine et al., 2020), especially in the offline setting.",,,0,not_related
"In order to estimate the model uncertainty accurately and alleviate the model exploitation problem, we follow previous works (Janner et al., 2019; Yu et al., 2020; Kidambi et al., 2020; Rafailov et al., 2021; Yu et al., 2021) and construct a bootstrap ensemble of K environment models {M̂i}Ki=1.",,,1,related
Model-based algorithms rely on an approximation of the environment’s dynamics (Sutton 1991; Janner et al. 2019).,,,0,not_related
Modelling the environment dynamics as a Gaussian distribution is common for continuous state-space applications (Janner et al. 2019; Yu et al. 2020; Kidambi et al. 2020; Yu et al. 2021).,,,0,not_related
"In the online setting, they tend to improve sample efficiency (Kalweit and Boedecker 2017; Janner et al. 2019; Feinberg et al. 2018; Buckman et al. 2018; Chua et al. 2018).",,,0,not_related
Model-based RL approaches typically use such dynamics’ models conditioned on the action as well as the state to make predictions (Janner et al. 2019; Yu et al. 2020; Kidambi et al. 2020; Argenson and Dulac-Arnold 2020).,,,0,not_related
"MOPO extends MBPO (Janner et al., 2019), which combines the soft-actor critic (SAC) policy gradient algorithm (Haarnoja et al.",,,0,not_related
"MOPO extends MBPO (Janner et al., 2019), which combines the soft-actor critic (SAC) policy gradient algorithm (Haarnoja et al., 2018a;b) with using learned dynamics models to generate short roll-outs.",,,0,not_related
"…shift, can result in model exploitation: the policy being trained learns to take advantage of model errors when optimizing the reward, leading to poor performance in the real environment (Cang et al., 2021; Rajeswaran et al., 2016; Janner et al., 2019; Clavera et al., 2018; Levine et al., 2020).",,,0,not_related
"Errors made by the learned environment models, such as those arising from poor generalization performance under distributional shift, can result in model exploitation: the policy being trained learns to take advantage of model errors when optimizing the reward, leading to poor performance in the real environment (Cang et al., 2021; Rajeswaran et al., 2016; Janner et al., 2019; Clavera et al., 2018; Levine et al., 2020).",,,0,not_related
Model-Based Policy Optimization (MBPO) [10].,,,0,not_related
"MBPO is only allowed to train for 100,000 timesteps after which its training is frozen due to its training being very computationally expensive (compared to SAC).",,,0,not_related
The results show that APEL and SSBAS correctly select MBPO as the best-performing learner and do not switch to SAC since it never (in the 10 million timesteps) starts outperforming MBPO.,,,1,related
We also run APEL and SSBAS on the Mujoco Half Cheetah domain with a SAC base learner and a MBPO base learner.,,,1,related
"The learning mechanisms used by RL learners can be broadly divided into four classes: (1) algorithms that learn (action) value functions in order to determine a policy, such as Qlearning [5] and DQN [6], (2) algorithms that learn policies directly through policy gradients, such as REINFORCE [7], (3) algorithms that combine the two through actor-critic methods, such as Soft Actor-Critic (SAC) [8] and Actor-Critic with Experience Replay (ACER) [9], and (4) algorithms that learn a model of the world and then use that model to learn a policy, e.g. Model-Based Policy Optimization (MBPO) [10].",,,0,not_related
The authors’ implementation of the MBPO algorithm is used with their hyperparameters and the stable baselines [21] implementation is used for the SAC algorithm.,,,0,not_related
We select two commonly adopted dyna-style MBRL algorithms – SLBO [6] and MBPO [7] as a foundation for evaluating value-aware approaches in continuous control.,,,1,related
"3 Evaluation on continuous control environments for value aware methods and baselines with MBPO [7], without tuning existing parameters, over 5 random seeds.",,,0,not_related
"Next, we describe our algorithm with which we find positive results in conjunction with the SLBO algorithm [6] on Swimmer-v1, Hopper-v1 and Ant-v1 and in conjunction with the MBPO algorithm [7] on Walker-v2 and HalfCheetah-v2.",,,1,related
"While several such methods have focused on how to better utilize a learned parametrized model [6, 7, 20], the choice of objective for model learning – specifically the learning of a dynamics model for predicting state transitions – has largely been overlooked.",,,0,not_related
We empirically test our proposed algorithm and novel upper bound on two recent dynastyle MBRL algorithms – SLBO [6] and MBPO [7].,,,1,related
"KL [66], total-variation [7] or Wasserstein distances [67].",,,0,not_related
"More recent research in MBRL has focused on efforts to overcome these shortcomings – including optimizing for auxiliary objectives [54, 55, 56], augmenting model-learning with exploration strategies [7, 57], meta-learning to closely intertwine the two objectives [58] and introducing inductive biases to the model-learning objective [59].",,,0,not_related
"Second, we evaluate Algorithm 2 together with our proposed and a prior value aware objective on several continuous control tasks, with two recent dyna-style MBRL algorithms – SLBO [6] and MBPO [7].",,,1,related
"3: Evaluation on continuous control environments for value aware methods and baselines with MBPO [7], without tuning existing parameters, over 5 random seeds.",,,0,not_related
"35th Conference on Neural Information Processing Systems (NeurIPS 2021).
data (e.g. ME-TRPO [6], MBPO [7]) to improve policy optimization.",,,0,not_related
"ME-TRPO [6], MBPO [7]) to improve policy optimization.",,,0,not_related
"There are also uncertainty-based and model-based methods that regularize the value function or policy with epistemic uncertainty estimated from model or value function [Janner et al., 2019; Yu et al., 2020; Uehara and Sun, 2021; Wu et al., 2021; Zhan et al., 2022].",,,0,not_related
"Recent works have proposed solutions to deal with this [24, 36, 40].",,,0,not_related
"[23] use forward and backward models in model-based policy optimization [19], a model based actor-critic method, in order to reduce accumulative model error while maintaining a similar update depth.",,,0,not_related
"Dynamics models combined with powerful search methods have led to impressive results on a wide variety of tasks such as Atari (Schrittwieser et al., 2020) and continuous control (Hafner et al., 2019a; Janner et al., 2019; Sikchi et al., 2021; Lowrey et al., 2018).",,,0,not_related
"Our work is also related to model-based RL methods which jointly learn dynamics and reward models to guide planning [45–47], policy search [48, 49], or combine both [50, 51].",,,0,not_related
"…have been proposed as the underlying cause with comprehensive analyses, and several mitigation strategies, such as model-based data augmentation (Janner et al., 2019a), the use of ensembles (Chen et al., 2021), network regularizations (Hiraoka et al., 2021), and periodically reseting the RL…",,,0,not_related
"…it is important to develop sample-efficient deep RL algorithms, that can learn efficiently even with limited amount of experience, and devising such efficient RL algorithm has been a important thread of research in recent years (Janner et al., 2019b; Chen et al., 2021; Hiraoka et al., 2021).",,,0,not_related
"However, naı̈vely doing this can lead to worse performance (e.g., on DMC tasks (Nikishin et al., 2022) and on MuJoCo gym tasks (Janner et al., 2019b)).",,,0,not_related
"Accelerate learning via model-based planning For sequential decision making problems, model-based planning is a powerful approach to improve sample efficiency and has achieved great success in applied domains such as game playing [79–81] and continuous control [82,83].",,,0,not_related
"As for the multi-joint dynamics with contact (MuJoCo) benchmark, advanced MBRL algorithms have been able to optimize the reward function [50], [51], [52].",,,0,not_related
[15] provided theoretical analysis on how to decide the simulation horizon.,,,0,not_related
"3), an alternative idea could be to learn a model of the environment itself to simulate the realistic feedback during the control-policy learning (Janner et al. 2019).",,,0,not_related
"Some model-based RL algorithms use the model just to generate additional data and update the policy using a model-free algorithm (Sutton, 1991; Janner et al., 2019).",,,0,not_related
"Techniques from MBRL need to mitigate model error either by learning a global, task-conditioned policy during model learning [45] (which then cannot be adapted to novel tasks on-the-fly), or by re-planning at every timestep to avoid error accumulation [15].",,,0,not_related
"This approach, however, requires models with sufficient long-term stability and strong generalization, which commonly applied blackbox model architectures such as CNNs often lack [45, 69, 72, 4].",,,0,not_related
"Our agent exhibited a different type of “causal confusion” similar to the model exploitation phenomena in reinforcement learning [8], where the cause of an action is attributed to a model with",,,0,not_related
"DROP MOPO MBPO BC SAC-off BEAR BRAC-v AWR CQL
random hopp.",,,1,related
"By comparing to MBPO, the effectiveness of reward penalty can be investigated while the comparison with MOPO helps reveal the potential of density ratio as the reward penalty.",,,0,not_related
"By incorporating the ratio estimation and policy regularization into an effective model-based method MBPO [13], we obtain our algorithm DROP.",,,1,related
"For model-based baselines, MBPO [13] and MOPO [42] are taken into comparison.",,,0,not_related
"14: end for
By incorporating the ratio estimation and policy regularization into an effective model-based method MBPO [13], we obtain our algorithm DROP.",,,1,related
"Following previous works [4, 13], we randomly choose a state from offline data Db and use the current policy πφ to perform h-step rollouts on the model ensemble.",,,1,related
"(5)
This bound guarantees the improvement under the true returns as long as the improvement under the model returns increases by more than the slack in the bound due to m, π (Janner et al., 2019; Levine et al., 2020).",,,1,related
"This bound guarantees the improvement under the true returns as long as the improvement under the model returns increases by more than the slack in the bound due to m, π (Janner et al., 2019; Levine et al., 2020).",,,1,related
"Following the notation of Janner et al. (2019), we denote the generalization error of a dynamics model on the state distribution under the true behavior policy as m = maxt Es∼dπbt DTV (p(st+1|st, at)||pφ(st+1|st, at)), where DTV represents the total variation distance between true dynamics p and…",,,1,related
"…proposed to relieve the issue of model bias, such as the use of multiple dynamics models as an ensemble (Chua et al., 2018; Kurutach
et al., 2018; Janner et al., 2019), meta-learning (Clavera et al., 2018), energy-based regularizer (Boney et al., 2019), game-theoretic framework (Rajeswaran et…",,,0,not_related
"A bound relating the true returns η[π] and the model returns η̂[π] on the target policy is given in Janner et al. (2019) as,
η[π] ≥ η̂[π]− [ 2γrmax( m + 2 π)
(1− γ)2 + 4rmax π (1− γ)
] .",,,0,not_related
"As the baseline of our framework is built upon MBPO implementation, we derive the “same hyperparameters” for our experiments and all the baseline algorithms.",,,1,related
"Further, the number of interactions with the true environment for outer loop policy were kept constant to 1000 for each epoch, same as MoPAC and MBPO.",,,0,not_related
Our rewards in Ant-v2 were comparable with MoPAC but still significantly better than MBPO.,,,0,not_related
"This will be compared with the existing approaches MoPAC (Morgan et al., 2021) and MBPO (Janner et al., 2019) on the benchmark MuJoCo control environments.",,,0,not_related
"Several experiments were conducted on the MuJoCo (Todorov et al., 2012) continuous control tasks with the OpenAI-Gym benchmark and the performance was compared with recent related works MoPAC (Morgan et al., 2021) and MBPO (Janner et al., 2019).",,,0,not_related
"Model Based Policy Optimisation (MBPO) (Janner et al., 2019) introduced the outer loop policy to collect transition to train approximate model and sample over it to train the policy.",,,1,related
"We consider pφ as the discounted state-action visitation corresponding to fφ (similarly p for f ) and superscript h to resemble the notations of (Janner et al., 2019).",,,1,related
", 2021) and MBPO (Janner et al., 2019) on the benchmark MuJoCo control environments.",,,0,not_related
"Now, we will derive the bounds on the performance improvement in a similar way as demonstrated in (Janner et al., 2019) and (Morgan et al., 2021), however with consideration and assumptions related to the convexity of the losses.",,,1,related
"Let the total variation distance between them be bounded by f (see (Janner et al., 2019)).",,,1,related
"Now, to realize the maximum improvement in the approximated MDP while using the policy parameters (η̃t), obtained from the shift model, we use a formulation motivated by the bound formulated in Lemma B.3 in (Janner et al., 2019).",,,1,related
"Then the difference in the returns between the real and the approximated model (Jr and J respectively) is
J (xt, η̃t)− Jr (xt, η̃t)≤2cmax (H−1)γH+1−HγH + γ
(1− γ)2 f
+ γH2 VmaxH f , Rf,H
using Lemma B.3 in (Janner et al., 2019).",,,1,related
"We would like to emphasize that our final rewards are eventually the same as achieved by MoPAC and MBPO, however the progress rate is faster for all our experiments with lesser true environment interactions.",,,1,related
"…ut,h)) c(xt,h, ut,h)
+ γH (pHφ (xt,H , ut,H)− pH(xt,H , ut,H))Vζ(xt,H)
≤2 cmax H−1∑ h=0 γh h f + γ H2 VmaxH f
=2 cmax (H − 1)γH+1 −HγH + γ
(1− γ)2 f + γ
H2 VmaxH f
where, |(ph(x, u) − phφ(x, u))| ≤ h f is inherited from Lemma B.2 in (Janner et al., 2019), the uncertainty in dynamics approximation.",,,1,related
", 2018) using samples from both datasets D ∪ D̂ similar to MBPO (Janner et al., 2019).",,,0,not_related
"Implementation details Following the recent practice (Chua et al., 2018; Janner et al., 2019; Yu et al., 2020), we model the dynamics (T̂ , R̂) using a bootstrap ensemble of neural networks.",,,1,related
"(9) can be optimized using various model-based RL algorithms, e.g. with planning (Chua et al., 2018) or using a model-free learner (Janner et al., 2019).",,,0,not_related
", 2018) or using a model-free learner (Janner et al., 2019).",,,0,not_related
"We store the generated experiences to a separate dataset D̂ and update the policy π with IPM-regularized soft actor-critic (SAC) (Haarnoja et al., 2018) using samples from both datasets D ∪ D̂ similar to MBPO (Janner et al., 2019).",,,1,related
", 2021], branched model rollouts [Janner et al., 2019] and exploration strategies [Kidambi et al.",,,0,not_related
"…optimizing for auxiliary objectives [Lee et al., 2020, Nair et al., 2020, Tomar et al., 2021], augmenting modellearning with exploration strategies [Janner et al., 2019, Kidambi et al., 2020], meta-learning to closely intertwine the two objectives [Nagabandi et al., 2018] and introducing…",,,0,not_related
"Further, while we do compare with a competitive MLE-based baseline [Luo et al., 2018], the current state of the art MBRL methods achieve significantly higher performance on the continuous control MuJoCo environments than seen in this paper (e.g. [Janner et al., 2019]).",,,1,related
"However, these methods rely on innovations in directions independent of the use of MLE, e.g. using auxiliary objectives [Lee et al., 2020, Nair et al., 2020, Tomar et al., 2021], branched model rollouts [Janner et al., 2019] and exploration strategies [Kidambi et al., 2020].",,,0,not_related
"During policy optimization in RL, by updating the current policy π to a new policy π̃, Schulman et al. (2015) prove that:
η(π̃|p) ≥ Lπ(π̃|p)− 2λγ
(1− γ)2 β 2, (1)
Lπ(π̃|p) = η(π|p) + 1 1− γEs∼Pπ,a∼π [ π̃(a|s) π(a|s)Aπ(s, a) ] ,
where λ = maxs |Ea∼π(a|s)[Aπ(s, a)]| is the maximum expected advantage following current policy π, and β = maxs DTV (π(·|s)‖π̃(·|s)) is the maximum total variation (TV) distance between π and π̃.",,,0,not_related
"In the context of model-based RL, Janner et al. (2019); Luo et al. (2019) formulate the lower bound for a certain policy’s performance on true environment in terms of the performance on the learned model.",,,0,not_related
"For monotonic policy optimization in RL, Schulman et al. (2015) propose to optimize a constrained surrogate objective, which can guarantee the performance improvement of updated policy.",,,0,not_related
"In standard RL, environment parameter p is fixed without any model discrepancy.",,,1,related
"In this paper, we focus on the generalization issue in RL, and aim to mitigate the model discrepancy of the transition dynamics between source and target environments.",,,1,related
"Figure 1 illustrates the last idea, which has received much attention recently (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"The follow-up work (Janner et al., 2019) extended PETS with policy learning.",,,0,not_related
"…et al., 2017; Ruiz et al., 2019), iii) randomizing trajectory synthesis (Andrychowicz et al., 2017; Lu et al., 2019), iv) selecting more diverse data contributors (Stasaski et al., 2020), and v) sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"Different from (Janner et al., 2019), our method is designed for dialogue agents’ discrete action space.",,,0,not_related
"In our method, each training trajectory has an overlap much larger than (Janner et al., 2019) has with the expert trajectory.",,,0,not_related
", 2020), and sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
"…parameters (Tobin et al., 2017; Ruiz et al., 2019), randomizing trajectory synthesis (Andrychowicz et al., 2017; Lu et al., 2019), selecting more diverse data contributors (Stasaski et al., 2020), and sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
", 2020), and v) sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",,,0,not_related
", 2020) and MBPO (Janner et al., 2019), as well as the standard baselines of SAC (Haarnoja et al.",,,0,not_related
"We compared
GELATO and its variants to contemporary model-based offline RL approaches; namely, MOPO (Yu et al., 2020) and MBPO (Janner et al., 2019), as well as the standard baselines of SAC (Haarnoja et al., 2018) and imitation (behavioral cloning).",,,1,related
"These benchmarks have been extensively studied by previous methods, including on-policy [18] [19], off-policy [20] [2] and model-based [21] [22] ones.",,,0,not_related
"Since almost all of the existing learning-based methods apply imitation learning to learn the simulator where GAIL is utilized in [36,25,24,31] and BC is adopted in most MBRL methods [1,6], we take GAIL and BC as baselines.",,,1,related
2) BC-based methods have good performance because of the alleviation of the compounding error due to short rollout length [6] while utilizing pφ to collect data.,,,0,not_related
"The second type of method begins from the perspective of learning policies in the environment, makes the most efficient use of the dynamic models, represented by model-based policy planning (POPLIN) [15], stochastic lower bound optimization (SLBO) [16], and model-based policy optimization (MBPO) [17], and draws upon theories to determine how to correctly make choices in the interaction between the real environment and the model.",,,0,not_related
"Thus, another important direction for future work would be to reduce sample complexity so as to increase the feasibility of real robot training, perhaps achievable via a model-based reinforcement learning approach [10, 11].",,,0,not_related
"Also, in the context of accelerating and generalizing skill-learning, the combination of learning and planning has received increasing attention [20, 37, 40, 41].",,,0,not_related
"35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.
data (e.g. ME-TRPO [6], MBPO [7]) to improve policy optimization.",,,0,not_related
"ME-TRPO [6], MBPO [7]) to improve policy optimization.",,,0,not_related
"Model-based reinforcement learning is one way to do this, and provides a framework for learning a policy from just a reward signal by optimizing for the control policy that takes actions which maximize the expected discounted return [Janner et al. 2019].",,,0,not_related
"However, model-based RL is an active area of research, and recently developed model-based approaches have been successful on many continuous control benchmarks (Janner et al., 2019).",,,0,not_related
"Alternatively, for model-based RL (e.g., (Janner et al. 2019)), the pipeline is also a linear structure with an additional module which corresponds to the dynamics model (see Figures 2(b) and 2(d), respectively).",,,0,not_related
", (Janner et al. 2019)), the pipeline is also a linear structure with an additional module which corresponds to the dynamics model (see Figures 2(b) and 2(d), respectively).",,,0,not_related
", 2020) and MBPO (Janner et al., 2019), as well as the standard baselines of SAC (Haarnoja et al.",,,0,not_related
"We compared GELATO and its variants to contemporary model-based offline RL approaches; namely, MOPO (Yu et al., 2020) and MBPO (Janner et al., 2019), as well as the standard baselines of SAC (Haarnoja et al., 2018) and imitation (behavioral cloning).",,,1,related
"Prior works 65 have shown remarkable performance to train deep visuomotor skills for a single task in low-data 66 regimes with imitation learning [11, 12, 13, 14, 15, 16], reinforcement learning [17, 18], and model 67 predictive control [19, 20].",,,0,not_related
"Our approach is similar to MBPO [23] with 1-step rollouts, except we don’t store virtual experiences in the replay buffer and instead sample fresh ones in every critic update.",,,1,related
"MBPO extends the Soft Actor-Critic (SAC) algorithm [19] (an off-policy, model-free AC method) by generating short model-based rollouts branched from real experiences that are then mixed together to augment the experience replay buffer.",,,1,related
"Recently, there has been a growing interest in extending AC methods by progressively learning and using a model of the environment dynamics [7, 8, 10, 15, 23].",,,0,not_related
"MBPO is successful in increasing the sample-efficiency of SAC, but learns its model via maximum likelihood, ignoring performance on the end task.",,,0,not_related
One of the most successful instantiations of this paradigm in deep RL is the Model-Based Policy Optimization (MBPO) algorithm [23].,,,0,not_related
"We use a probabilistic neural network (Nix and Weigend, 1994), which has been shown to be effective as an ensemble in model-based RL (Chua et al., 2018; Janner et al., 2019).",,,1,related
"Janner et al.
(2019); Buckman et al. (2018) uses ensembles to reduce model bias.",,,0,not_related
"Janner et al. (2019), proposed short model-generated branched rollouts starting from data collected on “real environment”.",,,0,not_related
"Research in deep reinforcement learning has observed that model-based algorithms have superior empirical sample-efficiency over model-free approaches [24] and thus has been preferred for applications where data collection is expensive or impossible such as offline reinforcement learning [25, 26].",,,0,not_related
"Additionally, the effects of model mismatch should be quantified in other state of the art algorithms in MBRL such as MBPO (Janner et al., 2019) and POPLIN (Wang and Ba, 2019).",,,0,not_related
"Recently, MBPO [20] showed that they can train a model of Mujoco environments that is accurate enough for nearly 200-step rollouts in terms of accumulated rewards.",,,0,not_related
"To achieve these goals, we can use a trained model of the environment similar to model-based reinforcement learning approaches [20, 23, 6], instead of interacting directly with the environment in MCTS. Recently, MBPO [20] showed that they can train a model of Mujoco environments that is accurate enough for nearly 200-step rollouts in terms of accumulated rewards.",,,0,not_related
"To achieve these goals, we can use a trained model of the environment similar to model-based reinforcement learning approaches [20, 23, 6], instead of interacting directly with the environment in MCTS.",,,0,not_related
The authors of the paper [1] first formulate and analyze a general implementation for MBPO with monotonic improvement at each step which uses a predictive model to optimize policy and utilizes the policy to collect data and train the model.,,,1,related
