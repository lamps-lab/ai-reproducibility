text,label_score,label,target_predict,target_predict_label
"We are left with the question: why does the network’s test performance improve dramatically upon continued training, having already achieved nearly perfect training performance? Recent answers to this question vary widely, including the difficulty of representation learning (Liu et al., 2022), the scale of parameters at initialisation (Liu et al.",,,1,related
Liu [16] proposes interpretations and metrics to characterize grokking.,,,0,not_related
"This avenue of study offers a macroscopic understanding of how neural networks work and has helped identify and interpret significant phenomena such as “grokking”, also known as delayed generalization where models exhibit improved generalization long after over-fitting their training set (Liu et al. 2022).",,,0,not_related
"…of study offers a macroscopic understanding of how neural networks work and has helped identify and interpret significant phenomena such as “grokking”, also known as delayed generalization where models exhibit improved generalization long after over-fitting their training set (Liu et al. 2022).",,,0,not_related
"From a macroscopic perspective, Liu et al. (2022) tackle delayed generalization or “grokking” using addition and modular addition tasks.",,,0,not_related
"1 Review of the Clock Algorithm As in past work, we find that after training both Model A and Model B, embeddings (Ea,Eb in Figure 1) usually describe a circle [8] in the plane spanned by the first two principal components of the embedding matrix.",,,1,related
"Mechanistic interpretability is closely related to training dynamics [8, 13, 1].",,,0,not_related
"Training phases Previous work (Power et al., 2022; Liu et al., 2022) used the terms confusion, memorization and comprehension in the phase diagram based on different hyperparameters, but in this paper they also refer to the phases along the training trajectory.",,,0,not_related
"We found that the dimensionality of the network layers (mainly the last layer) correlates with oscillations in training and validation performances (loss and accuracy) both in toy models (Liu et al., 2022) and transformer (Power et al.",,,0,not_related
"Others have studied microscopic phenomena that coincide or come in tandem with delayed generalization, such as the emergence of structure in embedding space (Liu et al., 2022) and the slingshot effect (Thilak et al.",,,0,not_related
"Recent work has shown that grokking is observed only with a certain range of hyperparameters (Power et al., 2022; Liu et al., 2022).",,,0,not_related
"Multiple recent papers have introduced synthetic tasks in order to better understand and interpret transformers [7, 29, 35, 53].",,,0,not_related
"This focuses our study on attention and feed-forward mechanisms, while avoiding the difficulty of learning representations, which may require complex nonlinear dynamics [13, 29, 40].",,,0,not_related
", word embeddings [33, 27], or grokking [29, 35]), factorized key-query and value-output matrices that may induce additional regularization effects [17], and non-linear feedforward layers, which may provide richer associative memories between sets of embeddings.",,,0,not_related
"In contrast to (Liu et al., 2022; Nanda et al., 2023) where post-processing (e.",,,0,not_related
"For modular addition, (Liu et al., 2022) discovers that ring-like representations emerge in training.",,,0,not_related
"In contrast to (Liu et al., 2022; Nanda et al., 2023) where post-processing (e.g., principal component analysis) is needed to obtain ring-like representations, the ring structures here automatically align to privileged bases, which is probably because embeddings are also regularized with L1.",,,0,not_related
"A generalization puzzle called grokking (Power et al., 2022) has also been understood by reverse engineering neural networks (Nanda et al., 2023; Chughtai et al., 2023; Liu et al., 2023; 2022).",,,0,not_related
"But what does a generalizing circuit imply about the origins of GPT-2’s greater-than capabilities—do they stem from from memorization [31, 4], or rich, generalizable representations of numbers [16]? The presence of a greater-than circuit does not preclude memorization.",,,0,not_related
"[16] train a toy transformer model on modular addition, and find that its number representations become structured only after it moves from overfitting to generalization.",,,0,not_related
"Grokking is thought to relate to the build-up of generalizable representations [Liu et al., 2022, Chughtai et al., 2023]; in a similar vein, we found that emergence of many tasks coincided with improvement in structural representations.",,,0,not_related
Liu et al. (2022) demonstrate that grokking generalization speed can be accelerated in a simple embeddingdecoder model by using a larger learning rate for the embedding than for the decoder.,,,0,not_related
"Liu et al. (2022) demonstrate that grokking generalization speed can be accelerated in a simple embeddingdecoder model by using a larger learning rate for the embedding than for the decoder. This is in line with findings by Heckel & Yilmaz (2020) on epoch-wise double descent, where decreasing learning rates in later layers (which learn faster) aligns pattern learning speeds. We consider this further evidence that both grokking and epoch-wise double descent occur as a result of similar learning dynamics resulting from different speeds of pattern development. Nanda & Lieberum (2022) investigate grokking through mechanistic interpretability, with findings in line with our results (specifically observing the development of a Type 3 pattern).",,,1,related
"Liu et al. (2022) demonstrate that grokking generalization speed can be accelerated in a simple embeddingdecoder model by using a larger learning rate for the embedding than for the decoder. This is in line with findings by Heckel & Yilmaz (2020) on epoch-wise double descent, where decreasing learning rates in later layers (which learn faster) aligns pattern learning speeds.",,,0,not_related
"Our findings agree with Liu et al. (2022c) in that grokking seems intrinsically linked to the relationship between performance and weight norms; and with Barak et al. (2023) and Nanda et al. (2023) in showing that the networks make continuous progress toward a generalizing algorithm, which may be…",,,0,not_related
"Liu et al. (2022b) construct further small examples of grokking, which they use to compute phase diagrams with four separate ‘phases’ of learning.",,,0,not_related
Liu et al. (2022a) study how Transformers learn group theoretic automata.,,,0,not_related
"In contrast to Power et al. (2022) but in line with Liu et al. (2022), grokking does not occur when both train and test loss improve together without the initial divergence, as shown in many of the figures in this paper, for example Figures 2 and 18.",,,0,not_related
"(2022) but in line with Liu et al. (2022), grokking does not occur when both train and test loss improve together without the initial divergence, as shown in many of the figures in this paper, for example Figures 2 and 18. The core issue is that the model has two possible solutions: memorization (with low train loss and high test loss) and a generalization (with low train loss and low test loss). In our case, the Fourier Multiplication Algorithm is the generalization solution. Intuitively, with very little training data, the model will overfit and memorize. With more training data, the model must generalize or suffer poor performance on both train and test loss. Since neural networks have an inductive bias favoring “simpler” solutions, memorization complexity scales with the size of the training set, whereas generalization complexity is constant. The two must cross at some point! Yet, the surprising aspect of grokking is the abrupt shift during training, when the model switches from memorization to generalization. The other component of grokking is phase transitions - the phenomena where models trained on a certain task develop a specific capability fairly rapidly during a brief period of training, as shown for the case of induction heads forming in transformer language models in Olsson et al. (2022) and our results in Appendix D.",,,0,not_related
"…decay is necessary for grokking: smaller amounts of weight decay causes the network to take significantly longer to grok (echoing the results on toy models from Liu et al. (2022)), and our networks do not grok on the modular arithmetic task without weight decay or some other form of regularization.",,,0,not_related
"Liu et al. (2022) construct small
examples of grokking, which they use to compute phase diagrams with four separate “phases” of learning.",,,0,not_related
"(2022) but in line with Liu et al. (2022), grokking does not occur when both train and test loss improve together without the initial divergence, as shown in many of the figures in this paper, for example Figures 2 and 18.",,,0,not_related
"1, we provide additional evidence that weight decay is necessary for grokking: smaller amounts of weight decay causes the network to take significantly longer to grok (echoing the results on toy models from Liu et al. (2022)), and our networks do not grok on the modular arithmetic task without weight decay or some other form of regularization.",,,1,related
"In subsequent work [7], the authors simplified the architecture to a single linear learnable encoder followed by a multilayer perceptron (MLP) decoder and showed that, even if the task is recast as a classification problem, grokking persists.",,,0,not_related
[7] argued that grokking is due to the competition between encoder and decoder.,,,0,not_related
Liu et al. (2022) further studied this phenomenon by focusing on addition and permutation as toy models.,,,0,not_related
"In [8], an effective theory of grokking has been proposed.",,,0,not_related
"In [8] a consistent observation has been made, namely larger weight decay leads in most cases to a larger parameter region where grokking is observed.",,,0,not_related
"The works [6, 8] do not study the differences between L1 and L2 regularisations.",,,0,not_related
This result partially explains the observation in [8] which relates grokking to structure formation and effective dimension decrease at the transition.,,,0,not_related
"On the other hand, we do observe phenomena related to neural collapse [5] and structure formation [8].",,,0,not_related
"The grokking phenomenon has been discussed within an effective theory approach [8], where an empirical connection between representation/structure formation and generalisation has been made.",,,0,not_related
"This is consistent with the observations of [6, 8] where a shorter grokking time has been reported for increased number of training samples and a larger weight decay.",,,0,not_related
"These results provide, some justification of the numerical observation in [6, 8, 9] that weight decay increases the parameter region where grokking is observed.",,,0,not_related
"Similarly, in [8] the authors argue that the grokking in deep models is related to structure formation.",,,0,not_related
Our findings differ from those of [8] in that we discuss ensemble/average phenomena.,,,1,related
"Preliminary theoretical work (31) suggests that the task structure imposes highly specific constraints on the representations that can achieve perfect generalization, and “sudden insight” occurs when these constraints are fulfilled.",,,0,not_related
"Setup We take the toy addition setup in (Liu et al., 2022), where each input digit 0 ≤ i ≤ p − 1 (output label 0 ≤ k ≤ 2(q − 1)) is embedded as a vector Ei (Yk).",,,1,related
"Architecture Similar to Liu et al. (2022), the decoder architecture is an MLP with hard coded addition.",,,0,not_related
"As reported in (Power et al., 2022; Liu et al., 2022), we see that there exists a critical training set size below which generalization is impossible.",,,1,related
"Several formal or informal attempts have been made to understand grokking: (a) (Liu et al., 2022) attributes grokking to the slow formation of good representations.",,,0,not_related
"The effective theory analysis in (Liu et al., 2022) only applies to algorithmic datasets, but not to other datasets with unknown optimal representations.",,,0,not_related
"This formula agrees with the observation that large weight decays γ and/or larger decoder learning rates ηD can make generalization happen faster (Power et al., 2022; Liu et al., 2022).",,,0,not_related
"Partial answers to Q1 are provided in recent studies: Liu et al. (2022) attribute grokking to the slow formation of good representations, Thilak et al. (2022) attempts to link grokking to the slingshot mechanism of adaptive optimizers, and Barak et al. (2022) uses Fourier gap to describe hidden…",,,0,not_related
"1 ALGORITHMIC DATASETS Setup We take the toy addition setup in (Liu et al., 2022), where each input digit 0 ≤ i ≤ p − 1 (output label 0 ≤ k ≤ 2(q − 1)) is embedded as a vector Ei (Yk).",,,1,related
"Modern research points towards a direction where, following an initial short-lived drop, validation performance picks up again after more training [28].",,,0,not_related
"released their paper ""Towards Understanding Grokking: An Effective Theory of Representation Learning""[4].",,,0,not_related
"A few weeks after, a subset of the same authors released another paper [6] in which they explain grokking behaviour by looking at loss landscapes and analizing model weight size, again with the toy setting they introduced before [4].",,,0,not_related
"[4] to look at grokking as tightly interconnected to the representations that have to be learned by the model to solve the task, but it challenges the notion that a clear encoder-decoder dichotomy is responsible for grokking.",,,0,not_related
"Additionally, related work [4, 5] seems to agree that grokking strongly correlates to representation learning, which is the process of finding relevant structures in complicated data like pictures, videos and text and thus essential to many applied machine learning tasks.",,,0,not_related
"In the paper Liu et al. released in October [4], the authors break down grokking behaviour as a problem of representation learning.",,,0,not_related
"released in October [4], the authors break down grokking behaviour as a problem of representation learning.",,,0,not_related
"To analyze the grokking phenomenon, the authors Liu et al. released their paper ""Towards Understanding Grokking: An Effective Theory of Representation Learning""[4].",,,0,not_related
