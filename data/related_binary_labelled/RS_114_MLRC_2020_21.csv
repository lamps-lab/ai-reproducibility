text,label_score,label,target_predict,target_predict_label
"Despite numerous existing algorithms for pruning (Singh & Alistarh, 2020; Zhu & Gupta, 2017; Gale et al., 2019; Jaiswal et al., 2022; Lin et al., 2020; Liu et al., 2021a; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020) and quantization (Dong et al.",,,0,not_related
"Sparse training (Mocanu et al., 2018; Jaiswal et al., 2022; Mostafa & Wang, 2019; Evci et al., 2020; Liu et al., 2021; Yuan et al., 2021; Yin et al., 2023; Kundu et al., 2021), on the other hand, starts with a (random) sparse network and updates network connectivity during training to search for good sparse neural network without any pre-training and dense training steps.",,,0,not_related
"On the other hand, unstructured sparsity [16]–[18] involves pruning elements in any position without constraints, leading to a high sparse ratio (over 80% sparse ratio in [9]) and a significant reduction in the number of FLOPs for training.",,,0,not_related
"A majority of prior arts mainly focus on ReLU-based [42], [43], structured [6], [7] or unstructured sparsity [8], [9] for sparse training.",,,0,not_related
"Structured sparsity has limitations in reducing computational complexity (less than 40% sparse ratio in [6]), while unstructured sparsity, despite saving a great number of operations (over 80% sparse ratio in [9]), is difficult to accelerate on hardware [20].",,,0,not_related
Recent pruning policies improve efficiency by starting with a sparse network (Evci et al. 2020).,,,0,not_related
"If considering s0 > 0, the layerwise sparsity of the initial mask follows the ERK distribution introduced in [31].",,,1,related
"PruneFL starts with a pre-selected node to train a global shared mask function, while FedDIP generates the mask function with weights following the Erdős-Renéyi-Kernel (ERK) distribution [31], as we will discuss in the later sections.",,,1,related
"Initial studies by [26] showed how neural networks produce accurate yet overconfident predictions, and it has been seen that this problem is exacerbated even further [13] by certain sparse training regimes like RigL [27].",,,0,not_related
Gradient growth [31]: It involves adding new connections based on the gradients values of the connections during the training process.,,,0,not_related
"Similar to the previous studies [25, 31, 32, 34], we use Cosine Annealing fdecay(t, α, Tend) = α2 (1 + cos( tπ Tend )) for topology update.",,,1,related
"Erdős-Rényi Kernel (ERK) [31]: Inspired by Erdős-Rényi initialization [28], ERK is formulated as ε(nl−1+nl+wl+hl) nl−1nlwlhl where ε indicates the sparsity level, n denotes the number of neurons at layer l, w and h are the width and the height of the l convolutional kernel.",,,0,not_related
"Inspired by SET, RigL [31] and SNFS [32] introduced gradient growth and momentum growth: the idea of using gradient and momentum information obtained by backward pass is to grow the promising connections that accelerate the exploration of optimal sparse topology.",,,0,not_related
"We know that uniform initialization assigns equal connections to each layer without considering the layer size and ERK allocates more connections to the layers with more parameters and less connections to the layers with fewer parameters [28, 31].",,,1,related
"Recent discoveries [10, 13, 25] demonstrate that layer-adaptive sparsity is the superior pruning scheme.",,,0,not_related
"[10], which is an extended Erdős-Rényi method for CNNs pruning, where layer-wise sparsity is selected by a closed-form criterion dependent on merely the layer architecture (e.",,,0,not_related
"[45, 10] adopted a global pruning threshold throughout all layers in the network to meet the model sparsity constraint.",,,0,not_related
"Table 1: Classification accuracy of sparse NNs for varying pruning rates κ based on our proposed method ART with L1, L2, and HyperSparse regularization LHS compared to dense models, and masks obtained by SNIP [20], GraSP [42], SRatio [34], LTH [9], IMP [16] and RigL [8].",,,1,related
In addition we evaluate IMP [16] and RigL [8] as dynamic pruning methods.,,,1,related
"The more resource efficient sparse→sparse methods sustain sparse NNs during training [4,8,20,32,34,37,42], whereas dense→sparse methods utilize all parameters before finding the final mask [9, 15, 16, 24, 31, 36].",,,0,not_related
"formes the methods SNIP [20], Grasp [42], SRatio [34], LTH [9] and RigL [8] on all sparsity levels.",,,0,not_related
"For example, RigL [8] iteratively prunes weights with low magnitude and therefore reactivates weights with highest gradient.",,,0,not_related
"The weights that are reactivated can be selected randomly [25] or determined by the gradients [3, 4, 8].",,,0,not_related
"[101] systematically investigate some dynamic sparse training methods (such as RigL [84], SET [93]) in RL.",,,0,not_related
[84] propose Rigged Lottery (RigL) that actives new connections during training by using gradient magnitude.,,,0,not_related
"After pruning, many PDT methods ([64, 84, 85, 86, 87]) directly obtain the subnetworks and do not require trainingfrom-scratch or fine-tuning process anymore.",,,1,related
"Graesser et al. [101] systematically investigate some dynamic sparse training methods (such as RigL [84], SET [93]) in RL.
Evci et al. [102] analyze the reasonability of dynamic sparse training.",,,0,not_related
Evci et al. [84] propose Rigged Lottery (RigL) that actives new connections during training by using gradient magnitude.,,,0,not_related
"A class of the PDT methods ([84, 93, 94, 95, 96, 97, 98]) take randomly initialized sparse network rather than dense network as the input model.",,,0,not_related
"By repeating the pruneand-grow cycle during training, this kind of method keeps searching for better sparse architecture, which is classified as dynamic sparse training in [84].",,,0,not_related
", inference), researchers have proposed various techniques (pruning [6, 13, 14, 40], sparse training [9, 21, 43], etc.",,,0,not_related
"Our method slightly differs from the original pruning and regrowing strategy [5, 10] in that we allow the pruned mask to regrow back in the same round, which allows the amounts of readjusted masks to differ across clients.",,,1,related
"• Enforcing model sparsity proves effective to reduce the training workload by learning a sparse sub-network [10, 34].",,,0,not_related
"Given a sparsity rate S, both the communication and the computation cost in federated learning will roughly reduce by S percent since they are proportional to the number of parameters [5, 10].",,,0,not_related
"The reduction in computation cost is also proportional to the degree of sparsity, but slightly lower, since our sparsity setting follows the ERK distribution [10] which assigns lower sparsity to layers with fewer redundant parameters.",,,0,not_related
"Afterward, each client can readjust its maskmc via pruning and regrowing [5, 10], with a readjustment ratio of αs (Line 10-12).",,,1,related
FedDST [5] utilizes dynamic sparse training [10] in generic FL setting.,,,0,not_related
"Roughly, for both benchmarks, it is known that sparsities lower than 90% can be achieved with approximately 1% accuracy loss relative to the original dense model, but accuracy rapidly decreases in the 90-95% range [27, 16], and that decreases are drastic at higher (≥ 95%) sparsities [67, 43].",,,0,not_related
"In this context, we implemented three leading sparse training methods: Gradual Magnitude Pruning (GMP) [81], RigL [16] and AC/DC [60], which we execute for an increasing number of epochs between 100 (standard) and 1000 (10x).",,,1,related
"In prior work, RigL executed >5x extended training for a 99%-sparse model only [16].",,,1,related
"By contrast, in sparse training methods, parameters are pruned from the model during training from scratch, either close to initialization [16, 37, 46, 70, 66], or progressively as the model is trained [24, 21, 65].",,,0,not_related
"[21] found that extended training did improved results for GMP in some cases, while RigL [16] and Powerpropagation [66] found diminishing improvements.",,,0,not_related
"Therefore, to create a more fair comparison, we consider estimated Floating-Point Operations (FLOPs) necessary for inference; these are computed as in [16].",,,1,related
"A subset of sparse training methods are dynamic, in the sense that weights may be reintroduced during training [16, 60].",,,0,not_related
"[27, 14, 21, 16, 67, 65, 60], and language modelling using the BERT-base model [12] on the GLUE benchmark datasets [73], e.",,,0,not_related
"While several novel ways of choosing or updating the sparsity mask choice (step 1), have been investigated, by and large, for the second step, that of optimizing the remaining weights, sparse training methods largely emulate the hyperparameters of the baseline dense model, including the total number of training epochs [21, 36, 16, 60].",,,0,not_related
"The RigL pruning method [16] is a common, high-performing benchmark for dynamic sparse training.",,,0,not_related
"While not the focus of this work, a similar motivation can also be found in dynamic sparse training methods, which aim to identify relevant sub-networks during training and promote the prunability of the network [70, 23, 26].",,,0,not_related
This sparse-to-sparse strategy with improved training efficiency facilitates ”single-shot” pruning [20] and Dynamic Sparse Training (DST) [21]–[23].,,,0,not_related
"RigL-PFL attains better results than SNIP, which veriﬁes the effectiveness of dynamically updating sparse networks.",,,0,not_related
4) RigL-PFL [23].,,,0,not_related
"In particular, RigL [23] activates more important parameters according to the instantaneous gradient information in the previous round of training.",,,0,not_related
"These graphs depict the results of the other two most competitive approaches (RigL-PFL, Hermes) compared to ASPFL.",,,0,not_related
"In contrast, RigL-PFL is unable to reach the desired 99% accuracy level, indicating the superior performance of ASPFL in terms of convergence speed.",,,0,not_related
"For Static, SNIP, RigL-PFL and ASPFL with sparse-to-sparse training, the communication overhead is similar because they have the same amount of remaining parameters.",,,0,not_related
"6 (c), ASPFL achieved a signiﬁcantly higher accuracy of 44% compared to the accuracies of only 40% and 28% achieved by Hermes and RigL-PFL at the 350th communication round, respectively.",,,0,not_related
"Sparse training is an efficient and effective way to add this type of regularization to a neural network [27,30,31].",,,0,not_related
"Following [37, 39], we apply a cosine decay scheduler to alleviate this problem:",,,1,related
"The criterion of pruning could be weight magnitude [44], gradient [37] and Hessian [35, 45], etc.",,,0,not_related
"Limited by the requirement for the pretrained model, some recent research [37, 38, 39, 40, 41, 42, 43] attempts to discover a sparse network directly from the training process.",,,0,not_related
"[51] use dynamic sparse training [55, 13, 16] methods to efficiently generate diverse networks for ensembling.",,,0,not_related
"Ensuring a fair comparison we only consider the dense-to-sparse training paradigm, as opposed to pruning at initialization [46, 66] and dynamic sparse training methods [13, 16].",,,1,related
"As the growing criterion, we choose the uniform random [40] and gradient [12] approaches, as they are widely used in the literature.",,,1,related
2 and gradient growth (we select the gradient-based growth method as it is known to provide good performance in this setup [12]).,,,1,related
"One of the most common choices for initialization schemes is the Erdős-Rényi (ER) method [40], and its convolutional variant, the ER-Kernel (ERK) [12].",,,0,not_related
"In DST, the neural network structure is constantly evolving by pruning and growing back weights during training [40, 3, 11, 12, 59, 56, 1, 9, 27].",,,0,not_related
"Most research in this domain is devoted to investigating the best growth criterion as the most influential factor in the performance of DST [3, 12, 11, 1], disregarding any potential contribution coming from the choice of the weight removal algorithm.",,,0,not_related
The second category selects weights based on the largest gradient magnitude [12].,,,0,not_related
We compare the results with the original dense model and static sparse training with ERK initialization.,,,1,related
"Due to its simplicity and effectiveness, the magnitude criterion has been a common choice in standard post-training pruning, as well as in sparse training [15, 12].",,,0,not_related
"One of the most common choices for initialization schemes is the Erdős-Rényi (ER) method [40], and its convolutional variant, the ER-Kernel (ERK) [12].",,,0,not_related
"Most notably, in computer vision, DST demonstrated that it is sufficient to use only 20% of the original parameters of ResNet50 to train ImageNet without any drop in performance [12, 37].",,,0,not_related
"DST Settings specific for dynamic sparse training: We use the ER initialization for the distribution of sparsity levels in MLP, and the ERK initialization for the convolutional models.",,,1,related
"CMagnitude CSET CMEST CSensitivity CSNIP |θ| |θ+|, |θ−| |θ|+ λ|∇θL(D)| |∇θL(D)| |θ| |θ||∇θL(D)| random growth × SET[40] MEST[59] Sensitivity[43] × gradient growth RigL[12] × × × ×",,,0,not_related
"The distribution of s λ adopts the ERK distribution model [7], and the mask matrix m is generated according to the ERK distribution model.",,,0,not_related
RigL [8] alternatively removes and revives weights based on magnitudes and gradients.,,,0,not_related
"This variability in spatial sparsity remains consistent across different layers and network types, irrespective of the specific unstructured sparsity technique employed [11, 8, 27].",,,0,not_related
"Figure 2: Spatial sparsity of common unstructured sparsity methods including Magnitude-based sparsity [11], RigL [8], GraNet [23].",,,0,not_related
"We do not delve deeply into this matter at present, but it is unequivocal that unstructured sparsity methods [11, 8, 27] allocate more weights to certain fixed visual points simultaneously.",,,1,related
The gradient-based growth rule forms new synaptic connections by selecting the pruned connections with the largest gradient obtained from the instantaneous weight gradient information in (Evci et al. 2020; Dettmers and Zettlemoyer 2019).,,,0,not_related
"This also excludes any strategy that requires, even if only intermittently, a full, dense gradient to be computed, such as [10].",,,0,not_related
"Another fashion is dynamic sparsity exploration [49, 50, 51, 52, 53, 54, 55, 56, 57, 58] which allows pruned connections can be re-activated in the latter training stage.",,,0,not_related
"…parameter redundancy (Aghajanyan et al., 2021), allowing arbitrary selection of trainable parameters for tuning without greatly degrading performance (Desai et al., 2019; Chen et al., 2020; Prasanna et al., 2020; Evci et al., 2020); thus, controllers (modules) might have higher degrees of freedom.",,,0,not_related
", 2021), allowing arbitrary selection of trainable parameters for tuning without greatly degrading performance (Desai et al., 2019; Chen et al., 2020; Prasanna et al., 2020; Evci et al., 2020); thus, controllers (modules) might have higher degrees of freedom.",,,0,not_related
"One approach is to determine nonzero weights dynamically, such as dense-to-sparse training [2], pruning [11], and sparse-to-sparse training [12, 13].",,,0,not_related
It is common to initialize sparse subnetworks θs randomly based on the uniform [40; 5] or nonuniform layer-wise sparsity ratios with Erdős-Rényi (ER) graph [39; 10; 35; 31].,,,0,not_related
Static sparse training determines the structure of the sparse network at the initial stage of training by using certain pre-defined layer-wise sparsity ratios [38; 39; 10; 33].,,,0,not_related
We follow the widely-used sparse training framework used in [39; 10].,,,1,related
RigL [10] and SNFS [5] propose to uses gradient information to grow weights.,,,0,not_related
"However, with the popularity of RigL [10], it is common to use a fixed set of layer-wise sparsities.",,,0,not_related
", magnitude) of the nonzero weights into consideration due to the crucial role of magnitude to dynamic sparse training [39; 10; 35].",,,0,not_related
"Dynamic sparse training (DST) [39; 10; 35], on the contrary, jointly optimizes the weights and sparse patterns during training, usually delivering better performance than the static ones.",,,0,not_related
Method Channel Sparsity 10% 20% 30% Standard RigL [10] 76.,,,1,related
", pruning r proportion of the least important parameters based on their magnitude, and immediately grow the same number of parameters randomly [39] or using the potential gradient [10].",,,0,not_related
"Taking the most representative DST approaches SET [39] and RigL [10] as examples, we measure the number of the Sparse Amenable Channels across layers in Figure 2, with v equals 0%, 20%, 30%, and 40%.",,,1,related
"[8] investigates DST for BERT language modeling tasks and shows Pareto improvement
over the dense model in terms of FLOPs.",,,0,not_related
"We report the theoretical FLOPs to be independent of the used hardware, as it is done in the unstructured pruning literature [31, 10].",,,1,related
"1 Performance Comparison with Pruning and Sparse Training Algorithms We compare PALS with a standard during-training pruning approach (GMP [68]), GraNet [31], and a well-known sparse training method (RigL [10]).",,,1,related
"While DST and GMP use fixed sparsification policies (fixed sparsity level (Stable in Figure 1) and constantly prune the network until the desired sparsity level is reached (Shrink in Figure 1), respectively) and require the final sparsity level before training, PALS exploits heuristic information from the network at each iteration to automatically determine whether to increase,
decrease, or keep the sparsity level at each connectivity update step.",,,0,not_related
"Method Shrink Stable Expand Adaptive Sparsity Schedule Automatically tuning sparsity level RigL [10] ✗ ✓ ✗ ✗ ✗ GMP [68] ✓ ✗ ✗ ✗ ✗ GraNet [31] ✓ ✓ ✗ ✗ ✗ PALS (ours) ✓ ✓ ✓ ✓ ✓ In this work, we take advantage of the successful mechanism of “Shrink” from during-training pruning (such as GraNet [31]) and “Stable” from DST (such as RigL [10]) and propose for the first time the “Expand” mechanism, to design a method to automatically optimize the sparsity level during training without requiring to determine it beforehand.",,,1,related
"[10] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.",,,0,not_related
It prunes the weights (as performed in GMP) while allowing for connection regeneration (as seen in Dynamic Sparse Training (DST) which will be explained in the following).,,,0,not_related
"GraNet gradually shrinks a network (here, we start from a dense network) during the training to reach a pre-determined sparsity level, while allowing for connection regeneration inspired by DST. GraNet algorithm is described in Appendix B.",,,1,related
PALS GraNet*[31] RigL*[10] GMP*[68] Dataset loss sparsity epochs loss sparsity epochs loss sparsity epochs loss sparsity epochs Electricity 0.,,,1,related
"The sparse topology can remain fixed (static) [38] or dynamically optimized during training (a.k.a Dynamic Sparse Training (DST)) [39, 10, 33, 20, 1, 61, 47].",,,0,not_related
"In this work, we take advantage of the successful mechanism of “Shrink” from during-training pruning (such as GraNet [31]) and “Stable” from DST (such as RigL [10]) and propose for the first time the “Expand” mechanism, to design a method to automatically optimize the sparsity level during training without
requiring to determine it beforehand.",,,1,related
"PALS is in essence inspired by the DST framework [39] and gradual magnitude pruning (GMP) [68, 31].",,,0,not_related
"PALS leverages the effective strategies of ""Shrink"" from during-training pruning (e.g., GraNet) and ""Stable"" from DST (e.g., SET, RigL).",,,0,not_related
"Figure 1: Schematic overview of the proposed method, PALS (Algorithm 1), Dynamic Sparse Training (DST) [39, 10], During-training pruning (Gradual Magnitude Pruning (GMP) [68], and GraNet [31]).",,,0,not_related
"The growth criteria can be random, as in the Sparse Evolutionary Training (SET) algorithm [39], or based on the gradient, as in the Rigged Lottery (RigL) algorithm [10].",,,0,not_related
"However, unstructured pruned models also have this advantages as it can lead to reduced computations for some hardware while maintaining a low number of parameters [19].",,,0,not_related
Such methods can improve efficiency in parameters for both training and inference [19].,,,0,not_related
"Specifically, it has been shown that the winning ticket can be transferred between datasets and tasks [19, 47, 13, 57].",,,0,not_related
"Other works demonstrated the generality of the winning ticket and showed that a single pruned network can be transferred across datasets and achieve good performance after fine-tuning [19, 47, 13] and even that LT can be used for non-natural datasets [57].",,,0,not_related
"[14] Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich.",,,0,not_related
"A third family of algorithms, called Dynamic Sparse Training (DST) techniques, aims at modifying the structure of the sparse network during the training process [18, 12, 34, 45, 14, 30].",,,0,not_related
"Their ability to improve performance over random pruning algorithms has been discussed in [39], which shows how even small perturbations of random pruning (Layer-wise Random Pruning such as ER [12] and ERK [14]) are able to outperform well-engineered PaI algorithms.",,,0,not_related
"The experimental setup is based on the ones used in [45, 14, 32].",,,0,not_related
"98 on MobileNet-V2, that is known to be difficult to prune due to its separable convolutions layers [54, 14].",,,0,not_related
"Algorithm Layer-wise Sparsity Uniform [54] s ∀ l ∈ [0, N) ER [12] 1 − n l−1+nl nl−1×nl ERK [14] 1 − n l−1+nl+wl+hl nl−1×nl×wl×hl Table 4: Dynamic Sparse Training Pruning Algorithms.",,,0,not_related
"based on random growth [12, 34], momentum [45], or absolute gradients [14].",,,0,not_related
"Algorithm Layer-wise Sparsity
Uniform [54] sl ∀ l ∈ [0, N) ER [12] 1 − n l−1+nl
nl−1×nl
ERK [14] 1 − n l−1+nl+wl+hl
nl−1×nl×wl×hl
Since the graph size of the proposed GE is based on the size of the input data, we selected three datasets with the same data sizes, namely CIFAR-10, CIFAR-100 [28], and the downscaled Tiny-ImageNet (of size 32 × 32 pixels) [11].",,,1,related
DSR [34] |θ| random random (zero layers) Uniform SNFS [45] |θ| momentum momentum Uniform RigL [14] |θ| gradient ✗ ERK SET-ITOP [32] |θ| random ✗ ERK,,,0,not_related
"This model has been benchmarked in [14, 27, 7].",,,0,not_related
"Sources are the following:
• SNIP https://github.com/mil-ad/snip • GraSP https://github.com/alecwangcq/GraSP • SynFlow https://github.com/ganguli-lab/Synaptic-Flow • ProsPR https://github.com/mil-ad/prospr • DSR and SNFS https://github.com/TimDettmers/sparse_learning • ITOP https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization • RigL https://github.com/nollied/rigl-torch9
• Uniform, ER and ERK https://github.com/VITA-Group/Random_Pruning",,,1,related
"This includes one-shot pruning methods (SNIP [Lee et al., 2018]; GraSP [Wang et al., 2020]; SynFlow [Tanaka et al., 2020]), dense-to-sparse training with dynamic masks (DST [Liu et al., 2020]; RigL [Evci et al., 2020]), and SAM-optimized IMP [Na et al., 2022].",,,0,not_related
Evci et al. [2020] proposes a rigged lottery (RigL) that regrows the dead weights yet with a large gradient flow.,,,0,not_related
"As IMP provides so-called sparse-to-sparse training, i.e., the mask is pre-defined, some work investigates dense-to-sparse training with dynamic masks [Evci et al., 2020, Liu et al., 2020].",,,0,not_related
", 2020]; RigL [Evci et al., 2020]), and SAM-optimized IMP [Na et al.",,,0,not_related
It has been demonstrated empirically that RigL can escape local minima and discover improved basins during optimization process.,,,0,not_related
"To generate more efficient models, researchers have explored various techniques for compressing DNNs without sacrificing their accuracy, such as pruning [15, 16, 17, 18, 19, 20], quantization [21, 22, 23, 24], knowledge distillation [25, 26, 27], and matrix low-rank approximation [28, 29, 30, 31].",,,0,not_related
"On the other hand, Mostafa and Wang (2019); Mocanu et al. (2018); Evci et al. (2020) proposed to leverage information gathered during the training process to dynamically update the sparsity pattern of kernels.",,,0,not_related
"Moreover, researchers have explored sparsifying NNs during training (Evci et al., 2020).",,,0,not_related
"The ERK distribution has been demonstrated to outperform uniform sparsity distributions by reallocating parameters to layers with fewer parameters (Mocanu et al., 2018; Evci et al., 2021).",,,0,not_related
"We propose a novel DST method, Structured RigL (SRigL), based on RigL (Evci et al., 2021).",,,1,related
"We achieve the desired overall sparsity by distributing the per-layer sparsity according to the Erdős-Rényi-Kernel (ERK) (Evci et al., 2021; Mocanu et al., 2018) distribution, which scales the per-layer sparsity based on the number of neurons and the dimensions of the convolutional kernel, if present.",,,1,related
"Dynamic Sparse Training (DST) methods such as RigL (Evci et al., 2021) are the state-of-the-art in sparse training methods, learning unstructured Sparse Neural Networks (SNNs) with 85–95% fewer weights than dense models, while maintaining similar generalization.",,,0,not_related
", 2018) removes weights with the smallest magnitude and adds weights randomly; similarly, RigL (Evci et al., 2021) prunes weights with the smallest magnitude and regrows weights that have large-magnitude gradients.",,,0,not_related
"• With the constant fan-in constraint, per-layer sparsity distributions such as Erd˝os-Rényi-Kernel (ERK) can be applied to the model.",,,0,not_related
"We achieve the desired overall sparsity by distributing the per-layer sparsity according to the ERK (Evci et al., 2021; Mocanu et al., 2018) distribution, which scales the per-layer sparsity based on the number of neurons and the dimensions of the convolutional kernel, if present.",,,1,related
"The new trend of dynamic sparse training shows that any random initialized sparse neural networks can achieves comparable accuracy to the dense neural networks (Ye et al., 2020; Evci et al., 2020; Hou et al., 2022; Ma et al., 2021a; Yuan et al., 2021).",,,0,not_related
"1) Experiment Setup: To answer the RQ3, we select 4 state-of-the-art model compression techniques proposed in the last two years (i.e., LAMP [30], Global [31], Uniform+ [32] and ERK [33]) to see whether our DeepArc framework can speed up the model compression efficiency.",,,1,related
", LAMP [30], Global [31], Uniform+ [32] and ERK [33]) to see whether our DeepArc framework can speed up the model compression efficiency.",,,0,not_related
", 2018) and gradient based (RIGL) (Evci et al., 2020) growth.",,,0,not_related
"We use ERK sparsity distribution (Mocanu et al., 2018; Evci et al., 2020) in our ViT experiments.",,,1,related
"Sparse Training including static sparse training (STATIC) and dynamic sparse training with random (SET) (Mocanu et al., 2018) and gradient based (RIGL) (Evci et al., 2020) growth.",,,0,not_related
"Though most results match previous work, we observe a significant improvement for the accuracy achieved by the SET algorithm compared to the implementation done in (Evci et al., 2020).",,,1,related
"We train 80% sparse ResNet-50 models on ImageNet to reproduce previous results reported in the literature (Gale et al., 2019; Evci et al., 2020).",,,1,related
"Using GMP and an extended training schedule, we are able to obtain sparse models that match or outperform the dense baseline, both in terms of accuracy and ROC-AUC values, even at high (≥ 99%) sparsities, while providing substantial improvements in theoretical FLOPs (computed as in [14]), and practical inference speed on CPU when using the DeepSparse inference engine [10].",,,1,related
", SET [20], RigL [22]) on SNN models (i.",,,0,not_related
"For the latter, we implement the sparse training methods (i.e., SET [20], RigL [22]) on SNN models (i.e., SET-SNN, RigL-SNN).",,,1,related
"Compared to RigL-SNN, NDSNN has up to 5.45% and 17.83% increase in accuracy at a sparsity of 99% for VGG-16 and ResNet-19, respectively.",,,0,not_related
"While for ResNet-19, NDSNN has 15.42%, 14.17%, 23.88% and 18.15% increase in accuracy (that is relatively 28.2%, 14.17%, 23.38%, 18.15% higher accuracy) compared to LTH-SNN, obtains 1.96%, 4.30%, 7.99%, 10.5% higher accuracy than SET-SNN and achieves 2.75%, 3.72%, 8.52%, 11.65% higher accuracy than RigL-SNN at a sparsity of 90%, 95%, 98% and 99%, respectively.",,,0,not_related
"Such a formulation is favorable because: (i) allowing the growth of connections has been shown to yield better sparse networks (Evci et al., 2020); and (ii) recovering weights to previous values is compatible with the finding that tuning pruned networks with earlier weight values or learning rate…",,,0,not_related
"Such a formulation is favorable because: (i) allowing the growth of connections has been shown to yield better sparse networks (Evci et al., 2020); and (ii) recovering weights to previous values is compatible with the finding that tuning pruned networks with earlier weight values or learning rate schedule is beneficial (Renda et al.",,,0,not_related
"In particular Li et al. (2020a) implemented randomly initialized sparse mask, FedDST Bibikar et al. (2022) built on the idea of RigL Evci et al. (2020) and mostly focussed on magnitude pruning on the server-side resulting in similar constraints.",,,0,not_related
"(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the ‘Lottery Ticket Hypothesis’ (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such that it preserves the network’s gradient flow. SynFlow (Tanaka et al. (2020)) uses iterative synaptic flow pruning to preserve the total flow of synaptic strength (global pruning at initialization with no data and no backpropagation). 3SP (van Amersfoort et al. (2020)) introduces compute-aware pruning of channels (structured pruning).",,,0,not_related
"Pruning after Training: Here sparse models are created using a three-stage pipeline– train a dense model, prune, and re-train (Han et al. (2015); Guo et al.",,,0,not_related
"(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the ‘Lottery Ticket Hypothesis’ (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such that it preserves the network’s gradient flow. SynFlow (Tanaka et al. (2020)) uses iterative synaptic flow pruning to preserve the total flow of synaptic strength (global pruning at initialization with no data and no backpropagation).",,,0,not_related
"(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the ‘Lottery Ticket Hypothesis’ (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such that it preserves the network’s gradient flow. SynFlow (Tanaka et al. (2020)) uses iterative synaptic flow pruning to preserve the total flow of synaptic strength (global pruning at initialization with no data and no backpropagation). 3SP (van Amersfoort et al. (2020)) introduces compute-aware pruning of channels (structured pruning). The main advantage of these methods is that they are compute and memory efficient in every iteration. However, they often suffer from significant loss of accuracy. Note that, the weight pruning mask is fixed throughout the training. Pruning during Training: For these methods, the weight pruning mask evolves dynamically with training. Methods of this category can belong to either sparse-to-sparse or dense-to-sparse training. In sparse-to-sparse training, we have a sparse model to start with (based on sparsity budget) and the budget sparsity is maintained throughout the training. SET (Mocanu et al. (2018)) pioneered this approach where they replaced a fraction of least magnitude weights by random weights for better exploration. DSR (Mostafa & Wang (2019)) allowed sparsity budget to be non-uniform across layers heuristically, e.",,,0,not_related
"(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the ‘Lottery Ticket Hypothesis’ (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such that it preserves the network’s gradient flow. SynFlow (Tanaka et al. (2020)) uses iterative synaptic flow pruning to preserve the total flow of synaptic strength (global pruning at initialization with no data and no backpropagation). 3SP (van Amersfoort et al. (2020)) introduces compute-aware pruning of channels (structured pruning). The main advantage of these methods is that they are compute and memory efficient in every iteration. However, they often suffer from significant loss of accuracy. Note that, the weight pruning mask is fixed throughout the training. Pruning during Training: For these methods, the weight pruning mask evolves dynamically with training. Methods of this category can belong to either sparse-to-sparse or dense-to-sparse training. In sparse-to-sparse training, we have a sparse model to start with (based on sparsity budget) and the budget sparsity is maintained throughout the training. SET (Mocanu et al. (2018)) pioneered this approach where they replaced a fraction of least magnitude weights by random weights for better exploration.",,,0,not_related
"(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the ‘Lottery Ticket Hypothesis’ (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity.",,,0,not_related
"(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the ‘Lottery Ticket Hypothesis’ (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such that it preserves the network’s gradient flow. SynFlow (Tanaka et al. (2020)) uses iterative synaptic flow pruning to preserve the total flow of synaptic strength (global pruning at initialization with no data and no backpropagation). 3SP (van Amersfoort et al. (2020)) introduces compute-aware pruning of channels (structured pruning). The main advantage of these methods is that they are compute and memory efficient in every iteration. However, they often suffer from significant loss of accuracy. Note that, the weight pruning mask is fixed throughout the training. Pruning during Training: For these methods, the weight pruning mask evolves dynamically with training. Methods of this category can belong to either sparse-to-sparse or dense-to-sparse training. In sparse-to-sparse training, we have a sparse model to start with (based on sparsity budget) and the budget sparsity is maintained throughout the training. SET (Mocanu et al. (2018)) pioneered this approach where they replaced a fraction of least magnitude weights by random weights for better exploration. DSR (Mostafa & Wang (2019)) allowed sparsity budget to be non-uniform across layers heuristically, e.g., higher sparsity for later layers. SNFS (Dettmers & Zettlemoyer (2019)) proposed to use momentum of each parameter as a criterion to grow the weights leading to increased accuracy.",,,0,not_related
"(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the ‘Lottery Ticket Hypothesis’ (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such that it preserves the network’s gradient flow.",,,0,not_related
"(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the ‘Lottery Ticket Hypothesis’ (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model.",,,0,not_related
"RigL (Evci et al. (2021)) activates/revives new weights ranked according to gradient magnitude (using infrequent full gradient calculation), i.e., masked weights receive gradients only after certain iterations.",,,0,not_related
"Network pruning is studied rigorously in literature for single-task models and there exist many pruning criteria based on weight magnitude [11, 26], connection sensitivity [10, 25], and even learning-based pruning methods [7, 19].",,,0,not_related
"DST methods (Mocanu et al., 2018; Bellec et al., 2017; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Liu et al., 2021b; Evci et al., 2020; Liu et al., 2021c;a) train the neural network with a fixed parameter count budget while gradually adjust the parameters throughout training.",,,0,not_related
"Gale et al. (2020) designed high-performance SpMM kernels targeted specifically at deep learning applications. Using SpMM as the fundamental component, pruning has been used to optimally sparsify the dense weight matrix in deep neural networks. Zhu & Gupta (2017) pruned the large models in an unstructured way and compared the accuracy of the models with their small-dense counterparts across a broad range of neural network architectures (CNN, LSTM etc.) and found that large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters. Yao et al. (2019) showed a novel fine-grained unstructured sparsity approach and corresponding pruning method to fixing the number of non-zeros in small regions of the sparse matrix in matrix row. Wang (2020) presented SparseRT, a system to support efficient inference with unstructured weight pruning on GPU.",,,0,not_related
"This is typically done through a single pruning step at the end of training (Zhu & Gupta, 2017) or potentially through some sparse training regime (Evci et al., 2019). These approaches have typically achieved between 90-99% reduction in model weights while maintaining and acceptable level of accuracy depending on the model and technique used (Hoefler et al., 2021). While the benefits to model size can be easily realised, increased computational efficiency can often be more difficult to achieve as sparse computation can be challenging to execute effectively on modern, highly parallel deep learning accelerators (Qin et al., 2022). As such, sparse training methods often fall into a gap between theoretical and practical efficiency. For example Mostafa & Wang (2019) demonstrated FLOP efficient deep residual CNN training with dynamic sparse reparameterisation techniques but struggled to achieve speed ups in practice.",,,0,not_related
"Gale et al. (2020) designed high-performance SpMM kernels targeted specifically at deep learning applications. Using SpMM as the fundamental component, pruning has been used to optimally sparsify the dense weight matrix in deep neural networks. Zhu & Gupta (2017) pruned the large models in an unstructured way and compared the accuracy of the models with their small-dense counterparts across a broad range of neural network architectures (CNN, LSTM etc.) and found that large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters. Yao et al. (2019) showed a novel fine-grained unstructured sparsity approach and corresponding pruning method to fixing the number of non-zeros in small regions of the sparse matrix in matrix row.",,,0,not_related
Gale et al. (2020) designed high-performance SpMM kernels targeted specifically at deep learning applications.,,,0,not_related
"Gale et al. (2020) designed high-performance SpMM kernels targeted specifically at deep learning applications. Using SpMM as the fundamental component, pruning has been used to optimally sparsify the dense weight matrix in deep neural networks. Zhu & Gupta (2017) pruned the large models in an unstructured way and compared the accuracy of the models with their small-dense counterparts across a broad range of neural network architectures (CNN, LSTM etc.",,,0,not_related
"Hence, layer-wise sparsity distributions such as the Erdös-Rényi-Kernel (Evci et al., 2020), Ideal Gas Quota (Chen et al., 2022b), and parameter leveling (Golubeva et al., 2021) are often used with sparse training to boost accuracies.",,,0,not_related
"For example, SOTA sparse training methods (Jayakumar et al., 2020; Evci et al., 2020) invest these FLOP savings into longer training schedules to close the accuracy gap and compensate for the inability to discover an optimal mask earlier in training.",,,0,not_related
"We adopt the default hyperparameters from RigL (Evci et al., 2020) for dynamic sparsity.",,,1,related
"In this work, we focus on improving the training efficiency (test-accuracy w.r.t training FLOPs) of DNNs.
Recent works (Evci et al., 2020; Jayakumar et al., 2020) have explored using weight sparsity to reduce the FLOPs spent in training.",,,1,related
"Inspired by this result, various dynamic sparse training (DST) methods (Ma et al., 2022; Evci et al., 2020; Liu et al., 2021a; Jayakumar et al., 2020) attempt to find optimal sparse subnetworks in a single training run.",,,0,not_related
"As a result, these techniques can sometimes even require more FLOPs than training the dense model (Ma et al., 2022; Evci et al., 2020; Jayakumar et al., 2020).",,,0,not_related
"Recent works (Evci et al., 2020; Jayakumar et al., 2020) have explored using weight sparsity to reduce the FLOPs spent in training.",,,0,not_related
"These methods either try to find the subnetworks at initialization (Tanaka et al., 2020; Wang et al., 2020a; de Jorge et al., 2020; Lee et al., 2018) or dynamically during training (Mocanu et al., 2018; Evci et al., 2020; Jayakumar et al., 2020; Raihan & Aamodt, 2020).",,,0,not_related
", 2018) or dynamically during training (Mocanu et al., 2018; Evci et al., 2020; Jayakumar et al., 2020; Raihan & Aamodt, 2020).",,,0,not_related
"%} using three sparse training methods: static sparsity, SET (Mocanu et al., 2018) and RigL (Evci et al., 2020).",,,1,related
"Sparsity Setup For enabling the SIFT transformations, we use the RigL (Evci et al., 2020) algorithm in its default hyperparameter settings (α = 0.3,∆T = 100), with the drop-fraction (α) annealed using a cosine decay schedule for 75% of the training run.",,,1,related
"Hence, layer-wise sparsity distributions such as the Erdös-Rényi-Kernel (Evci et al., 2020), Ideal Gas Quota (Chen et al.",,,0,not_related
"During training, DST methods periodically update the sparse connectivity of the network; e.g., in Mocanu et al. (2018); Evci et al. (2020) authors remove a fraction ζ of the parameters θs and add the same number of parameters to the network to keep the sparsity level fixed.",,,1,related
Estimating the FLOPs (floating-point operations) and parameter count is a commonly used approach to analyze the efficiency gained by a sparse neural network compared to its dense equivalent network Evci et al. (2020); Sokar et al.,,,0,not_related
We use gradients for weight regrowth Evci et al. (2020).,,,1,related
The starting point of our implementation is based on the sparse evolutionary training introduced as SET in Mocanu et al. (2018)3 to which we added the gradient-based connections growth proposed in RigL Evci et al. (2020).,,,1,related
"However, there exists various approaches for weight regrowth including, random Mocanu et al. (2018); Mostafa & Wang (2019), gradientbased Evci et al. (2020); Dai et al. (2019); Dettmers & Zettlemoyer (2019); Jayakumar et al. (2020), localitybased Hoefler et al. (2021), and similarity-based…",,,0,not_related
"Estimating the FLOPs (floating-point operations) and parameter count is a commonly used approach to analyze the efficiency gained by a sparse neural network compared to its dense equivalent network Evci et al. (2020); Sokar et al. (2021). Number of parameters indicates the size of the model, which directly affects the memory consumption and also computational complexity. FLOPs estimates the time complexity of an algorithm independently of its implementation. In addition, since existing deep learning hardware is not optimized for sparse matrix computations, most methods for obtaining sparse neural networks only simulate sparsity using a binary mask over the weights. Consequently, the running time of these methods does not reflect their efficiency. Besides, developing proper pure sparse implementations for sparse neural networks is currently a highly researched topic pursued by the community Hooker (2021). Thus, as our paper is, in its essence, theoretical, we decided to let this engineering research aspect for future work.",,,0,not_related
It has been shown in Evci et al. (2020) that adding the zero-connections with the largest gradients magnitude in the DST process accelerates the learning and improves the accuracy.,,,0,not_related
"Estimating the FLOPs (floating-point operations) and parameter count is a commonly used approach to analyze the efficiency gained by a sparse neural network compared to its dense equivalent network Evci et al. (2020); Sokar et al. (2021). Number of parameters indicates the size of the model, which directly affects the memory consumption and also computational complexity.",,,0,not_related
Estimating the FLOPs (floating-point operations) and parameter count is a commonly used approach to analyze the efficiency gained by a sparse neural network compared to its dense equivalent network Evci et al. (2020); Sokar et al. (2021).,,,0,not_related
"In this section, we compare NeuroFS with RigL Evci et al. (2020), which is a DST method mainly designed for classification; it uses gradient for weight regrowth when updating the sparse connectivity in the DST framework.",,,1,related
"We use gradients for weight regrowth Evci et al. (2020). For each hidden layer h(l), NeuroFS performs the following two steps: 1.",,,1,related
"At the same time, sparse training (Mocanu et al., 2018; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021c; Schwarz et al., 2021) was proposed that can train a randomlyinitialized sparse neural network from scratch while dynamically optimizing the sparse…",,,0,not_related
"Following a mainstream convention in many sparse training papers (Frankle & Carbin, 2019; Gale et al., 2019; Evci et al., 2020; Lee et al., 2019; Liu et al., 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as…",,,1,related
"Following a mainstream convention in many sparse training papers (Frankle & Carbin, 2019; Gale et al., 2019; Evci et al., 2020; Lee et al., 2019; Liu et al., 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al.",,,1,related
"More importantly, oBERT produces a completely different layerwise sparsity pattern from magnitudebased pruning approaches, which is consistent with the patterns that are commonly observed in sparse computer vision models: deeper layers tend to have higher sparsities than lower layers (Evci et al., 2020; Kusupati et al., 2020; Tanaka et al., 2020; Liu et al., 2021b).",,,0,not_related
"…pattern from magnitudebased pruning approaches, which is consistent with the patterns that are commonly observed in sparse computer vision models: deeper layers tend to have higher sparsities than lower layers (Evci et al., 2020; Kusupati et al., 2020; Tanaka et al., 2020; Liu et al., 2021b).",,,0,not_related
"At the same time, sparse training (Mocanu et al., 2018; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021c; Schwarz et al., 2021) was proposed that can train a randomlyinitialized sparse neural network from scratch while dynamically optimizing the sparse connectivity with promising performance.",,,0,not_related
"• Rigging the Lottery (RigL) (Evci et al., 2020) is a leading sparse training method that updates the topology of sparse neural networks during training via a prune-and-grow scheme.",,,0,not_related
"We have also verified that various during-training pruning methods like GMP [55], RigL [7], and GraNet [32] could improve the performance of OOD detection, which are often more efficient.",,,1,related
"We show the performance of three methods: GMP [55], RigL [7], and GraNet [32].",,,1,related
"Methods are proposed to drop the unimportant weights [13, 55, 7, 32].",,,1,related
"As a remedy, DST methods (Bellec et al., 2017; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021a;b;c; Mocanu et al., 2018; Mostafa & Wang, 2019; Graesser et al., 2022) were introduced to train the neural networks under a given parameter budget while mask change is allowed during…",,,0,not_related
"For the growing criterion, we test both random growth SDST(SET) (Mocanu et al., 2018; Liu et al., 2021c) and gradient-based growth SDST(RigL) (Evci et al., 2020).",,,1,related
"For SDST methods, we test both grow methods, i.e., SDST(SET) (Mocanu et al., 2018) which grows connections randomly and SDST(RigL) (Evci et al., 2020) which grows connections via gradient.",,,1,related
"Following Evci et al. (2020), we specify the hyper-parameters of DST through sparsity distribution, update schedule, drop criterion, and grow criterion.",,,1,related
", 2018) which grows connections randomly and SDST(RigL) (Evci et al., 2020) which grows connections via gradient.",,,0,not_related
"However, recent advances in dynamic sparse training (DST) (Evci et al., 2020; Liu et al., 2021a;b;c; Mocanu et al., 2018) for the first time show that pruning-during-training methods ∗Corresponding author.",,,0,not_related
"Following Evci et al. (2020); Liu et al. (2021c), only parameters of fully connected layers and convolutional layers will be pruned.",,,0,not_related
"At initialization, we use the commonly adopted Erdős-Rényi-Kernel (ERK) strategy (Evci et al., 2020; Dettmers & Zettlemoyer, 2019; Liu et al., 2021c) to allocates higher sparsity to larger layers.",,,1,related
"Layer-wise sparsity ratio and masks mG,mD are determined using Erdős-Rényi-Kernel (ERK) graph topology (Evci et al., 2020) and are fixed throughout the training.",,,0,not_related
"Layer-wise sparsity ratio and masks mG,mD are determined using Erdős-Rényi-Kernel (ERK) graph topology (Evci et al., 2020) and are fixed throughout the training.",,,0,not_related
"As a remedy, DST methods (Bellec et al., 2017; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021a;b;c; Mocanu et al., 2018; Mostafa & Wang, 2019; Graesser et al., 2022) were introduced to train the neural networks under a given parameter budget while mask change is allowed during training.",,,0,not_related
"Out of simplicity, we increase the density by growing the connections with the largest gradient magnitude (Evci et al., 2020).",,,1,related
", 2017), RIGL (Evci et al., 2020), SNFS (Dettmers & Zettlemoyer, 2020) and DNW (Wortsman et al.",,,0,not_related
"We compare our approach against Incremental (Zhu & Gupta, 2018), STR (Kusu-pati et al., 2020), Global Magnitude (Singh & Alistarh, 2020), WoodFisher (Singh & Alistarh, 2020), GMP (Gale et al., 2019), Variational Dropout (Molchanov et al., 2017), RIGL (Evci et al., 2020), SNFS (Dettmers & Zettlemoyer, 2020) and DNW (Wortsman et al., 2019 MobileNetV1.",,,1,related
"…in this area includes STR (Kusupati et al., 2020), Top-KAST (Jayakumar et al., 2020), CS (Continuous Sparsification) (Savarese et al., 2020), RigL (Evci et al., 2020), WoodFisher (Singh & Alistarh, 2020), PSGD (Kim et al., 2020), GraNet (Liu et al., 2021a), Powerprop (Schwarz et al., 2021),…",,,0,not_related
"Recent works in this area includes STR (Kusupati et al., 2020), Top-KAST (Jayakumar et al., 2020), CS (Continuous Sparsification) (Savarese et al., 2020), RigL (Evci et al., 2020), WoodFisher (Singh & Alistarh, 2020), PSGD (Kim et al., 2020), GraNet (Liu et al., 2021a), Powerprop (Schwarz et al., 2021), ProbMask (Zhou et al., 2021), GPO (Wang et al., 2022), OptG (Zhang et al., 2022) and STDS (Chen et al., 2022).",,,0,not_related
", 2020), RigL (Evci et al., 2020), WoodFisher (Singh & Alistarh, 2020), PSGD (Kim et al.",,,0,not_related
"…wheresoever equally, and yet becomes totally ignorant of nowadays pruning researches like sparsity budget allocation, e.g., Erdős-Rényi (Mocanu et al., 2018) and Erdős-Rényi-Kernel (ERK) (Evci et al., 2020), or commonsense in this area like “Leave at least one path from input through output”.",,,1,related
"This design endows our method with versatility while treating weight wheresoever equally, and yet becomes totally ignorant of nowadays pruning researches like sparsity budget allocation, e.g., Erdős-Rényi (Mocanu et al., 2018) and Erdős-Rényi-Kernel (ERK) (Evci et al., 2020), or commonsense in this area like “Leave at least one path from input through output”.",,,1,related
", 2018) and Erdős-Rényi-Kernel (ERK) (Evci et al., 2020), or commonsense in this area like “Leave at least one path from input through output”.",,,0,not_related
Evci et al. (2020) builds on this approach to give a method that does not require frequent gradient re-computation.,,,0,not_related
"Implementations: We follow the settings in (Evci et al., 2020; Sundar & Dwaraknath, 2021).",,,1,related
"Consistent with widely used sparse training methods (Evci et al., 2020; Liu et al., 2021), the deterministic mask stops updating near the end of the training process.",,,0,not_related
"We perform a comprehensive empirical evaluation of CigL, comparing it with the popular baseline method RigL (Evci et al., 2020).",,,1,related
Datasets & Model Architectures: We follow the settings in Evci et al. (2020) for a comprehensive comparison.,,,1,related
"Inspired by the widely-used sparse training method RigL (Evci et al., 2020), we believe a larger weight/gradient magnitude implies that the weight is more helpful for loss reduction and needs to be activated.",,,1,related
"The sparse topology is usually controlled by a mask, and various sparse training methods have been proposed to find a suitable mask to achieve comparable or even higher accuracy compared to dense training (Evci et al., 2020; Liu et al., 2021; Schwarz et al., 2021).",,,0,not_related
"…magnitude,
are designed (Mocanu et al., 2018; Bellec et al., 2018; Frankle & Carbin, 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021; Özdenizci & Legenstein, 2021; Zhou et al., 2021; Schwarz et al., 2021; Yin et al.,…",,,0,not_related
"Sparse training is gaining increasing attention and has been used in various deep neural network (DNN) learning tasks (Evci et al., 2020; Dietrich et al., 2021; Bibikar et al., 2022).",,,0,not_related
"(Cited in Section 5) [17] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.",,,0,not_related
"In supervised learning, especially computer vision, many promising results have been achieved with sparsity over the last few years [11, 17, 31].",,,0,not_related
Graesser et al. [20] compared DST methods such as SET [33] and RigL [17] in many deep RL environments.,,,0,not_related
[20] compared DST methods such as SET [33] and RigL [17] in many deep RL environments.,,,0,not_related
"C V
] 1
3 Fe
b 20
23
tee (Han et al., 2015; Evci et al., 2020).",,,0,not_related
"Sparse training serves as an effective tool to boost the performance of network sparsity (Hoefler et al., 2021; Evci et al., 2020; Sanh et al., 2020).",,,0,not_related
"For example, RigL (Evci et al., 2020) removes smaller-magnitude weights and revives weights with larger-magnitude gradients.",,,0,not_related
"Notably, the speedups attained through RigL surpass those obtained through GMP.",,,1,related
"This is because RigL initializes the network layers with the target sparsity at the outset of training, while GMP progressively achieves the target sparsity during training.",,,0,not_related
"We set the hyper-parameters α = 0.3, Tend = 80, and ∆T = 10 (introduced in the original paper (Evci et al., 2020)).",,,1,related
"On the algorithmic side, there are several interesting proposals for sparse training algorithms (Dettmers & Zettlemoyer, 2019; Kusupati et al., 2020; Evci et al., 2020; Jayakumar et al., 2020; Schwarz et al., 2021), i.e. variants of stochastic gradient descent (SGD) which aim to keep as many…",,,0,not_related
"As such, SparseProp can provide direct support for pruning-during training methods like Gradual Pruning (Zhu & Gupta, 2017), RigL (Evci et al., 2020) or AC/DC (Peste et al., 2021), which assume a fixed sparsity mask for any forward / backward pass, and can be modified to support more complex methods (Jayakumar et al., 2020), which specify different sparsities for weights and gradients.",,,0,not_related
"…while trying to maximize sparsity in the models’ internal representations (Zhu & Gupta, 2017; Lis et al., 2019; Dettmers & Zettlemoyer, 2019; Zhang et al., 2020; Wiedemann et al., 2020; Kusupati et al., 2020; Evci et al., 2020; Jayakumar et al., 2020; Peste et al., 2021; Schwarz et al., 2021).",,,0,not_related
"Finally, we examine the performance of SparseProp on the RigL method (Evci et al., 2020), a dynamic sparse training technique, to train a sparse ResNet18 model.",,,1,related
"As noted, there has been a significant amount of work on SGD-like algorithms for sparse training of DNNs, balancing accuracy while trying to maximize sparsity in the models’ internal representations (Zhu & Gupta, 2017; Lis et al., 2019; Dettmers & Zettlemoyer, 2019; Zhang et al., 2020; Wiedemann et al., 2020; Kusupati et al., 2020; Evci et al., 2020; Jayakumar et al., 2020; Peste et al., 2021; Schwarz et al., 2021).",,,0,not_related
"On the algorithmic side, there are several interesting proposals for sparse training algorithms (Dettmers & Zettlemoyer, 2019; Kusupati et al., 2020; Evci et al., 2020; Jayakumar et al., 2020; Schwarz et al., 2021), i.",,,0,not_related
"3, Tend = 80, and ∆T = 10 (introduced in the original paper (Evci et al., 2020)).",,,1,related
"As such, SparseProp can provide direct support for pruning-during training methods like Gradual Pruning (Zhu & Gupta, 2017), RigL (Evci et al., 2020) or AC/DC (Peste et al.",,,0,not_related
"As such, SparseProp can provide direct support for pruning-during training methods like Gradual Pruning (Zhu & Gupta, 2017), RigL (Evci et al., 2020) or AC/DC (Peste et al., 2021), which assume a fixed sparsity mask for any forward / backward pass, and can be modified to support more complex…",,,0,not_related
"Compared to traditional unstructured sparsity (Frankle and Carbin, 2018; Lee et al., 2018; Evci et al., 2020) or channel/block structured sparsity algorithms (Wen et al.",,,0,not_related
"Compared to traditional unstructured sparsity (Frankle and Carbin, 2018; Lee et al., 2018; Evci et al., 2020) or channel/block structured sparsity algorithms (Wen et al., 2016; Li et al., 2016; He et al., 2017), adopting N:M masks has negligible evaluation degradation and progressively co-design…",,,0,not_related
"In light of this, follow-up works with higher training efficiency can be roughly categorized into three groups: (i) find sparse networks once at initialization by measuring the importance of connections on the loss, eliminating the need for the complex iterative optimization schedule [47, 77] ; (ii) identify the winning tickets in Transformers at a very early training stage via low-cost schemes and then merely train these early tickets until convergence [79, 15]; (iii) use an alternating pruning and growing schedule to dynamically update model sparsity patterns throughout training, suitable for general architectures [23, 13].",,,0,not_related
"Several studies have been carried out to identify subnetworks across the model that can provide the best transferability (Gale et al., 2019; Evci et al., 2020; Lee et al., 2021; Guo et al., 2021; Hu et al., 2021).",,,0,not_related
"Such observation has been made by some sparse training papers, e.g., RigL [16] notes that “sparse training methods benefit significantly from increased training steps”.",,,0,not_related
", RigL [16] notes that “sparse training methods benefit significantly from increased training steps”.",,,0,not_related
"During sparse training, a certain percentage of connections are removed to save memory (Bellec et al., 2017; Evci et al., 2020).",,,0,not_related
"We calculate the gradient variance and correlation of the ResNet-50 on CIFAR-100 from RigL (Evci et al., 2020) and SET (Mocanu et al., 2018) at different sparsities including 0%, 50%, 80%, 90%, and 95%.",,,1,related
"Aligned with popular sparse training methods (Evci et al., 2020; Özdenizci & Legenstein, 2021; Liu et al., 2021), we choose piecewise constant decay schedulers for learning rate and weight decay.",,,1,related
"Despite of the good performance, such large models are not applicable when memory or computational resources are limited (Bellec et al., 2017; Evci et al., 2020; Liu et al., 2022).",,,0,not_related
", 2018), RigL (Evci et al., 2020), BSR-Net (Özdenizci & Legenstein, 2021) and ITOP (Liu et al.",,,0,not_related
"We add our AGENT to three recent sparse training pipelines, namely SET (Mocanu et al., 2018), RigL (Evci et al., 2020), BSR-Net (Özdenizci & Legenstein, 2021) and ITOP (Liu et al., 2021).",,,1,related
"pruning and growth criteria are proposed, such as weight/gradient magnitude, random selection, and weight sign (Mocanu et al., 2018; Bellec et al., 2018; Frankle & Carbin, 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021; Özdenizci & Legenstein, 2021; Zhou et al., 2021b; Schwarz et al., 2021; Huang et al., 2022; Liu et al., 2022).",,,0,not_related
"We summarize additional experimental results for the BSR-Net-based Özdenizci & Legenstein (2021), RigLbased Evci et al. (2020), and ITOP-based Liu et al. (2021) models.",,,1,related
"Sparse training (Mocanu et al., 2018; Evci et al., 2020; Liu et al., 2022) is one of the most popular classes of methods to improve efficiency in terms of space (e.g. memory storage) and is receiving increasing attention.",,,0,not_related
"iteratively updated with various criteria (Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021; Özdenizci & Legenstein, 2021).",,,1,related
"Implementations: Aligned with the choice of Evci et al. (2020); Sundar & Dwaraknath (2021); Özdenizci & Legenstein (2021), the parameters of the model are optimized by SGD with momentum.",,,1,related
"In RigL-based results, we follow the settings in Evci et al. (2020); Sundar & Dwaraknath (2021).",,,1,related
"As for the blue curve for our A-RigL, it is always on the top of the green curve for RigL, indicating that the speedup is successful.
methods in RigL-based models Evci et al. (2020).",,,1,related
"…selection, and weight sign (Mocanu et al., 2018; Bellec et al., 2018; Frankle & Carbin, 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021; Özdenizci & Legenstein, 2021; Zhou et al., 2021b; Schwarz et al., 2021; Huang et al.,…",,,0,not_related
"L G
iteratively updated with various criteria (Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021; Özdenizci & Legenstein, 2021).",,,1,related
"Sparse training (Mocanu et al., 2018; Evci et al., 2020; Liu et al., 2022) is one of the most popular classes of methods to improve efficiency in terms of space (e.",,,0,not_related
"Crucially, following [11], the updated inverse (HUj+1) −1 can be calculated efficiently by removing the first row and column, corresponding to j in the original H, from the inverse of (HUj ) −1 in (3)For example, structured (column-wise) pruning ResNet50 to 50% structured sparsity without accuracy loss is challenging, even with extensive retraining [30], while unstructured pruning to 90% sparsity is easily achievable with state-of-the-art methods [6, 39].",,,1,related
"…finding a fixed sparse mask at the initialization as we mentioned in introduction, on the other hand, dynamic sparse training allows the sparse mask to be updated during training, e.g., (Mocanu et al., 2018; Mostafa and Wang, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021a,c,d).",,,1,related
"…pruning (Mariet and Sra, 2015; He et al., 2017; Gale et al., 2019; Suau et al., 2018), non-uniform pruning (Mocanu et al., 2016), expander-graph-related techniques (Prabhu et al., 2018; Kepner and Robinett, 2019) Erdös-Rényi (Mocanu et al., 2018) and Erdös-Rényi-Kernel (Evci et al., 2020).",,,0,not_related
"Random pruning has also been considered in static sparse training such as uniform pruning (Mariet and Sra, 2015; He et al., 2017; Gale et al., 2019; Suau et al., 2018), non-uniform pruning (Mocanu et al., 2016), expander-graph-related techniques (Prabhu et al., 2018; Kepner and Robinett, 2019) Erdös-Rényi (Mocanu et al., 2018) and Erdös-Rényi-Kernel (Evci et al., 2020).",,,0,not_related
"On the other hand, dynamic sparse training allows the sparse mask to be updated (Mocanu et al., 2018; Mostafa and Wang, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021c,d,a; Peste et al., 2021).",,,0,not_related
"Rather than growing weights randomly (as in the line 9 of Algorithm 1), RigL grows them with the highest magnitude gradients.",,,0,not_related
"Different from the traditional dynamic sparse training methods, which explore the connections in a layer-wise manner [40, 34, 13] and can hardly cover smaller RF, we propose a more fine-grained sparse training strategy for TSC.",,,0,not_related
"Introduced as a new training paradigm before the lottery ticket hypothesis, DST starts from a sparse network and allows the sparse connectivity to be evolved dynamically with a fixed number of parameters throughout training [40, 13, 35, 60, 10].",,,0,not_related
"4.6 Case Study on other DST Method
Likewise, the effectiveness of the proposed DSN method is also analyzed in the presence of other DST methods, such as RigL [13].",,,0,not_related
"Likewise, the effectiveness of the proposed DSN method is also analyzed in the presence of other DST methods, such as RigL [13].",,,0,not_related
"However, “dynamic pruning” (Evci et al., 2020) has also been used during training to find sparser architectures from dense models.",,,0,not_related
"Inspired by RigL [14], devices only rate partial model parameters (e.",,,0,not_related
"Although RigL [14] tries to reduce memory consumption, it needs to compute gradients for all parameters, which is computationally expensive and may lead to straggling issues in federated learning.",,,0,not_related
"Such negative impact becomes more challenging when pruning towards an extremely tiny subnetwork, as the biased initial subnetwork can deviate significantly from the optimal structure, resulting in poor accuracy [14].",,,0,not_related
"Inspired by RigL [14], devices only rate partial model parameters (e.g., a single layer) at a time, where the top-K importance scores are stored locally and uploaded to the server, significantly reducing memory, computation, and communication cost.",,,0,not_related
"The other category is dynamic sparse training [14], [25], [26].",,,0,not_related
"To limit these computations, sparse networks have been thoroughly investigated in the past few years [43, 4, 27, 41, 21, 38, 3, 6, 32, 22, 42, 25], and significant efforts have been devoted to their efficient hardware implementation [8, 29].",,,0,not_related
"First, Erdos-Rényi-Kernel (ERK) [30, 3] proposes to scale the global sparsity ratio (= ratio of zeros to the total parameter count) with a layer-wise factor.",,,0,not_related
"Most of the persistent pruning literature makes use of hard thresholding to increase the pruning ratio [9, 4, 3].",,,0,not_related
"First, Erdos-Rényi-Kernel (ERK) [30, 3] proposes to scale the global sparsity ratio (= ratio of zeros to the total parameter count) with a layer-wise factor. so as to induce a higher (smaller) sparsity for layers with more (less) parameters, e.g. the first convolution layers of a ResNet-50 remain dense, thereby preserving accuracy.",,,0,not_related
"It has the advantage of being straightforward, and our experiments reveal that it results in better accuracy/sparsity tradeoffs as ERK or LAMP.",,,0,not_related
RIGL [3] builds on gradient momentum to resurrect some of the zeroed weights.,,,0,not_related
"Most methods use gradient-magnitude [3], weight-magnitude [9, 43, 17], or a mix of both [36] to select the weights to set to zero after some preliminary training.",,,0,not_related
Both ERK and LAMP target higher accuracy at the cost of an increased complexity (as measured in FLOPS).,,,0,not_related
"As a consequence, as shown experimentally [3], after global thresholding, those kernels with larger weights end-up in being relatively less sparse than the kernels associated to the smaller resolutions channels of wider layers.",,,0,not_related
"GraNet [25], and other similar approaches like [3, 26], update the pruning mask every few thousands iterations, by regenerating connections when pruning others.",,,0,not_related
"For completeness, comparison is also provided with DNW [38], RIGL [3] and GraNet [25] (see Section 2 for the presentation of those methods), implemented as recommended by their respective authors.",,,1,related
"ResNet-50 [10] and MobileNetv1 [13] have been trained on ImageNet [2] using the same standard hyper-parameters, number of epochs (=100), and data-augmentation as previous related works [3, 17, 42, 25], leading to a dense baseline accuracy of 77.",,,0,not_related
"It has been widely documented [30, 3, 20] that the network prediction accuracy is more impacted when pruning occurs close to the input.",,,0,not_related
"RigL [14] updated the sparsity topology of the sparse network during training using the same magnitudebased weights dropping method while growing back the weights using top-k absolute largest gradients, achieving better accuracy than static mask training under same sparsity.",,,0,not_related
"For other baselines, we select SNIP [10] and GraSP [11] as the static mask training baselines while adopting DeepR [32], SNFS [22], DSR [13], SET [12], RigL [14], MEST [23], RigL-ITOP [1] as the dynamic mask training baselines as shown in Table II.",,,1,related
We use the same sparsity initialization method ERK in the state-of-the-art sparse training method such as RigL [14] and ITOP [1].,,,1,related
"To improve the flexibility, dynamic mask training has been proposed [12, 13, 14], where the sparse mask is periodically updated by drop-and-grow to search for better Drop-andgrow Non-active state (weight=0)",,,0,not_related
"The second limitation is that the existing methods sparsify networks layer-wise with a uniform sparsity ratio, which typically leads to inferior performance compared with the non-uniform layer-wise sparsity [20, 39, 40], especially for deep architectures [41].",,,0,not_related
"We do this because [20] showed that the layer-wise sparsity obtained by this scheme outperforms the other well-studied sparsity ratios [19, 37, 39].",,,1,related
"Many works have been proposed, focusing on improving the performance of sparse training for supervised image classification tasks by introducing different criteria for connection regrowth [20, 34, 59, 49, 73, 50].",,,0,not_related
"Aligned with many prior works [20, 54, 27], we find that the weight magnitude is an effective metric for estimating the connection importance (#4).",,,1,related
", 2020), model compression based on weights and gradient magnitude (Evci et al., 2020), and improved generalization performance by estimating the loss landscape (Foret et al.",,,0,not_related
"…weight- or/and function-space regularizers (Kirkpatrick et al., 2017; Pan et al., 2020), model compression based on weights and gradient magnitude (Evci et al., 2020), and improved generalization performance by estimating the loss landscape (Foret et al., 2021) and avoiding sharp minima…",,,0,not_related
The value of pl is decided based on ErdősRényi-Kernel scaling (used in (Evci et al. 2021; Raihan and Aamodt 2020)).,,,1,related
"Sparse training imposes sparsity constraints during network training [4, 13, 14, 42, 43, 45, 49].",,,0,not_related
"[8], Mostafa and Wang [31], and Wortsman et al.",,,0,not_related
"However, MobileNetv2 can be trained on the CIFAR-100 dataset with a 99% weight sparsity using ERK.",,,0,not_related
"To confirm this, we trained
GGR only using either gradient-based insertion or random insertion using a sparsity of 90% and ERK.",,,1,related
"We implement two different approaches for selecting the layer density at initialization: Erdős-Rényi-Kernel (ERK) (Mocanu et al., 2018b; Evci et al., 2020) and uniform density.",,,1,related
"Researchers have shown many times that large, sparse models outperform dense, small models with equal parameter count significantly (Evci et al., 2020; Mostafa & Wang, 2019).",,,0,not_related
"While GGR still achieves the best accuracy, the network cannot be shrunk as well as with ERK initialization.",,,0,not_related
"We implement two different approaches for selecting the layer density at initialization: Erdős-Rényi-Kernel (ERK) (Mocanu et al., 2018b; Evci et al., 2020) and uniform density.",,,1,related
Looking at the individual results with a sparsity of 90% and ERK reveals that GGR is performing worse than previous work when applied to large models.,,,0,not_related
"RigL (Evci et al., 2020) improved the criteria introduced in SET.",,,0,not_related
"The percentage of weights to be redistributed is decreased every epoch using the cosinefunction as suggested by the authors of RigL (Evci et al., 2020).",,,1,related
"Previous work shows that zero-initialization of newly inserted weights leads to equal or even slightly better performance than gradient-based insertion (Evci et al., 2020).",,,0,not_related
"The best example for that is VGG16 which fails to learn using RigL and DSR using a uniform layer sparsity of 99%, while GGR performs nearly as good as with ERK initialization.",,,0,not_related
[139] took inspiration from the recent sparse training work [140] and dynamically extracted and trained sparse sub-networks.,,,0,not_related
"We compare with global magnitude (GM) following the same schedule as CAP, as well as the state-of-the-art SViTE [Chen et al., 2021] paper, which trains the sparse model from scratch using a variant of RigL [Evci et al., 2020], but for a total of 600 epochs.",,,1,related
"Many other criteria exist, such as gradient magnitude [Evci et al., 2020] or “rates of change” in the weights [Sanh et al.",,,0,not_related
"In addition to the sparse training from scratch with periodic updates of the sparsity weights with some saliency criterion for weight elimination and regrowth [Evci et al., 2020] one can consider alternating compressed/decompressed training (AC/DC), proposed in [Peste et al., 2021].",,,0,not_related
"Many other criteria exist, such as gradient magnitude [Evci et al., 2020] or “rates of change” in the weights [Sanh et al., 2020].",,,0,not_related
"The only existing prior work on unstructured ViT pruning is SViTE [Chen et al., 2021], which performed careful customization of the RigL pruning method [Evci et al., 2020] to the special case of ViT models.",,,1,related
", 2021], which performed careful customization of the RigL pruning method [Evci et al., 2020] to the special case of ViT models.",,,0,not_related
", 2021] paper, which trains the sparse model from scratch using a variant of RigL [Evci et al., 2020], but for a total of 600 epochs.",,,0,not_related
"To name a few, Rigging the Lottery [10] edits the network connections and jointly updates the learnable weights.",,,0,not_related
"We take a simple 5-layer MLP to clarify this operation: its dimensions are [512, 100, 100, 100, 10] with 4 weight matrices w1 ∈ R512×100, w2 ∈ R100×100, w3 ∈ R100×100, and w4 ∈ R100×10.",,,1,related
"Another promising direction is to obtain sparse network by dynamic sparse training [10, 29].",,,0,not_related
"The potential of randomly initialized network is pioneeringly explored by the Lottery Ticket Hypothesis [11], and further investigated by [28, 2, 39].",,,0,not_related
"In spirit, our effort can be likened to methods like RigL (Evci et al., 2020), which discovers the wiring of a sparse neural network during training rather than pruning a dense network post training.",,,1,related
"The criterion of pruning could be weight magnitude [18], gradient [15] and Hessian [35, 49], etc.",,,0,not_related
"Following [15, 9], we apply a cosine decay scheduler to alleviate this problem:",,,1,related
"Limited by the requirement for the pre-trained model, some recent research [15, 2, 9, 30, 43, 37, 38] attempts to discover a sparse network directly from the training process.",,,0,not_related
Mocanu et al. (2018); Evci et al. (2020) specify each layer with a random topology in which larger layers are allocated with higher sparsity than smaller layers.,,,0,not_related
"The former identifies redundant model weights by leveraging heuristics-based metrics such as weight magnitudes [6, 17, 19, 11, 22, 37, 31, 36, 38], gradient magnitudes [21, 23, 24, 39, 40], and Hessian statistics [41–46].",,,0,not_related
"ER networks rewired with DST: Test Accuracies for an ER(p) VGG16 with a fixed mask and after rewiring edges with RiGL (Evci et al., 2020; Liu et al., 2021) on CIFAR10.",,,1,related
"For sparse to sparse training with DST, we us weight magnitude as importance score for pruning (with prune rate 0.5) and gradient for growth.",,,1,related
"99 an ER network of some initial sparsity and further pruned to a final sparsity (initial → final) while modifying the mask using the RiGL (Evci et al., 2020) algorithm.",,,1,related
"In the DST experiments, we use the same setup as random pruning, and modify the mask every 100 iterations.",,,1,related
"These can be partially remedied by using the rewiring strategy of Dynamic Sparse Training (DST) (Evci et al., 2020).",,,0,not_related
"61 In addition to the rewiring experiments shown in Table 2, we use Dynamical Sparse Training to prune an already sparse ER network to a higher sparsity and see if this can achieve the same performance as performing DST starting from a denser network.",,,1,related
"However, it further relies on edge rewiring steps that sometimes require the computation of gradients of the corresponding dense network (Evci et al., 2020).",,,0,not_related
"Our main results could also be interpreted as theoretical justification for Dynamic Sparse Training (DST) (Evci et al., 2020; Liu et al., 2021.; Bellec et al., 2018), which prunes random networks of moderate sparsity.",,,1,related
"These can be partially remedied by using the rewiring strategy of Dynamic Sparse Training (DST) (Evci et al., 2020",,,1,related
"Sparse to sparse training with DST Final test accuracy for VGG16 on CIFAR10 is reported where the model is initialized with an ER network of some initial sparsity and further pruned to a final sparsity (initial → final) while modifying the mask using the RiGL (Evci et al., 2020) algorithm.",,,1,related
"Our main results could also be interpreted as theoretical justification for Dynamic Sparse Training (DST) (Evci et al., 2020; Liu et al., 2021.",,,1,related
"This shortcoming could be potentially addressed by targeted rewiring of random edges with Dynamical Sparse Training (DST) that starts pruning from an ER network (Liu et al., 2021.",,,0,not_related
"Dynamical Sparse Training To improve randomly pruned networks at extremely high sparsities, we employ the RiGL algorithm (Evci et al., 2020) to obtain Table 2.",,,1,related
"Similarly, RigL 161 updates the topology of a sparse DNN during training by 162 considering a measure based on the magnitude of parame- 163 ters and infrequent gradient calculations.",,,0,not_related
"As mentioned, we select GMP [36], AMC [21], 448 GSM [27], DSR [33], SM [34], DNW [37], RigL [35], 449 DPF [29], and STR [38] as our competitors.",,,0,not_related
"Over 154 this dataset, the most prominent results are delivered by 155 DSR [33], SM [34], RigL [35], GMP [36], DNW [37], and 156 STR [38] algorithms.",,,0,not_related
"We compare Global MP with various popular SOTA algorithms that are well known for pruning, such as SNIP [42], SM [7], DSR [6], DPF [9], GMP [18], DNW [19], RigL [8], and STR [10].",,,1,related
"Another popular SOTA technique, RigL [8], also iteratively prunes and re-grows weights every few iterations.",,,0,not_related
"Similarly, many SOTA papers do not use Global MP for benchmarking and miss out on capturing its remarkable performance [8], [10], [18], [2], [19].",,,0,not_related
"Using this dataset, we compare Global MP with SOTA algorithms like GMP [18], DSR [6], DNW [19], SM [7], RigL [8], WoodFisher [63], MFAC [62], DPF [9], and STR [10].",,,1,related
"Furthermore, many SOTA algorithms miss out on benchmarking their algorithms against Global MP and hence are unable to capture its efficacy [8], [10], [18], [2], [19].",,,0,not_related
"Many pruning techniques have been developed over the years, which use first or second order derivatives [40], [41], [1], gradient based methods [42], [43], [44], sensitivity to or feedback from some objective function [9], [45], [46], [47], [48], distance or similarity measures [49], Bayesian optimisation [50], regularization-based techniques [51], [10], [52], [53], [54], and magnitude-based criterion [8], [17], [18],",,,0,not_related
Rigging the Lottery (RigL) [8] allocates sparsity based on the number of parameters in a layer.,,,1,related
", SNIP [2], GraSP [3], RigL [5], ITOP [6], SET [19], DSR [4], and MEST [1], is provided in Tab.",,,0,not_related
"Research on the sparse training strategies [47, 48, 49] can be categorized into fixed-mask and sparsemask sparse training, where the former aims to make it feasible that the training of the pruned models can be implemented on edge devices [2, 3, 50, 51, 52] and the latter studies how to reduce memory costs along with the computation during training [53, 19, 4, 54, 5].",,,0,not_related
"We compare the accuracy, training FLOPs, and memory costs of our framework with the most representative sparse training works [2, 3, 53, 54, 4, 19, 5, 1] at different sparsity ratios.",,,1,related
"The baseline results of “DST methods Min.” only consider the minimum memory costs requirement for DST methods [2, 3, 53, 54, 4, 19, 5, 1, 6], which ignores the memory overhead such as the periodic dense back-propagation in RigL [5], dense sparse structure searching at initialization in [2, 3], and the soft memory bound in MEST [1].",,,0,not_related
"” only consider the minimum memory costs requirement for DST methods [2, 3, 53, 54, 4, 19, 5, 1, 6], which ignores the memory overhead such as the periodic dense back-propagation in RigL [5], dense sparse structure searching at initialization in [2, 3], and the soft memory bound in MEST [1].",,,0,not_related
"Another category is Dynamic Sparse Training (DST), which usually starts the training from a randomly selected sparse structure [4, 5].",,,0,not_related
"The Dynamic Sparse Training (DST) method shows its superior performance by continuously changing its sparse model structure during training to search for a better sparse architecture, making it a desirable sparse training paradigm [5].",,,0,not_related
"The comparison of key features between SpFDE and other representative sparse training works, i.e., SNIP [2], GraSP [3], RigL [5], ITOP [6], SET [19], DSR [4], and MEST [1], is provided in Tab.",,,1,related
"The reason is that sparse training algorithms (e.g., MEST and RigL) may force less important weights/locations to be changed during sparse training, regardless of whether the sparse structure has already been stabilized.",,,0,not_related
"We also adapt representative sparse training methods (SNIP (34), RigL (19)) to the CL setting by combining them with DER++ (SNIP-DER++, RigL-DER++).",,,1,related
"We also use the best hyperparameter setting reported in (8; 56) for CL methods, and in (19; 34) for CL-adapted sparse training methods.",,,1,related
"A straightforward idea is to directly combine existing sparse training methods, such as SNIP (34), RigL (19), with a rehearsal buffer under the CL setting.",,,0,not_related
"data from a single training task (19; 66), TDM considers also the importance of weights w.",,,0,not_related
"Recently, another stream of work, sparse training (4; 19; 34) has emerged as a new training trend to achieve training acceleration, which embraces the promising training-on-the-edge paradigm.",,,0,not_related
"Moreover, we evaluate the training FLOPs (19), and memory footprint (66) (including feature map pixels and model parameters during training) to demonstrate the efficiency of each method.",,,1,related
"To overcome these drawbacks, dynamic mask methods (4; 16; 19; 44; 45) adjust the sparsity topology during training while maintaining low memory footprint.",,,0,not_related
"The second includes methods which prune before training like Single-shot Network Pruning (SNIP) [8] and Gradient Signal Preservation (GraSP) [9], or prune during training like Sparse Momentum (SM) [10] and Rigged Lottery (RigL) [11].",,,0,not_related
"• Rigged Lottery (RigL) [11] saves on the computational cost of SM by defining the sparsities of each layer beforehand, then operating layer by layer instead of across all layers.",,,0,not_related
"This recipe has been proven to be effective as shown in the previous experiments and many prior works [33, 40, 12, 28, 11, 18, 17, 21, 36, 52, 60, 15, 37, 38, 8, 39, 23, 9].",,,0,not_related
"Some other metrics such as gradientbased [54, 9], Hessian based [27], connection sensitivity [28], and salient-based [35, 28] are also used.",,,0,not_related
"Unstructured Sparsity prunes the model without any sparsity pattern constraint [43, 17, 28, 12, 14, 60, 18, 31, 51, 6, 14, 26, 9, 4, 34].",,,0,not_related
"4) From-scratch while learning sparsity pattern (Figure 1i) [51, 6, 14, 26, 9, 4, 34, 58, 10], which trains a sparse model from scratch while learning sparsity patterns simultaneously.",,,0,not_related
"Another line of effort in the DNN community is to propose different methods to compress the models, such as quantization [45, 25, 55, 57, 53], sparsification [11, 18, 17, 21, 36, 52, 60, 15, 37, 38, 8, 39, 23, 9], and distillation [44, 22, 46, 49].",,,0,not_related
"2) Fine-tuning with iterative pruning (Figure 1g) [11, 18, 17, 21, 36, 52, 60, 15, 37, 38, 8, 39, 23, 9], which trains a dense model and then iterates between pruning and re-training.",,,0,not_related
"For sparse training, we follow a similar drop and grow method that was proposed in RigL [47] to explore the sparse model architecture during the sparse training process.",,,1,related
The initial selection of A element could be a random process [47] or restricted to the top-K proportion of weights by magnitude [64].,,,0,not_related
"Then, for each training epoch, the dense model consumes 3fD FLOPs for forward and backward path [47], sparse model consumes 3fS FLOPs for forward and backward path.",,,0,not_related
RigL [47] grew back the weights with top-k largest gradients.,,,0,not_related
"The second one, sparse training [47], gives up the hypothesis that the dense model could guide the sparsification process [19] and directly trains a model with fixed sparsity from scratch.",,,0,not_related
"These are a special case of sparse neural networks (Gale et al., 2019; Dettmers and Zettlemoyer, 2019; Evci et al., 2020) which are similar in that they only use a subset of parameters, but differ because they have potentially irregular sparsity patterns.",,,0,not_related
"In computer vision tasks, it has been demonstrated that different CNN layers have different redundancy, and rule- or heuristic-based unstructured pruning [9, 18, 31] may be used to find the appropriate layer-wise sparsities to reduce their redundancy.",,,0,not_related
"In areas such as machine learning and neural networks, sparsity is universally present, a phenomenon primarily resulting from two aspects: (1) Activation sparsity from the activation function ReLU, roughly 50% sparsity [27]; (2) weight sparsity that derived from the pruning algorithms, along with sparsity training algorithms, up to 90% sparsity [7].",,,0,not_related
"1 INTRODUCTION Sparse matrix computation is widely employed in various applications, including scientific computation [14, 18, 26], neural network training [6, 7, 19, 24, 25], language processing models [4, 34] and so on.",,,0,not_related
The sparse training method RigL [7] is applied in our experiment to ensure that the model weights are sparse.,,,1,related
"More recently, sparse learning Evci et al. (2020); Kundu et al. (2021b); Dettmers & Zettlemoyer (2019); Raihan & Aamodt (2020); Kundu et al.",,,0,not_related
"In particular, recently proposed sparse learning (SL) strategies Evci et al. (2020); Kundu et al. (2021b); Mocanu et al. (2018); Dettmers & Zettlemoyer (2019); Raihan & Aamodt (2020) effectively train weights and associated binary sparse masks to allow only a fraction of model parameters to be updated during training, potentially enabling the lucrative reduction in both the training time and compute cost Qiu et al. (2021); Raihan & Aamodt (2020), while creating a model to meet a target parameter density denoted as d, and is able to yield accuracy close to that of the unpruned baseline.",,,0,not_related
"In particular, recently proposed sparse learning (SL) strategies Evci et al. (2020); Kundu et al. (2021b); Mocanu et al. (2018); Dettmers & Zettlemoyer (2019); Raihan & Aamodt (2020) effectively train weights and associated binary sparse masks to allow only a fraction of model parameters to be updated during training, potentially enabling the lucrative reduction in both the training time and compute cost Qiu et al.",,,0,not_related
"In particular, recently proposed sparse learning (SL) strategies Evci et al. (2020); Kundu et al.",,,0,not_related
"We now compare our SPDST mask initialization, with that of parameter density distribution evaluated via ERK+ Huang et al. (2022); Evci et al. (2020).",,,1,related
"More recently, sparse learning Evci et al. (2020); Kundu et al.",,,0,not_related
"In particular, recently proposed sparse learning (SL) strategies Evci et al. (2020); Kundu et al. (2021b); Mocanu et al. (2018); Dettmers & Zettlemoyer (2019); Raihan & Aamodt (2020) effectively train weights and associated binary sparse masks to allow only a fraction of model parameters to be…",,,0,not_related
"In particular, recently proposed sparse learning (SL) strategies Evci et al. (2020); Kundu et al. (2021b); Mocanu et al.",,,0,not_related
"FedDST Bibikar et al. (2021), on the other hand, leveraged the idea of RigL Evci et al. (2020) to perform sparse learning of the clients, relied on a large number of local epochs to avoid gradient noise, and focused primarily on only highly non-IID data without targeting ultra-low density d.",,,0,not_related
"More recently, sparse learning Evci et al. (2020); Kundu et al. (2021b); Dettmers & Zettlemoyer (2019); Raihan & Aamodt (2020); Kundu et al. (2020), a popular form of model pruning, has gained significant traction as it can yield FLOPs advantage even during training.",,,0,not_related
"The two growing strategies are gradient-based growth [12, 19]",,,0,not_related
"Follow-up works further introduce weight redistribution [12, 48], gradient-based weight growth [12, 19], and extra weights update in the backward pass [27, 54] to improve the sparse training performance.",,,0,not_related
"More recently, the Lottery Ticket Hypothesis [12] has sparked interest in techniques that provide the storage and computation benefits of sparse models directly during training [35, 8].",,,0,not_related
"Sparse Language Models Sparsely activated language models have been considered in a few forms (Evci et al., 2020; Mostafa and Wang, 2019; Dettmers and Zettlemoyer, 2019), but the Mixtureof-Experts (MoE) model is of particular note.",,,0,not_related
"Additionally, fine-tuning a sparse network will actually cost the same or even more time than a dense one [3].",,,0,not_related
"While post-training quantization can be an efficient and successful technique for quantizing models without any retraining (Frantar and Alistarh, 2022), in the case of pruning the gold standard is still training a separate model for every target sparsity level (Zhu and Gupta, 2017; Singh and Alistarh, 2020; Evci et al., 2020; Peste et al., 2021); the latter can be an expensive procedure, which would still rely on powerful computational resources to obtain the sparse models in the first place.",,,0,not_related
"Hence, we can approximate the number of MAC operations as 2Nc as done in [21].",,,1,related
Evci et al. (2020) further extended ER to CNN and brings significant gains to sparse CNN training with the Erdős-Rényi-Kernel (ERK) ratio.,,,0,not_related
"Weight Grow: The most common ways to grow new weights are random-based growth (Mocanu et al., 2018) and gradient-based growth (Dettmers & Zettlemoyer, 2019; Evci et al., 2020).",,,0,not_related
"A long-standing research topic, recent attempts on sparsity (Mocanu et al., 2018; Liu et al., 2021b;c; Evci et al., 2020; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Chen et al., 2021) train intrinsically sparse neural networks from scratch using only a small proportion of parameters and FLOPs (as illustrated in Figure 2).",,,0,not_related
"This ratio determines the computational FLOPs (floating-point operations) of the sparse model and has a significant impact on its final performance (Evci et al., 2020; Liu et al., 2022a; Hoang et al., 2023).",,,0,not_related
"A long-standing research topic, recent attempts on sparsity (Mocanu et al., 2018; Liu et al., 2021b;c; Evci et al., 2020; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Chen et al., 2021) train intrinsically sparse neural networks from scratch using only a small proportion of parameters and…",,,0,not_related
"While there is an upsurge in increasingly efficient ways for sparse training (Bellec et al., 2018; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021c; Jayakumar et al., 2020; Chen et al., 2021; Schwarz et al., 2021; Jiang et al.)",,,0,not_related
"[7] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.",,,0,not_related
Backward propagation has the same computation characteristics as forward propagation but has two times the computation expense of the forward propagation [9].,,,0,not_related
"Although a higher compression rate can be achieved, the unstructured sparsity pattern (Sun et al. 2017; Goli and Aamodt 2020; Lin et al. 2020; Evci et al. 2020) cannot be directly employed to commercial",,,0,not_related
"Although a higher compression rate can be achieved, the unstructured sparsity pattern (Sun et al. 2017; Goli and Aamodt 2020; Lin et al. 2020; Evci et al. 2020) cannot be directly employed to commercial Dataset Model Sparsity mAP(%) Speedup",,,0,not_related
The prune-redistribute-regrowth cycle is adopted in (Dettmers and Zettlemoyer 2019; Mostafa and Wang 2019; Evci et al. 2020) to change weights according to different criteria dynamically.,,,1,related
5 3 7 – RigL (Evci et al. 2020)∗ Unstructured 50%×1 W – – 76.,,,1,related
"In sparse masks, the number of incoming/outgoing connections is not identical for all the neurons in the layer (Evci et al., 2020b) and this raises direct concerns against the blind usage of dense network initialization for sparse subnetworks.",,,0,not_related
"Yet, (Evci et al., 2020b) also showed that completely random re-initialization of sparse subnetworks can lead the sparse masks to converge to poorer solutions.",,,0,not_related
"They usually prunes weights based on the magnitude and grows weights back (Mocanu et al., 2018) at random or based on the gradient (Evci et al., 2020a; Liu et al., 2021; Chen et al., 2022; 2021a).",,,0,not_related
"Aware of the sensitivity and negative impact of changing initialization identified by (Evci et al., 2020c), we point that that linear scaling will not hurt the original sparse mask’s initialization, thanks to the BatchNorm layer which will effectively absorb any linear scaling of the weights.",,,1,related
"Most pruning research since then has followed this approach (Zhou et al., 2019; Evci et al., 2020; Mostafa & Wang, 2019; Bellec et al., 2018; Dettmers & Zettlemoyer, 2019; Mocanu et al., 2018; You et al., 2020; Chen et al., 2020).",,,0,not_related
"However, a large body of literature has shown that a high number of carefully chosen parameters can be removed (i.e. pruned) while maintaining the network’s predictive performance (LeCun et al., 1990; Molchanov et al., 2017; Evci et al., 2020; Su et al., 2020; Lee et al., 2019; Wang et al., 2020).",,,0,not_related
"pruned) while maintaining the network’s predictive performance (LeCun et al., 1990; Molchanov et al., 2017; Evci et al., 2020; Su et al., 2020; Lee et al., 2019; Wang et al., 2020).",,,0,not_related
"…networks (Han et al., 2015; LeCun et al., 1990; Hassibi et al., 1993; Wang et al., 2019; Li et al., 2016), or throughout training (Srinivas & Babu, 2016; Louizos et al., 2018; Evci et al., 2020; Mostafa & Wang, 2019; Bellec et al.,
2018; Dettmers & Zettlemoyer, 2019; Mocanu et al., 2018).",,,0,not_related
"Training sparse neural networks has been extensively explored (Bellec et al., 2018; Evci et al., 2020; Mocanu et al., 2018; Liu et al., 2021b; 2020; 2021a).",,,0,not_related
"FLOPs behaviour of Sparse ERK networks in DRL
As reported in Evci et al. (2020), using ERK sparsity distribution often doubles the FLOPs needed for sparse models compared using the uniform sparsity distribution.",,,0,not_related
"…(Frankle et al., 2019; Zhou et al., 2019), dynamic sparse training (Dettmers & Zettlemoyer, 2019; Mostafa & Wang, 2019; Mocanu et al., 2018; Bellec et al., 2018; Evci et al., 2020; Liu et al., 2021) and one-shot pruning (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020; Liu & Zenke, 2020).",,,0,not_related
"Code Our code is built upon the TF-Agents (Guadarrama et al., 2018), Dopamine (Castro et al., 2018), and RigL (Evci et al., 2020) codebases.",,,1,related
"Within network sparsity In Figure 4 (left) we turn our attention to the question of distributing parameters within networks and compare two strategies; uniform and ERK (Evci et al., 2020).",,,1,related
"Inline with previous observations made in speech (Kalchbrenner et al., 2018), natu-
ral language modelling (Li et al., 2020) and computer vision (Evci et al., 2020), in almost all environments, sparse networks found by pruning achieve significantly higher rewards than the dense baseline.",,,0,not_related
"Due to weight sharing in convolutional layers, ERK sparsity distribution doubles the FLOPs required at a given sparsity (Evci et al., 2020), which we also found to be the case with the convolutional networks used by DQN in the Atari environments (see subsection A.",,,0,not_related
"However gradient based growth (Evci et al., 2020) seems to have a limited effect on performance.",,,0,not_related
"Rigged Lottery (RigL) (Evci et al., 2020): is the same as SET, except the new connections are activated using the gradient signal (highest magnitude) instead of at random.",,,0,not_related
", 2020) and computer vision (Evci et al., 2020), in almost all environments, sparse networks found by pruning achieve significantly higher rewards than the dense baseline.",,,0,not_related
"Due to weight sharing in convolutional layers, ERK sparsity distribution doubles the FLOPs required at a given sparsity (Evci et al., 2020), which we also found to be the case with the convolutional networks used by DQN in the Atari environments (see subsection A.2 for further discussion).",,,0,not_related
", 2019), dynamic sparse training (Dettmers & Zettlemoyer, 2019; Mostafa & Wang, 2019; Mocanu et al., 2018; Bellec et al., 2018; Evci et al., 2020; Liu et al., 2021) and one-shot pruning (Lee et al.",,,0,not_related
"Known as Dynamic sparse training (DST), such approaches have been shown to match pruning results, making it possible to train sparse networks efficiently without sacrificing performance (Dettmers & Zettlemoyer, 2019; Evci et al., 2020).",,,0,not_related
"Comparison of our model with SOTA pruning methods, DPF [29], STR[23], LAMP[27], RiGL[11], and SuRP[19].",,,1,related
"DPF [29], STR[23], LAMP[27], RiGL[11], and SuRP[19] are the SOTA methods that use a large sparsity (+50×) and maintain a reasonable accuracy.",,,0,not_related
"Dozens of earlier studies [22, 17, 7] have demonstrated that unstructured sparsity is able to reach negligible performance degradation under",,,0,not_related
"The use of a binary mask is originated from many traditional unstructured sparsity methods [7, 16].",,,0,not_related
"4.3, we further show the performance of LBC and traditional unstructured sparsity methods including RigL [7],
GMP [41], STR [17] under similar total compression rates.",,,1,related
"3, we further show the performance of LBC and traditional unstructured sparsity methods including RigL [7],",,,1,related
RigL [7] alternately removes and revives weights according to their magnitudes and dense gradients.,,,0,not_related
"Sparse models for these accelerators are obtained through the many pruning and sparsification techniques, [23, 25, 36, 48, 49, 58, 62] including channel pruning [37, 55] and advanced compression [34].",,,0,not_related
"Inspired from (Evci et al., 2020), we take similar steps to update the local mask on each client.",,,1,related
"We highlight the contributions we make to extend the traditional sparse training techniques like Rigl (Evci et al., 2020), and Set (Mocanu
et al., 2018) to the proposed decentralized sparse training technique as follows: (i) To deal with the data heterogeneity and to learn different sparse models…",,,1,related
"We highlight the contributions we make to extend the traditional sparse training techniques like Rigl (Evci et al., 2020), and Set (Mocanu",,,1,related
"The mask is initialized based on Erdos-Renyi Kernel (ERK) (Evci et al., 2020), which assigns higher sparsities to layers with more parameters and lower sparsities to layers with fewer parameters.",,,0,not_related
"We highlight the contributions we make to extend the traditional sparse training techniques like Rigl (Evci et al., 2020), and Set (Mocanu
et al., 2018) to the proposed decentralized sparse training technique as follows: (i) To deal with the data heterogeneity and to learn different sparse models for each client, decentralized sparse training operates on the local client instead of operating on the centralized device.",,,1,related
"Other criteria include gradient [7], low-rank decomposition [47], Hessian [22], etc.",,,0,not_related
[7] proposed RigL for training sparse models without the need of a “lucky” initializations.,,,0,not_related
Some works [7] keep the first layer dense because the sparsity of the first layer will have a significant effect on the performance but has little effect on reducing the model size.,,,0,not_related
Neural network pruning can be roughly categorized into structured pruning and unstructured pruning [7].,,,0,not_related
"The decay rule can be polynomial [25], sinusoidal [7], or other forms.",,,0,not_related
"We follow RigL [7], which regenerates part of the pruned connections based on the gradient magnitude.",,,1,related
Evci et al. [7] proposed RigL for training sparse models without the need of a “lucky” initializations.,,,0,not_related
"We compressed the pre-trained state-of-the-art deraining models by the same FLOPs, for a fair comparison to the most classical (l1 [13]) and modern (erk [8], lamp [15]) pruning methods.",,,1,related
"Considering that the original data is not available, we mainly compare with the alternative magnitude-based pruning methods, including the most classical l1 regularization [13], and the most modern methods of erk [8] and lamp [15].",,,1,related
"In practice, various attempts have been made to compress the heavy CNN models, including quantization [11, 12, 23], pruning [8, 13, 15, 21], distillation [3, 7, 14], and so on.",,,0,not_related
"These works adjust structures of sparse networks during training, including Deep Rewiring (DeepR) [2], Sparse Evolutionary Training (SET) [26], Dynamic Sparse Reparameterization (DSR) [30], Sparse Networks from Scratch (SNFS) [8], and Rigged Lottery (RigL) [10].",,,0,not_related
"Sparse Models in DRL [10, 36] show that finding a sparse model in DRL is difficult due to training instability.",,,0,not_related
"Many works [2, 26, 30, 8, 10] also try to train a sparse neural network from scratch without having to pre-trained dense models.",,,0,not_related
", SET [26] and RigL [10], can train a 90%-sparse network (i.",,,0,not_related
"RigL [10], which uses dynamic sparse training by dropping and growing connections with magnitude and gradient criteria, respectively, the same as RLx2’s topology evolution procedure.",,,0,not_related
The topology evolution in RLx2 is made by adopting the RigL [10] method.,,,1,related
"Our RLx2 algorithm, which contains both topology evolution [10] (using the same scheme in TE) and accurate value estimation (using multi-step TD targets [19]), is able to achieve a performance close to TE+Q∗ without the need for pre-trained expert Q-values.",,,1,related
"Algorithm 1 Topology Evolution [10] 1: Nl: Number of parameters in layer l 2: θl: Parameters in layer l 3: Mθl : Sparse mask of layer l 4: sl: Sparsity of layer l 5: L: Loss function 6: ζt: Update fraction in training step t 7: for each layer l do 8: k = ζt(1− sl)Nl 9: Idrop = ArgTopK(−|θl Mθl |, k) 10: Igrow = ArgTopKi/ ∈θl Mθl\Idrop(|∇θlL, k|) 11: Update Mθl according to Idrop and Igrow 12: θl ← θl Mθl 13: end for",,,1,related
"In particular, we apply a gradient-guided topology search scheme [10] to enable dynamic network evolution.",,,1,related
"Topology evolution RLx2 drops and grows connections with magnitude and gradient criteria, respectively, which has been adopted in RigL [10] for deep supervised learning.",,,0,not_related
"Gradient-based regrowth e.g., momentum [Dettmers and Zettlemoyer, 2019] and gradient [Evci et al., 2020], shows strong results in image classification,
whereas random regrowth outperforms the former in language modeling [Dietrich et al., 2021].",,,0,not_related
", 2021b]; one SST method: ERK [Evci et al., 2020]; and one pruning at initialization approach: SNIP [Lee et al.",,,1,related
"To ver-
ify the effectiveness of Sup-tickets, we apply it to various sparse training methods, including 3 DST methods: SET, RigL [Evci et al., 2020], and GraNet [Liu et al., 2021b]; one SST method: ERK [Evci et al., 2020]; and one pruning at initialization approach: SNIP [Lee et al., 2018].",,,1,related
"Among them, sparse neural network training [Mocanu et al., 2018, Evci et al., 2020, Bellec et al., 2018] stands out and receives growing attention recently due to its high efficiency in both the training and inference phases.",,,0,not_related
", 2018] and its CNNs variant Erdős-Rényi-Kernel (ERK) [Evci et al., 2020] allocates lower sparsity to smaller layers, avoiding the layer collapse problem [Tanaka et al.",,,0,not_related
"Inspired by the graph theory, Erdős-Rényi (ER) [Mocanu et al., 2018] and its CNNs variant Erdős-Rényi-Kernel (ERK) [Evci et al., 2020] allocates lower sparsity to smaller layers, avoiding the layer collapse problem [Tanaka et al., 2020] and achieving stronger results than the uniform sparsity in general.",,,0,not_related
", momentum [Dettmers and Zettlemoyer, 2019] and gradient [Evci et al., 2020], shows strong results in image classification,",,,0,not_related
"Inspired by the graph theory, Erdős-Rényi (ER) [Mocanu et al., 2018] and its CNNs variant Erdős-Rényi-Kernel (ERK) [Evci et al., 2020] allocates lower sparsity to smaller layers, avoiding the layer collapse problem [Tanaka et al., 2020] and achieving stronger results than the uniform sparsity in…",,,0,not_related
"Besides the sparse structures, in the most sparse training literature [Dettmers and Zettlemoyer, 2019, Evci et al., 2020, Mostafa and Wang, 2019, Liu et al., 2021b], it is usually a safe choice to keep the other training configurations, such as optimizers, hyperparameters, and learning rate…",,,0,not_related
"ify the effectiveness of Sup-tickets, we apply it to various sparse training methods, including 3 DST methods: SET, RigL [Evci et al., 2020], and GraNet [Liu et al.",,,1,related
"%) usually remains below the full dense training under a regular training epoch number [Evci et al., 2020, Liu et al., 2021b].",,,0,not_related
Pruning aware training is currently an actively researched topic and a few works have presented impressive results that increase the sparsity of the kernel/weight parameters up to 80% [21] during the training.,,,0,not_related
"This evaluation shows that a nominal sparsity range of 50% to 80% is achievable for most networks with minor accuracy loss, which was also the similar sparsity presented in [21].",,,0,not_related
[16] observed that the FLOP count of a ResNet-50 model at a fixed global sparsity can vary by a factor of over 2× due to differences in the individual sparsities of convolutional layers with differing output dimensions.,,,0,not_related
"Our baselines are iterative magnitude pruning [48], RigL with the Erdos-Renyi-Kernel (ERK) sparsity distribution [16], Soft Threshold Weight Reparameterization (STR) [25], probabilistic masking (ProbMask) [47], OptG [46], and Top-KAST with Powerpropagation and ERK [24; 37].",,,1,related
"Spartan belongs to the family of “parameter dense” training algorithms that maintains a dense parameter vector θ ∈ R throughout training [48; 24], in contrast to “parameter sparse” training algorithms that adhere to a Õ(k) memory budget for representing the parameters of a ksparse model [3; 31; 32; 13; 16].",,,0,not_related
"For Top-KAST, we exclude the first convolutional layer from pruning (following [37; 16]) and we use fully dense backward passes (i.",,,1,related
"Since Spartan retains a dense parameter vector and computes dense backward passes during training, it incurs higher memory and computational costs in each iteration than methods like RigL [16] that use both a sparse parameters and sparse backward passes.",,,0,not_related
"We do not prune biases and batch-normalization parameters, as they only account for a small fraction of the total number of parameters yet are crucial for obtaining well-performing models [19].",,,1,related
"Current techniques can be mainly categorised as the iterative sparsification of a densely initialised network (Gale et al., 2019) or techniques that maintain constant sparsity
throughout learning (e.g. Evci et al., 2020; Jayakumar et al., 2020).",,,0,not_related
"It is also worth pointing out that existing hand-designed sparsity distributions (e.g. Mocanu et al., 2018; Evci et al., 2020) would result in a different pattern, allocating equal sparsity to layers 2-4, whereas our empirical results suggest this might not be optimal in all cases.",,,0,not_related
", 2021b) or by starting the pruning not from a dense but a sparse random architecture (Evci et al., 2020; Liu et al., 2021b).",,,0,not_related
"This can also be achieved with the help of core sets (Zhang et al., 2021b) or by starting the pruning not from a dense but a sparse random architecture (Evci et al., 2020; Liu et al., 2021b).",,,0,not_related
"It can therefore be beneficial to start pruning from a sparse random architecture rather than a dense network (Evci et al., 2020; Liu et al., 2021b), which saves computational resources.",,,0,not_related
", 2021a), but also to reduce the computational burden associated with deep learning (You et al., 2020; Evci et al., 2020; Liu et al., 2021b).",,,0,not_related
"Recent strategies [14, 30] dynamically extract and train sparse subnetworks instead of training the full models.",,,0,not_related
"However, the empirical success of the popular sparse algorithms implies that the tolerance to such error can be quite large in practice [7], [21], [22], [25], [30], which enables us to implement sparse training in FL for communication efficiency, and meanwhile utilizes the properties of sparse models to address the unreliable communications.",,,0,not_related
"Moreover, the sparse topology’s updates based on parameter magnitudes and infrequent gradient calculations in [25] loosened the limitation on the size relationship between sparse model and the corresponding dense model, which further reduced the computation cost for sparse learning.",,,0,not_related
"Sparsity enabled Communication Efficiency and Similarity assisted Bias Reduction: To save computing resources and training/inference time, sparse learning on large neural networks has been widely deployed in the deep learning field [7], [21], [22], [25], [30].",,,0,not_related
"We train the sparse dynamic convolution following an iterative pruning process [11, 13].",,,1,related
"We conjecture that the sparse property [16,18] has reduced the redundancy in high-resolution feature maps in our HRCA and leads to higher performance and efficiency.",,,1,related
"In sparse training and efficient Auto-ML algorithms, it is a common practice to estimate future ranking of models with current parameters and their gradients [22, 67], or with parameters after a single step of gradient descent update [9, 51, 53].",,,0,not_related
"As for random pruning, every layer can be uniformly pruned with the same pre-defined pruning ratio (Mariet and Sra, 2015; He et al., 2017; Gale et al., 2019) or the pruning ratio can be varied for different layers such as Erdö-Rényi (Mocanu et al., 2018) and Erdö-Rényi Kernel (Evci et al., 2020).",,,1,related
", 2021a) explores the sparsity pattern in a prune-and-grow scheme according to some criteria (Mocanu et al., 2018; Mostafa and Wang, 2019; Dettmers and Zettlemoyer, 2019; Evci et al., 2020; Ye et al., 2020; Jayakumar et al., 2020; Liu et al., 2021b).",,,0,not_related
"…for random pruning, every layer can be uniformly pruned with the same pre-defined pruning ratio (Mariet and Sra, 2015; He et al., 2017; Gale et al., 2019) or the pruning ratio can be varied for different layers such as Erdö-Rényi (Mocanu et al., 2018) and Erdö-Rényi Kernel (Evci et al., 2020).",,,1,related
"…sparse training (Mocanu et al., 2018; Liu et al., 2021a) explores the sparsity pattern in a prune-and-grow scheme according to some criteria (Mocanu et al., 2018; Mostafa and Wang, 2019; Dettmers and Zettlemoyer, 2019; Evci et al., 2020; Ye et al., 2020; Jayakumar et al., 2020; Liu et al., 2021b).",,,0,not_related
"All these types of unstructured pruning only reduce the memory footprint [9, 8].",,,0,not_related
"Most of the pruning methods focus on pruning the weights [8, 9, 12, 18].",,,0,not_related
"5.3, we discuss IP and SP for more sophisticated sparse training methods, namely LTs [15] and the DST methods SET [49] and RigL [13].",,,1,related
Table 2 shows training and test accuracy for the IP- and SP versions of SET and SNIP for a VGG16 on CIFAR-10 as well as RigL for a ResNet50 on ImageNet.,,,0,not_related
"For training sparse networks, we distinguish between (i) pruning at initialization (PaI) [9,35,66,71,75] which prunes the network at initialization and fixes zeroed parameters during training, (ii) finding the sparse architecture to be finally trained by iterative train-prune-reset cycles, a so called lottery ticket (LT) [14,15], and (iii) dynamic sparse training (DST) [13,39,49] which prunes the network at initialization, but allows the pruning mask to be changed during training.",,,1,related
"We demonstrate this by achieving SOTA results with the application of IP to SOTA standard PaI, LT, DST as well as classical pruning methods.",,,1,related
"Furthermore, we want to check if IP boosts the SOTA methods LT and RigL as well.",,,1,related
"3, we discuss IP and SP for more sophisticated sparse training methods, namely LTs [15] and the DST methods SET [49] and RigL [13].",,,1,related
SET regrows coefficients randomly whereas RigL regrows those with high gradient magnitude.,,,0,not_related
Related work covers general pruning and pruning before training and DST. Training a sparse model allows to learn non-zero FB coefficients and FBs jointly from scratch.,,,1,related
"5 compare SP and IP on various sparse training and other pruning methods, namely:
DST randomly prunes the model at initialization.",,,1,related
"As shown in this work, using expensive pre-training to find a better pruning mask via LTs or adapting suppR during training via DST further improves IP’s performance.",,,0,not_related
"For SP, more expensive or sophisticated methods like LT and DST improve sparse training results compared to PaI.",,,0,not_related
"became of interest, providing the benefits of reduced memory requirements and runtime not only for inference but also for training [13, 14, 35, 47, 49, 55, 66, 71, 75].",,,0,not_related
RigL [13] improves this by recovering those weights with the biggest gradient magnitude.,,,0,not_related
"Dynamic sparse training [10,13,39,49] adjusts pruning masks during training to ensure sparse networks while adapting the architecture to different conditions.",,,0,not_related
DST and LT on CIFAR-10.,,,0,not_related
"IP improves DST and LTs significantly, see Figs.",,,0,not_related
"The pruning mask is updated each 1, 500 iterations for SET and 4, 000 for RigL.",,,1,related
"Table 1 compares IP and SP on the SOTA pruning methods RigL [13], GMP [17] and FT [59].",,,1,related
Rigging the Lottery (RigL) Evci et al. (2020) enhances the sparse network training by editing the network connectivity along with the optimizing the parameter by taking advantages of both weight magnitude and gradient information.,,,0,not_related
"ER and ERK typically achieve better performance on CNNs than the naive uniform distribution, i.e., allocating the same sparsity to all layers [31].",,,0,not_related
"Given that the most the state-of-the-art GANs are constructed based on convolutional neural networks (CNNs), we initialize both G(z) and D(x) with the Erdős-Rényi-Kernel (ERK) graph topology [36], which automatically allocates higher sparsity to larger layers and lower sparsity to smaller ones.",,,0,not_related
", momentum [38] and gradient [36] shows strong results in convolutional neural networks, whereas random regrowth outperforms the former in language modeling [40].",,,0,not_related
"Sparse initialization of D(x,θD) 2: G(z,θsG)← ERK(G(z,θ), sG) .",,,1,related
"Output: Sparse Generator G(z,θsG), Sparse discriminator D(x,θsD )
1: D(x,θsD )← ERK(D(x,θ), sD) .",,,1,related
"Given that the most the state-of-the-art GANs are constructed based on convolutional neural networks (CNNs), we initialize both G(z) and D(x) with the Erdős-Rényi-Kernel (ERK) graph topology [36], which automatically allocates higher sparsity to larger layers and lower sparsity to smaller ones.",,,0,not_related
"While not initially designed for SST, Erdős-Rényi (ER) [35] and ErdősRényi-Kernel (ERK) [36] are two advanced layer-wise sparsities introduced from the field of graph theory with strong results.",,,0,not_related
"We consider unstructured sparsity (individual weights are removed from a network) in this paper, not only due to its promising ability to preserve performance even at extreme sparsities [15, 36] but also the increasing support for sparse operations on the practical hardware [42–45].",,,1,related
"While not initially designed for SST, Erdős-Rényi (ER) [35] and ErdősRényi-Kernel (ERK) [36] are two advanced layer-wise sparsities introduced from the field of graph theory with strong results.",,,0,not_related
"As demonstrated in Evci et al. (2020a), allowing new connections to grow yields improved flexibility in navigating the loss surfaces, which creates the opportunity to
5
escape bad local minima and search for the optimal sparse connectivity Liu et al. (2021b).",,,0,not_related
"Note that newly added connections are not activated in the last sparse topology, and are initialized to zero since it establishes better performance as indicated in (Evci et al., 2020a; Liu et al., 2021b).",,,1,related
"However, our flying bird first removes the parameters with the lowest magnitude, which ensures a small term of the first-order Taylor approximation of the loss and thus limits the impact on the output of networks (Evci et al., 2020a).",,,1,related
"As two major components in the dynamic sparsity exploration (Evci et al., 2020a), we conduct thorough ablation studies in Table 4 and 5.",,,1,related
"…the huge family of sparse training (Mocanu et al., 2016; Evci et al., 2019; Mostafa & Wang, 2019; Liu et al., 2021a; Dettmers & Zettlemoyer, 2019; Jayakumar et al., 2021; Raihan & Aamodt, 2020), the recent methods Evci et al. (2020a); Liu et al. (2021b) lead to the state-of-the-art performance.",,,0,not_related
"This training pipeline, called as Flying Bird (FB), is motivated by the latest sparse training approaches (Evci et al., 2020b) to further reduce robust generalization gap in AT, while ensuring low training costs.",,,0,not_related
"And then, it allows new connectivity with the largest gradient to grow to reduce the loss quickly (Evci et al., 2020a).",,,0,not_related
"Comprehensive results of these subnetworks at 80% and 90% sparsity are reported in Table 1, where the chosen sparsity follows routine options (Evci et al., 2020a; Liu et al., 2021b).",,,1,related
The current state-ofthe-art [21] and [5] solve this task via one single round of training.,,,0,not_related
"We compare ASNI-I against its counterparts [45,21,5] where the two last ones are the state-of-the-art methods.",,,1,related
"87 Rigging the Lottery, [5] 5,120,000 74.",,,0,not_related
"Neural networks have been shown to have great expressive power even when the weights are randomly initialized (Frankle & Carbin, 2018; Evci et al., 2020; Ramanujan et al., 2020).",,,0,not_related
"Current techniques range from simple approaches like gradual magnitude pruning [17, 56], which periodically drops the fraction of the weights with lowest magnitude, followed by model finetuning, to dynamic techniques like Soft Threshold Reparametrization [32], Movement Pruning [49], or Rigging the Lottery [11], which adapt mask selection during training itself.",,,0,not_related
"Surprisingly, properly-tuned gradual magnitude pruning is often competitive with more complex methods [51, 11, 14, 46].",,,0,not_related
"[11, 51, 50, 46], do not directly take the behavior of acceleration methods into account, while existing speedup-aware structured pruning methods are not straightforward to adapt to the unstructured case.",,,0,not_related
"Note that, some recent advances [8, 51] advocate incremental pruning that removes a small portion of weights each time.",,,0,not_related
"For instance, RigL [8] re-allocates the removed",,,1,related
"Note that our OptG sorts the weights in a global manner to automatically decide a layer-wise sparsity budget, thus avoiding the rule-of-thumb design [8] or complex hyper-parameter tuning for learning sparsity distributions [16].",,,1,related
"For instance, RigL [8] removes a small fraction of weights and activates new ones iteratively, while Zhu et al.",,,0,not_related
"Besides, we compare our OptG with several the state-of-the-arts including SNIP [25], GraSP [45], SET [34], GMP [11], SynFlow [44], DNW [46], RigL [8], GSM [6], STR [22] and GraNet [29].",,,1,related
"Majorities of existing methods implement layer-wise sparsity using a static or dynamic design [8,24].",,,0,not_related
"Recent advances advocate during-training sparsity which consults the sparsity process throughout network training [8, 22].",,,0,not_related
"On CIFAR-100, our OptG provides significantly better accuracy against other gradient-driven approaches including GrasP [45] and RigL [8], which demonstrates the superiority of optimizing the gradient-driven criteria in network sparsity.",,,0,not_related
"The other direction is through sparse mask exploration [2, 31, 7], where a sparsity in neural networks are maintained during the training process, while the fraction of the weights is explored based on random or heuristics methods.",,,0,not_related
"[7] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.",,,0,not_related
The fast optimizer [4] and sparse training [5] have reduced the training algorithm complexity.,,,0,not_related
"Choosing s is also a hard search problem [22, 23].",,,0,not_related
"Gradient based growth criteria is used in the context of growing sparse connections (Liu et al., 2017; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; ab Tessera et al., 2021; Evci et al., 2022) and when initializing neural networks (Dauphin & Schoenholz, 2019).",,,0,not_related
"In addition to undertaking iterative pruning, algorithms can iteratively grow connections, working to ensure that the optimal set of interconnections is retained [17].",,,0,not_related
Using such adaptive techniques it is now possible to create accurate networks with 90% sparsity on ImageNet [17] and Transformers [13].,,,0,not_related
This stem performs a 7 × 7× 3 (RGB color values) convolution on the input image [17].,,,0,not_related
"Pruning techniques primarily focused on reducing computational overheads are also in use [17, 53].",,,0,not_related
"We begin on the server by initializing a server network θ(1) and a sparse maskm(1), following the layer-wise sparsity distribution described in (Evci et al. 2020).",,,1,related
"Note that following the convention of (Mocanu et al. 2018; Evci et al. 2020; Liu et al. 2021c), FedDST so far only considers element-wise unstructured sparsity.",,,1,related
"We begin on the server by initializing a server network θ1 and a sparse maskm1, following the layer-wise sparsity distribution described in (Evci et al. 2020).",,,1,related
"As in (Evci et al. 2020; Dettmers and Zettlemoyer 2019), we use a cosine decay update schedule for αr,",,,1,related
"As in (Evci et al. 2020; Dettmers and Zettlemoyer 2019), we use a cosine decay update schedule for αr,
αr = α
2
( 1 + cos ( (r − 1)π Rend )) .",,,1,related
"However, suitable values for α are still smaller than in RigL; for α ∈ [0.001, 0.05], FedDST significantly outperforms other methods.",,,1,related
The client-side mask readjustment procedure is familiar and takes inspiration from RigL (Evci et al. 2020).,,,1,related
At the core of FedDST is a judiciously designed federated approach to dynamic sparse training (Evci et al. 2020).,,,0,not_related
"In RigL (Evci et al. 2020), the authors initialize the sparsity mask randomly and perform layer-wise magnitude pruning and gradient-magnitude weight growth.",,,0,not_related
"For RandomMask, we randomly sample weights at the server, then perform layer-wise magnitude pruning, following the ERK sparsity distribution (Evci et al. 2020), before the first round, and perform FedAvgM on this sparse network.",,,1,related
"More works show sparsity can emerge at initialization (Lee, Ajanthan, and Torr 2019; Wang, Zhang, and Grosse 2020) or can be exploited in dynamic forms during training (Evci et al. 2020).",,,0,not_related
", 2020b), or in dynamic forms throughout training (Evci et al., 2020) by updating model parameters and architecture typologies simultaneously.",,,0,not_related
"…works reveal that sparsity patterns might emerge at the initialization (Lee et al., 2018), the early stage of training (You et al., 2019) and (Chen et al., 2020b), or in dynamic forms throughout training (Evci et al., 2020) by updating model parameters and architecture typologies simultaneously.",,,0,not_related
"For RigL, we use the PyTorch implementation of Sundar & Dwaraknath (2021).",,,1,related
"Results of 98% sparsity in Table 2 show that RMDA consistently outdoes RigL, indicating regularized training could be a promising alternative to pruning.",,,1,related
"We run RigL with 1000 epochs, as its performance at the default 500 epochs was unstable, and let
RMDA use the same number of epochs.",,,1,related
"When the desired structure is (unstructured) sparsity, a popular approach is pruning that trims a given dense model to a specified level, and works like (Gale et al., 2019; Blalock et al., 2020; Evci et al., 2020; Verma & Pesquet, 2021) have shown promising results.",,,0,not_related
"We compare RMDA with a state-of-the-art pruning method RigL (Evci et al., 2020).",,,1,related
It is commonly believed (Evci et al. 2020; Frankle and Carbin 2020; Malach et al. 2020; Zhou et al. 2019) that finding the “important” weight values is crucial for retraining a small pruned model.,,,0,not_related
"As expected, RigL does not speed up training (the pioneering work has unstructured sparsity and does not achieve speed up on GPU) but surprisingly Pixelfly outperforms both dense and RigL in terms of accuracy while achieving 2.1× speedup.",,,0,not_related
"RigL aims to sparsify model weights/parameters, so we use it as a baseline in MLP-based models (Mixer).",,,1,related
"On the WikiText-103 1State-of-the-art sparse training methods require up to 5× more training epochs compared to dense models [Evci et al., 2020] 2An unstructured sparse model with 1% nonzero weights can be as slow as a dense model [Hooker, 2020]",,,1,related
"Specifically, we achieve training speed up on both MLP-Mixer and ViT models by up to 2.3× wall-clock time with no drop in accuracy compared to the dense model and up to 4× compared to RigL, BigBird and other sparse baselines.",,,1,related
"On the WikiText-103
1State-of-the-art sparse training methods require up to 5× more training epochs compared to dense models [Evci et al., 2020]
2An unstructured sparse model with 1% nonzero weights can be as slow as a dense model [Hooker, 2020]
language modeling task, we speed up GPT-2 Medium…",,,1,related
"Figure 6: Comparison with a representative sparse training baseline RigL [Evci et al., 2020].",,,1,related
"For a fair comparison, we conduct the experiment on Mixer-S/32 model for 100 epochs because RigL aims for sparsity on weights, while we aim for both weights & attention.",,,1,related
"Additionally, we include the “The Rigged Lottery” (RigL) method [12] with Erdős-Rényi-Kernel (ERK) weight density.",,,1,related
"The top-performing methods we consider here are Soft Threshold Reparametrization (STR) [41], Alternating Compressed/DeCompressed Training (AC/DC) [57] and “The Rigged Lottery” (RigL) [12].",,,1,related
"We also consider the “The Rigged Lottery” (RigL) method [12], which achieves close to state-of-the-art ImageNet results by allowing for dynamic weight pruning and re-introduction with long finetuning periods, to be a regularization method.",,,1,related
", S(2)ViTE [11] employs a prune-and-grow strategy to explore a larger pruning space under the lottery ticket hypothesis [21].",,,0,not_related
"In comparison with the multishot results, we observe that SYNFLOW, SNIP, and MAGNITUDE pruning outperform RIGL on this task for the extreme sparsity levels (compare Fig.",,,1,related
"1Note that Evci et al. (2020) use percentage of pruned parameters for their plots, i.e. 1−sparsity in our paper.",,,1,related
"Note that the version of SNIP used in the original paper is essentially the singleshot approach, which indeed performs worse than RIGL (compare Fig.",,,1,related
"For our benchmark data, we construct similar networks as for the multishot experiments – i.e. depth 6 and width 100 fully connected networks – and run the available implementation of RIGL with default parameters as suggested in the paper, and Adam optimization with the same parameter settings as for all other considered methods.",,,1,related
"Our results on Circlematch those results, with RIGL being able to match ground truth ticket performance for sparsity .5 and .1.",,,1,related
"We show in Appendix B that neither dynamic sparse training (Evci et al., 2020) nor LT fine-tuning techniques (Liu et al.",,,1,related
"The original results reported in Evci et al. (2020) indicate that for their considered (classification) tasks, RIGL outperforms other dynamic sparse training methods, and that for target sparsity levels of .1 and lower, performance quickly deteriorates for all methods.1 In the original paper, there…",,,1,related
"Yet, RIGL allows for efficient computations, saving FLOPS by only infrequently updating gradients, which render it the method of choice for target sparsities of ≥ .1 in specific applications.",,,0,not_related
"Beyond LT pruning, many more methods have been developed to reduce computational resources and perform structure learning, including dynamic sparse training (Evci et al., 2020; Liu et al., 2021b), adaptations (Frankle et al.",,,0,not_related
"For regression tasks, we see a similar trend, with RIGL performing comparably good as SNIP for sparsity levels ≥ .1, but the performance decreases rapidly for more extreme target sparsitiy levels.",,,1,related
"The original results reported in Evci et al. (2020) indicate that for their considered (classification) tasks, RIGL outperforms other dynamic sparse training methods, and that for target sparsity levels of .1 and lower, performance quickly deteriorates for all methods.1 In the original paper, there was no exploration of the more extreme sparsities considered here, nor a comparison to ticket pruning other than SNIP.",,,1,related
"…pruning, many more methods have been developed to reduce computational resources and perform structure learning, including dynamic sparse training (Evci et al., 2020; Liu et al., 2021b), adaptations (Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a; Liu et al.) of Iterative Magnitude…",,,0,not_related
"We show in Appendix B that neither dynamic sparse training (Evci et al., 2020) nor LT fine-tuning techniques (Liu et al., 2021a; Renda et al., 2020) find architectures that are competitive with our constructed ground truth tickets.",,,1,related
"RIGL While the main focus of our paper are lottery tickets, we here briefly discuss results for RIGL (Evci et al., 2020), a state-of-the-art dynamic sparse training approach, which results in sparsified and trained network architectures which are comparable to trained ’weak’ tickets.",,,1,related
"Generally, for the regression tasks we observe that RIGL is outperformed by the state-of-the-art ticket pruner SYNFLOW and iterative MAGNITUDE pruning.",,,1,related
"Another line of methods [180], [181], [182], [183], [184] focus on jointly learning the sparse structures with model parameters by considering different downstream hardware features, e.",,,0,not_related
"[32, 6] automatically reallocate parameters across layers during training via controlling the global sparsity.",,,0,not_related
"granularity non-parametric parametric weight-level [6, 8, 51, 24, 20, 31, 42, 32, 5] [45, 40, 28, 50, 20] channel-level [44, 14] [21, 26, 47, 28, 18]",,,0,not_related
"We note that we follow the advice of Evci et al. (2020) and Dettmers & Zettlemoyer (2019) and do not prune biases and batch-normalization parameters, since they only amount to a negligible fraction of the total weights, however keeping them has a very positive impact on the performance of the…",,,1,related
", 2019), where specific criteria have been proposed that take the particular network architecture into consideration (Zhu & Gupta, 2017; Gale et al., 2019; Evci et al., 2020; Lee et al., 2020).",,,0,not_related
"Evci et al. (2020) propose a reformulation of the ERDŐS-RÉNYI KERNEL (ERK) (Mocanu et al., 2018) to take the layer and kernel dimensions into account when determining the layerwise sparsity distribution.",,,0,not_related
"…on the magnitude of their current values has established itself as the approach of choice (Lee et al., 2019), where specific criteria have been proposed that take the particular network architecture into consideration (Zhu & Gupta, 2017; Gale et al., 2019; Evci et al., 2020; Lee et al., 2020).",,,0,not_related
"A couple of methods have been proposed for training deep neural networks from scratch using sparse connections and sparse training (Dettmers & Zettlemoyer, 2019; Mocanu et  al., 2018; Bellec et  al., 2017; Mostafa & Wang, 2019; Evci et  al., 2019; Zhu & Jin, 2019).",,,0,not_related
"The other reference works (except SET [14] and RigL [11] that use uniform sparsity) use non-uniform sparsity, which leads to a higher computation FLOPs compared to the uniform sparsity under the same sparsity ratio.",,,0,not_related
"1 Sparsity Scheme in Sparse Training on the Edge It is common to see that sparse training works [9, 10, 15, 14, 13, 12, 11, 45] represent the training speed performance using the training FLOP count.",,,0,not_related
"7×, which is 250 epochs, to compare with the RigL with 5× longer training, which is 500 epochs as reported in [11].",,,0,not_related
RigL [11] proposes to iteratively update sparse model topology during training by calculating dense gradients only at the update step.,,,0,not_related
"RigL [11] is a recent milestone of dynamic sparse training works, which has considerable improvements compared to previous works.",,,0,not_related
", GraSP [10], SNIP [9], RigL [11], SNFS [12], DSR [13], SET [14], and DeepR [15].",,,0,not_related
"2 Sparse Training with Dynamic Sparsity Mask To reduce the computation as well as memory footprint during the whole training phase, sparse training is exploited in many works [15, 14, 13, 12, 11], which can adjust the sparsity topology during training as well as maintain a low memory footprint.",,,0,not_related
"Furthermore, sparse training with dynamic sparsity mask such as SET [14], DeepR [15], DSR [13], RigL [11], and SNFS [12] have been proposed, showing great potential towards end-toend edge training.",,,0,not_related
", RigL [11]) that use gradients of the dense model to find the weights to grow back, we only use sparse gradients to identify less important",,,1,related
"To date, various types of compression strategies, such as network pruning [14, 15, 36, 59, 27, 13, 1, 48, 9, 62, 35, 17, 12, 3, 2, 1, 24, 52, 8, 38, 32], bit-precision reduction [15, 54, 42, 49], low-rank approximation [55, 39, 57, 56], knowledge distillation [21, 40] and structured matrix-based construction [44, 28, 6], have been proposed and explored.",,,0,not_related
"Unstructured sparse models are models where weights of dense operations are made to contain many (almost) zeros (Gale et al., 2019; Evci et al., 2020), which do not contribute to the operation’s result and thus, in principle, can be skipped.",,,0,not_related
"Despite the empirical success [19, 63, 55, 11], the theoretical justification of winning tickets remains elusive except for a few recent works.",,,0,not_related
"…and numerous techniques have been proposed, which differ in how weights are identified for removal and the schedule for introducing sparsity/allowing recovery (Cun et al., 1990; Hassibi et al., 1993a; Ström, 1997; Louizos et al., 2017; See et al., 2016; Evci et al., 2019; Narang et al., 2017).",,,0,not_related
introduces a negligible amount of extra FLOPs (over baseline methods) we only show such values in the extended training setting to provide a fair comparison to the setup in [13].,,,1,related
"We orient ourselves primarily on the experimental setup in [13] & [14], both of which present techniques among the strongest in the literature.",,,1,related
"We also provide results using the Erdos-Renyi Kernel [13], a redistribution of layerwise sparsity subject to the same fixed overall budget.",,,1,related
Rigging the Lottery (RigL) [13] instead activates new weights by,,,0,not_related
"…on the weights of neural networks via weight sparsity (Frankle & Carbin, 2019; Gale et al., 2019; Bellec et al., 2017; Mostafa & Wang, 2019; Evci et al., 2019; Dettmers & Zettlemoyer, 2019; Mocanu et al., 2018; Jayakumar et al., 2020), or techniques that dynamically route activations to…",,,0,not_related
"In particular, we find that gradient-based re-allocation (Evci et al., 2019) results in a collapse of the explored network parameters (Figure 11), which we mitigate through the use of random parameter re-allocation.",,,1,related
"We found that the cosine decay of the pruning
ratio introduced in Evci et al. (2019) outperforms constant pruning schedules and leads to a reduction of the changes in network topology during training.",,,1,related
"Dynamic sparsity In DynSparse (Mocanu et al., 2018; Bellec et al., 2017; Liu et al., 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2019; Liu et al., 2021a), the sparse connectivity pattern is evolved during training.",,,0,not_related
"In particular, during the re-allocation step of DynSparse training, we use random re-allocation of pruned weights instead of gradient-based techniques as in RigL (Evci et al., 2019).",,,1,related
"However, so far, the limited performance on language modeling task (Evci et al., 2019) has resulted in DynSparse training not seeing wide adoption for large-scale language modeling tasks despite recent advances (Jayakumar et al.",,,0,not_related
"Current sparsity methods can be distinguished into approaches that impose sparsity on the weights of neural networks via weight sparsity (Frankle & Carbin, 2019; Gale et al., 2019; Bellec et al., 2017; Mostafa & Wang, 2019; Evci et al., 2019; Dettmers & Zettlemoyer, 2019; Mocanu et al., 2018; Jayakumar et al., 2020), or techniques that dynamically route activations to only interact with a subset of the network weights via conditional sparsity (Shazeer et al.",,,0,not_related
"To this end, we adopt and investigate DynSparse training techniques (Dettmers & Zettlemoyer, 2019; Evci et al., 2019) for pre-training of BERT bidirectional language encoder (Devlin et al., 2018) based on the highly scalable Transformer architecture (Vaswani et al., 2017).",,,1,related
"To this end, we adopt and investigate DynSparse training techniques (Dettmers & Zettlemoyer, 2019; Evci et al., 2019) for pre-training of BERT bidirectional language encoder (Devlin et al.",,,1,related
"…is dynamic sparsity (DynSparse), which reduces FLOPs while only requiring training of sparse subsets of the over-parameterized network (Bellec et al., 2017; Mostafa & Wang, 2019; Evci et al., 2019; Dettmers & Zettlemoyer, 2019; Mocanu et al., 2018; Jayakumar et al., 2020; Liu et al., 2021a).",,,0,not_related
"Given that DynSparse training has been primarily developed for vision architectures (Dettmers & Zettlemoyer, 2019; Evci et al., 2019) and did not show competitive performance on the language tasks, we find it necessary to reassess some of the algorithm choices for BERT.",,,1,related
"However, so far, the limited performance on language modeling task (Evci et al., 2019) has resulted in DynSparse training not seeing wide adoption for large-scale language modeling tasks despite recent advances (Jayakumar et al., 2020).",,,0,not_related
"One of the most promising candidates for weight sparse training is dynamic sparsity (DynSparse), which reduces FLOPs while only requiring training of sparse subsets of the over-parameterized network (Bellec et al., 2017; Mostafa & Wang, 2019; Evci et al., 2019; Dettmers & Zettlemoyer, 2019; Mocanu et al., 2018; Jayakumar et al., 2020; Liu et al., 2021a).",,,0,not_related
"However, the algorithm show large differences in the exact re-allocation criteria, which range from random re-allocation (Bellec et al., 2017; Mocanu et al., 2018; Liu et al., 2019; 2021a) to a directed evolution based on momentum (Dettmers & Zettlemoyer, 2019) or gradients (Evci et al., 2019).",,,0,not_related
", 2019; 2021a) to a directed evolution based on momentum (Dettmers & Zettlemoyer, 2019) or gradients (Evci et al., 2019).",,,0,not_related
"This evolution leads to a joint exploration of both network topology and parameters, which has been shown to outperform static sparsity baselines (Bellec et al., 2017; Mostafa & Wang, 2019; Evci et al., 2019; Dettmers & Zettlemoyer, 2019).",,,0,not_related
"Many variants have been proposed for both structured [22, 31, 37, 61] and unstructured [13, 14, 16, 21] pruning.",,,0,not_related
"B Pruned CNN+ReLU [16], [20] Pruned Trasformer+ReLU [54] sparse /sparse DNN.",,,0,not_related
"[26], Dynamic Sparse Reparamterization (DSR) [27] or the Rigged Lottery (RigL) [6].",,,0,not_related
"While initial approaches mostly focused on pruning models after training [15, 22], contemporary algorithms optimize the sparsity structure of a network while training its parameters [8, 30] or even remove connections before any training whatsoever [24, 39].",,,0,not_related
"A more sophisticated approach, Erdös-Renyi-Kernel (ERK), sets the density of a convolutional layer with kernel size w× h, fan-in nin and fan-out nout proportional to (w+ h+ nin + nout)/(w · h · nin · nout) [8, 30].",,,0,not_related
[8] rely on the instantaneous gradient to revive weights but follow SET to maintain the initial layerwise sparsity distribution during training.,,,0,not_related
"Well-engineered LSQ could avoid this and enforce proper redistribution of compression across layers (see [8, 11, 30] for existing baselines).",,,0,not_related
"We additionally compare AC/DC with Top-KAST and RigL, in terms of the validation accuracy achieved depending on the number of training FLOPs.",,,1,related
"RigL can lead to state-of-the-art accuracy results even compared to post-training methods; however, to achieve high accuracy it requires significant additional data passes (e.g. 5x) relative to the dense baseline.",,,0,not_related
"GFLOPs Inference EFLOPs Train
Dense 0 76.84 8.2 3.14
AC/DC 95 73.14± 0.2 0.11× 0.53× RigL1× 95 67.5± 0.1 0.08× 0.08× RigL1× (ERK) 95 69.7± 0.17 0.12× 0.13× Top-KAST 95 fwd, 50 bwd 71.96 0.08× 0.22×
STR 94.8 70.97 0.04× - WoodFisher 95 72.12 0.09× -
AC/DC 98 68.44± 0.09 0.06× 0.46× Top-KAST 98 fwd, 90 bwd 67.06 0.05× 0.08×
STR 97.78 62.84 0.02× - WoodFisher 98 65.55 0.05× -
ResNet50 Results.",,,1,related
"GFLOPs Inference EFLOPs Train
Dense 0 76.84 8.2 3.14
AC/DC 80 76.3± 0.1 0.29× 0.65× RigL1× 80 74.6± 0.06 0.23× 0.23× RigL1×(ERK) 80 75.1± 0.05 0.42× 0.42× Top-KAST 80 fwd, 50 bwd 75.03 0.23× 0.32×
STR 79.55 76.19 0.19× - WoodFisher 80 76.76 0.25× -
AC/DC 90 75.03± 0.1 0.18× 0.58× RigL1× 90 72.0± 0.05 0.13× 0.13× RigL1× (ERK) 90 73.0± 0.04 0.24× 0.25× Top-KAST 90 fwd, 80 bwd 74.76 0.13× 0.16×
STR 90.23 74.31 0.08× - WoodFisher 90 75.21 0.15× -
Table 2: ResNet50/ImageNet, high sparsity results.",,,1,related
The only method which obtains higher accuracy for the same sparsity is the version of RigL [16] which executes for 5x more training epochs than the dense baseline.,,,0,not_related
"For AC/DC and Top-KAST, the first and last layers are kept dense, whereas for RigL, only the first layer is kept dense; however, this has a negligible impact on the number of FLOPs.",,,0,not_related
"The sparsity level is computed with respect to all the parameters, except the biases and Batch Normalization parameters and this is consistent with previous work [16, 52].",,,0,not_related
"For example, the RigL technique [16] randomly ∗Correspondence to Alexandra Peste: alexandra.peste@ist.ac.at
35th Conference on Neural Information Processing Systems (NeurIPS 2021).
removes a large fraction of connections early in training, and then proceeds to optimize over the sparse support, providing savings due to sparse back-propagation.",,,1,related
"Then, our results are quite close to RigL2×, with half the training epochs, and less training FLOPs.",,,1,related
"Moreover, RigL does not prune the first layer and the depth-wise convolutions, whereas for the results reported we do not impose any sparsity restrictions.",,,1,related
"GFLOPs Inference EFLOPs Train
Dense 0 71.78 1.1 0.44
AC/DC 75 70.3± 0.07 0.34× 0.64× RigL1× (ERK) 75 68.39 0.52× 0.53×
STR 75.28 68.35 0.18× - WoodFisher 75.28 70.09 0.28× -
AC/DC 90 66.08± 0.09 0.18× 0.56× RigL1× (ERK) 90 63.58 0.27× 0.29×
STR 89.01 62.1 0.07× - WoodFisher 89 63.87 - -
Semi-structured Sparsity.",,,1,related
"RigL [16] prunes weights at random after a warm-up period, and then periodically performs weight re-introduction using a combination of connectivity- and gradient-based statistics, which require periodically evaluating full gradients.",,,0,not_related
"For example, the RigL technique [16] randomly ∗Correspondence to Alexandra Peste: alexandra.",,,0,not_related
"Additionally, we experiment with extending the number of training iterations for AC/DC at 90% and 95% sparsity two times, similarly to Top-KAST and RigL which also provide experiments for extended training.",,,1,related
"Compared to purely sparse training methods, such as Top-KAST or RigL, AC/DC requires dense training phases.",,,0,not_related
"The comparison between AC/DC, Top-KAST and RigL presented in Figure 3 shows that AC/DC is similar or surpasses Top-KAST 2x at 90% and 95% sparsity, and RigL 5x at 95% sparsity both in terms of training FLOPs and validation accuracy.",,,0,not_related
"At the same time, due to dense training phases, AC/DC has higher FLOP requirements relative to RigL or Top-KAST at the same sparsity.",,,0,not_related
"This finding helps to explain several observations (1) for gradual magnitude pruning (GMP), it is always optimal to end pruning before the second learning rate drop [77, 13]; (2) dynamic sparse training (DST) benefits from a monotonically decreasing pruning rate with cosine or linear update schedule [8, 9]; (3) rewinding techniques [12, 54] outperform fine-tuning as rewinding retrains subnetworks with the original learning rate schedule whereas fine-tuning often retrains with the smallest learning rate.",,,0,not_related
"For this reason, we focus on gradient-based regeneration proposed in Rigged Lottery ( RigL) [9], i.",,,1,related
"Different from the existing works for pruning understanding which mainly focus on dense-to-sparse training [42] (training a dense model and prune it to the target sparsity), we also consider sparse-to-sparse training (training a sparse model yet adaptively re-creating the sparsity pattern) which recently has received an upsurge of interest in machine learning [44, 3, 9, 48, 8, 37, 36].",,,0,not_related
"Dynamic Sparse Training (DST) [44, 3, 48, 8, 9, 36, 35, 25] is another class of methods that prune models during training.",,,0,not_related
"For example, Liu et al. [35] illustrated for the first time the true potential of DST, demonstrating significant training/inference efficiency improvement over the dense training.",,,0,not_related
"Again, we use the gradient as the importance score for regeneration, same as the regrow method as used in RigL [9].",,,1,related
"This setting is also appealing to GMP [77, 13] and DST [44, 9, 48, 37] in which most of the pruned models are continually trained with the current learning rate for some time.",,,0,not_related
The key factor of DST is that it starts from a random initialized sparse network and optimizes the sparse topology as well as the weights simultaneously during training (sparse-to-sparse training).,,,0,not_related
"Compared with the methods [33, 69] that require updating all the weights in the backward pass, our method is much more training efficient, as around 2/3 of the training FLOPs is owing to the backward pass [9, 72].",,,0,not_related
", SET [44], RigL [9], and ITOP [37], in which the sparsity is fixed throughout training, GraNet starts from a denser yet still sparse model and gradually prunes the sparse model to the desired sparsity.",,,0,not_related
"We prune the weights with the smallest magnitude, as it has evolved as the standard method when pruning happens during training, e.g., GMP [77, 13] and DST [44, 9, 37].",,,1,related
"All accuracies are in line with the baselines reported in the references [8, 11, 67, 9, 37].",,,1,related
"For the uniform sparsity, the first convolutional layer with 7× 7 kernels is kept dense, the same as in [16].",,,0,not_related
"This may explain why S-GaP achieves better accuracy than RigL, SET, and DSR (see Table 2).",,,0,not_related
"[16] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.",,,0,not_related
RigL [16] proposes to use magnitude-based pruning and gradient-flow-based growth that update sparse model topology during training.,,,0,not_related
"Please note that models with the non-uniform sparsifying distribution in Table 2 already have the last FC layer pruned, thus the experiment setup is the same as the ones in [16].",,,1,related
"Note that previous work update weights either greedily (e.g., RigL [16]) or randomly (e.g., SET [14] and DSR [15]).",,,0,not_related
"In Table 10, we perform additional experiments to supplement Table 2 by pruning the last FC layer using the S-GaP method and comparing them with [16].",,,1,related
1 n/a n/a n/a RigL [16] non-uniform (ERK) 75.,,,0,not_related
"The results in Table 10 and Table 2 indicate that the accuracy of the ResNet-50 models with sparse and dense FC layers are similar, and both of them outperform the state-of-the-art results in [16].",,,0,not_related
"For non-uniform sparse ResNet-50, the improvement over [16] is 1.",,,0,not_related
We observe that the improvement over [16] is 1.,,,1,related
"Methods based on sparse mask exploration, such as DeepR [13], SET [14], DSR [15], and RigL [16] maintain the target sparsity in all layers throughout the training process and selectively explore a small fraction of the weights periodically.",,,0,not_related
"Pruning algorithms proposed in other works [2, 20, 4] are designed to recover pruned weights by zero-initialization instead of random values, so that the recovered weights do not affect the outputs of the networks.",,,0,not_related
"Literature on network pruning has been historically focused on accuracy [14, 5] with recently work on robustness [7, 22].",,,0,not_related
"Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible.",,,0,not_related
We introduce the regurgitating tickets interpretation (RTI) based on the prior work in Evci et al. (2020b) and test its causal relationship with the success of IMP using repellent and attractive losses.,,,1,related
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin. We find that this is not necessarily the case at very low or high sparsities. The LTH has spawned the idea that lottery tickets could be generated in a mostly data-independent fashion without relying on the very costly IMP. This would open up the possibility of fully sparse training without any prior dense training. For example, Morcos et al. (2019) hint at ""the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets."" Our evidence for the RTI indicates that such efforts might be misguided, as the construction of a lottery ticket requires prior knowledge of a dense solution. This is in line with Frankle et al. (2020b), who empirically examine current attempts at data-independent pruning at initialization (PAI), and show that these attempts significantly underperform compared to IMP.",,,1,related
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin. We find that this is not necessarily the case at very low or high sparsities. The LTH has spawned the idea that lottery tickets could be generated in a mostly data-independent fashion without relying on the very costly IMP. This would open up the possibility of fully sparse training without any prior dense training. For example, Morcos et al. (2019) hint at ""the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets.",,,1,related
Evci et al. (2020b) claim that lottery tickets lie in the same basin as the solution they are pruned from.,,,0,not_related
"Examples include SNIP (Lee et al., 2018), GraSP (Evci et al., 2020b) and SynFlow (Tanaka et al., 2020).",,,0,not_related
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin. We find that this is not necessarily the case at very low or high sparsities. The LTH has spawned the idea that lottery tickets could be generated in a mostly data-independent fashion without relying on the very costly IMP. This would open up the possibility of fully sparse training without any prior dense training. For example, Morcos et al. (2019) hint at ""the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets."" Our evidence for the RTI indicates that such efforts might be misguided, as the construction of a lottery ticket requires prior knowledge of a dense solution. This is in line with Frankle et al. (2020b), who empirically examine current attempts at data-independent pruning at initialization (PAI), and show that these attempts significantly underperform compared to IMP. We hope that an improved understanding of IMP and the LTH, will allow future work to design better pruning and sparse optimization algorithms. In summary, our key contributions are as follows: 1. We demonstrate that the lottery ticket hypothesis can be extended to large networks and show that it is possible to train large neural networks from scratch without rewinding by using a sufficiently large batch size. This reinforces the hypothesis of Frankle et al. (2020a) that finding lottery tickets is possible when training is stable with respect to linear mode connectivity. 2. We introduce the regurgitating tickets interpretation (RTI) based on the prior work in Evci et al. (2020b) and test its causal relationship with the success of IMP using repellent and attractive losses.",,,1,related
Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH.,,,0,not_related
Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin.,,,0,not_related
"Very low and high sparsities are not connected by a linear low error path, in contrast to Evci et al. (2020b), although the angular distances are still considerably smaller than those of reinitializations. 2 Alternative stabilization of the lottery ticket hypothesis Frankle et al. (2020a) link the applicability of IMP with the stability of training.",,,0,not_related
"Given the RTI, there are no clear advantages to using IMP with resetting, instead of ordinary iterative pruning.",,,0,not_related
"To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50.",,,1,related
"In the formulation of the RTI, we informally characterize the successive optima from IMP as similar.",,,1,related
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin.",,,1,related
Evci et al. (2020b) explain that the success of lottery tickets lies in relearning the same solution as the larger net that they were pruned from.,,,0,not_related
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50.",,,1,related
"As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations.",,,0,not_related
"Using the RTI, we can explain the relation of IMP with stability.",,,1,related
"Very low and high sparsities are not connected by a linear low error path, in contrast to Evci et al. (2020b), although the angular distances are still considerably smaller than those of reinitializations.",,,0,not_related
"Based upon Frankle et al. (2020a) and Evci et al. (2020b), we hypothesized and tested the regurgitating tickets interpretation (RTI) as an explanation for the lottery ticket hypothesis.",,,1,related
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin. We find that this is not necessarily the case at very low or high sparsities. The LTH has spawned the idea that lottery tickets could be generated in a mostly data-independent fashion without relying on the very costly IMP. This would open up the possibility of fully sparse training without any prior dense training. For example, Morcos et al. (2019) hint at ""the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets."" Our evidence for the RTI indicates that such efforts might be misguided, as the construction of a lottery ticket requires prior knowledge of a dense solution. This is in line with Frankle et al. (2020b), who empirically examine current attempts at data-independent pruning at initialization (PAI), and show that these attempts significantly underperform compared to IMP. We hope that an improved understanding of IMP and the LTH, will allow future work to design better pruning and sparse optimization algorithms. In summary, our key contributions are as follows: 1. We demonstrate that the lottery ticket hypothesis can be extended to large networks and show that it is possible to train large neural networks from scratch without rewinding by using a sufficiently large batch size. This reinforces the hypothesis of Frankle et al. (2020a) that finding lottery tickets is possible when training is stable with respect to linear mode connectivity.",,,1,related
"However, it is not impossible that other methods to find lottery tickets exist which do not suffer from the RTI, and our discussion of RTI should hence not necessarily be taken as an absolute statement on the LTH but only of the LTH in relation to IMP.",,,1,related
"…training, for example, transferring existing lottery tickets Morcos et al. (2019); Mehta (2019), pruning weights during training (You et al., 2019), or dynamically changing the mask during training (Evci et al., 2020a; Savarese et al., 2020; Dettmers and Zettlemoyer, 2019; Kusupati et al., 2020).",,,0,not_related
"Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations.",,,1,related
"To do so, we see potential to rely on other forms of sparsity, including dynamic weight sparsity (Evci et al., 2019) and conditional activation sparsity, improving the capability to handle multiple languages and data domains within the same architecture (Fedus et al.",,,1,related
"To do so, we see potential to rely on other forms of sparsity, including dynamic weight sparsity (Evci et al., 2019) and conditional activation sparsity, improving the capability to handle multiple languages and data domains within the same architecture (Fedus et al., 2021).",,,1,related
"Second, unlike other DST methods that use the values of non-existing (masked) weights in the evolution process, SET uses only the values of existing sparse connections.",,,0,not_related
"Dynamic sparse training (DST) [3, 13, 25, 39, 40, 45] has recently emerged, aiming to achieve training efficiency and inference efficiency.",,,0,not_related
"Recently, DST has emerged in other domains like text classification [38], feature selection [2], lifelong learning [58], and federated learning [72]. ar X iv :2
10 6.",,,0,not_related
"In most of the DST algorithms that are applied in the supervised setting, the dynamic evolution of the sparse topology is performed each training epoch.",,,0,not_related
This is the traditional metric used in the literature to compare a DST method against its dense counterpart.,,,0,not_related
Our experimental results show that DST brings other favorable advantages to the DRL agents besides memory and computation efficiency.,,,1,related
"In the rest of this section, we will explain the details of our proposed DST method for the TD3 algorithm (DS-TD3).",,,1,related
"DST methods show their success in outperforming dense neural networks with high sparsity levels in supervised classification tasks [9, 45, 49].",,,0,not_related
"DST has emerged and showed its success in many other fields as well [25, 44].",,,0,not_related
"In [9, 13, 14, 29], the gradient information is used to determine which connections would be changed",,,0,not_related
"training (DST) [3, 13, 25, 39, 40, 45] has recently emerged, aiming to achieve training efficiency and inference efficiency.",,,0,not_related
We follow the method described in [13] to calculate the FLOPs.,,,1,related
"For the · update schedule, it contains: (i) the update interval ∆T, which is the number of training iterations between two sparse topology updates; (ii) the end iteration Tend, indicating when to stop updating the sparsity connectivity, and we set Tend to 80% of total training iterations in our experiments; (iii) the initial fraction α of connections that can be pruned or grow, which is 50% in our case; (iv) a decay schedule of the fraction of changeable connections fdecay(t, α,Tend) = α 2 (1+cos( t×π Tend )), where a cosine annealing is used, following [24, 25].",,,1,related
"To meet this challenging demand, we draw inspirations from the latest sparse training works [24, 25] that dynamically extract and train sparse subnetworks instead of training the full models.",,,1,related
"Infrequent gradient calculation [24] is adopted in our case, which computes the gradients in an online manner and only stores the top gradient values.",,,1,related
"Our SViTE method (and its variants S(2)ViTE and SViTE+) is inspired from state-of-the-art sparse training approaches [24, 25] in CNNs.",,,1,related
"Newly added connections are not activated in the last sparse topology, and are initialized to zero since it produces better performance as demonstrated in [24, 25].",,,0,not_related
"As illustrated in [24], such fashion amortizes the",,,0,not_related
"Furthermore, gradient information from the backward pass is utilized to guide the update of the dynamic sparse connectivity [28, 24], which produces substantial performance gains.",,,0,not_related
"Grow criterion: Similar to [24, 25], we active the new units with the highest magnitude gradients, such as ‖ ) ∂A(l,h) ‖`1 and ‖ ∂L(X) ∂W (l,1) j,· ‖`1 for the hth attention head and the jth neuron of the MLP (W ), respectively.",,,1,related
"For a consistent description, we follow the standard notations in [24, 25].",,,1,related
"We start by demonstrating the efficacy of our method on the ImageNet dataset for image classification, where we train a sparse ResNet-50 as in previous works [7, 10].",,,1,related
"Lastly, Rigging the Lottery (RigL) [7] is a recent and highly performant sparse-to-sparse method that matches or surpasses the performance of pruning-based methods.",,,0,not_related
"For image modelling, Top-KAST outperforms existing sparse-to-sparse training approaches, such as Sparse Evolutionary Training (SET) [26] and matches Rigging the Lottery (RigL) [7] on ImageNet across a range of floating-point operations (FLOPs) budgets.",,,0,not_related
"While there is a plethora of works proposing increasingly efficient ways to prune dense networks for sparse inference (dense-to-sparse training) [45, 27, 5], the field has only more recently begun to look at approaches that start training at the desired sparsity (sparse-to-sparse training) [26, 3, 28, 7].",,,0,not_related
"Unstructured IMP (Han et al., 2015; Frankle & Carbin, 2018) serves as an effective method to find these winning tickets, and Dynamic Sparse Training (Mostafa & Wang, 2019; Mocanu et al., 2018; Evci et al., 2020) is also capable of identifying subnetworks with promising performance.",,,0,not_related
", 2015; Frankle & Carbin, 2018) serves as an effective method to find these winning tickets, and Dynamic Sparse Training (Mostafa & Wang, 2019; Mocanu et al., 2018; Evci et al., 2020) is also capable of identifying subnetworks with promising performance.",,,0,not_related
Parameters are pruned based on magnitude and grown back at random [23] or based on gradient [25] or momentum [24] information.,,,0,not_related
"It essentially sparsifies the network at a fine-grained level and is demonstrated to achieve an extremely high compression rate and high accuracy performance [4], [5], [6].",,,0,not_related
"While some recent data-driven, unstructured approaches achieved higher levels of compression on this benchmark Evci et al. (2020), these results show the potential of RED as an efficient, portable and privacy compliant data-free, structured pruning method.",,,0,not_related
"Particularly, errors can reach zero with sufficient training [21] for models that are not constrained by capacity (d ≥ 0.",,,0,not_related
"Some works rewire weights every training iteration [18, 22, 29–31], while others rewire every hundreds of training steps [21] or after an entire pass through the data [19,20].",,,0,not_related
"To solve this trade-off, previous works assign some parts of the model to be language specific: Language specific decoders (Dong et al., 2015), Language specific encoders and decoders (Firat et al., 2016) and Language specific hidden states and embeds (Wang et al., 2018).",,,0,not_related
"…upon the higher-order representation provided by an internal node, beyond following gradient descent to optimize the weights for connections to the node (Rumelhart et al. 1985), a reasonable next step is to use the loss gradient to help select the growth of new connections (Evci et al. 2019).",,,0,not_related
"…cases should be expected to improve the performance of the network; even if redundant information is fed forward in some cases, denser networks are more likely to contain a “winning ticket” than randomly selected sparse networks (see Evci et al. 2019 for empirical results supporting this notion).",,,0,not_related
"Recently, a new rewiring approach has been proposed that selects new connection growth based on the magnitude of the loss gradient for possible connections (Evci et al. 2019).",,,0,not_related
"However, as suggested by Evci et al. (2019), forming potentially redundant connections between adjacent layers can help knock networks out of local optima (or into a “winning lottery ticket”), so restricting such connections entirely is not necessarily a favorable approach, as it would preclude…",,,0,not_related
"Other approaches from literature are grow-and-prune strategies [3, 8, 11, 12, 18] which, starting from sparse networks, successively add and remove neurons or connections while training the networks.",,,0,not_related
"Following Evci et al. (2019); Gale et al. (2019), we conduct experiments to compare this strong baseline with `1-norm filters pruning while employing CLR on CIFAR-10, CIFAR-100 and ImageNet.",,,1,related
"In addition, it would be interesting to study more advanced pruning algorithms such as [41, 42], especially iterative or inherently sparse ones that could explore structures corresponding to prohibitively large networks.",,,0,not_related
"Also, lottery ticket related researches [9], [11], [14] mostly focus on finding a single winning ticket for a single task, which cannot be directly translated to the multitask problem.",,,0,not_related
"In another case [31], the training schedule was extended for some networks by 5× in order to reach the same accuracy as the dense model.",,,0,not_related
"Sparse training is a line of work where traditional architectures are trained with sparse instead of dense layers and the number of parameters allowed during training is restricted to a percentage of the dense layers (Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Mostafa & Wang, 2019).",,,0,not_related
We calculate theoretical FLOP requirements in a manner similar to Evci et al. [2020] (exact details in the supplementary material).,,,1,related
"Evaluated on image classification, the central claims of Evci et al. [2020] hold true—RigL outperforms existing sparse-to-sparse training methods and can also surpass other dense-to-sparse training methods with extended training.",,,0,not_related
"Similar to Evci et al. [2020], we assume that algorithms utilize sparsity during training.",,,1,related
"RigL [Evci et al., 2020a] also uses magnitudes for pruning, yet they employ the absolute gradients for weight growing.",,,0,not_related
"Some researchers (such as [Evci et al., 2020a]) conjecture that dynamic and adaptive masks during training may be better, thus introduce another group of methods featured by dynamic masks.",,,0,not_related
"[Evci et al., 2020b] (GradFlow) present a gradient flow perspective to explain why LTH happens.",,,0,not_related
"[10] show that if they trained with RigL sparse CNNs for a long enough time, they can reach the performance of the dense counterparts.",,,0,not_related
"To enhance a faster convergence, [7] and [10] introduced the idea of using momentum and gradient information (quantified in two methods named, Sparse Momentum and RigL, respectively) from non-existing connections during the regrow steps.",,,0,not_related
"Typical methods include, but are not limited to, network pruning (Lin et al., 2020c; Evci et al., 2020; Lin et al., 2020a), tensor decomposition (Jaderberg et al.",,,0,not_related
"Additionally, we include comparisons to recent works on weight rewinding and dynamic sparsity, in particular SNIP (Lee et al., 2018), DSR (Mostafa and Wang, 2019), SNFS (Dettmers and Zettlemoyer, 2019), and RiGL (Evci et al., 2020).",,,1,related
"Evci et al. (2020) have shown promising results on NNs pruned at initialization where the pruning ratio across layers is adjusted by Erdős-Rényi kernel method, as introduced by Mocanu et al. (2018).",,,0,not_related
", 2018), DSR (Mostafa and Wang, 2019), SNFS (Dettmers and Zettlemoyer, 2019), and RiGL (Evci et al., 2020).",,,0,not_related
"[14] that requires expanding the model size), several researchers [4, 29, 30, 7, 9] tried to enable dynamic mask changes during the training process.",,,0,not_related
"Recently, a new line of research that aims to train sparse models from scratch [14, 7, 9] has emerged.",,,0,not_related
"[9] work, as it aims to solve a different issue within the same problem.",,,0,not_related
"[9] that aims to reduce memory footprint for sparse training from scratch, Zhou et al.",,,0,not_related
"While a different line of research suggested more extreme setting which restrict the memory footprint to the compressed model size [4, 29, 30, 7, 9], we argue that our method is orthogonal to it and both methods could be easily be combined.",,,0,not_related
"The results in Table 3 and Table 4 show that the proposed method outperform the baseline and previous methods [32, 16, 5].",,,1,related
"The current state-ofthe-art for DST is RigL (Evci et al., 2020), which maintains a fixed layer-wise sparsity distribution, prunes parameters with the smallest magnitude, and re-activates weights with the largest-magnitude gradients.",,,1,related
"In contrast, while RigL is also shown to be influenced by the layer-wise sparsity distribution (Evci et al., 2020), the premise of DST is precisely that the locations of the trainable parameters within each layer matter fundamentally.",,,0,not_related
"…A.12 we compare different heuristics for distributing trainable parameters between network layers – in particular, uniform density per layer (uniform), equal number of parameters per layer (EPL), equal number of parameters per filter (EPF) and the ERK distribution used in (Evci et al., 2020).",,,1,related
"In RigL (Evci et al., 2019a), the authors consider the case of SADt−1:t = 2k, where k is dynamically calculated for each layer during training.",,,1,related
"…other is a one-stage scheme, which adopts the dynamic method to alternatively optimize parameters and prunes network architectures based on different criteria (Bellec et al., 2017; Mocanu et al., 2018; Mostafa & Wang, 2019; Evci et al., 2019b; Kusupati et al., 2020; Dettmers & Zettlemoyer, 2019).",,,0,not_related
"One is a two-stage scheme, which discovers a sparse neural architecture by pruning a well-trained dense network and then uses the same or even greater computational resources to retrain the sparse models (Nvidia, 2020; Evci et al., 2019b; Han et al., 2015; Frankle & Carbin, 2018).",,,0,not_related
"It is difficult to find the optimal sparse architecture (connections) and optimal parameters (Evci et al., 2019b) simultaneously during training sparse CNNs and Transformers although SET-MLP could easily outperform dense MLP (Bourgin et al., 2019).",,,0,not_related
"However, compared with training dense neural networks from scratch, to achieve the same performance, RigL needs 5× more training time.",,,0,not_related
RigL can achieve state-of-the-art results on training unsturctured sparse networks from scratch.,,,0,not_related
"RigL (Evci et al., 2019a) uses the magnitudebased method to prune and the periodic dense gradients to regrow connection.",,,0,not_related
"Before the advent of N :M fine-grained structured sparsity, there exist many state-of-the-art methods to generate sparsity models, including DSR (Mostafa & Wang, 2019), RigL (Evci et al., 2019a), GMP (Gale et al., 2019), and STR (Kusupati et al., 2020).",,,0,not_related
"Evci et al. (2020b) found that sparse neural networks that are initialized by a dense initialization e.g., He et al. (2015), suffer from a poor gradient flow, whereas DST can improve the gradient flow during training significantly.",,,0,not_related
"…redistribution (Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Liu et al., 2021), gradient-based weight growth (Dettmers & Zettlemoyer, 2019; Evci et al., 2020a), and extra weights update in the backward pass (Raihan & Aamodt, 2020; Jayakumar et al., 2020) to improve the sparse training…",,,0,not_related
"…expressibility of sparse training, especially for extreme sparsities, (2) in reducing training and inference costs (3) in understanding the underlying mechanism of dynamic sparse training (DST) (Mocanu et al., 2018; Evci et al., 2020a), (4) in preventing overfitting and improving generalization.",,,0,not_related
"It also helps to avoid the dense over-parameterization bias introduced by the gradient-based methods e.g., The Rigged Lottery (RigL) (Evci et al., 2020a) and Sparse Networks from Scratch (SNFS) (Dettmers & Zettlemoyer, 2019), as the latter utilize dense gradients in the backward pass to explore new…",,,0,not_related
"B. Implementation Details of RigL-ITOP in Section 4.2
In this Appendix, we describe our replication of RigL (Evci et al., 2020a) and the hyperparameters we used for RigLITOP.",,,1,related
"We train sparse ResNet-50 for 100 epochs, the same as Dettmers & Zettlemoyer (2019); Evci et al. (2020a).",,,1,related
"More importantly, our method requires only 2× training time to match the performance of dense ResNet-50 at 80% sparsity, far less than RigL (5× training time) (Evci et al., 2020a).",,,1,related
"Sparse Network Optimization to study Network Dynamics Apart from being used as pruning criteria, optimization information has been used to investigate aspects of sparse networks, such as their loss landscape (Evci et al., 2019), how they are impacted by SGD noise (Frankle et al.",,,0,not_related
"…from being used as pruning criteria, optimization information has been used to investigate aspects of sparse networks, such as their loss landscape (Evci et al., 2019), how they are impacted by SGD noise (Frankle et al., 2019a), the effect of different activation functions (Dubowski, 2020) and…",,,0,not_related
"Pruning during Training Another branch of pruning is Dynamic Sparse Training, which uses information gathered during the training process, to dynamically update the sparsity pattern of these sparse networks (Mostafa & Wang, 2019; Bellec et al., 2017; Mocanu et al., 2018; Dettmers & Zettlemoyer, 2019; Evci et al., 2019).",,,0,not_related
"…Another branch of pruning is Dynamic Sparse Training, which uses information gathered during the training process, to dynamically update the sparsity pattern of these sparse networks (Mostafa & Wang, 2019; Bellec et al., 2017; Mocanu et al., 2018; Dettmers & Zettlemoyer, 2019; Evci et al., 2019).",,,0,not_related
"However, while this work has had some success, focusing on initialization alone has proven to be inadequate (Frankle et al., 2020; Evci et al., 2019).",,,0,not_related
"More recently, RigL [18] introduced gradient-based regrowing to get rid of the extra computation and storage caused by SNFS.",,,0,not_related
"This observation contradicts the common belief that gradient-based weight regrowth achieves better performance than random-based regrowth [15, 18].",,,0,not_related
"More recently, RigL [18] introduced gradient-based regrowing to get rid of the extra computa-",,,0,not_related
"We compare ST-RNNs with strong state-of-the-art DST baselines including SET, SNFS, and RigL and a dense-to-sparse method, ISS.",,,1,related
"Only very recently, dynamic sparse training (DST) was begun to be studied to enable training sparse neural networks from scratch, with a few approaches including Sparse Evolutionary Training (SET) [47], Dynamic Sparse Reparameterization (DSR) [49], Sparse Networks from Scratch (SNFS) [15], Rigged Lottery (RigL) [18].",,,0,not_related
"We can see that, with 33% parameters, all gradient-based methods (SNFS and RigL) fail to match the performance of the dense-to-sparse method (ISS), whereas random-based methods (SET, ST-LSTM) can all outperform ISS and the dense model.",,,1,related
"Different from other neural networks, RNNs are relatively more challenging to be compressed [18, 57].",,,0,not_related
"This is the main difference between ST-RNNs with gradient-based sparse training techniques such as RigL and SNFS. Gradient-based regrowing heavily depends on the gradient of every parameter and they still require a dense forward pass at least once per D T iterations, whereas our method keeps a clearly sparse backward pass and requires smaller FLOPs.",,,0,not_related
"However, previous approaches are mainly for CNNs and MLPs.",,,0,not_related
"Note that its variant Erdős-Rényi-Kernel proposed by Evci et al. (2020) scales back to ER for RNNs, as no kernels are involved.",,,1,related
"For instance, while “The Rigged Lottery” (RigL) achieves state-of-the-art sparse training results with various CNNs, it fails to match the performance of the iterative pruning method (Gale et al., 2019) in the RNN setting (Evci et al., 2020).",,,0,not_related
"iterative pruning method in the RNN setting (Evci et al., 2020).",,,1,related
"Methods that leverage gradient-based weight growth (SNFS and RigL) have shown superiority on performance over the methods using random-based weight growth for CNNs (Evci et al., 2020).",,,0,not_related
"It has been shown by Evci et al. (2020) that while state-ofthe-art sparse training method (RigL) achieves promising performance with various CNN models, it fails to match the performance of pruning in RNNs.",,,0,not_related
"Recently, some dynamic sparse training (DST) approaches (Mocanu et al., 2018; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020) have been proposed to bring efficiency to the training phase as well.",,,0,not_related
"However, all these works mainly focus on CNNs and MLPs, and they are not designed to match state-of-the-art performance for RNNs.",,,0,not_related
It has been shown that the choice of sparse initialization (sparsity distribution) is important for sparse training in Frankle & Carbin (2019); Kusupati et al. (2020); Evci et al. (2020).,,,0,not_related
"Later, some works focus on designing sparse CNNs based on Expander graphs and show comparable performance against the corresponding dense models (Prabhu et al., 2018; Kepner & Robinett, 2019).",,,0,not_related
"The long-term dependencies and repetitive usage of recurrent cells make RNNs more difficult to be sparsified (Kalchbrenner et al., 2018; Evci et al., 2020).",,,0,not_related
"In line with the previous studies (Mocanu et al., 2018; Mostafa & Wang, 2019; Evci et al., 2020), both static sparse networks and small-dense networks fall short of Selfish-RNN.",,,0,not_related
We follow the way of calculating training FLOPs proposed by Evci et al. (2020).,,,1,related
"In line with the previous studies (Mocanu et al., 2018; Mostafa & Wang, 2019; Evci et al., 2020), both static sparse networks and small dense networks fall short of Selfish-RNN.",,,0,not_related
"• Our analysis shows two surprising phenomena in the setting of RNNs contrary to CNNs (1) random-based weight growth performs better than gradient-based weight growth, and (2) uniform sparse distribution performs better than Erdős-Rényi (ER) sparse distribution.",,,0,not_related
"However, previous sparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs) and Convolutional Neural Networks (CNNs), failing to match the performance of dense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting.",,,0,not_related
"RigL (Evci et al., 2020) went one step further by activating new weights with the highest magnitude gradient.",,,0,not_related
"This is because current sparse training algorithms typically use a fixed sparse network architecture or a fixed sparsity pattern for a number of iterations [9, 10].",,,0,not_related
"L G
] 2
F eb
2 02
1
Mostafa & Wang, 2019b; Evci et al., 2019; Anonymous, 2021a; Jayakumar et al., 2020).",,,0,not_related
"Very recently, based on the Lottery Ticket Hypothesis (Frankle & Carbin (2019)), RigL (Evci et al. (2019); Jayakumar et al. (2020)) was introduced as a novel method for training sparse models without the need of a ""lucky initialisation""; it can match and sometimes exceed the performance of pruning…",,,0,not_related
"Very recently, based on the Lottery Ticket Hypothesis (Frankle & Carbin (2019)), RigL (Evci et al. (2019); Jayakumar et al. (2020)) was introduced as a novel method for training sparse models without the need of a ""lucky initialisation""; it can match and sometimes exceed the performance of pruning based approaches.",,,0,not_related
"Currently, the sparse training concept has started to be a de facto approach for e icient training of ANNs, as demonstrated in (Bellec et al., 2018; Dettmers & Zettlemoyer, 2019; Mostafa & Wang, 2019b; Evci et al., 2019; Anonymous, 2021a; Jayakumar et al., 2020).",,,0,not_related
"Sparsification can be considered a growing trend in the training of neural networks [16, 17].",,,0,not_related
Evci et al. (2020) derived an algorithm for training sparse neural networks according to LTH and applied it to character-level language modeling on WikiText-103.,,,0,not_related
"To address this research gap, we turn our attention to lottery ticket hypothesis (LTH) [20, 27, 31, 50, 76, 81], a fast-rising field that investigates the sparse trainable subnetworks within full dense networks.",,,1,related
"Previous works have primarily focused on efficient inference [35, 20, 24] and some on training costs [9].",,,0,not_related
"A couple of methods have been proposed for training deep neural networks from scratch using sparse connections and sparse training [14, 41, 7, 42, 17, 55].",,,0,not_related
RigL [3] maintains sparsity throughout training and does not require dense gradients during most iterations of training.,,,0,not_related
"In addition to IMP we analyze two dynamic sparsity methods: Discovering Neural Wirings (DNW) [19] and Rigged Lottery Tickets (RigL) [3], algorithms where the connectivity changes throughout training.",,,1,related
"In comparison to newer dynamic training algorithms including RigL and Top-KAST [3, 11], our modified DNW algorithm is likely still less computationally efficient for most applications.",,,1,related
"As in [2, 3], DNW [19] maintains sparsity throughout training.",,,0,not_related
"We also consider Lottery Ticket (LT) [5] graphs, DNW [19] graphs, and RigL graphs [3] which are respectively produced by the following three algorithms:",,,0,not_related
"Though earlier work on sparse networks focused primarily on pruning after training, researchers have recently shown interest in pruning early in training [5, 14, 17, 18] or dynamically as training progresses [2, 19, 3].",,,0,not_related
"[42] and then further explored within the literature [11, 13, 44], sparse training consists in training the network with a constant level of sparsity, at first spread randomly with uniform probability and then adjusted during steps which combine 1) pruning of a certain portion of the weights, according to a certain criterion, and 2) regrowing an equivalent amount of weights, depending on another criterion.",,,0,not_related
"el to each layer in the network by pruning the weights with smallest magnitude in each layer [5]. The Erdosh-Renyi Kernel (ERK) baseline corresponds to a budgeted layer-wise pruning suggested in RigL [13]. Each layer’s number of pruned weights is proportional to p= c o +c i +k h +k w c o c i k h k w ; (17) where c is the number of output dimensions, c i is the number of input dimensions, and k h;k w a",,,0,not_related
" levels of sparsity as training progresses. Several other works present magnitude-based pruning methods [18,47,19,16, 3,39]. Alternatives to magnitude-based pruning involve using gradient information [13,30,21,10,8], covariance in1 arXiv:2011.09058v1 [cs.CV] 18 Nov 2020 Layerwise Generator Layerwise Optimizer AFCLE Teacher Layer 1 Layer i É Layer 2 É AFCLE Student Layer 1 Layer i É Layer 2 É Batch-norm Stats Inp",,,0,not_related
"cient CNNs for on-device execution. Examples include smart-home security, factory automation, and mobile applications. One common technique for improving CNN computational efﬁciency is weight pruning [29,47,18,13]. The removal of network weights allows the network to occupy a smaller memory footprint and achieve a faster execution time. Another common technique for improving a CNN’s runtime efﬁciency is quanti",,,0,not_related
"ittle or no loss of accuracy [27,38,26]. Most methods for CNN compression require retraining on the original training set to achieve a high compression rate. For example, compressing through sparsity [29,47,18,13] requires training on the original data. Applying post-training quantization usually results in poor network accuracy [26] unless special care is taken in adjusting network weights [38]. Handling low-",,,0,not_related
"[ !&quot;&amp;#&apos;  %% !&amp;#$ ! &amp; !# # 5]): a global magnitude threshold is used to prune weights. (Uniform [5]): a uniform layer-wise sparsity budget is used. (ERK [13]): a layer-wise sparsity budget weighted to prune more weights from larger convolutional layers, as described in Equation17.    !# Soft Threshold Reparameterization with data, as reported in [ ",,,1,related
"There is a large body of work on the topic of sparse neural networks [17, 18, 19, 20, 21, 22, 23], and many studies derive sophisticated approaches to optimize the sparsity pattern [24, 25, 26, 27].",,,0,not_related
"Randomly pruning network weights typically impairs overall network performance unless special care is taken, such as intelligent sparse initialization schemes [52], [53], [55], [89] or dynamic rewiring during the training [51], [54].",,,0,not_related
"This achieves the same objective as methods like Rigged Lottery (Evci et al., 2020), Sparse Networks From Scratch (Dettmers & Zettlemoyer, 2019), which periodically prune and grow weights based on gradient momentum.",,,0,not_related
"This is similar to many recent pruning during training methods that employ momentum as the importance metrics to rank weights (Ding et al., 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020).",,,0,not_related
"(3) Momentum Based Pruning (MoP) (Ding et al., 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020): Notice that as we have non-stationary data and thus it is also reasonable to measure the weight importance by its momentum (calculated with the exponential moving average of gradients).",,,1,related
"Besides, various methods (Dettmers & Zettlemoyer, 2019; Ding et al., 2019; Evci et al., 2020) are proposed to keep a sparse model through the training process.",,,0,not_related
"An extension of Erdős-Rényi method (originally given by Mocanu et al. (2018)) accounting for convolutional layers, as proposed by Evci et al. (2020).",,,1,related
"Recent discoveries (Gale et al., 2019; Evci et al., 2020) demonstrate that, given an appropriate choice of layerwise sparsity, simply pruning on the basis of weight magnitude yields a surprisingly powerful unstructured pruning scheme.",,,0,not_related
"(2018)) accounting for convolutional layers, as proposed by Evci et al. (2020). The numbers of nonzero parameters of sparse convolutional layers are scaled proportional to 1− n l−1+nl+wl+hl nl−1·nl·wl·hl , where n denotes the number of neurons at layer l, and w, h denotes the width and height of the lth layer convolutional kernel.",,,0,not_related
"A recent work by Evci et al. (2020) proposes a magnitude-based dynamic sparse training method, adopting layerwise sparsity inspired from the network science approach toward neural network pruning (Mocanu et al., 2018).",,,0,not_related
"…layer); for larger nets, authors use global MP. Morcos et al. (2019) consider transferring the “winning ticket” initializations, using the global MP. Evci et al. (2020) proposes a training scheme for sparsely initialized neural networks, where the layerwise sparsity is given by the Erdős-Rényi…",,,0,not_related
"An extension of Erdős-Rényi method (originally given by Mocanu et al. (2018)) accounting for convolutional layers, as proposed by Evci et al. (2020).",,,1,related
"Recent discoveries (Gale et al., 2019; Evci et al., 2020) demonstrate that, given an appropriate choice of layerwise sparsity, simply pruning on the basis of weight magnitude yields a surprisingly powerful unstructured pruning scheme.",,,0,not_related
"(2018)) accounting for convolutional layers, as proposed by Evci et al. (2020). The numbers of nonzero parameters of sparse convolutional layers are scaled proportional to 1− n l−1+nl+wl+hl nl−1·nl·wl·hl , where n l denotes the number of neurons at layer l, and w, h denotes the width and height of the lth layer convolutional kernel.",,,0,not_related
"A recent work by Evci et al. (2020) proposes a magnitude-based dynamic sparse training method, adopting layerwise sparsity inspired from the network science approach toward neural network pruning (Mocanu et al., 2018).",,,0,not_related
"Evci et al. (2020) proposes a training scheme for sparsely initialized neural networks, where the layerwise sparsity is given by the Erdős-Rényi kernel method; the method generalizes the scheme initially proposed by Mocanu et al. (2018) to convolutional neural networks.",,,0,not_related
"Based on the works of their fellow researchers, a group of researchers from Google and DeepMind has come up with a Rigged Lottery (RigL) [6] method for training SNNs with fixed complexity without accuracy loss.",,,0,not_related
"Dettmers and Zettlemoyer (2019) proposed using momentum values, whereas Evci et al. (2020) used gradient estimates directly to guide the selection of new connections, reporting results that are on par with pruning algorithms, and has been applied to vision transformers (Chen et al. 2021), language…",,,0,not_related
"However, we don’t know how to find Lottery Tickets (LTs) efficiently; while RigL (Evci et al. 2020), a recent DST method, requires 5× the training steps to match dense NN generalization.",,,1,related
"One way to avoid this challenge altogether is to dynamically change the mask to exploit signals from later in training (Dettmers & Zettlemoyer, 2019; Evci et al., 2020).",,,0,not_related
"Current sparse training algorithms either use a fixed sparse neural network architecture, or fixes a particular architecture for a number of iterations [12, 13].",,,0,not_related
"Relatedly, motivated by compression, many recent works [11, 18, 8, 6] study sparse neural networks; studying their effectiveness on learning architectural bias from data would be an interesting direction.",,,0,not_related
"In parallel to this race for ever-larger models, an emerging subfield has explored the prospect of training smaller subnetworks in place of the full models without sacrificing performance [11–16].",,,0,not_related
"In recent years, sparse training proved its success in achieving the same performance with dense neural networks for single task standard supervised/unsupervised learning, while having much faster training speed and much lower memory requirements [30, 2, 4, 5, 14, 32].",,,0,not_related
"The number of specific feature maps in each hidden layer spect l is as follows: [2, 2, 5, 6, 30].",,,0,not_related
"Works from [5, 32] also show that the sparse training achieves better performance than iteratively pruning a pre-trained dense model and static sparse neural networks.",,,0,not_related
"Previous works have primarily focused on efficient inference [23, 28, 29, 35, 44] and some on training costs [12].",,,0,not_related
"Pragmatic systems must measure this compute over the duration of their lifetime; not just measuring inference cost [34, 20, 24] but also update cost [10].",,,0,not_related
"In [5], it is shown that such metaheuristics approaches always lead to very-well performing sparse topologies, even if they are based on a random process, without the need of a pre-trained model and a lucky initialization as done in [6].",,,0,not_related
"Recently, many works have emerged to achieve both, training efficiency and inference efficiency, based on adaptive sparse connectivity [26,28,20,3,5].",,,0,not_related
"– In addition, with the help of our proposed distance metric, we confirm and complement the findings from [5] by being able to quantify how different are the sparse and, at the same time, similarly performing topologies obtained with adaptive sparse connectivity.",,,1,related
"Very recently, instead of using the momentum, The Rigged Lottery [5] grows the zero-weights with the highest magnitude gradients to eliminate the extra floating point operations required by Sparse Momentum.",,,0,not_related
"One effective way to optimize the sparse topology is adaptive sparse connectivity, a technique based on connection pruning followed by connection regrowing, which has shown good performance in the previous works [26,28,3,5].",,,0,not_related
"Second, by use of dynamic network rewiring rules over training time that keeps network sparsity below a given threshold (Mocanu et al., 2018; Bellec et al., 2018; Yan et al., 2019; Evci et al., 2020).",,,1,related
"[12] and Zhu and Gupta [31] prevents the use of the optimization in Equation 4, which is strictly necessary to fit the RTRL training computations on accelerators without running out of memory.",,,0,not_related
The last few years have also seen a resurgence of interest in sparse neural networks – both their properties [13] and new methods for training them [12].,,,0,not_related
"Recently, many methods to induce sparsity in neural networks have shown that it is possible to train models with an overwhelming fraction of the weights being 0 [25, 10, 9, 23, 8, 41].",,,0,not_related
"While there are a variety of approaches to compressing neural networks, such as novel design of micro-architectures [15, 16, 17], dimensionality reduction of network parameters [18, 19], and training of dynamic sparse networks [20, 21, 22], in this work we will focus on neural network pruning.",,,0,not_related
"Modern pruning techniques are quite successful at finding these sparse solutions and prune over 95% of the weights in a network, whilst leaving raw performance intact [24, 43, 18, 15, 75].",,,0,not_related
"This theory sparked a body of research examining the behaviour and obtainment of these winning tickets [18, 76, 54, 13, 15, 79, 17, 50], as well as some criticism [47, 19].",,,0,not_related
"Some methods fight this by pruning during training [48, 18, 15, 75], which reduces the problem somewhat.",,,0,not_related
"One way, is to initialise with a sparse distribution and then train with the periodical interchanging of which parameters are considered pruned and which are ‘grown back ’ [4, 14, 15].",,,1,related
"We compare (gRDA) with the Erdős-Rényi-Kernel of [15], variational dropout [42] and a reinforcement-learning based AutoML method [32].",,,1,related
"For example, unstructured pruning (Hooker et al., 2019; Gale et al., 2019; Evci et al., 2019) and weight specific quantization (Zhen et al.",,,0,not_related
"[23, 24, 27, 45], which prune during regular training and can additionally allow the re-introduction of weights during training; (4) variational or regularization-based methods, e.",,,0,not_related
"We also find that global magnitude (GM) is quite effective, surpassing many recent dynamic pruning methods, which also adjust the sparsity distribution across layers [24, 27, 45].",,,0,not_related
", 2018) and convolutional models for computer vision (Zhu & Gupta, 2018; Elsen et al., 2019; Evci et al., 2020; Kusupati et al., 2020).",,,0,not_related
"…findings hold for large but sparse audio synthesis models (Kalchbrenner et al., 2018) and convolutional models for computer vision (Zhu & Gupta, 2018; Elsen et al., 2019; Evci et al., 2020; Kusupati et al., 2020).
original weights and the sparse weights—for the 60% sparse ROBERTA models.",,,0,not_related
"Following this pivotal work, several studies have been carried to understand the role of initialization, the e ect of the pruning criterion used and the importance of retraining the sub-networks [3, 4, 5, 6, 7, 8] for the success of lottery tickets.",,,0,not_related
", 2019), SNFS (Dettmers & Zettlemoyer, 2019), RigL (Evci et al., 2020) and DPF (Lin et al.",,,0,not_related
"Sparse Networks From Scratch (SNFS) (Dettmers & Zettlemoyer, 2019) utilizes momentum of the weights to re-allocate weights across layers and the Rigged Lottery (RigL) (Evci et al., 2019) uses the magnitude to drop and the periodic dense gradients to regrow weights.",,,0,not_related
"The “+ ERK” suffix implies the usage of ERK budget (Evci et al., 2020) instead of the original sparsity budget.",,,1,related
"5× (Gale et al., 2019) and RigL5× (Evci et al., 2019) show that training the networks longer increases accuracy.",,,0,not_related
"Unstructured sparsity has been extensively studied and includes methods which use gradient, momentum, and Hessian based heuristics (Evci et al., 2020; Lee et al., 2019; LeCun et al., 1990; Hassibi & Stork, 1993; Dettmers & Zettlemoyer, 2019), and magnitude-based pruning (Han et al.",,,0,not_related
", 2019) and RigL5× (Evci et al., 2020) show that training the networks longer increases accuracy.",,,0,not_related
"Figure 6 also shows that the last layers through STR are denser than that of the other methods which is contrary to the understanding in the literature of non-uniform sparsity (Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Gale et al., 2019).",,,0,not_related
"It can be a fixed budget like the ERK (Erdos-Renyi-Kernel) heuristic described in RigL (Evci et al., 2020).",,,0,not_related
"RigL, SNFS, DSR, and DPF were compared in their original form.",,,0,not_related
SNFS and RigL are state-of-the-art in sparse-to-sparse training but fall short of GMP for the same experimental settings.,,,0,not_related
"Table 1 summarizes that the non-uniform sparsity baselines like SNFS, SNFS+ERK and RigL+ERK can have up to 2-4× higher inference cost (FLOPs) due to non-optimal layer-wise distribution of the parameter weights.",,,0,not_related
"Sparse Networks From Scratch (SNFS) (Dettmers & Zettlemoyer, 2019) utilizes momentum of the weights to re-allocate weights across layers and the Rigged Lottery (RigL) (Evci et al., 2020) uses the magnitude to drop and the periodic dense gradients to regrow weights.",,,0,not_related
"It can be a fixed budget like the ERK (Erdos-Renyi-Kernel) heuristic described in RigL (Evci et al., 2019).",,,0,not_related
"STR was compared against strong state-of-the-art baselines in various sparsity regimes including GMP (Gale et al., 2019), DSR (Mostafa & Wang, 2019), DNW (Wortsman et al., 2019), SNFS (Dettmers & Zettlemoyer, 2019), RigL (Evci et al., 2019) and DPF (Lin et al., 2020).",,,0,not_related
"This is referred to as sparse-to-sparse training and a lot of recent work (Mostafa & Wang, 2019; Bellec et al., 2018; Evci et al., 2020; Lee et al., 2019; Dettmers & Zettlemoyer, 2019) aims to do sparse-to-sparse training using techniques which include re-allocation of weights to improve accuracy.",,,0,not_related
"This work is among several recent papers to propose that merely sparsifying at initialization can produce high performance neural networks (Mallya et al., 2018; Zhou et al., 2019; Ramanujan et al., 2020; Evci et al., 2020).",,,0,not_related
"Despite the widespread use of compression techniques, articulating the trade-offs of compression has overwhelming centered on change to overall accuracy for a given level of compression (Ström, 1997; Cun et al., 1990; Evci et al., 2019; Narang et al., 2017; Gale et al., 2019).",,,0,not_related
"llowing [14]). We run CS with s 0 2f0:0; 0:01; 0:02; 0:03; 0:05gyielding 5 tickets with varying sparsity levels. Table2summarizes the results achieved by CS, IMP, and state-of-the-art pruning methods [25, 26, 27, 28, 29], where reported sparsity levels are computed for the set of parameters that each method prunes. IMPydenotes IMP run for 12 rounds, i.e. using a larger training budget. Differences in each method’s me",,,1,related
The Rigged Lottery (RigL) [10] addressed the high computational cost by using infrequent gradient information.,,,0,not_related
"…dynamic sparse
1 https://github.com/zahraatashgahi/CTRE.
training methods use magnitude as a pruning criterion, weight regrowing approaches are of different types, including random (Mocanu et al., 2018; Mostafa & Wang, 2019) and gradient-based regrowth (Evci et al., 2020; Jayakumar et al., 2020).",,,1,related
"While most dynamic sparse training methods use magnitude as a pruning criterion, weight regrowing approaches are of different types, including random [52, 57] and gradient-based regrowth [10, 28].",,,0,not_related
● RigL Evci et al. (2020).,,,0,not_related
"The Rigged Lottery (RigL) (Evci et al., 2020) addressed the high computational cost by using infrequent gradient information.",,,0,not_related
"Moreover, even if it would be possible to run FC-MLP, this comparison is outside the scope of this paper and it would be redundant as it has been shown in [6, 24, 44, 52, 53, 73] that SET-MLP typically outperforms its fully connected counterparts.",,,0,not_related
"recently, by modifying the sparsity distribution of Erd}os– Rényi introduced in [52], RigL [24] can match and sometimes exceed the performance of pruning-based approaches.",,,0,not_related
", sparse evolutionary training (SET) [52], DEEP-R [4], dynamic sparse reparameterization (DSR) [55], sparse momentum [20], ST-RNNs [43], and rigged lottery (RigL) [24].",,,0,not_related
"We demonstrate that HYDRA (Sehwag et al., 2020) and Robust-ADMM (Ye et al., 2019) yield better results when used with non-uniform strategies determined by ERK (Evci et al., 2020) and LAMP (Lee et al., 2021) (cf. Section 4.3).",,,1,related
"Also in conventional network pruning, non-uniform compression strategies have been proven effective, for instance, ERK by Evci et al. (2020) and LAMP by Lee et al. (2021).",,,0,not_related
"Concatenated architectures such as VGG16 possess relatively high sparsity in the middle layers (Lee et al., 2021; Evci et al., 2020).",,,0,not_related
", 2019) yield better results when used with non-uniform strategies determined by ERK (Evci et al., 2020) and LAMP (Lee et al.",,,0,not_related
"For link prediction in Ogbl-Collab, we adopt 28-layer ResGCNs.",,,1,related
"To test the scalability of DGLT, we further use a large-scale dataset called Ogbl-Collab Hu et al. (2020) for link prediction. Finally, we examine our algorithm for graph classification on D&D Dobson & Doig (2003) and ENZYMES Borgwardt et al.",,,1,related
"For Ogbl-Collab, in order to simulate a real collaborative recommendation application, we take the cooperation before 2017 as the training edge, the cooperation in 2018 as the validation edge and the cooperation in 2019 as the testing edge.",,,1,related
LTH is initially observed in dense networks and is broadly found in many fields Evci et al. (2020); Frankle et al. (2020); Malach et al. (2020); Ding et al. (2021); Chen et al. (2020a; 2021); Sui et al. (2021).,,,0,not_related
"To answer RQ2, we conduct experiments on the Ogbl-Collab dataset using ResGCNs as the backbone.",,,1,related
"To test the scalability of DGLT, we further use a large-scale dataset called Ogbl-Collab Hu et al. (2020) for link prediction.",,,1,related
"…finding a fixed sparse mask at the initialization as we mentioned in introduction, on the other hand, dynamic sparse training allows the sparse mask to be updated during training, e.g., (Mocanu et al., 2018; Mostafa and Wang, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021a,c,d).",,,1,related
"training [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], even before looking at the data, in order to reduce training cost.",,,0,not_related
"The other line of work focuses on reducing computation through various model pruning techniques (Han et al., 2015; Frankle & Carbin, 2018; Evci et al., 2020).",,,0,not_related
"niques have their limitations and often come at the cost of reduced accuracy [2], [5], [11]–[13], [18], [21], [22], [24], [28], [29].",,,0,not_related
"In contrast, recent works propose dynamic weight training strategies where different compact subnets will be dynamically activated at each training iteration (Mocanu et al., 2018; Mostafa & Wang, 2019; Raihan & Aamodt, 2020; Evci et al., 2020; Liu et al., 2023).",,,0,not_related
"Evci et al. (2020b) further expand the Erdős-Rényi graph to convolution neural networks, demonstrating large performance improvements.",,,0,not_related
"• ERK (Evci et al., 2020a; Mocanu et al., 2018) initializes sparse networks with a Erdős-Rényi graph where small layers are usually allocated more weights.",,,0,not_related
"We can confirm, using our two random methods (Rand and ERK) on Figure 3, that they always yield σ ≤ 0, no matter the settings.",,,1,related
"For example, sparse training (Evci et al., 2020; Yuan et al., 2021) and low-precision training (Yang et al.",,,0,not_related
"For example, sparse training (Evci et al., 2020; Yuan et al., 2021) and low-precision training (Yang et al., 2020; Zhao et al., 2021) are two active research areas for efficient training that can effectively reduce training costs, such as computing FLOPs and memory.",,,0,not_related
"Recent studies on unstructured pruning [96, 97] demonstrate that ResNet50 experiences significant accuracy degradation only when the pruning rate exceeds 90%.",,,0,not_related
"While post-training quantization can be efficient and successful without any retraining (Frantar & Alistarh, 2022), in the case of pruning the gold standard is still training a separate model for every target sparsity level (Zhu & Gupta, 2017; Singh & Alistarh, 2020; Evci et al., 2020; Peste et al., 2021), which can be expensive.",,,0,not_related
"…can be efficient and successful without any retraining (Frantar & Alistarh, 2022), in the case of pruning the gold standard is still training a separate model for every target sparsity level (Zhu & Gupta, 2017; Singh & Alistarh, 2020; Evci et al., 2020; Peste et al., 2021), which can be expensive.",,,0,not_related
"Therefore, it is also interesting to study how PQI are related to various pruning methods Evci et al. (2020); Hoefler et al. (2021).",,,0,not_related
"It is also possible to use other pruning methods such as random pruning (Evci et al., 2020; Liu et al., 2022), SNIP (Lee et al., 2018), hessian-based (Yu et al., 2021), iterative (Han et al., 2015; Frankle & Carbin, 2018) or progressive (Liu et al., 2021).",,,0,not_related
"It is also possible to use other pruning methods such as random pruning (Evci et al., 2020; Liu et al., 2022), SNIP (Lee et al.",,,0,not_related
All further hyperparameters of RigL are adopted from [4].,,,0,not_related
"As described in [4], RigL performes better with a longer training duration.",,,0,not_related
"1 Comparing layerwise pruning ratios achieved on WideResNet28 (blue) and DenseNet40 (orange) using Erdos-Renyi Kernel layerwise [18], Layer-adaptive Sparsity for the Magnitude-based Pruning (LAMP) [51], and uniform (baseline) schemes.",,,0,not_related
"1: Comparing layerwise pruning ratios achieved on WideResNet28 (blue) and DenseNet40 (orange) using Erdos-Renyi Kernel layerwise [18], Layer-adaptive Sparsity for the Magnitude-based Pruning (LAMP) [51], and uniform (baseline) schemes.",,,0,not_related
point out that all these algorithms can be expressed with a drop-and-grow framework and propose RigL which uses the gradient magnitude as the grow criterion [8].,,,0,not_related
"To reduce the cost, there is a growing interest in developing sparse training algorithms [30, 2, 8, 18, 26, 27].",,,0,not_related
Evci et al. point out that all these algorithms can be expressed with a drop-and-grow framework and propose RigL which uses the gradient magnitude as the grow criterion [8].,,,0,not_related
"Compared with nonstructured dynamic sparse training (RigL [8]), our DSB has slightly lower accuracy at 0.5 and 0.6 sparsity, and the accuracy gap widens as the sparsity gets higher.",,,0,not_related
"An issue with RigL is that it needs to compute the full gradient every few iterations, which may not be affordable for large models.",,,0,not_related
"Following previous dynamic sparse training algorithms [8, 26, 27, 18], we select the k weights with the smallest magnitudes (i.",,,1,related
"Different from previous work which selects new parameters based on dense gradients [8] or dense weights [18], we select blocks of new parameters directly based on the input value and output gradient of each layer, making our algorithm purely sparse.",,,1,related
"Compared with nonstructured dynamic sparse training (RigL [8]), our DSB has slightly lower accuracy at 0.",,,1,related
"For example, RigL [8] drops the weights with the smallest magnitudes and grows the same amount of weights with the largest gradients.",,,0,not_related
"Different from previous work that computes the gradients for all weights and adds the weights with the largest gradients[8, 26, 27], we use dOut and In to estimate the importance of weights.",,,1,related
We use the same sparsity initialization method ERK in the state-of-the-art sparse training method such as RigL (Evci et al. 2020) and ITOP (Liu et al.,,,1,related
"2020), SIS (Verma and Pesquet 2021) and RigL (Evci et al. 2020), respectively.",,,0,not_related
"To improve the flexibility, dynamic mask training has been proposed (Mocanu et al. 2018; Mostafa and Wang 2019; Evci et al. 2020; Liu et al. 2021b; Ma et al. 2021), where the sparse mask is periodically updated by drop-andgrow to search for better subnetworks with high accuracy, where in the drop…",,,0,not_related
"RigL (Evci et al. 2020) updated the sparsity topology of the sparse network during training using the same magnitude-based weights dropping method while growing back the weights using top-k absolute largest gradients, achieving better accuracy than static mask training under same sparsity.",,,0,not_related
"…mask training baselines while adopting DeepR (Bellec et al. 2018), SNFS (Dettmers and Zettlemoyer 2019), DSR (Mostafa and Wang 2019), SET (Mocanu et al. 2018), RigL (Evci et al. 2020), MEST (Yuan et al. 2021), RigL-ITOP (Liu et al. 2021b) as the dynamic mask training baselines as shown in Table 2.",,,0,not_related
We use the same sparsity initialization method ERK in the state-of-the-art sparse training method such as RigL (Evci et al. 2020) and ITOP (Liu et al. 2021b).,,,1,related
"To improve the flexibility, dynamic mask training has been proposed (Mocanu et al. 2018; Mostafa and Wang 2019; Evci et al. 2020; Liu et al. 2021b; Ma et al. 2021), where the sparse mask is periodically updated by drop-andgrow to search for better subnetworks with high accuracy, where in the drop process we deactivate a portion of weights from active states (nonzero) to non-active states (zero), vice versa for the growing process.",,,1,related
"…6.5%, 6.4%, 6.5%, 4.9% and 1.4% higher accuracy performance at 98% sparsity ratio compared to SNIP (Lee, Ajanthan, and Torr 2019), GraSP (Wang, Zhang, and Grosse 2020), SynFlow (Tanaka et al. 2020), STR (Kusupati et al. 2020), SIS (Verma and Pesquet 2021) and RigL (Evci et al. 2020), respectively.",,,0,not_related
"Post-training pruning methods have a high computational cost because they require training an overparameterized dense model first, while pruning from scratch methods have shown promise, but have been outperformed by dynamic sparsity methods such as Rigging the Lottery (RigL) (Evci et al., 2020).",,,0,not_related
"RigL (Evci et al., 2020) and Sparse Network From Scratch (SNFS) (Dettmers & Zettlemoyer, 2019) use a cosine annealing schedule, while Zhu & Gupta (2017); Mostafa & Wang (2019) use a cubic schedule.",,,0,not_related
"ER was adapted to Erdős RènyiKernel (ERK) ratios by (Evci et al., 2020), to work better with Convolutional Neural Networks (CNNs).",,,0,not_related
"ER was adapted to Erdős RènyiKernel (ERK) ratios by (Evci et al., 2020), to work better with Convolutional Neural Networks (CNNs).",,,0,not_related
"The only existing prior work on unstructured ViT pruning is SViTE [Chen et al., 2021], which applied the general RigL pruning method [Evci et al., 2020] to the special case of ViT models.",,,1,related
"Many other criteria exist, such as gradient magnitude [Evci et al., 2020] or “rates of change” in the weights [Sanh et al.",,,0,not_related
"Future work should also be able to extend our pruner to structured compression of ViT models, or employ our oViT pruner inside different, more computationally-intensive pruning algorithms such as RigL [Evci et al., 2020].",,,1,related
"Many other criteria exist, such as gradient magnitude [Evci et al., 2020] or “rates of change” in the weights [Sanh et al., 2020].",,,0,not_related
", 2021], which applied the general RigL pruning method [Evci et al., 2020] to the special case of ViT models.",,,0,not_related
"We compare with global magnitude (GM) following the same schedule as oViT, as well as the state-of-the-art SViTE [Chen et al., 2021] paper, which trains the sparse model from scratch using a variant of RigL [Evci et al., 2020], but for a total of 600 epochs.",,,1,related
"In addition to the sparse training from scratch with periodic updates of the sparsity weights with some salincy criterion for weight elimination and regrowth [Evci et al., 2020] one can consider alternating compressed/decompressed training (AC/DC), proposed in [Peste et al., 2021].",,,0,not_related
", 2021] paper, which trains the sparse model from scratch using a variant of RigL [Evci et al., 2020], but for a total of 600 epochs.",,,0,not_related
"The other category is dynamic sparse training (Mocanu et al., 2018; Dettmers & Zettlemoyer, 2019; Evci et al., 2020).",,,0,not_related
"Inspired by RigL (Evci et al., 2020), devices only rate partial model parameters (e.",,,0,not_related
"Although RigL Evci et al. (2020) tries to reduce memory consumption, it needs to compute gradients for all parameters, which is computationally expensive and may lead to straggling issues in federated learning
Federated Neural Network Pruning.",,,0,not_related
"After pruning, we grow the pruned parameters with the largest gradient magnitude, like RigL (Evci et al., 2020).",,,1,related
"Such negative impact becomes more challenging when pruning towards an extremely tiny subnetwork, as the biased initial subnetwork can deviate significantly from the optimal structure, resulting in poor accuracy (Evci et al., 2020).",,,0,not_related
"Inspired by RigL (Evci et al., 2020), devices only rate partial model parameters (e.g., a single layer) at a time, where the topK importance scores are stored locally and uploaded to the server, significantly reducing memory, computation, and communication costs.",,,0,not_related
"Note, centralized training has shown significant benefits with sparse learning with FLOPs reduction during forward operations [8], and potential training speed-up of up to 3.",,,0,not_related
"FedDST [2], on the other hand, leveraged the idea of RigL [8] to perform sparse learning of the clients and relied on magnitude pruning at the server-side that does not necessarily adhere to the layer sensitivity towards a target density.",,,0,not_related
"More recently, sparse learning [5, 8, 19, 32], a popular form of model pruning, has gained significant traction due to its popularity in yielding FLOPs advantage and potential speed-up even during training.",,,0,not_related
"In particular, recently proposed sparse learning strategies [5, 8, 19, 30, 32] effectively train weights and associated binary sparse masks to allow only a fraction of model parameters to be updated during training, potentially enabling the lucrative reduction in both the training time and FLOPs [31, 32], while creating a model to meet a target parameter density denoted as d, and is able to yield accuracy close to that of the unpruned baseline.",,,0,not_related
"In particular, initial balanced or pyramidal sparsity ratios seem to be able to improve the performance of RiGL.",,,0,not_related
"; Evci et al., 2020a;b).",,,0,not_related
"Dynamical Sparse Training In order to improve the expressiveness of ER networks and achieve extremely sparse WLTs, ER networks can be rewired with the help of DST. Specifically, we use the algorithm RiGL (Evci et al., 2020a).",,,1,related
"This way, we also provide a missing theoretical foundation for dynamic sparse training approaches (Evci et al., 2020a; Liu et al., 2021.",,,1,related
"This insight presents a theoretical justification for pruning approaches that start from random ER masks like Dynamic Sparse Training (Evci et al., 2020a; Mocanu et al., 2018).",,,0,not_related
"…Training (Mocanu et al., 2018; Liu et al., 2021a) which explores the sparsity pattern in a prune-and-grow scheme according to some criteria (Mocanu et al., 2018; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020a; Ye et al., 2020; Jayakumar et al., 2021; Liu et al., 2021b).",,,0,not_related
"Apart from pre-specified pruning rate, pruning ratio can be varied for different layers such as Erdö-Rényi (Mocanu et al., 2018) and Erdö-Rényi Kernel (Evci et al., 2020a).",,,0,not_related
"As suggested in (Evci et al., 2020b), scaling after pruning preserves the gradient flow of the neural network.",,,0,not_related
"In recent studies, some sparse networks not only decrease storage and computational requirements but also achieve higher inference scores than dense networks [10], suggesting the potential utility of sparse structure in decreasing the overfit.",,,0,not_related
"Google AI group proposed this modified version of Erdős–Rényi algorithm (Evci et al. 2020), originally published by Mocanu et al.",,,0,not_related
"Google AI group proposed this modified version of Erdős–Rényi algorithm (Evci et al. 2020), originally published by Mocanu et al. 2018) for pruning convolutional layers in neural networks.",,,0,not_related
"It was further improved it with a training scheme for sparsely initialized neural networks, where the layerwise sparsity is determined by the Erdős–Rényi Kernel (ERK) method (Evci et al. 2020).",,,0,not_related
"Older works maintained a static graph [28] and dealt only with feedforward networks but newer methods such as dynamic sparse training (DST)[5, 22] have been proposed for both feedforward networks and RNNs which dynamically improve the sparse graph and provide better performance.",,,0,not_related
`1 [5] erk [2] lamp [7] Ours original [12] FLOPs 168 G 287 G PSNR 20.,,,0,not_related
"There are many other ways to obtain pruned neural networks (e.g., Janowsky, 1989; LeCun et al., 1990; Han et al., 2015; Zhu & Gupta, 2017; Evci et al., 2020).",,,0,not_related
"In spirit, our effort can be likened to methods like RigL (Evci et al., 2020), which discovers the wiring of a sparse neural network during training rather than pruning a dense network post training.",,,1,related
"338 [38] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.",,,0,not_related
"Past works in network pruning have explored effective techniques to find efficient subnetworks (Lee et al., 2019; Evci et al., 2020; He et al., 2022; 2023) and zero out redundant parameters.",,,0,not_related
"On the other hand, LTH-based (Chen et al., 2020a; Evci et al., 2020) methods can be borrowed to find the mask by several iterations, but it is prohibitively time-consuming.",,,1,related
"Table 1 compares the inference accuracy, inference FLOPS, and model size of the proposed method with pruning (Gale et al., 2019), and with two sparsity training methods: RigL (Evci et al., 2020) and MEST+EM&S (Yuan et al., 2021).",,,1,related
", 2019), and with two sparsity training methods: RigL (Evci et al., 2020) and MEST+EM&S (Yuan et al.",,,0,not_related
"In addition, it would be interesting to study more pruning algorithms such as (Evci et al., 2020; Lin et al., 2020; Wang et al., 2020; Aghasi et al., 2016; Verma & Pesquet, 2021), especially iterative or inherently sparse ones that could explore structures corresponding to prohibitively large…",,,0,not_related
"In addition, it would be interesting to study more pruning algorithms such as (Evci et al., 2020; Lin et al., 2020; Wang et al., 2020; Aghasi et al., 2016; Verma & Pesquet, 2021), especially iterative or inherently sparse ones that could explore structures corresponding to prohibitively large networks, thus solving the problem of size when embedding in fully connected space.",,,0,not_related
"Several recent works (Su et al., 2020; Evci et al., 2020) empirically verify “lottery ticket hypothesis”, i.e., there exists sub-networks (i.e., winning tickets) that can reach comparable generalization performance as the original pre-trained DNN if re-trained.",,,0,not_related
"Several recent works (Su et al., 2020; Evci et al., 2020) empirically verify “lottery ticket hypothesis”, i.",,,0,not_related
"Immediately after magnitude pruning, we explore the same number of p-proportion of new weights with the largest magnitude gradients as in [15]:",,,1,related
"Following RigL3 [15], the FLOPs are calculated with the total number of multiplications and additions layer by layer for a given layer S.",,,0,not_related
"Further, [13, 15] leverage the gradient information in the backward pass to guide the optimization of sparse connectivity and demonstrate substantial performance improvement.",,,0,not_related
"Following RigL3 [15], the FLOPs are calculated with the total number of multiplications and additions layer by layer for a given layer Sl. Briefly speaking, with ERK distribution, the training FLOPs of a sparse Wide ResNet28-10 at sparsity S = 0.8 and S = 0.9 are 33.7% and 16.7% of the dense model, respectively.",,,1,related
"To address this problem, we turn our attention to dynamic sparsity [3, 15, 47, 52, 55], a recently emerged sparse training area that enables training sparse neural networks from scratch by dynamically optimizing the sparse connectivities.",,,1,related
"For the convolutional layers, we use the kernel variant, Erdős-Rényi-Kernel (ERK) as introduced in [15].",,,1,related
"Over-Parameterization [15, 47], which requires thousands of training epochs for extremely sparse models to explore sufficient parameters in the space-time manifold.",,,0,not_related
"In this section, we conduct various experiments to validate the effectiveness of SIS in terms of test accuracy vs. sparsity and inference time FLOPs vs. sparsity by comparing against RigL (Evci et al., 2020).",,,1,related
"(Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2020; Evci et al., 2020) computes weight magnitude and reallocates weights at every step of model training.",,,0,not_related
"…SMAPE FLOPs
Dense 12.2 4.53G 18.6 927.73G 8.3 41.26M
SNIP (Lee et al., 2019) 14.3 2.74G 24.6 398.92G 10.1 21.45M LRR (Renda et al., 2020) 13.7 2.61G 23.1 339.21G 9.3 14.47M RigL (Evci et al., 2020) 13.9 2.69G 22.4 326.56G 10.2 15.13M SIS (Ours) 13.1 2.34G 21.1 290.38G 9.7 14.21M
N-BEATS on M4.",,,1,related
"sparsity by comparing against RigL (Evci et al., 2020).",,,0,not_related
"2) Layer-wise sparsity decided by ERK only depends on the overall sparsity target and layer configurations (e.g. kernel size, input/output channels).",,,0,not_related
Training epochs: Training for fine-tuning a sparse neural networks usually requires more epochs for convergence [15].,,,0,not_related
"In order to estimate the layer-wise redundancy, we use the Erdos-Renyi kernel (ERK) [15].",,,1,related
Note that ri can be negative if the current sparsity (si) is larger than the sparsity estimated by ERK.,,,1,related
"3.4 16: end if 17: end if 18: end for 19: end while 20: Output Layer wise sparse scheme N iv
In order to estimate the layer-wise redundancy, we use the Erdos-Renyi kernel (ERK) [15].",,,1,related
[15] propose a training scheme for unstructured sparse neural networks and present the Erdos-Renyi kernel (ERK) (extension of [17]) to heuristically select the layer-wise sparsity.,,,0,not_related
ERK is a heuristic method to decide the layer-wise unstructured sparsity given an overall sparsity target (i.e. model size) which does not require design space exploration or hyper-parameter search.,,,0,not_related
DominoSearch also uses a layer-wise penalty factor to balance the heuristic layer-wise redundancy of parameters[15] and layer-wise computational complexity (e.,,,0,not_related
"Thus, we use the layer-wise sparsity generated by ERK as a layer-wise redundancy guidance, which can be formulated as:
ri = ei − si
maxi=Li=1 |ei − si| (7)
where ei is the sparsity of layer i decided by ERK.",,,1,related
"Recent discoveries demonstrate that by carefully selecting [14, 15] or learning [16] the layer-wise sparsity, the sparse DNNs can achieve higher accuracy than their uniform counterparts.",,,0,not_related
Evci et al. [15] propose a training scheme for unstructured sparse neural networks and present the Erdos-Renyi kernel (ERK) (extension of [17]) to heuristically select the layer-wise sparsity.,,,0,not_related
"However, ERK cannot be directly applied to select the layer-wise N:M schemes because: 1) ERK gives layer-wise sparsity in the continuous domain while N:M sparsity is discrete.",,,0,not_related
"One pruning method removes the weights with the least magnitude and randomly makes new connections to learn a sparse ANN structure (Evci et al., 2019).",,,0,not_related
"Pruning methods are used to learn a sparser ANN structure (Pérez-Sánchez, 2018; Evci et al., 2019).",,,0,not_related
"Pruning methods are used to learn a sparser ANN structure (Pérez-Sánchez, 2018; Evci et al., 2019).",,,0,not_related
"The method is further evolved for convolution layers considering both the magnitude and gradient of the weights(Evci et al., 2020a). Various analysis for explaining the LTH have been attempted in the past. Researchers Evci et al. (2020b) explain empirically why the LTH works through gradient flow at different stages of the training. Despite previous attempts to explain why the Lottery Ticket Hypothesis works, the underlying phenomenon associated with the hypothesis still remains ill-understood. All of these studies related to LTH identify that a sparse sub-network can be trained instead of a complete network and the network needs to be connected from input to output layers. However, none of them try to explain the LTH and the properties of the pruned network through the lens of spectral graph theory. The network connectivity can be described from the graph expansion point of view, where any subset of vertices of size less than or equal to half of the number of vertices in a graph, is adjacent to at least a fraction of the number of vertices in that set; for details, see (Lubotzky, 2010). Graphs satisfying this property are known as expander graphs. The Ramanujan Graph is a special graph in a bounded degree expander family, where the eigenbound is maximal (Nilli, 1991). This leads to a maximum possible sparsity of a network while preserving the connectivity. In this paper, we initiate a study to observe the characteristics of a pruned sub-network from the spectral properties of its adjacency matrix, which, has not been reported previously. We represent a feed-forward neural network as a series of connected bipartite graphs. Both weighted and unweighted bi-adjacency matrices are considered. The Ramanujan graph properties of each of the bipartite layers are studied. We use the results of Hoory (2005) on the bound of spectral gap for the weight matrix of a pruned network.",,,0,not_related
"The method is further evolved for convolution layers considering both the magnitude and gradient of the weights(Evci et al., 2020a). Various analysis for explaining the LTH have been attempted in the past. Researchers Evci et al. (2020b) explain empirically why the LTH works through gradient flow at different stages of the training.",,,0,not_related
Researchers Evci et al. (2020b) explain empirically why the LTH works through gradient flow at different stages of the training.,,,0,not_related
"The method is further evolved for convolution layers considering both the magnitude and gradient of the weights(Evci et al., 2020a).",,,0,not_related
"Many follow-up works [Morcos et al., 2019; Zhou et al., 2019; Frankle et al., 2020a; Savarese et al., 2020; Wang et al., 2020; Ramanujan et al., 2020; Evci et al., 2020; Frankle et al., 2021] advance the idea and keep challenging the conventional wisdom on neural network pruning.",,,0,not_related
"As described in [22], the final test accuracy would decrease if the pruning interval is too big or small.",,,0,not_related
"The requirement of a dense gradient in Evci et al. (2019) and Zhu & Gupta (2018) prevents the use of the optimization in Equation 4, which is strictly necessary to fit the RTRL training computations on accelerators without running out of memory.",,,0,not_related
"The last few years have also seen a resurgence of interest in sparse neural networks – both their properties (Frankle & Carbin, 2019) and new methods for training them (Evci et al., 2019).",,,0,not_related
"It essentially sparsifies the network at a fine-grained level and is demonstrated to achieve an extremely high compression rate and high accuracy performance [11, 7, 29].",,,0,not_related
"B Pruned CNN+ReLU [16], [21] Pruned Trasformer+ReLU [55] sparse /sparse DNN.",,,0,not_related
"Many variants have been proposed for both structured [25, 36, 41, 65] and unstructured [16, 17, 19, 26] pruning.",,,0,not_related
"[16] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.",,,0,not_related
"Possible directions of future study include:
• Bringing the RG framework to other sparsficiation methods, like RigL [12], which can similarly be viewed as RG schemes;
• Bringing the RG framework to study winning tickets beyond computer vision, such as in natural language processing [3, 31, 45], reinforcement learning [45], and lifelong learning [5];
• Computing and classifying systems by their critical exponents (such as γ in Eq.",,,0,not_related
"Possible directions of future study include: • Bringing the RG framework to other sparsficiation methods, like RigL [12], which can similarly be viewed as RG schemes; • Bringing the RG framework to study winning tickets beyond computer vision, such as in natural language processing [3, 31, 45], reinforcement learning [45], and lifelong learning [5]; • Computing and classifying systems by their critical exponents (such as γ in Eq.",,,0,not_related
"Evci et al. (2019) propose a reformulation of the ERDŐS-RÉNYI KERNEL (ERK) (Mocanu et al., 2018) to take the layer and kernel dimensions into account when determining the layerwise sparsity distribution.",,,0,not_related
"We note that we follow the advice of Evci et al. (2019) and Dettmers and Zettlemoyer (2019) and do not prune biases and batch-normalization parameters, since they only amount to a negligible fraction of the total weights, however keeping them has a very positive impact on the performance of the…",,,1,related
"Gale et al. (2019), Evci et al. (2019) and Lin et al. (2020) consider this when presenting their results.",,,0,not_related
"Although existing LTH techniques raise very intriguing observations, most of them provide only empirical evidence to verify the LTH [71, 12, 1, 47, 69, 54, 5, 53, 26, 8, 7, 11].",,,0,not_related
"Similar to our work, there exists a line of work in the area of dynamic sparse training for unstructured pruning [50, 13, 32, 9, 27] that gradually prunes the model to the target sparsity during training.",,,0,not_related
"…many interesting works have addressed this training strategy by proposing different algorithms for optimizing the topology during training (Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Bellec et al., 2018; Jayakumar et al., 2020; Liu et al., 2021b; Raihan & Aamodt, 2020).",,,0,not_related
"We follow the method described in (Evci et al., 2020) to calculate the FLOPs.",,,1,related
"Similar to our work, there exists a line of work in the area of dynamic sparse training for unstructured pruning [50, 13, 32, 9, 27] that gradually prunes the model to the target sparsity during training.",,,0,not_related
"Furthermore, [31] proposed a drop and grow strategy with the magnitude of the parameters and the gradients.",,,0,not_related
"Various dynamic methods [17]–[20], [24], [28]–[31] have been proposed, and they have shown better performance than static methods.",,,0,not_related
We follow the method described in [11] to calculate the number of FLOPs required for training which is based on the total number of multiplications and additions layer by layer.,,,1,related
"In [11, 7, 25, 12], the gradient information is used to determine which connections would be changed during the evolution phase.",,,0,not_related
"…uniform layer-wise magnitude pruning (Zhu & Gupta, 2017), magnitude pruning with heuristic layer-wise budgets or reallocation of weights (Dettmers & Zettlemoyer, 2019; Evci et al., 2020) and learnable sparsity methods like l1 regularization (Louizos et al., 2018) and STR (Kusupati et al., 2020).",,,0,not_related
", 2016), gradual uniform layer-wise magnitude pruning (Zhu & Gupta, 2017), magnitude pruning with heuristic layer-wise budgets or reallocation of weights (Dettmers & Zettlemoyer, 2019; Evci et al., 2020) and learnable sparsity methods like l1 regularization (Louizos et al.",,,0,not_related
"s Following the convention in (Evci et al., 2020), multiplication and addition are counted as two operations.",,,1,related
"For non-uniform sparse ResNet-50 model, the improvement over RigL5× (Evci et al., 2020) is 1.0% and 1.5% at 80% and 90% sparsity, respectively.",,,1,related
", 2018), DSR (Mostafa & Wang, 2019), and RigL (Evci et al., 2020) maintain the target sparsity in all layers throughout the training process and † Equal Contribution.",,,0,not_related
"As shown in Table 1, our implementation has slightly higher accuracy, which indicates that our code base in PyTorch is comparable with the original implementation in Evci et al. (2020), and can be used for reproducing and extending the training of RigL without causing fairness issue.",,,1,related
"We observe that the improvement over RigL5× (Evci et al., 2020) is 1.3% (77.9% vs. 76.6",,,1,related
"Methods based on sparse mask exploration, such as DeepR (Bellec et al., 2018), SET (Mocanu et al., 2018), DSR (Mostafa & Wang, 2019), and RigL (Evci et al., 2020) maintain the target sparsity in all layers throughout the training process and
† Equal Contribution.",,,0,not_related
"In Table 10, we perform additional experiments to supplement Table 1 by pruning the last FC layer using the C-GaP method and comparing them with Evci et al. (2020).",,,1,related
"In this section, we provide the implementation details of the RigL (Evci et al., 2020).",,,1,related
"We also implement the RigL and RigL5× (Evci et al., 2020) using our code base on PyTorch (please refer to Appendix B.",,,1,related
"For the experiments with 100 training epochs, Most of the important hyper-paramters can be found in Evci et al. (2020).",,,0,not_related
"However, Evci et al. (2020) doesn’t provide detailed hyper-parameter settings for 500-epoch training.",,,0,not_related
"RigL (Evci et al., 2020) and NeST (Dai et al., 2019) propose to use magnitude-based pruning and gradient-flowbased growth that update sparse model topology during training.",,,0,not_related
"Please note that models with the non-uniform sparsifying distribution in Table 1 already have the last FC layer pruned, thus the experiment setup is the same as the ones in Evci et al. (2020).",,,1,related
"The results in Table 10 and Table 1 indicate that the accuracy of the ResNet-50 models with sparse and dense FC layers are similar, and both of them outperform the state-of-the-art results in Evci et al. (2020).",,,0,not_related
"Note that previous works update weights either greedily (e.g., RigL (Evci et al., 2020)) or randomly (e.g., SET (Mocanu et al., 2018) and DSR (Mostafa & Wang, 2019)).",,,1,related
"For the uniform sparsity, the first convolutional layer with 7× 7 kernels is kept dense, the same as in Evci et al. (2020).",,,0,not_related
"We also implement the RigL and RigL5× (Evci et al., 2020) using our code base on PyTorch (please refer to Appendix B.2 for details).",,,1,related
"We include the SNIP (Lee et al., 2019) and SET (Mocanu et al., 2018) results in uniform sparsity that are reproduced in Evci et al. (2020).",,,1,related
"2 OUR IMPLEMENTATION DETAILS OF RIGL In this section, we provide the implementation details of the RigL (Evci et al., 2020).",,,1,related
"We observe that the improvement over RigL5× (Evci et al., 2020) is 1.",,,1,related
"For non-uniform sparse ResNet-50 model, the improvement over RigL5× (Evci et al., 2020) is 1.",,,0,not_related
"As for the uncertainty measurement, LTH and RigL show better performance than dense networks.",,,0,not_related
"Numerous approaches (Mocanu et al., 2016; Evci et al., 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Liu et al., 2021a; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Jayakumar et al., 2020; Raihan & Aamodt, 2020; Liu et al., 2021b) study such dynamic sparsity, often matching state-of-the-art training performance (Liu et al.",,,0,not_related
"From Figure 11, we observe that LTH and RigL are capable of maintaining the generalization ability of dense networks at a sparsity level of 21%.",,,1,related
"And with the scope of Hessian traces for weight flatness, the subnetworks located by LTH are winning tickets on CIFAR-10 and CIFAR-100, while RigL fails to locate the flat local minima.",,,0,not_related
"…sparsification regimes such as magnitude pruning (Han et al., 2016), lottery ticket hypothesis (Frankle & Carbin, 2019), random pruning, pruning at initialization (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020), and dynamic sparse training (Evci et al., 2020; Liu et al., 2021b).",,,0,not_related
"Overall, identifying sparse neural networks from dynamic sparse training (e.g., RigL) is a great option for preserving generalization ability, interpretability, and uncertainty, but not for maintaining flat geometric of learned loss surfaces.",,,0,not_related
"All RigL experiments follow the recent SOTA training configurations (Liu et al., 2021b).",,,0,not_related
All results and analyses about RigL are referred to Appendix A2.,,,1,related
"We choose the top-performing algorithm, RigL (Evci et al., 2020; Liu et al., 2021b), which starts from a random sparse network and encourages the connectivity to evolve dynamically based on a grow-and-prune strategy.",,,1,related
"…(Mocanu et al., 2016; Evci et al., 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Liu et al., 2021a; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Jayakumar et al., 2020; Raihan & Aamodt, 2020; Liu et al., 2021b) study such dynamic sparsity, often matching state-of-the-art…",,,0,not_related
", 2020), and dynamic sparse training (Evci et al., 2020; Liu et al., 2021b).",,,0,not_related
