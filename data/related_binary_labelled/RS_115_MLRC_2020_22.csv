text,label_score,label,target_predict,target_predict_label
"They argue that raw attention fails to capture syntactic structures in text and may not contribute to predictions as commonly assumed (Mohankumar et al., 2020).",,,0,not_related
"To make attention explanation, technical solutions have also been explored by optimizing input representation (Mohankumar et al., 2020), regularizing learning objectives (Moradi et al.",,,0,not_related
"[0,10) [10,20) [20,30) [30-40) ≥40 SASRec AC-SASRec 0.",,,0,not_related
"[0,10) [10,20) [20,30) [30-40) ≥40 BERT4Rec",,,0,not_related
"Although much recent work has been done on explainability in the computer vision and natural language processing [Masoomi et al., 2021; Mohankumar et al., 2020; Tsang et al., 2020], this problem has been overlooked in the case of time series forecasting [Tonekaboni et al.",,,0,not_related
"There has been an ongoing debate in the literature regarding the interpretability of the attention mechanism (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Moradi et al., 2019; Mohankumar et al., 2020).",,,0,not_related
And the attention mechanism is able to provide faithful explanations [49]– [51].,,,0,not_related
"To give a sense of the success of these techniques, it suffices to say that some of them are now integrated as default explainability tools in widespread cloud machine learning services, while in areas such as natural language processing, researchers are starting to use such methods as the gold standard against which they judge the quality of other explanations (Mohankumar et al., 2020).",,,0,not_related
[99] proposed the Orthogonal and Diversity based LSTMmodels.,,,0,not_related
"We assume a generic model framework and notations [57, 99].",,,1,related
In general attention based explanations are characterised as faithful and plausible [99].,,,0,not_related
(Mohankumar et al. 2020) explores to modify the LSTM cell with diversity driven training to enhance explainability and transparency of attention modules.,,,0,not_related
"For regularization methods, we conducted a grid search with parameter grid [0.1, 0.3, 0.5, 1, 5, 10] for CONICITY and [0.1, 0.3, 0.5, 1, 5, 10, 20] for TYING.",,,1,related
"Taking a step further, we applied two regularization techniques, TYING and CONICITY, originally aimed at increasing faithfulness of attention explanations, with the hypothesis that the issue underpinning disagreements and unfaithfulness is the same – representation entanglement in the hidden space.",,,1,related
"C L
] 1
1 M
ay 2
02 3
tanglement significantly improve the faithfulness of attention-based explanations (Mohankumar et al., 2020; Tutek and Šnajder, 2020).",,,0,not_related
CONICITY aims to increase the angle between each hidden representation and the mean of the hidden representations of a single instance.,,,0,not_related
"Although both works introduced other methods of enforcing differences between hidden states, namely orthogonal-LSTM and masked language modeling as an auxiliary task, we opt for CONICITY and TYING as they were both shown to be more efficient and more stable in practice.",,,0,not_related
"In Table 2 we report correlation scores on the test splits of all datasets for regularized models (CONICITY, TYING) and their unregularized variants (BASE).",,,1,related
"tanglement significantly improve the faithfulness of attention-based explanations (Mohankumar et al., 2020; Tutek and Šnajder, 2020).",,,0,not_related
"We include the results for all datasets across training epochs for regularized models (CONICITY, TYING) when compared to their unregularized, BASE variants.",,,1,related
"We hypothesize that the cause of saliency method disagreements is rooted in representation entanglement and experimentally show that agreement can be significantly improved by regularization techniques such as tying (Tutek and Šnajder, 2020) and conicity (Mohankumar et al., 2020).",,,1,related
"To counteract this issue, we employ two regularization schemes that have been shown to improve the faithfulness of the attention mechanism as a method of interpretability: CONICITY (Mohankumar et al., 2020) and TYING (Tutek and Šnajder, 2020).",,,1,related
"Attention mechanism depends on accurately distributing attention weights for each prediction, and its vulnerability to perturbation leads to incorrect translation or over translation in neural machine translation [2], [3].",,,0,not_related
"[69] argue that when the attention distribution is computed on input representations that are very similar to each other, they cannot provide very meaningful explanations.",,,0,not_related
"For this purpose, the authors of [69] diversify the hidden representations over which the distribution are computed for more faithful explanations.",,,0,not_related
"Some works also aim to employ masks as the analytical tools to indicate the importance (Kitada and Iyatomi 2020; Mohankumar et al. 2020), attention head (Fong and Vedaldi 2017), or the contributions of the pixels in the image to the model outputs (Voita et al. 2019).",,,0,not_related
"Some works also aim to employ masks as the analytical tools to indicate the importance (Kitada and Iyatomi 2020; Mohankumar et al. 2020), attention head (Fong and Vedaldi 2017), or the contributions of the pixels in the image to the model outputs (Voita et al.",,,0,not_related
"The attention mechanism [45, 46] is one of the most popular methods, which uses the computer vision system to mimic the human visual system so as to focus on the regions of interest.",,,0,not_related
"Attention mechanism [30], one of the early approaches that incorporated explainability during the training phase, fail to consider network layers succeeding the attention layer [31], [32].",,,0,not_related
[12] show that minimizing hidden state conicity in a BiLSTM improves the Pearson correlation of attention weights with Integrated Gradients [13] attributions.,,,0,not_related
"Several researchers proposed kernels designed around constituent parse trees to capture sentence grammatical structure (Miller et al. 2000; Zelenko, Aone, and Richardella 2003; Moschitti 2006).",,,0,not_related
Mohankumar et al. (2020) follows up prior work to note that the distribution of attention fails to fall on important words and strays to unimportant tokens.,,,0,not_related
"However, recent extensive evaluations in NLP tasks (Serrano & Smith, 2019; Jain & Wallace, 2019; Mohankumar et al., 2020) have shown that the attention may not weigh the features that dominate the model output more than other features.",,,0,not_related
"Following prior studies (Jain & Wallace, 2019; Mohankumar et al., 2020), we conduct extensive experiments on six exemplar tasks, for which attention models are widely applied.",,,0,not_related
"…puzzles in inherent attention explanations, such as whether attention is directly explainable (Jain & Wallace, 2019; Serrano & Smith, 2019; Wiegreffe & Pinter, 2019; Brunner et al., 2020), or how attention behaves (Clark et al., 2019; Bai et al., 2021; Michel et al., 2019; Mohankumar et al., 2020).",,,0,not_related
", 2020), or how attention behaves (Clark et al., 2019; Bai et al., 2021; Michel et al., 2019; Mohankumar et al., 2020).",,,0,not_related
"For example, Mohankumar et al. (2020) explored why attention fails to explain LSTM models and pointed out that the similar hidden states in the encoder restrict the significance of attention weights.",,,0,not_related
"Although, the usefulness of the interpretation of attention weights is controversial [97, 138, 214].",,,0,not_related
"However, the attention can still scatter in irrelevant channels as long as it attends to the most important one [11].",,,0,not_related
"To improve the faithfulness of attentions, [33, 43] regularize the hidden representations on which the attention is computed over; [17] applies attention weights on losses of pre-defined individual rationale candidates’ predictions.",,,0,not_related
"Practitioners have shown that attention-based explanations are generally not faithful (Jain and Wallace, 2019; Serrano and Smith, 2019), but that they may
be plausible (Wiegreffe and Pinter, 2019; Mohankumar et al., 2020; Vashishth et al., 2019).",,,0,not_related
"ness of explanations generated by non-transformer based models (Ross et al., 2017b; Liu and Avci, 2019; Moradi et al., 2021; Mohankumar et al., 2020; Tutek and Snajder, 2020).",,,0,not_related
"…and Pinter, 2019; Pruthi et al., 2020; Ghorbani et al., 2019) while others used auxiliary objectives to improve the faithfulness of explanations generated by non-transformer based models (Ross et al., 2017b; Liu and Avci, 2019; Moradi et al., 2021; Mohankumar et al., 2020; Tutek and Snajder, 2020).",,,0,not_related
"Yet to which extend those attention models help in interpreting how the model is operating has recently become a novel debate [3, 4].",,,0,not_related
"…Marshall, and Wallace 2016) and through different ways of human rationale integration, e.g., by learning a mapping between human rationales and machine attention (Bao et al. 2018) or ensuring the diversity among the hidden representations learned at different time steps (Mohankumar et al. 2020).",,,0,not_related
"Existing work (Bahdanau, Cho, and Bengio 2014; Mohankumar et al. 2020), however, takes human rationales as gold information that is entirely trustworthy, which is typically not the case in practice; indeed, studies from human computation have found the reliability of human-contributed rationales to…",,,0,not_related
"Prior research (Zaidan, Eisner, and Piatko 2007; Zhang, Marshall, and Wallace 2016) has shown that human rationales represent valuable input for improving model performance and for identifying explainable input features in model prediction (Bahdanau, Cho, and Bengio 2014; Mohankumar et al. 2020).",,,0,not_related
"In addition, we compare against rational-aware models: 1) LSTM-ortho and LSTM-diversity, both proposed in (Mohankumar et al. 2020).",,,1,related
"Despite that, the validity of this analysis method is a subject undergoing intense discussion and study in NLP (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Moradi et al., 2019; Mohankumar et al., 2020; Tutek and Snajder, 2020, i.a.).",,,0,not_related
"Focusing also on recurrent-encoders, Mohankumar et al. (2020) introduce a modification to recurrent encoders to reduce repetitive information across different words in the input to improve faithfulness of explanations.",,,0,not_related
", 2014), attention-based (Mohankumar et al., 2020; Tutek and Šnajder, 2020; Ghaeini et al., 2018; Lee et al., 2017), and occlusion-based (DeYoung et al.",,,0,not_related
"Such extractive explanations typically use either gradient-based (Sundararajan et al., 2017; Li et al., 2015; Denil et al., 2014), attention-based (Mohankumar et al., 2020; Tutek and Šnajder, 2020; Ghaeini et al., 2018; Lee et al., 2017), and occlusion-based (DeYoung et al., 2019; Poerner et al.,…",,,0,not_related
"For courtesy reasons, we anonymize the papers surveyed, except Paper 3 (Mohankumar et al., 2020) which was the only paper that did not exhibit the Great Misalignment Problem.",,,1,related
A recent work on interpretability of attention distributions shows that the attention distributions could be utilized in specifically accustomed DL models to provide faithful and plausible explanations for models’ predictions [19].,,,0,not_related
"If higher attention weights imply a more significant impact on the model’s predictions, then we consider attention distributions as faithful explanations, while we can also consider them as plausible explanations if they provide human-understandable justification for the model’s predictions [19].",,,0,not_related
"For better alignment, (Tutek and Šnajder, 2020) utilizes masked language model (MLM) loss and (Mohankumar et al., 2020) invents orthogonal LSTM representations.",,,0,not_related
"For better alignment, (Tutek and Šnajder, 2020) utilizes masked language model (MLM) loss and (Mohankumar et al., 2020) invents orthogonal LSTM representations.",,,0,not_related
"The above works have inspired some to find ways to make attention more faithful and/or plausible, by changing the nature of the hidden representations attention is computed over using special training objectives (e.g., Mohankumar et al., 2020; Tutek and Snajder, 2020).",,,0,not_related
Mohankumar et al. (2020) observe high similarity between the hidden representations of LSTM states and propose a diversity-driven training objective that makes the hidden representations more diverse across time steps.,,,0,not_related
"Recently, however, it has been pointed out that rank correlations often misrepresent the relationship between the two due to the noise in the order of the low rankings [46]; we concurred with this, so we used Pearson’s correlations.",,,0,not_related
"QED exists in between relatively unstructured explanation forms on the one hand, such as attention distributions (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019; Mohankumar et al., 2020) or sequential outputs (Camburu et al.",,,0,not_related
"In some views, attention in deep models can also be regarded as local interpretations [60], [61], [62].",,,0,not_related
[19] suggested an additional constraint in the learning objective to force this representation to be sparse.,,,0,not_related
"The sparsity constraint can be expressed in many different ways, which have different but marginal effects on convergence speed, or on the resulting explanation [19,13].",,,0,not_related
"Faithfulness, a widely discussed problem [19,3], focuses on whether the weight associated with a token reflects its influence on the prediction.",,,0,not_related
"Plausibility refers to the extent to which the attention map can resemble human reasoning [19,31].",,,0,not_related
", “plausibility”) on proxy tasks without real human participating [5, 50, 86].",,,0,not_related
", “plausibility”) on proxy tasks without real human participations [5, 51, 84].",,,0,not_related
"(2019) Natural Language Inference LSTM-CRF Section 4 Mohankumar et al. (2020) Sentiment Analysis, Text Classification, Natural Language Inference, Paraphrase Detection and Question Answering LSTM Sections 4, 8 and 9.",,,0,not_related
"…581
solutions to three baseline explanations methods.582 Their results show that their solutions are an im-583 provement over the baselines.584
Mohankumar et al. (2020) propose the introduc-585 tion of more diversity in the hidden states learned586 by LSTMs, allowing to observe elements…",,,1,related
"266
Mohankumar et al. (2020) investigate attention 267 on top of LSTMs (attention-LSTMs).",,,0,not_related
"Similar concepts have been investigated for neural network models [1] and various methods of human reason integration, such as learning a mapping between human rationales and machine attention [31] or assuring variety among hidden representations learned at different time steps [32].",,,0,not_related
"…equipped with attention mechanism (Bahdanau et al., 2014) for NLP tasks, researchers often regard the weights assigned by the attention layer to different parts of input text as indicators of their importance to the model prediction (Mohankumar et al., 2020; Yang et al., 2016; Wang et al., 2016).",,,0,not_related
"The conicity metric [46], C, is nothing but the mean value of ATM ∀vi ∈ V",,,0,not_related
Attention distributions have been found to offer plausible explanations of predictions of deep learning models [46].,,,0,not_related
"To address this challenge, prior studies in the software engineering domain have employed various Explainable AI approaches on transformer-based code models (Kenny and Keane, 2021; Mohankumar et al., 2020; Kobayashi et al., 2020; Liu et al., 2021).",,,0,not_related
", punctuations) frequently attract high attention [23], resulting in wrongtranslation or over-translation in NMT [13].",,,0,not_related
"Another work line aims to make attention better indicative of the inputs’ importance [23], [57].",,,0,not_related
"Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the analytical results to enhance the…",,,0,not_related
"…caused many deaths and traffic interruption Ours:
days of heavy snow in countryside left many deaths and transportation disrupted Ref:
Smith, 2019), which can be attributed to that unimportant words (e.g., punctuations) are frequently assigned with high attention weights (Mohankumar et al., 2020).",,,0,not_related
"Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the analytical results to enhance the NMT performance.",,,0,not_related
", punctuations) are frequently assigned with high attention weights (Mohankumar et al., 2020).",,,0,not_related
"Ongoing research in xAI community examines many different approaches, with the two of them being in the focus of current work: modelspecific attention-based (Mohankumar et al., 2020) and model-agnostic (Lundberg and Lee, 2017) explanations.",,,0,not_related
"The implementation used in this work has been adapted from the source code1 provided by the authors of the original paper (Mohankumar et al., 2020).",,,1,related
An interesting approach to LSTM with attention has been proposed in Mohankumar et al. (2020).,,,0,not_related
"Yet to which extend those attention models help in interpreting how the model is operating has recently become a novel debate [4, 24].",,,0,not_related
"Pluciński and Klimczak (2021) used the same approach, but also employed an orthogonalisation technique (Mohankumar et al., 2020).",,,0,not_related
"Despite the differences we found between our observations and the observations reported by [2], we still see the potential value of the methods they propose.",,,1,related
"These papers often provide results that either prove a 32 correlation between the attention weights and predictions [11][10], or the ambiguity between attention weights and the 33 performance of the rest of the model [6][7], or somewhere in between [9].",,,0,not_related
"Unlike Jain and Wallace (2019), and for the same reasons as Mohankumar et al. (2020) and Kitada and Iyatomi (2020), we conducted an experiment with Pearson’s correlation coefficient.",,,0,not_related
It is worth to notice that attention weights can have a role in starting to fostering explainability [27].,,,0,not_related
