text,label_score,label,target_predict,target_predict_label
"The third group includes one-class classification methods that assume the availability of only normal training data (Liu et al., 2021; Lv et al., 2021; Park et al., 2020).",,,0,not_related
"one-class classification (OOC) (Liu et al., 2021; Lv et al., 2021; Park et al., 2020; Xu et al., 2019): only visual data corresponding to the normal state is used as training data, and an input test video is classified as normal or abnormal based on its deviation from the learnt",,,0,not_related
"Regarding VAD, we compare AnomalyCLIP against stateof-the-art methods with different supervision setups, including one-class (Park et al., 2020; Liu et al., 2021; Lv et al., 2021), unsupervised (Zaheer et al.",,,1,related
"However, OOC methods can be particularly in-e ff ective in complex real-world applications where normal activities are diverse.",,,0,not_related
"VAD can be addressed as an out-of-distribution detection problem, i.e . one-class classification (OOC) (Liu et al., 2021; Lv et al., 2021; Park et al., 2020; Xu et al., 2019): only visual data corresponding to the normal state is used as training data, and an input test video is classified as normal or abnormal based on its deviation from the learnt normal state.",,,0,not_related
"Different memory-based auto-encoders [16, 18, 20] have been proposed to reconstruct images with features from memory bank to limit the generalization ability.",,,0,not_related
"Memory mechanism [16, 19, 20] are frequently used, memorizing the normal prototypes.",,,0,not_related
[28] proposed a memory module where prototypes of normality are stored.,,,0,not_related
"It should be pointed out that the experimental results of [48], [52], [53], and [54] are the best results obtained on our equipment according to their experimental settings, because we unfortunately did not get their official results for this dataset, since we keep the experimental equipment used unchanged and follow the optimal settings of the method and believe that",,,1,related
"[49], [50], [51], [52], [53], [54], [55].",,,0,not_related
"SOTA methods, including AE- and GAN-based methods [9], [11], [48], [50], [51], [52], [53], [54], [55].",,,0,not_related
"[10] [11] [19] [12] proposed to combine prediction with reconstruction and built a pool of prototype features for encoding normal dynamics, a few frames were also leveraged to fine-tune hyperparameters to adapt to new scenes.",,,0,not_related
Existing unsupervised or weakly supervised approaches leverage either the unpredictability of human behaviors [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] or the divergence in deep features [13] [14] [15] [16] [17] [18] [19] [20] between normal and abnormal events.,,,0,not_related
"To generalize to novel circumstances, meta learning-based methods such as [11] [12] introduced adjustable feature representations which can adapt to new domains.",,,0,not_related
"Memory-based Learning has been widely explored in computer vision [41, 72, 29, 46, 15, 21, 33, 62, 47, 69, 60, 89, 27, 51, 4, 71, 30, 19].",,,0,not_related
"[33], [34], and inpainting masked regions [15], [35], [36], were adopted to improve the reconstruction result.",,,0,not_related
"These generally train a reconstruction model, then either reconstruct the current frame or try to predict the next frame, signaling an anomaly when reconstruction error is high [10, 39, 25, 36, 54, 48].",,,0,not_related
"One commonly used proxy-task in semi-supervised VAD methods is frame reconstruction [9,10,11,12,13] in which, an unsupervised neural network is trained on normal frames, assuming that the reconstruction error would be comparatively higher for abnormal frames.",,,0,not_related
"ity is achieved by exploiting that models learn their training data’s underlying patterns, which implicitly makes them learn a notion of normality corresponding to their training data [5].",,,0,not_related
The idea is to detect anomalies by their deviation from the learned model of normality [1]–[5].,,,0,not_related
"[5], who propose a memory module, which represents a storage of latent representations of normal image frames.",,,0,not_related
"Since “prediction can be considered as a reconstruction of the future frame using previous ones” [5], the model can be adapted to predict the next frame with minimal effort using the same underlying",,,0,not_related
"To this end, we can employ many methods to generate the reference features, such as sampling key features by coreset subsampling algorithm [35], generating prototype features by memory module [28], or learning codebook features through vector quantization [65] or sparse coding techniques [55].",,,1,related
"Many previous works attempt to train AutoEncoders [6, 28, 57, 8, 17, 64], Variational AutoEncoders [23] and GANs [42, 1, 29, 40] to reconstruct the input images.",,,0,not_related
"The core idea of most unsupervised AD methods is to compare with normal samples to distinguish anomalies [46, 35, 60, 11, 28, 58].",,,0,not_related
"com/zhaojiachen1994/DIP-Anomaly-Detection methods [30], [31], which use the reconstruction errors as anomaly scores.",,,0,not_related
"Therefore, some papers [30], [31], [45] developed semi-supervised deep",,,0,not_related
"[44] proposed using a memory module with an update scheme, where items in the memory record the normal patterns of the training data.",,,0,not_related
"More recently, some works [7], [13], [44] have applied memory networks to the VAD task in surveillance videos.",,,0,not_related
"Recent researchers mainly adopt deep auto-encoders [2], [4], [24]–[28] for self-supervised learning, e.",,,0,not_related
"In previous studies, some investigators have developed anomaly detection approaches that learn only normal events [2-4] to detect abnormal events from surveillance camera movies.",,,0,not_related
"Memory-augmented AEs [8, 27] have been introduced to add a memory module for anomaly detection in image and video data.",,,0,not_related
"Considering that memory can be used to record prototypical patterns of normal data, a number of studies [8,27] proposed to augment AEs with a memory module for image or video anomaly detection.",,,0,not_related
"Due to the ability to store and retrieve important information, memory networks have been proposed and successfully applied to a wide range of domains [8, 10, 16, 27, 36, 38].",,,0,not_related
"AE is also commonly used for anomaly detection in various domains [6,8,27,41,46,47] since anomalies are generally difficult to reconstruct, and thus, they have a higher reconstruction error than normal samples.",,,0,not_related
", the first category [2], [3], [8], [23]–[25] trains the models on full video frames, the second one [4], [5], [9]– [11] works with the assistance of object detection, and the last one [6], [12] uses not only object detection but also external datasets to help improve performance.",,,0,not_related
"Due to the lack of abnormal events for supervision, the models learned by existing methods [2], [3], [8] would recklessly fit correlations between non-causal features (e.",,,0,not_related
"There are several categories of anomaly detection approaches, including dictionary learning methods [12, 13, 20, 47, 64, 84], probabilistic models [2, 3, 24, 30, 38, 51, 52, 69, 86], change detection frameworks [14, 34, 45, 55], distance-based models [33, 35, 58,59,62,67,68,71,72,75,77] and reconstruction-based approaches [22,27,28,43,44,49,53,57,63,65,76].",,,0,not_related
"To boost the performance of frame-level or cube-level methods, researchers explored the inclusion of various components, such as memory modules [27, 46, 57] or masked convolutional blocks [65].",,,0,not_related
"Thus, to better leverage the reconstruction error of AEs in anomaly detection, researchers explored a few alternatives, from the use of dummy [33] or pseudo-anomalies [5, 26] to the integration of memory modules [27,46,57].",,,0,not_related
"AutoEncoder (AE) and Variational AE have been the first and most popular models for this purpose [51, 76, 7, 50, 35, 21, 28, 40, 25, 78].",,,0,not_related
"Recently, the memory mechanism is exploited to further constrain model’s capability on reconstructing abnormal samples [21, 40, 25, 78].",,,0,not_related
"Unsupervised anomaly detection methods are trained on unlabeled anomaly-free samples, while they are tested on both normal and anomalous samples [18].",,,0,not_related
"By assuming that anomaly samples are not available during the training stage, considerable progress has been made in anomaly-free detection studies under the one-classification framework [4, 5, 30, 32], self-supervised framework [2, 8, 9, 19, 38, 42, 45], GAN-based framework [29, 34, 36, 50], and autoencoderbased framework [10, 14, 28, 53, 55].",,,0,not_related
"Moreover, the memory module is also leveraged to record prototypical patterns of normal data via update mechanisms [4], [29].",,,0,not_related
"Finally, similar to the previous work [6], [29], [30], we measure the average area under the curve (AUC) by computing the area under the receiver operating characteristic",,,1,related
"[43] presented a new update scheme of the memory module over [42], in which both normal and",,,0,not_related
"Due to the lack of published results on human-related anomalies on the UCF-Crime dataset, we implement the code of literature [11, 35, 39], where [39] takes [35] as the backbone.",,,1,related
This relies on using generative models to learn the representations of normal samples in video clips by minimising the reconstruction error [18] [28] [31] [32] [35] [42].,,,0,not_related
"Compared with anomaly detection [3, 23, 26], open-set 3D semantic segmentation (O3D) is more challenging, for it also needs to assign labels to seen-class data simultaneously, as shown in Figure 1.",,,0,not_related
"Park et al. propose augmenting a U-net style encoderdecoder (i.e., a network model for future frame prediction and reconstruction) by using a Memory Module trained to store important features of the normal mode.",,,1,related
"The function of the Memory Module[3] is primarily to assist AE in recording normal frame features, weaken the representation ability of CNN, and then achieve the purpose of distinguishing normal frames from abnormal frames.",,,0,not_related
"In order to solve this problem, there is a method of adding a Memory Module in AE.",,,0,not_related
"Subsequent improvements to the Memory Module have significantly reduced the consumption of the Memory Module, resulting in good results.",,,0,not_related
"However, prediction-based frameworks [37, 39, 40] utilize a dedicatedly designed network that directly maps a query frame into a futuristic frame.",,,0,not_related
"Generally, the judgement is based on reconstruction or prediction errors [5].",,,0,not_related
"We compare the proposed method with Conv-AE [11], ConvLSTM-AE [38], AE-Conv3D [15], Unmasking [39], Stacked RNN [21], Frame-Pred [16], MemAE [12], sRNNAE [22] and MNAD [5] on CUHK Avenue dataset in terms of",,,1,related
"The frame-level anomaly score scrFR(n + t) for the nth test is calculated by normalizing PSN R( În+t , In+t ) as follows [5], [16]:",,,1,related
[5] adopt a strategy to update the memory module even during testing.,,,0,not_related
"1) Frame-Level Anomaly Detection: We compare the framelevel AUCs of the proposed method with MPPCA [41], MPPC+SFA [35], MDT [35], Unmasking [39], Conv-AE [11], ConvLSTM-AE [38], Stacked RNN [21], Frame-Pred [16], MemAE [12], LatSp-AG [42], sRNN-AE [22], DeepOC [43], MNAD [5] and ROADMAP [17] on UCSD dataset as listed in Table III.",,,1,related
"In Table II, we compare AUCs of the proposed method with Conv-AE [11], TSC [21], Stacked RNN [21], MemAEnonSpar [12], Frame-Pred [16], MemAE [12], MNAD [5] and sRNN-AE [22] in the frame-level evaluation.",,,1,related
"trained Frame-Pred [16] without constructing dictionaries or memory, the memory-based MNAD [5], and the proposed method.",,,0,not_related
We select the memory-based MNAD [5] attaining the top AUC on UCSD Ped2 in the frame-level,,,1,related
The other hyper-parameters are set following [36]: λc=0.,,,1,related
"[36], [37], [38], [39] is mostly supposed to be an unsupervised learning problem which the model learns to describe normality",,,0,not_related
"To prove the efficiency of the method proposed in this paper, we compare our model with the SOTA model, including reconstruction-based methods such as R-VAD [45], ConvVAD [39], MEM-VAD [62], LAD [10], GMFC-VAE [63], MemAE [64], and C-VAD [65]; future frame prediction-based methods such as VEC [66], FPVAD [26], CPNet [9], ConvVRNN [67], Attention-VAD [68], D-VAD [69], and S-VAD [70]; and methods based on multiple information sources such as ASSVAD [71], MPED-RNN [72], ST-CAE [73], AnoPCN [74], and PR-AD [75].",,,1,related
"Motivated by a line of research on memory-augmented anomaly detection [8, 24], we put constraints on the learnable parameters to contrast differency of memory items.",,,1,related
"To enhance the ability to model complex distributions, the variational autoencoder [6] has improved the autoencoder, and the memory structures [4, 12] are introduced to enhance modeling of normal modes.",,,0,not_related
"Existing works mainly learn the difference between abnormal behavior and normal behavior at a local level [2, 4, 7, 10, 12], ignoring the multi-level context of abnormal snippets and failing to identify abnormal behaviors that are similar to normal behavior.",,,0,not_related
"Ano-pred [35], MNAD [38] and MPN [39] all detect anomaly by the prediction and reconstruction of the whole video frame.",,,0,not_related
"Ano-pred [35], MNAD [38] and MPN [39] all detect anomaly by the prediction and reconstruction of the whole",,,0,not_related
"With the powerful feature representation capability of CNNs, convolutional autoencoders are widely used to reconstruct training data, such as CONV-AE [36], MNAD [38] and MPN [39].",,,0,not_related
"In order to better compare different detection methods, the proposed Stage I (underwater near-vertical human detection) is combined with Ano-pred, MNAD and MPN, and modified these methods into two-stage methods.",,,0,not_related
"It is worth differentiating the definition of unsupervised VAD [13] from one-class VAD since the latter is being referred to as unsupervised in some studies [14, 15, 10, 16, 17, 18].",,,1,related
4 shows the results of video frames restored by our method compared with the output of existing reconstruction-based [33] and prediction-based methods [19].,,,1,related
"We do not employ other assists such as optical flow [4], adversarial training [19], extraction of the foreground objects [30], or memory enhancement [8, 33].",,,0,not_related
"We compared our method on itwith some existing methods, including baseline methods: f-AnoGAN [26], AE [18], VAE [27], and MNAD [28]; state-of-the-art methods: S-T [6], SPADE [15], DREAM [24], Pachcore [5], RD4AD [12], and GCAD [18].",,,1,related
"Recent autoencoder-based methods rely on accurate reconstructions of normal images and inaccurate reconstructions of anomalous images [8, 12, 20, 43].",,,0,not_related
"Generative models such as autoencoders [6, 8, 12, 20, 33, 43, 55] and GANs [2, 21, 45, 58, 59] have been used extensively for anomaly detection.",,,0,not_related
"[43] Hyunjong Park, Jongyoun Noh, and Bumsub Ham.",,,0,not_related
"To prove the effectiveness of the proposed HN-MUM, we compare it with different methods, which can be specific divided into traditional methods [30, 32, 35, 38, 45], reconstruction based methods [1, 6, 10, 12, 29, 48], and prediction based methods [3, 8, 24, 31, 46], on the publicly available datasets.",,,1,related
"This improves the quality of obtained video frame features and the accuracyof subsequent detection.ComparedwithMNAD [30], the performance of ourmethod is slightly better, despite the use ofmemory.",,,0,not_related
"The purpose of introducing memory between the encoder and decoder of AE is to resolve the limited representation of the original AE features [30], which means the detection error can be reduced by recording asmany feature prototypes through memory as possible.",,,0,not_related
"ComparedwithMNAD [30], the performance of ourmethod is slightly better, despite the use ofmemory.",,,0,not_related
"The precondition of prediction is that normal behavior is regular and predictable, but the abnormal is unpredictable [30], so the prediction error can be used to determinewhether the predicted video frame is anomalous or not.",,,0,not_related
"According to the experimental settings in the literature [10, 30, 33, 40], the network model was implemented through the deep learning framework Pytorch [49].",,,0,not_related
"[30] followed this innovation and put forward a different strategy, namely, using multiple prototypes to represent various patterns of normal video frames for unsupervised anomaly detection.",,,0,not_related
"Based on the previous work [10, 30], Peak Signal to Noise Ratio (PSNR) was applied to calculate the relationship",,,0,not_related
"We compared with the recent anomaly detection methods on three datasets, including MPPCA [50], AMDN [51], AMC [54], MNAD [30], BiTraP [57], etc.",,,1,related
"We compared with the recent anomaly detection methods on three datasets, including MPPCA [50], AMDN [51], AMC [54], MNAD [30], BiTraP [57], etc., as shown in Table 1, and
the performance of the other methods was all derived from the corresponding original papers.",,,1,related
"Existing works [2, 6, 14] often use the designed autoencoder (AE) to tackle this problem: an encoder learns to extract features from only normal training video frames, and a decoder generates the predicted target frame by using the extracted features.",,,0,not_related
"At test time, following the existing approaches for VAD [2, 6, 10], we predict framelevel anomaly scores and calculate these scores by using the Peak Signal to Noise Ratio (PSNR).",,,1,related
"However, several researchers [5, 6, 8] observe that AEs sometimes reconstruct anomalies well, indicating that the reconstruction difference between normal and abnormal data may not be discriminative enough to detect the anomalies.",,,0,not_related
[40] introduces memory modules to explicitly model the normal mode.,,,0,not_related
"The global anomaly branch adopts the method in [40], so the result is the same.",,,1,related
"This paper follows the experimental protocol in [12, 35, 40], and evaluates the performance of the model by comparing the area under the ROC curve, namely, the AUC (%) value.",,,1,related
"[40] do not notice that due to the view difference, the subtle difference between the distant objects are weakened, while those of near objects are magnified, so near objects are more likely to be located as abnormal.",,,0,not_related
"Among these approaches, memory-based models [8], [9], [10] achieve significant improvement in performance.",,,0,not_related
"techniques [1], [2], [7], [8], [9], [10] are proposed to localize",,,0,not_related
"[15], HF2VAD [13], and MNAD [24] in Table 1.",,,0,not_related
"Namely, we compare to Variational Model (VM) Steger (2001) - a handcrafted similarity measure designed for robustness to different conditions, MNAD Park et al. (2020) - an autoencoder with a memory module, f-AnoGAN Schlegl et al. (2017) - a generative model trained for the reconstruction of anomaly free images, AE / VAE Sakurada & Yairi (2014) - an autoencoder / variational autoencoder, Student Teacher (ST) Bergmann et al. (2020) - a student network aimed to give better reconstruction for the normal data, SPADE Cohen & Hoshen (2020) - a density estimation method using a pyramid of deep ResNet features, PatchCore (PCore) Roth et al. (2022) - a state-of-the-art method for structural anomalies, improving SPADE scoring function, GCAD Bergmann et al. (2022) - a reconstruction based method, based on both local and global deep ResNet features.",,,1,related
"Namely, we compare to Variational Model (VM) Steger (2001) - a handcrafted similarity measure designed for robustness to different conditions, MNAD Park et al. (2020) - an autoencoder with a memory module, f-AnoGAN Schlegl et al. (2017) - a generative model trained for the reconstruction of anomaly…",,,1,related
Hyunjong used feature loss rather than reconstruct loss to simplified the complexity of memory bank and enhanced the effect of memory items [20].,,,0,not_related
"The methods mainly include Classification-based [9, 16], Density-based [37, 51], Distance-based [45] and Reconstructionbased [11, 29].",,,0,not_related
[118] introduced an attention-based memory addressing mechanism and proposed to update the memory pool during the testing phase to ensure that the network can better represent normal events.,,,0,not_related
"Overly powerful deep networks may lead to missing anomalous events as normal due to overgeneralization [42, 118].",,,0,not_related
"With the rise of deep learning in computer vision tasks [136, 184, 191], recent approaches preferred to extracting features representations in an end-to-end framework with deep Auto-Encoders (AE) [21, 46, 87, 90, 118], Generative Adversarial Networks (GAN) [9, 17, 56, 69, 113, 188], and Vision Transformers (ViT) [36, 65, 179].",,,0,not_related
"For example, the dominant prediction-based methods [84, 87, 90, 118] in UVAD route can only give the prediction error of the current input in a single-step execution, while the Informative anomaly score needs are",,,0,not_related
"Mainstream unsupervised datasets [78, 94] and UVAD methods [14, 84, 118] only consider single-scene videos, while the real world always contains multiple scenes, which constitutes another challenge for UVAD methods.",,,0,not_related
"(Stephen and Menon 2020) follows the MemAE framework and presents a new readable and updatable memory scheme, where memory banks also update at test time.",,,0,not_related
"Furthermore, Dictionary learning or sparse coding are also prominent video anomaly detection techniques [17,18].",,,0,not_related
"We have used ShanghaiTech dataset and compared the propsoed model performance with various methods such as predictions of normal frames based on anomaly detection techniques with unsupervised learning [9,17], feature patterns based on unsupervised learning [60,61], and skeleton patterns based on unsupervised learning [62,63].",,,1,related
"In order to solve this problem, reconstructionbased methods combine memory mechanisms, image masking strategy, and pseudo-anomaly [9], [11], [15].",,,0,not_related
"Inspired by normality representation [10, 26] in the OCC methods, we encode normal patterns across all normal video sets into compact prototypes which are the centroids of normal instances.",,,1,related
"Our method achieves AUC score of 97.43% with I3D RGB features and 96.02% with C3D RGB features, outperforming existing state-of-the-art one-class classification (OCC) [11, 24, 26, 3, 47] and weakly supervised methods [48, 51, 33, 40, 46, 9, 38, 17, 29].",,,1,related
"Consistent with the results on ShanghaiTech, our method outperforms all OCC [11, 34] and weakly supervised approaches [33, 48, 52, 51, 46, 9, 38, 17, 29] by large margins.",,,1,related
"02% with C3D RGB features, outperforming existing state-of-the-art one-class classification (OCC) [11, 24, 26, 3, 47] and weakly supervised methods [48, 51, 33, 40, 46, 9, 38, 17, 29].",,,0,not_related
"However, due to a lack of prior knowledge of abnormalities, such OCC methods show relatively low performance compared to wVAD methods [46].",,,0,not_related
"Contrary to the aforementioned OCC methods, we leverage the prototypes to refine the initial noisy prediction of the MIL-based classifier.",,,1,related
"Some recent papers [10, 26] solved the problem by introducing normality prototypes.",,,0,not_related
"Our model exceeds OCC methods [31, 11] by a minimum of 47.74% in AP.",,,1,related
"It is inspired by the previous memory-based methods [32, 13, 10, 26].",,,0,not_related
"Meanwhile, One-Class Classification (OCC) approaches [11, 24, 50, 19, 10, 26, 21] focus on encoding frequently occurred patterns of normal data as a form of centroids [28] or latent vector [50] to train an one-class classifier.",,,0,not_related
"It allows the model to obtain compact decision boundaries for normal instances [10, 40, 26].",,,0,not_related
In particular MNAD [32] and MPN [26] set up for comparison in the same settings also show this trend.,,,0,not_related
"[32] propose the compact loss and separation loss to reduce the distance of the nearest feature and enhance the diversity of memory patterns, respectively.",,,0,not_related
"Moreover, the memory module remains effective and outperforms the reconstructed results on the predictive model [32].",,,0,not_related
"Instead, abnormal appearance in video frames may be reconstructed partially, or even completely [9, 32].",,,0,not_related
"Despite the specific design, conventional frame prediction-based approaches take a short sequence as input, which may leak anomalous motion patterns existing in those sequences to the synthesized future frame [17, 10, 4, 26, 32, 8], hurting the detection of abnormal motion patterns.",,,0,not_related
"This is different from previous works [9, 32, 26, 4, 19] that either uses a hard threshold to select memory keys, select only the most similar memory key or linearly combine all the keys.",,,0,not_related
"The primary comparison methods are memory-augmented models, which are MemAE [9], MNAD [32], AMMC [4] and MPN [26].",,,0,not_related
The memory size is same as MNAD [32] and MPN [26] at 10.,,,0,not_related
[32] and MPN [26] provide results for both.,,,1,related
"These approaches take prediction errors as anomaly clues and demonstrate promising performance [4, 32, 10].",,,0,not_related
"Recent algorithms can be broadly classified into reconstruction based approaches [13, 15, 26, 29, 30], which try to classify frames based on the reconstruction error, and prediction based approaches [22, 19, 7, 9], which attempt to predict a future frame, primarily by using generative adversarial networks (GANs) [14].",,,0,not_related
[Park et al. 2020] addressed this drawback by introducing feature compactness loss and feature separateness loss.,,,0,not_related
"To resolve this issue, the memory module [Gong et al. 2019; Park et al. 2020] is incorporated into reconstructionbased methods.",,,0,not_related
"Park
et al. [Park et al. 2020] addressed this drawback by introducing feature compactness loss and feature separateness loss.",,,0,not_related
"1 in the prediction method, respectively [13].",,,0,not_related
"In Figures 7–9, score1 is used to represent the model scores in literature [13], score2 is used to represent themodel scores in this paper, the label is used to represent the label of the testing videos, and the threshold is the division value of abnormal and normal behavior.",,,0,not_related
"In Figures 7–9, it is shown that the partial anomaly scores and corresponding division thresholds on the test set given by the prediction model of this paper and the literature [13] in the Ped1, Ped2, and Avenue datasets.",,,0,not_related
[13] improved the model proposed by Gong et al.,,,0,not_related
"Te loss function has 4 parts, the intensity loss Lossp [7], which constrains pixel similarity, the feature aggregation loss Lossf g and separation loss Lossf s[13], which ensure that the memory module can record typical normal behavior, and the edge loss Lossg, which Skip Connection Memory",,,0,not_related
"So, the model in this paper can more accurately divide the normal and abnormal behavior frames than the model in the literature [13].",,,0,not_related
"In order to evaluate the model performance, we visualize the refactoring error obtained by the prediction model of literature [13] and this paper.",,,1,related
6 in reconstruction and prediction methods [13].,,,0,not_related
"Since the sum of feature similarity and pixel error weights is set to 1 [13], this paper sets λ � 1 − η − φ.",,,0,not_related
"Terefore, this paper sets η around the value of literature [13] and then adjusts the value of the newly added weight φ.",,,1,related
"To further enhance its distinguishing power for diverse scenarios on different roads over time, we sparsify the attention-based query mechanism (defined in Equation (7)) with two constraints [53, 54], including a consistency loss L1 and a contrastive loss L2, denoted by:",,,1,related
"This technique has been largely employed on computer vision tasks, such as few-shot learning [51, 52] and anomaly detection [53, 54].",,,0,not_related
"For example, when the memory module is omitted, the AUC score of PA-MAE on Ped2 is 97.66% while the value of MemAE and MNAD is only 91.70% and 94.30%, respectively.",,,1,related
"Using only 10 memory items to capture normal patterns, but thanks to two new losses named feature compactness loss and separateness loss, MNAD maintains the diversity and discrimination of memory items.",,,0,not_related
The table shows that PA-MAE often outperforms MemAE and MNAD.,,,1,related
"In video anomaly detection, several memoryaugmented autoencoders are proposed to detect anomalies [9, 22], in which a memory module is built over the latent space between encoder and decoder.",,,0,not_related
"When the memory module is used, the AUC of PA-MAE is also higher than the value of MemAE and MNAD in almost cases except on Avenue where the AUC score of MNAD is slightly higher than that of PA-MAE.",,,0,not_related
The first set is to compare PA-MAE with two models (MemAE[9] and MNAD[22]) that are based on the memory network.,,,0,not_related
"To address the issue, some recent studies [9, 15, 22] have added memory modules in between the encoder and decoder to weaken the capacity of an autoencoder on abnormal events.",,,0,not_related
"Continuing the idea of applying memory, the authors in [22] proposed a newmemory model named MNAD that is more efficient than MemAE in terms of memory space and performance.",,,0,not_related
"%) score of PA-MAE and two memory augmented networks MemAE[9] and MNAD[22] on the three datasets namely Ped2, Avenue, and ShanghaiTech.",,,0,not_related
"Table 1 presents the AUC(%) score of PA-MAE and two memory augmented networks MemAE[9] and MNAD[22] on the three datasets namely Ped2, Avenue, and ShanghaiTech.",,,0,not_related
"Prediction-based methods learn to predict frames or flow maps in video clips, including inpainting intermediate frames and predicting future frames [5, 11, 12, 28, 30, 36, 48, 59, 64].",,,0,not_related
"As a consequence, prediction-based anomaly detection methods have developed rapidly during the last few years [13], [14].",,,0,not_related
Reconstruction-based methods usually use AutoEncoders (AEs) [8]–[11] or Generative Adversarial Networks (GANs) [12]–[14] to learn the distribution of normal data and then determine whether a test sample is anomalous based on how well it can be recovered.,,,0,not_related
"Other works utilize the memory mechanism to memorize normal patterns (Gong et al. 2019; Park, Noh, and Ham 2020; Liu et al. 2021; R. et al. 2021) and use metalearning (Lu et al. 2020) to enhance model’s generalization capability to the unseen normal scenarios.",,,0,not_related
"The anomaly detection approaches are typically categorized into dictionary learning methods [33–35, 40, 48], probabilistic models [32, 37, 42, 43, 63–67], distance-based approaches [7,8,18,21,44,46,49,68– 71], reconstruction-based methods [6,15,17,20,23,36,39,41, 47, 72, 73] and change detection frameworks [9, 16, 74, 75].",,,0,not_related
"To reduce the generalization capability, researchers employed various techniques, such as adding memory modules [6, 13, 17] or masked convolutional blocks [20].",,,0,not_related
"Both the clustering [12] and memory modules [13], [14] limit the generalization ability of the network from the features themselves, while lacking a more direct way to manipulate the features in anomalous regions.",,,0,not_related
"Some improvements were made on Memae and proposed MNAD [20], which uses the query features extracted by encoder and the feature vector obtained by prototype weighting to perform a splicing in the channel dimension, and then feeds the spliced features into the decoder.",,,0,not_related
1 Encoder and decoder Shared encoder: Previous studies[3-6] usually input sequential frame sequences into the convolutional encoder in a stacked manner to obtain its encoding features.,,,0,not_related
"The neural network structure of the motion decoder is as follows: In this paper, modifications are made to the decoder structure proposed by Park et al.[5], including deletion of skip-connection operation and cancellation of all operations that fuse the intermediate features of the encoder in the decoder.",,,0,not_related
"After calculating the PSNR of each frame in the test video, this paper normalized the PSNR of all frames in the test video into [0,1] following previous work[3-7][9][13], and used the following formula to calculate the normal score of each frame: ˆ ( , ) min ( ) 1 max min PSNR t t PSNR S t PSNR PSNR     (12)",,,1,related
The neural network structure of the appearance decoder is as follows: This paper follows the decoder structure proposed by Park et al.[5] and changes its input channel number to 512.,,,0,not_related
"In recent years, many researchers[3-13] have trained neural network models describing normal events by using convolutional autoencoders to reconstruct or predict future frames without abnormal samples, and then judge whether anomalies occur in the test phase according to the differences between reconstructed frames and ground truth.",,,0,not_related
"The encoded feature 4 ( )af f of the fourth frame image, the appearance decoder ( )aE  , the
underlying feature cf input in the skip-connection structure and the reconstructed fourth frame 4̂I have the following relationship:
4 4̂ and ( , )a a a cf f I E f f  (3)
The neural network structure of the appearance decoder is as follows: This paper follows the decoder structure proposed by Park et al.[5] and changes its input channel number to 512.",,,0,not_related
"The neural network structure of the shared encoder is as follows: In this paper, the encoder structure proposed by Park et al.[5] is modified as follows: (1) The number of input channels of the encoder is changed to 3, so that the encoder encodes a frame image with the number of channels 3 separately each time; (2) BatchNorm layer and ReLU layer are added to the last convolutional layer of the encoder, which can ensure that all feature representations finally obtained have similar distribution and ensure that subsequent incremental calculations are consistent; (3) The output of the intermediate process is retained, but only the output of the intermediate process of the fourth frame is retained, so as to help the appearance decoder rebuild the fourth frame by integrating the underlying features of the encoder with the structure of skip-connection.",,,0,not_related
"4 Normality Score In this paper, the PSNR is used to measure the quality of the predicted image[3-5], and the calculation formula of PSNR is shown as follows:",,,0,not_related
"The relationship between mf , motion
decoder ( )mE  , and predicted increment diagram Î generated by the incremental fusion prediction module is as follows:
ˆ ( )m mI E f  (4)
The neural network structure of the motion decoder is as follows: In this paper, modifications are made to the decoder structure proposed by Park et al.[5], including deletion of skip-connection operation and cancellation of all operations that fuse the intermediate features of the encoder in the decoder.",,,1,related
"For continuous frame sequences 1I , 2I , 3I and 4I , the shared encoder is  sE  , and the following relation exists:
Proc. of SPIE Vol. 12454 124540D-2
( ) 1, 2,3,4i s if E I i  (2)
The neural network structure of the shared encoder is as follows: In this paper, the encoder structure proposed by Park et al.[5] is modified as follows: (1) The number of input channels of the encoder is changed to 3, so that the encoder encodes a frame image with the number of channels 3 separately each time; (2) BatchNorm layer and ReLU layer are added to the last convolutional layer of the encoder, which can ensure that all feature representations finally obtained have similar distribution and ensure that subsequent incremental calculations are consistent; (3) The output of the intermediate process is retained, but only the output of the intermediate process of the fourth frame is retained, so as to help the appearance decoder rebuild the fourth frame by integrating the underlying features of the encoder with the structure of skip-connection.",,,1,related
"[17] to introduce a memory enhancement module to learn a limited number of archetypal features that can best represent normal samples, which record various normal behavior patterns and store them in memory.",,,0,not_related
"Indeed, while we learn the memory through contrastive learning, MemAE and others [21, 42] learned it via the pixel-wise reconstruction loss.",,,0,not_related
"Some works used memory in the latent space of an auto-encoder [21, 42] for AD.",,,0,not_related
"Unlike previous memory bank equipped methods [21, 42], our normal memory layers cover the normal class at multiple scales and not only improve anomaly detection but also the quality of the learned representations.",,,0,not_related
"Compared to previous memory bank equipped anomaly detectors [4, 21, 23, 42], our model is the first to memorize the normal class at several scales allowing it to be more robust to anomaly sizes.",,,0,not_related
"Unlike MemAE [19] and LMCNet [29], we utilize a memory module with new update and query schemes in prototype branch inspired by MNAD [15] to avoid excessive restrictions on representing normal patterns.",,,1,related
"Besides, many prediction-based methods [15], [25] tend to record global normal patterns while ignoring local spatiotemporal representation.",,,0,not_related
"In contrast, some prediction-based methods [15], [25] overfocus on global normality.",,,0,not_related
Prediction-based methods [15]–[17] typically take frame sequences consisting of several consecutive frames as inputs to generate future frames.,,,0,not_related
"In prediction-based VAD methods [15], [17], [22], [23], [25], [35], [36], the models usually take a number of consecutive video frames as input to generate the next frame.",,,0,not_related
"In recent years, many reconstruction-based VAD methods have been proposed [15], [18], [19], [21], [31], [32].",,,0,not_related
"signed based on autoencoder [15], [18], [19].",,,1,related
"However, the powerful prediction ability also allow them to predict some anomalies instances well and cause missed detections [15].",,,0,not_related
We design the prototype branch to memorize the prototype features with a memory module from whole training videos inspired by MNAD [15] and consider the prototypes as global normality.,,,1,related
"[15] devised a new memory module for VAD model to record normal prototype features, which can represent more diverse patterns than the memory module in MemAE [19].",,,0,not_related
"tion [54], image colorization [55], anomaly detection [56], [57] and video-based person ReID [53].",,,0,not_related
"At test-time, reconstruction is expected to be degraded for OOD samples, allowing for their detection [4, 5].",,,0,not_related
"Several VAD approaches [1, 10, 22, 27, 29, 30, 39, 46] employ Autoencoders (AEs), Generative Adversarial Nets (GANs) and their variants under the assumption that the models that are explicitly trained on normal data may not be successful to reconstruct abnormal event as such samples are usually absent in the training set.",,,0,not_related
"[9], [10] try to maintain the prototypes of the basic feature in the normal frames and reconstruct frames with them.",,,0,not_related
"Anomaly detection pretext tasks: Recently, deep learning approaches [8, 7, 11, 15, 14, 9, 21, 27, 23, 24, 3, 34, 32, 20, 12] have shown their effectiveness for detecting abnormal events in videos.",,,0,not_related
"Self-supervised methods [8, 21, 9, 20, 33, 10, 2, 16, 18, 14, 7, 27, 31, 15, 3, 32] use some pretext tasks to learn normal appearance and motion features from training data.",,,0,not_related
"Many previous methods used reconstruction to learn normality [8, 9, 10, 21, 20, 33, 2, 16].",,,0,not_related
An extension of this approach was proposed by [21] which learns spatio-temporal patch prototypes.,,,0,not_related
"A widely used self-supervised approach for anomaly detection consists in reconstructing normal samples from a low dimensional representations [8, 21, 9, 20, 33, 10, 2, 16, 18].",,,0,not_related
"Reconstruction-based or predictionbased methods [4], [11], [25] detect anomalies based on the reconstruction error or prediction error between testing data as ground truth and representations from the normal data.",,,0,not_related
[25] Proposed a memory module that can update during the detection stage.,,,0,not_related
"proposed the Memory networks [31], memory methods have been developed and enhanced, It has also been extensively used and exhibited effect in video abnormal event detection tasks [11], [20], [25].",,,0,not_related
"However, the reconstruction quality of autoencoder is unstable in practice since autoencoder has strong generalization ability and the anomalous regions cannot be eliminated in the reconstructed image [8].",,,0,not_related
"As such, AE has been widely applied for anomaly detection in various areas [46, 47, 48, 49].",,,0,not_related
"Due to the powerful representation capability of DNN, the AE trained by normal signals with diverse patterns tends to generalize well and may reconstruct abnormal signals with small errors [46, 47].",,,0,not_related
"1(a), the MadeGAN is designed by integrating semi-supervised learning based on memory-augmented deep auto-encoder (MemAE) [46, 47] with adversarial training, which will be detailed in the following subsections.",,,0,not_related
"To mitigate the drawback of traditional AEs, we leverage a memory module to recognize the diversity of normal patterns and reduce the generalizability of AE to abnormal signals as inspired by the work from [46, 47].",,,1,related
"Examples of such methods include AE-SSIM [27], MemAE [18], MGNAD [19], and DAAD [28], which mainly employ image reconstruction techniques.",,,0,not_related
"To prevent the recovery of abnormal images, some studies [18], [19], [28], [32] introduce memory modules that store normal features.",,,0,not_related
"It is more likely to avoid confusion over abnormal samples with plain representations [8]; ii) reconstruct-based: Reconstruct methods can be divided into AE-baesd [2, 10, 11] and GAN-based [9, 12, 13].",,,0,not_related
"For comparison, we apply MNAD-P w/o Mem [30] as our baseline to learn the semantic pool for video anomaly prediction and obtain the results of 71.3% on UCSD Ped2 [18] and 67.5% on CUHK Avenue [24].",,,1,related
"For comparison, we apply MNAD-P w/o Mem [30] as our baseline to learn the semantic pool for video anomaly prediction and obtain the results of 71.",,,1,related
"Although our simple anomaly detection model is outperformed by MNAD-P on CUHK Avenue [24] and some other methods on UCSD Ped2 [18] in video anomaly detection, such methods use a few additional modules, such as 3D CNNs, memory guiders, optical flow estimators, or some other constraints, which lead to more training time and computational cost, while ours just consists of our CSE and a simple decoder.",,,0,not_related
Table 1: Quantitative comparison with the state of the art for anomaly detection and MNAD-P w/o Mem [30] for anomaly prediction.,,,0,not_related
"Compared to theMNAD-P w/o Mem [30], our method achieves better performance by replacing the encoder with the proposed CSE.",,,1,related
"• We also combine the proposed CSE and a simple decoder as an AE, similar to MNAD-P w/o Mem [30], to detect anomalies, which exhibits strong competitiveness with low complexity and small computational cost.",,,1,related
"Taking advantage of the unpredictable characteristics of abnormal frames, prediction-based methods [3, 17, 21, 25, 30] are proposed, which first use the previous frames to predict the current frame, and then calculate the prediction error to quantify the extent of abnormalities of the current frame.",,,0,not_related
[30] employed a different update strategy and presented a more compact memory considering the diversity of normal patterns.,,,0,not_related
"In addition, we also give the results of the video anomaly detection task through the channel-selected shift encoder (CSE) and decoder, similar toMNAD-Pw/oMem [30].",,,1,related
"Note that EPAP-Netv0 is our baseline, MNAD-P w/o Mem [30].",,,1,related
"Figure 8: What is video anomaly prediction? Existing methods like MNAD-P [30] can only detect anomalies in the frame 174 or 561, but our EPAP-Net can make a judgment on the future frame 175 or 562, which has not happened yet.",,,0,not_related
"[36] imitated and improved the memory module, which can be updated in both training and testing phases, and introduced a compactness loss and a separateness loss to make the stored memory items more sparse and robust.",,,0,not_related
"Compared to previous methods [1, 12, 36] which memorize the features of the whole video frame, RCM-MemAE makes the stored prototypes more compact and also improves the separability between prototypes.",,,0,not_related
"Among them, MPPCA [21], MDT [34] are handcrafted feature-basedmethods; Conv-AE [16], ConvLSTM-AE [30], MemAE [12], Cluster-AE [4] and MNAD-R [36] are reconstruction-based methods; Frame-Pred [27], Attention-Prediction [53], MNAD-P [36], AMMC-Net [1] and VEC [45] are prediction-based methods; Table 1: Frame-level video anomaly detection comparison",,,0,not_related
"Memory augmented networks are now widely used in many video analysis tasks, such as video summarization [23], video prediction [22] and video anomaly detection [12, 36].",,,0,not_related
"Among them, MPPCA [21], MDT [34] are handcrafted feature-basedmethods; Conv-AE [16], ConvLSTM-AE [30], MemAE [12], Cluster-AE [4] and MNAD-R [36] are reconstruction-based methods; Frame-Pred [27], Attention-Prediction [53], MNAD-P [36], AMMC-Net [1] and VEC [45] are prediction-based methods;
ST-CAE [50], Reconstruction&Prediction[40] and AnoPCN [44] are hybrid methods.",,,0,not_related
"There are other works [18, 36, 45] that have verified that prediction-based methods yield to better anomaly detection performance than reconstruction methods when analyzing moving objects.",,,0,not_related
"HSNBM also shows superior results compared to other methods that incorporate memory modules [1, 12, 36], because HSNBM considers more local characteristics of the scene when storing memory prototypes, while adding more dimensional information to the scene normality pattern through region segmentation.",,,0,not_related
"To overcome this problem, memory-enhanced autoencoder (MemAE) carrying memory bank has been proposed [1, 12, 36] to improve the model’s ability to discriminate abnormal frames during inference by storing the patterns extracted from normal frames.",,,0,not_related
"Following the popular evaluation settings in the video anomaly detection community [8, 11, 17, 27, 32, 36], we report the area under the receiver operating characteristics curve (AUC) to evaluate the performance of the proposed framework.",,,1,related
"At present, many methods [28, 36] will insert one or more memory modules into the bottleneck of the autoencoder to form a memory autoencoder (MemAE) to reduce the representation ability of the network.",,,0,not_related
"The majority of existing methods [7, 23, 26, 29, 32, 34, 35, 38] rely on pixel-based features that are learned from complete frames, image patches, or regions of interest.",,,0,not_related
"To address thia issue, several works [8, 23, 26] introduced a memory module into deep AE.",,,0,not_related
"Compared with the pixel-based models Frame Pred [16], CT-D2GAN [7], AnoPCN [34], CAC [32], MNAD [26], F2PN [21] andAMMC-Net [2], our STGformer obtains significant performance improvements, since our model can reduce the negative influences of background noises.",,,0,not_related
"Among them, Frame Pred [16], CT-D2GAN [7], AnoPCN [34], CAC [32], MNAD [26], F2PN [21] and AMMC-Net [2] are pixelbased methods; while MPED-RNN [25], ST-GCAE [24], MTS [28], PoseCVAE [12], Normal Graph [20], HSTGCNN [39] andMoPRL [37] are pose-based models.",,,0,not_related
"Consequently, they often capture insufficient high-level semantic features, which hinders them from discriminating abnormal events [22, 51, 68].",,,0,not_related
"To improve the quality and discriminative power of low-level learning, various networks or techniques have been explored such as convolutional auto-encoder [24], U-Net [37], adversarial learning [21, 57, 74], memory module [22, 39, 51], foreground localization [73, 80] and transformer [18].",,,0,not_related
"These approaches hold the hypothesis that anomalous regions are hard to be reconstructed since the model trained on normal samples only [3, 12, 27, 39, 40, 50].",,,0,not_related
"Following [26, 29, 40], a prototype diversity loss is defined to ensure the diversity of learned prototypes",,,0,not_related
"Similar to [26, 29], we adopt two memory learning losses, i.",,,0,not_related
"The backbone of our framework is a U-Net model [25], from which we remove the ReLU activation functions of the encoder and decoder since they restrict the partial feature representation [14].",,,1,related
"The reconstruction method assumes that the reconstruction errors in anomalous regions are significant, so some researchers suggest using memory to limit the reconstruction effect of abnormal areas [13] [14] [16].",,,0,not_related
"This issue can be addressed by recently introduced memory reconstruction strategies, including Learning Memory-guided Normality for Anomaly Detection (MNAD) [14] and Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection (MemAE) [16].",,,0,not_related
"Inspired by the MNAD reconstruction method [14], Our approach adds an encoder II to the network structure, retaining skip connections and batch normalization of per-layer convolution, while employing a novel memory update policy.",,,1,related
"This paper defines a joint loss function comprising three losses [14]: reconstruction, feature compactness and feature separation loss, denoted as Lrec, Lcomp, and Lsepa, respectively, where α and β adjust the balance of the loss function, as follows:",,,0,not_related
"[15], AMMC-Net [16], and FastAno [26] leverage the frame prediction approach as we adopted.",,,0,not_related
"Frame prediction methods [15], [16], [22] share the same principle as the reconstruction methods, except that they are designed to reconstruct a future frame from a series of past frames.",,,0,not_related
"MNAD [36] A, P, S Added a memory module to record prototypical patterns of normal data in memory items, training it with compactness and separateness losses.",,,0,not_related
[36] proposed a memory module that updates items in the memory while assuring that these represent prototypical patterns of normal data.,,,0,not_related
"To reinforce above assumption, Gong et al. (MemAE) [9] and Park et al. (LMN) [10] introduced a memory mechanism for recording normal patterns among training data.",,,0,not_related
(LMN) [10] introduced a memory mechanism for recording normal patterns among training data.,,,0,not_related
"larger than the ones of normal samples, some methods [15], [16], [27], [29] combine AE and the traditional memory",,,0,not_related
"3) Evaluation: Following [27], [41], the area under the curve (AUC) of the receiver operating characteristic (ROC) of the image level and pixel level are used to measure the performance of model.",,,0,not_related
"Recently, Some existing works introduce memory bank to computer vision tasks [50], [51], such as fake video detection [52] and anomaly detection [27], [29].",,,0,not_related
[27] proposes a combination network of AE and memory module for video anomaly detection.,,,0,not_related
"However, the powerful reconstruction ability of AE is not beneficial for visual anomaly detection [15], [27].",,,0,not_related
"Moreover, Existing methods [15], [27] adopt a scheme, which concatenates the outputs of last encoder and the reads of the memory module as the input of the decoder.",,,0,not_related
"As shown in Figure 1 (1) and Figure 1 (2), the query generated by the traditional method is a logical reorganization of features at the same location but from different channels of the original feature map [16], [27].",,,0,not_related
"3) Evaluation: Following [27], [41], the area under the curve (AUC) of the receiver operating characteristic (ROC)",,,0,not_related
The work [12] further improves the original memory module and introduces a new loss function to improve the learning of the memory module.,,,0,not_related
"However, anomalous events are incidental and diverse resulting in the impracticality of collecting the sufficient anomalous samples [11], [12].",,,0,not_related
"Some methods are proposed [11], [12], [17], [29] to reduce the reconstruction ability of AE by introducing memory networks.",,,0,not_related
"Our method demonstrates the capability of detecting different anomalous events and even presents better performance than memory-based methods, such as MemAE and MNAD, that are specially designed to deal with various anomalies.",,,0,not_related
7) Memory-Guided Normality for Anomaly Detection (MNAD) [74]: MNAD uses a memory module to record multiple prototypes that represent diverse representations of normalities for unsupervised anomaly detection.,,,0,not_related
"In this article, we equip it on the MNAD [74] model, which is still denoted SSPCAB.",,,1,related
"Specifically, in the highway scene, our method presents a better performance of detecting different anomalies than memory-based methods, i.e., MemAE and MNAD, which are specially designed to deal with various anomalies.",,,0,not_related
7) Memory-Guided Normality for Anomaly Detection (MNAD) [74]: MNAD uses a memory module to record,,,0,not_related
"For anomaly detection in aerial videos, comparing with the commonly used reconstruction-based framework [31], [66], [67], [68], [69], [70], [71], [72], [73], [74] where target values are equal to the inputs, it is more natural to predict the",,,0,not_related
"our prediction-based framework with a commonly used reconstruction-based methodology [31], [66], [67], [68], [69], [70], [71], [72], [73], [74].",,,0,not_related
"Memory-based and GAN-based
methods, namely, Skip-GAN, MemAE, and MNAD, show superior performance in this scene.",,,0,not_related
"In recent years, many methods for video anomaly detection [23, 34, 43] have been proposed with great success.",,,0,not_related
"Anomaly detection in literature can be categorized into 3 groups; which are memory-based methods [12], one-class classification methods [16] and current",,,0,not_related
"AE [6,13,16,25] and GAN [26,29,39] are intuitive choices of reconstruction models.",,,0,not_related
[25] introduce a memory module to select the most similar embedding in embedding storage of normal samples to restrict the generalization on anomalies.,,,0,not_related
"Among them are (1) reconstruction-based approaches such as Conv-AE [29], TSC [15], StackRNN [15], MemAE [12], MNAD [9], etc.; (2) prediction-based methods: Frame-Pred [8], PGDLE [10], IPR [37].",,,0,not_related
"Among them are (1) reconstruction-based approaches such as Conv-AE [29], TSC [15], StackRNN [15], MemAE [12], MNAD [9], etc.",,,0,not_related
[9] employed the same network structure as Gong et al.,,,0,not_related
"As revealed by the existing works [8, 9, 42], Peak Signal to Noise Ratio (PSNR) is a more accurate method to determine the picture’s grade, shown as following:",,,0,not_related
"In compared to the method MNAD [9] with a memory module, its performance illustrated by the experimental outcomes on 3 datasets has been elevated, whether it is a reconstruction task or a prediction task.",,,0,not_related
"[9], developed a hybrid framework integrating optical flow reconstruction and frame prediction to carry out video anomaly detection.",,,0,not_related
"[56] 32 MemAE [12] 38 PGDLE [10] 42 MNAD [9] 67 Ours 46
1 3",,,0,not_related
[9] adopted multiple prototypes to represent different models of normal video frames.,,,0,not_related
"Future frame prediction is another prevalent video anomaly detection (VAD) methodology, which commonly achieves a better anomaly detection accuracy than reconstruction-based methods [9, 34, 35].",,,0,not_related
"Notably, prediction may be regarded of as a reconstruction of the future frame based on prior frames [9].",,,0,not_related
"Following previous work [3], [19], The PSNR of each test frame is normalized to the range [0, 1] and the following formula calculates the regular score of each frame:",,,0,not_related
"The prediction-based VAD methods [3], [18], [19] consider the instances with lower prediction errors as normal while considering unpredictable events as abnormal.",,,0,not_related
"However, we evaluate the quality of predicted frames utilizing the Peak Signal to Noise Ratio (PSNR), which has proven to be a more effective measure [19].",,,0,not_related
"Similarly, LMN [11] proposes to manually update the memory module, along with a triplet loss to better reflect normal behavior patterns.",,,0,not_related
[11] introduced memory into autoencoders in video anomaly detection.,,,0,not_related
"It is worth pointing out that training only on normal data is often claimed as unsupervision like [11], [13], [30] recently.",,,0,not_related
"[17] integrated a memory network with an encoder-decoder scheme, where components in the memory network record prototypical",,,0,not_related
"As we can see, ST-AE and MNAD score lower in AUC than the proposed method, although they achieve a high value on public datasets such as UCSD [40] and ShanghaiTech [41].",,,1,related
One is a baseline spatial-temporal AE network (ST-AE) [12] and the other is the state-of-the-art video anomaly detection based on memory-guided normality for anomaly detection (MNAD) [17].,,,0,not_related
"If there are multiple attentions, the output of multihead cross‐attention is shown in Equation (12).",,,0,not_related
"⋯ ( ) M MCA = CA + CA + + CA / , i i i i 1 2 (12)",,,1,related
"(2) A dictionary is trained where patterns of normal events are recorded based on their high-level semantic features, and the score of abnormality is calculated with the help of the dictionary [23, 29, 33].",,,0,not_related
"Compared with real-time models, TSSTGM is still a little lower than P w/Mem model [23].",,,0,not_related
"The comparison approaches include FSCN [29], Conv-AE [7], STAE [3], P w/Mem [23], ConvLSTM-AE [18], STAE-grayscale [34], STAE-optflow [34], Optical flow-GAN [25], ST-CaAE [14], STAN [13] and the methods proposed by Gao et al.",,,0,not_related
"[23] propose a model, called P w/Mem, using a memory module with an update scheme where items in the memory record prototypical patterns of normal data.",,,0,not_related
"Either separately training the feature extraction and clustering steps [48,13,14,41,32], or jointly training both steps end-to-end [1,11,34,3,27,2].",,,0,not_related
"[31] both proposed to record prototypes of normal data by inserting a memory bank into AE, which can enhance the ability of AE to model normal behavior patterns.",,,0,not_related
"In addition, in order to make prototypes have the characteristics of compactness and diversity, we follow the work [31] using feature compaction loss and feature separation loss to constrain prototypes.",,,1,related
"[31] followed this trend and presented an anomaly detection method that uses multiple prototypes to consider various patterns of normal data, which can obtain more compact and sparse memory bank.",,,0,not_related
"In addition, MemAE [11], MNAD [31], and MPN [27], are most-related methods to our approach.",,,0,not_related
[31] have enhanced the ability of AE to model normal frames by establishing a memory bank.,,,0,not_related
"Compared with previous memory bank based methods [11,31], DLAN can automatically learn prototypes online without additional memory consumptions.",,,0,not_related
"The use of memory mechanisms [16, 43] or multi-modal data (e.",,,0,not_related
"To address this, some follow-up studies attempt to boost the accuracy through incorporating memory modules [16, 43], modeling optical flows [29], redesigning specific architectures [58], etc.",,,0,not_related
"In previous work, researchers define video content patterns that are different from usual patterns are anomalies [1-5].",,,0,not_related
There mainly are two methods: 1) unsupervised learning methods [1-5] train the detection model with only normal samples and the different patterns in test part are considered as anomalies.,,,0,not_related
Gong et al. (MemAE) [4] and Park et al. (LMN) [5] introduce a memory bank into the AE for anomaly detection.,,,0,not_related
2) Weakly supervised learning methods [5-10] distinguish abnormal and normal events with video-level annotations.,,,0,not_related
(LMN) [5] introduce a memory bank into the AE for anomaly detection.,,,0,not_related
"VAD is an important yet challenge task, which has been researched for decades [1-10].",,,0,not_related
"The reconstruction error defines the novelty score [40,15,59]: GAN and flow-based invertible models have been exploited for this purpose [71,80,51].",,,0,not_related
"Different from previous work [10], considering the diversity of normal events, only part of the query vectors are written into the memory pool instead of all, which helps the pool to record general patterns of normal events.",,,0,not_related
"However, most existing methods [3, 4, 10] do not consider the difference of spatial and temporal features in isolation.",,,0,not_related
"Follow previous works [3, 10], e is defined as the peak signal-to-noise ratio between the predicted frames Ît and the ground truth It, as follows:",,,0,not_related
"Since most surveillance videos are normal, some previous works [2][3][4][5] treat anomaly detection as an unsupervised learning task.",,,0,not_related
"To detect whether a frame is anomalous or not, we use PSNR to calculate the abnormal score [2] [3] [5] [7] [13].",,,1,related
"According to the previous work [1] [5] [13], we calculate the Area Under Curve(AUC) of the Receiver Operation Characteristic(ROC), and compare it with hand-crafted based method [15] [20], prediction-based method [5], convolution LSTM based method [16] [17] [21], memory-based method [2] [13], and two-stream based method [1] [3] [4] [7].",,,1,related
"We compare our method with well-designed state-of-the-art methods including Stacked RNN [36], ConvAE [14], AE [37], MemAE [37], and MMNP [38].",,,1,related
The former uses only normal videos to train a deep model to understand the archetypal space-time patterns of normal occurrences through the solution of a proxy problem like reconstructing [13] or predicting video frames [14].,,,0,not_related
[14] proposed an encoder-decoder with a learned memory module to predict future frames.,,,0,not_related
"…gains with respect to SSMTL (Georgescu et al., 2021), while also attaining superior results compared to other recent state-of-the-art methods (Astrid et al., 2021a,b; Bertasius et al., 2021; Chang et al., 2022; Dong et al., 2020; Doshi and Yilmaz, 2020a,b; Georgescu et al., 2022; Gong et…",,,0,not_related
"…Cheng et al., 2015; Feng et al., 2017; Hinami et al., 2017; Kim and Grauman, 2009; Mehran et al., 2009; Wu et al., 2010), reconstruction-based (Astrid et al., 2021a; Gong et al., 2019; Hasan et al., 2016; Huang et al., 2022a,b,c; Luo et al., 2017, 2022; Nguyen and Meunier, 2019; Park et al.,…",,,0,not_related
"In addition, we note that our approach is different from the method proposed by Astrid et al. (2021a), since this related method aims to reconstruct unmodified frames from pseudo-abnormal frames without changing the learning procedure, i.e. without reversing the gradients, as we do.",,,1,related
"1* Corresponding author Various methods related to the unsupervised anomaly detection [2, 3, 4, 5] have been explored.",,,0,not_related
"Also, in [18], they introduced a memory module with items that capture prototypical models of the inlier class with a new update system.",,,0,not_related
"methods include Conv-AE[4], 3D-Conv[35], MemAE[1], and MNAD-R[6], while MNAD-P[6], AMMC-Net[8], FramePred[5], VEC[7], C2-D2GAN[3], and Transanomaly[14], are TABLE IV AUROC PERFORMANCE (%) COMPARISON WITH PREVIOUS WORKS ON UCSD PED2, CUHK AVENUE AND SHTECH DATASETS",,,0,not_related
"First, reconstruction-based methods include Conv-AE[4], 3D-Conv[35], MemAE[1], and MNAD-R[6], while MNAD-P[6], AMMC-Net[8], FramePred[5], VEC[7], C2-D2GAN[3], and Transanomaly[14], are
prediction-based methods and HF2-VAD[2] is the hybrid method.",,,0,not_related
"with a memory module[1], [6], [8] have been extensively studied.",,,0,not_related
"To alleviate this generalizing issue, an AE with a memory module[1], [6], [8], has been proposed in recent years.",,,0,not_related
"Dataset Future Frame Prediction [79] MLEP [78] MNAD [114]
Street Scene [123] 3130 48000* 37 Subway Entrance [1] 1000 1000* 43 UCF-Crime [150] 2105 10000* 2
*: In MLEP, the training process gets randomly video snippets as input, so we can only get the number of iterations needed to obtain the result.",,,1,related
"From the results is presented in Table 10, it is clear that almost all of the state-of-the-art approaches primarily focused on unsupervised or weakly-supervised deep learning are becoming increasingly pervasive on video tasks, specifically anomaly analysis tasks because of the overall performance of these methods outperforming all traditional methods on two different public benchmark datasets by a large margin such as Future Frame Prediction [79], MLEP [78], Memoryguided Normality [114].",,,0,not_related
"Street Scene [123] Subway Entrance [1] UCF-Crime [150] Method
AUC EER AUC EER AUC EER
Future Frame Prediction [79] 56.53 46.14 71.72 32.22 66.53 38.67 MLEP [78] 53.46 30.49 77.30 46.63 55.08 47.05 MNAD [114] 57.25 44.36 69.37 35.69 65.53 39.69
number of epochs and iterations with the best result of each method showed in Table 12.",,,1,related
"Notable unsupervised learning methods for anomaly analysis in videos [79, 100, 114, 182].",,,0,not_related
"In this section, the goal of our experiments is to evaluate three benchmark methods including Future Frame Prediction [79], MLEP [78] and MNAD [114] on three most commonly used datasets namely Street Scene [123], Subway Entrance [1], and UCF-Crime [150] in terms of EER and ROC-AUC metrics at frame-level to evaluate these results of these methods performed on diferent anomalous video datasets.",,,0,not_related
"In this section, the goal of our experiments is to evaluate three benchmark methods including Future Frame Prediction [79], MLEP [78], and MNAD [114] on three most commonly used datasets, namely, Street Scene [123], Subway Entrance [1], and UCF-Crime [150] in terms of EER and ROC-AUC metrics at frame-level to evaluate these results of these methods performed on different anomalous video datasets.",,,0,not_related
"Table 11 shows percentages of three prominent methods namely Future Frame Prediction [79], MLEP [78] and MNAD [114] on three abnormal video datasets in terms of AUC and EER metrics at frame-level.",,,0,not_related
Dataset Future Frame Prediction [79] MLEP [78] MNAD [114],,,0,not_related
"Table 11 shows percentages of three prominent methods, namely, Future Frame Prediction [79], MLEP [78], and MNAD [114] on three abnormal video datasets in terms of AUC and EER metrics at frame-level.",,,0,not_related
"With regard to the AUC metric, the number of MNAD method is the highest igure of the three well-known methods on the Street Scene dataset at 57.25%, however, the igure of this method on the Subway Entrance dataset is the lowest at 69.37%.",,,0,not_related
"Furthermore, the MNAD method is also slightly lower than that of the Future frame prediction method on the UCF-Crime dataset at 65.53% compared to at 66.53%.",,,0,not_related
", structural [53] or semantic [39, 46]), memory mechanism [19, 20, 29], iteration mechanism [12], image masking strategy [47], and pseudo-anomaly [9, 32].",,,0,not_related
Anomaly Detection aims to detect unusual samples in data.,,,0,not_related
"Deep Anomaly Detection [62, 63, 56, 36, 48, 3, 76, 51, 5, 16, 53, 54, 55, 20] takes the advantage of DNNs to have better scalability and performance on high dimensional data.",,,0,not_related
"They were adopted in several tasks such as object tracking [14, 43], anomaly detection [16,36], predictive learning [19,28,29], and few-shot learning [8, 23, 45].",,,0,not_related
"Hence, video anomaly detection has been attracting an increasing amount of research interest, with most of the recent approaches heavily dependent on end-to-end trained complex deep learning based approaches [11, 32, 43].",,,0,not_related
"Recent algorithms can be broadly classified into reconstruction based approaches [15, 17, 36, 41, 43], which try to classify frames based on the reconstruction error, and prediction based approaches [8, 11, 29, 32], which attempt to predict a future frame, primarily by using generative adversarial networks (GANs) [16].",,,0,not_related
"A common technique used by several recent works [22, 32, 40, 43] is to normalize the computed statistic for each test video independently, including the ShanghaiTech dataset.",,,0,not_related
"The results show that the DPU is more memory-efficient than the memory module in previous work [17], [18].",,,0,not_related
"UCSDPed1 is relatively poor, whilst for CUHK Avenue, the AUC is better than most methods, except FlowNet-Unet-GAN [14], MemAE [17], LMN [18], MPD [19].",,,0,not_related
"Performance on UCSDPed1 is relatively poor, whilst for CUHK Avenue, the AUC is better than most methods, except FlowNet-Unet-GAN [14], MemAE [17], LMN [18], MPD [19].",,,0,not_related
"We achieve 75 fps for anomaly detection with a GeForce GTX TITAN X, faster than other state of the art methods with the same setting [18].",,,1,related
"As in previous work [18], the DPU inputs the encoding feature maps, which are outputs of the encoder part of U-net, to generate a pool of dynamic prototypes.",,,0,not_related
"However, MemAE [17], LMN [18] and MPD [19] have more parameters than our models which is shown in table III.",,,0,not_related
"A memory module is proposed into the AE to address these problems [17], [18].",,,0,not_related
"Since normal patterns in training and testing sets may be different, the memory items are updated during training and testing time, with the use of a predefined threshold to prevent updating on anomaly patterns [18].",,,0,not_related
"[4], [5], [9], [10], [11] or a prediction task [2], [12], [13], [14].",,,0,not_related
"By weakening the strong generalization ability with a new memory module and maximizing the margin between abnormal and normal samples via few-shot learning, it makes the model more sensitive to abnormal behaviors and improves the model’s discrimination ability, compared with both prediction-based models(4,28) and memory-based models.(3,9) By observing the above tables, we can further draw the following conclusions.",,,0,not_related
"In [36], they added a new update system to the memory module for",,,0,not_related
"[44] further improved performance by introducing extra loss functions such as intra-class distance and inter-class distance, but its application scope was limited to the computer vision field.",,,0,not_related
"We also compare our method with the state-of-the-art video prediction methods, including FFP, latent space autoregression (LSA) [47], memory-guided normality method (MNN) [48], ADL [20], deep multi-branch mask network (DMMNet) [49], future frame prediction network (F2PN) [50] and non-local U-Net (NU-Net) [51].",,,1,related
The performance of MNN is also influenced by the size of memory.,,,0,not_related
"And we observe that the methods which use memory networks to keep the representations, such as MemAE [7], MNAD-R [21], MNAD-P [21], and AMMC-Net [3], have a limited performance for anomaly detection on Avenue and ShanghaiTech.",,,0,not_related
"Within our best knowledge, we compare our AMSRC with stateof-the-art methods, including: (1) classic video anomaly detection methods: MPPCA [10], MPPC+SFA [19], and MDT [19]; (2) reconstruction-based methods: ConvAE [8], ConvLSTM-AE [17], MemAE [7], andMNAD-R [21]; (3) prediction-basedmethods: FramePred [13], MNAD-P [21], and VEC [31]; (4) hybrid and other methods: Stacked RNN [18], AMC [20], AnoPCN [29], AMMC-Net [3], and HF2-VAD [14].",,,1,related
"Reconstruction-based methods [7, 8, 17, 21] usually train an autoencoder on normal data and expect abnormal data to incur larger reconstruction errors at test time, making abnormal data detectable from normal ones.",,,0,not_related
The trained models for MNAD and MOVAD on ShanghaiTech dataset are not publicly available.,,,0,not_related
"Performance (AUC) drop in FFPN [18], MNAD [23], and MOVAD [7] due to the Slow-Fast-Freeze effect.",,,0,not_related
"For detecting anomalies, both MNAD and FFPN compute the Peak Signal to Noise Ratio (PSNR) between the actual frame and predicted frame.",,,0,not_related
"Performance drops of FFPN [18] with adversarial ShanghaiTech dataset (top), MNAD [23] with adversarial Avenue dataset (middle), MOVAD [7] with adversarial Avenue dataset (bottom).",,,1,related
"(iii) Finally, we used our new datasets simulating the Wi-Fi deauthentication attack to evaluate performance of several state-of-the-art DNNbased anomaly detection methods: Future Frame Prediction [18], Memory-guided Normality for Anomaly Detection (MNAD) [23], Modular Online Video Anomaly Detector (MOVAD) [7].",,,1,related
"The recent DNN-based methods can be categorized into two main groups, prediction-based [4, 6, 16, 18] and reconstructionbased [9, 11, 20, 22, 23].",,,0,not_related
We notice that the AUC drops from 0.855 to 0.795 for the FFPN approach and from 0.885 to 0.819 for the MNAD model.,,,1,related
"In this section, we evaluate the performance of existing state-of-the-art approaches, namely the Future Frame Prediction (FFPN) [18], Memory-guided Normality for Anomaly Detection (MNAD) [23], and Modular Online Video Anomaly Detector (MOVAD) [7] on the generated adversarial datasets.",,,1,related
"Memory guided normality for anomaly detection is proposed in (Park et al., 2020).",,,0,not_related
"Most existing anomaly detection methods, such as autoencoder-base methods [13, 18, 38, 71, 73], GAN-base methods [39, 45, 48, 68], selfsupervised methods [2, 11, 12, 25, 50, 56, 60], and one-class classification methods [7,8,40,43], assume that only normal data can be accessed during training.",,,0,not_related
"Methods in [4, 5] apply the memory module to restrict the space of prototypical normal patterns, which prevents abnormal patterns from being decoded.",,,0,not_related
"Moreover, in addition to the reconstruction loss (L2 loss), the feature compactness loss and feature separateness loss [5] are applied in order to enhance the memory space’s representativeness and discriminability, which are expressed as follows.",,,0,not_related
"Video anomaly detection (VAD) aims to identify anomalous events on both the object level [7, 22, 68] and frame level [35, 39, 51] by techniques such as skeleton trajectory modeling [43], weakly supervised learning [69], attention [47], temporal pose graph [38], self-supervised learning [10] and autoencoders [3].",,,0,not_related
"More specifically, we neither use the normal class annotations as in one class classification (OCC) approaches [12, 37, 54], nor binary annotations used by the weakly supervised anomaly detection systems [50,67,69,74].",,,1,related
"Note that, the term ‘unsupervised’ in literature often refers to OCC approaches which assume all normal training data [11, 37, 64, 66].",,,0,not_related
"Similarly to [12], discriminative normality action features prototypes are learnt based on nearest neighbor distances within the memory space via a loss that favors compactness of data samples around prototypes:",,,0,not_related
"Similarly to [11, 12], we incorporate a memory block in our framework in order to model diverse normality patterns.",,,1,related
"In addition, training 3 is much faster than in [12]: only 70 min on ShanghaiTech, 9 min on Avenue and 4 min on UCSD ped2 with a single NVIDIA TITAN X (PASCAL) GPU.",,,0,not_related
"∗Equal contributions reconstruction-based methods [10, 11, 12, 4, 13].",,,0,not_related
"Instead, we propose to tackle these challenges by extending the mainstream reconstruction assumption on which most state-of-the-art methods [3, 11, 12, 13] are implicitly based: Given a normality model, normal observations are easier to reconstruct from a low-dimensional representation than abnormal observations.",,,0,not_related
"Contrarily to [12], we incorporate a third term LOLE that adds orthogonality constraints within the memory space.",,,1,related
We also use the same triplet loss Ltr introduced in [12].,,,1,related
Similar to [12] the most similar memory item mk to the query h is defined as the soft nearest neighbor:,,,0,not_related
"To ensure the comparability between different methods, we calculate AUC for the frame-level prediction [4,5,7].",,,1,related
"[5] propose to use a memory module with a new update scheme where items in the memory record prototypical patterns of normal data, and it represents the SOTA solution of unsupervised anomaly detection.",,,0,not_related
"To discourage this behavior, Park et al. (2020) introduce MNAD, an autoencoder with an integrated memory module.",,,0,not_related
"Following [2, 11, 4, 7, 5, 10, 9, 8], we normalize PSNR(Pt,Pt) of each video clip to the range [0, 1] to obtain the final normality score St.",,,1,related
"Since the first five frames of each clip cannot be predicted, they are ignored in the evaluation, following other prediction-based methods [4, 6, 5, 9].",,,0,not_related
"Following this circumstance, most of the deep-learning based VAD models [4, 5, 6, 7, 8, 9, 10] are trained to recognize the normal patterns during training, and detect the frames with the outlying patterns during testing.",,,0,not_related
Dataset Model STAN [10] Abn GAN [25] HyAE [7] Mem AE [11] AD [26] IntAE [6] MNAD -Recon [5] Fast Ano [9] Base Base+R Base+R+M (Ours) Avenue [1] 81.,,,0,not_related
"In the context of one-class distillation for unsupervised AD, the student model is expected to generate highly different representations from the teacher when the queries are anomalous samples [11, 26].",,,0,not_related
"To address this issue, memory mechanism [11, 16, 26] , image masking strategy [42, 46] and pseudo-anomaly [28, 45] are incorporated in reconstruction-based methods.",,,0,not_related
"These tasks include, but not limited to, sample reconstruction [2,5,11,16,26,34,38,48], pseudo-outlier augmenFigure 1.",,,0,not_related
"Notably, traditional AE-based methods [5, 11, 16, 26] detect anomalies utilising pixel differences, whereas we perform discrimination with dense descriptive features.",,,0,not_related
"Therefore detecting abnormal behavior in video automatically has attracted increasing attentions in recent years [22, 43, 55, 56, 73, 86].",,,0,not_related
"However, this framework usually misidentifies the small probability events in a scene as the abnormal events, because it is infeasible to collect all possible normal events [41, 52].",,,0,not_related
"However, it remains as an challenging task due to the unbounded and rare property of abnormal events [18, 37, 41, 52].",,,0,not_related
"Four models apply different methods to generate diversified trajectories, i.e., Social-STGCNN and SGCN model the pedestrian trajectories as a bi-variate Gaussian distribution, while STAR and STGAT add the random Gaussian noise to their predictions.",,,0,not_related
"We use SGCN’s Shi et al. (2021a) TP model architecture, and follow the loss function and anomaly scoring function designed by 4 stat-of-the-arts AD methods: MNAD Park et al. (2020), P-NET Zhou et al. (2020), RSRAE Lai et al. (2020), and GEPC Markovitz et al. (2020), and thus construct some trajectory ADmodels.",,,1,related
"C. Wang, C. Liang, X. Chen, H. Wang: Preprint submitted to Elsevier Page 8 of 14
In addition, we analyze 4 state-of-the-arts stochastic TP models, including Social-STGCNN Mohamed et al. (2020), STAR Yu et al. (2020), STGAT Huang et al. (2019) and SGCN Shi et al. (2021a).",,,1,related
"Solutions of the each stage are extracted from two parts: (1) 5 state-of-the-arts AD models analyzed in Table 1; (2) 5 state-of-the-art TP models, including SGCN Shi et al. (2021a) (1), LB-EBM Pang et al.",,,0,not_related
"Events that cannot be described by the normal model during the test are considered anomalies [4], which are usually judged based on reconstruction or prediction errors.",,,0,not_related
"The proposed method is an attention-based encoder-decoder, and achieves comparable
performance to the Frame-Pred method with an AUC of 71.0%, which is better than 70.5% of MNAD.",,,0,not_related
"MNAD uses several memory items to record prototypical patterns of normal activities, but the ShanghaiTech dataset contains a variety of scenarios.",,,0,not_related
"[4] propose an update method of memory module, which uses the coding features of sample to retrieve memory items and update them during training and testing.",,,0,not_related
"In order to evaluate the proposed method, we compare the AUC of the proposed method with Conv-AE [8], Stacked RNN [7], MemAE-nonSpar [2], Frame-Pred [5], MemAE [2] and MNAD [4] on the ShanghaiTech dataset.",,,1,related
"In order to evaluate the proposed method, we compare the AUC of the proposed method with MPPCA [14], MPPC+SFA [11], MDT [11], Unmasking [15], Conv-AE [8], ConvLSTMAE [17], Stacked RNN [7], Frame-Pred [5], MemAE [2] and MNAD [4] on the UCSD Ped1 dataset.",,,1,related
Some heatmaps of normalized prediction errors of the proposed method and MNAD for anomaly detection on the UCSD Ped1 test set are shown in Fig.,,,0,not_related
"Owing to its flexibility, it has been widely adopted for solving various vision problems including movie understanding [52], video object segmentation [53]–[55], image generation [56], VQA [57], and anomaly detection [58], [59].",,,0,not_related
"Besides, we use a variable r as the initial weight of adaptive fusion network with FC(2R)
4. https://pyod.readthedocs.io/en/stable/, https://github.com/ 7fantasysz/MSCRED,https://github.com/cvlab-yonsei/MNAD,https: //github.com/Vniex/BeatGAN, https://github.com/d-ailin/GDN
→ BN → Sigmoid → Multiply.",,,1,related
"We present case studies of MNAD, UODA and our method AMSL as shown in Fig.",,,1,related
"0
0.2
0.4
0.6
0.8
1
F 1
OCSVM ConvLSTM-COMP MNAD-R AMSL
Fig.",,,1,related
"We compare the performance of four methods: OCSVM, ConvLSTM-COMPOSITE, MNAD-R and AMSL.",,,1,related
"MNAD and ConvLSTM are proposed for video data, which may not be suitable for multivariate time series.",,,0,not_related
"Each column is the instance whether is correctly detected by our method AMSL, MNAD and UODA.",,,1,related
"[39], neural machine translation [40], anomaly detection [41], [42], [43].",,,0,not_related
"It has two variants: one with the prediction task (MNAD-P), another with the reconstruction task (MNAD-R).",,,0,not_related
"In
10
Normal AbnormalNormal AbnormalNormal Abnormal
UODA:
Ours:
MNAD:
UODA:
Ours:
MNAD:
UODA:
Ours:
MNAD:
UODA:
Ours:
MNAD:
UODA:
Ours:
MNAD:
UODA:
Ours:
MNAD:
Ac ce le ra tio
n
Time
Ac ce le ra tio
n
Time
Ac ce le ra tio
n
Time
Ac ce le ra tio
n
Time
Ac ce le ra tio
n
Time
Ac ce le ra tio
n
Time
Fig.",,,1,related
"• MNAD [43], which is an encoder-decoder model based on a memory module for video anomaly detection.",,,0,not_related
[17] improved this drawback by introducing feature compactness loss and feature separateness loss.,,,0,not_related
"Previous deep learning methods for anomaly detection and segmentation are usually based on reconstructionbased neural network architectures, such as autoencoders (AE) [3], [5], [11], [15], [17], variational autoencoders (VAE) [14], [23], and generative adversarial networks (GAN) [1], [20].",,,0,not_related
"Most of the above works use pixel-wise metrics between input and output to detect anomalies [5, 13, 4, 16].",,,0,not_related
"We compare our methods with the existing state-of-the-art approaches: FFP-MC [11], DAML [9], MemAE [4], and MNAD [16].",,,1,related
"Automated anomaly detection has attracted significant attention because of its importance for surveillance systems and public security [12, 11, 19, 1, 4, 16].",,,0,not_related
", anatomical structures) across persons, we incorporate a memory bank [21] M ∈ R to store the common patterns.",,,1,related
"Apart from the single-view internal learning and cross-view mutual distillation loss functions, the compactness (L) and separateness (L) constraints as in [21], are used to regularize the memory bank,",,,0,not_related
"Given two adjacent slices, a U-shape network constituted by convolution layers, residual groups [3] and a memory bank [21], synthesizes r − 1 intermediate slices.",,,0,not_related
"Similarly, methods based on autoencoders (Bergmann et al., 2019b; Park et al., 2020) first encode input images with a low dimensional latent sample, and then decode that sample to minimize a pixelwise reconstruction error.",,,0,not_related
"To compare our methods to the rapidly developing field of algorithms for AD in surveillance videos, we choose to run the Memoryguided Normality for Anomaly Detection (MNAD [7]) method on our dataset.",,,1,related
"In recent years, many new techniques have been suggested, pushing forward performance on such benchmarks [6, 7].",,,0,not_related
"More recent methods have been able to outperform using pretraining and self-supervised learning [12, 6, 7].",,,0,not_related
"use reconstruction loss, feature compactness loss, and feature separateness loss in [24].",,,0,not_related
"For the anomaly detection task, we evaluate our methods on a memory-guided autoencoder network from [24].",,,1,related
"Since [24] deals with image and video data, we change the network structure for the network anomaly detection.",,,1,related
"Finally, separatedness loss [18] is used to help learn a diverse feature representation and improve the discriminative power of the codebook.",,,0,not_related
"The second group of approaches to anomaly detection with deep learning is to reconstruct or predict future ‘normal’ video frames from sparse feature representations [3, 15, 27], sometimes augmented with memory modules [18], and/or optical-flow images [13, 19, 20].",,,0,not_related
"The total loss function consists of the prediction loss Lpred, embedding loss Lembed, the so-called commitment loss [26] Lcommit and feature separatedness loss [18] Lsep.",,,1,related
"Our method is most similar to the memory-augmented autoencoder [18], where the features at the bottleneck are appended to the closest entries from a learnt codebook containing a small number of codes.",,,1,related
"Following [18] we remove the last batch normalization layer and the last ReLU activation layer, because ReLU cuts off negative values, possibly restricting the diverse feature representation.",,,1,related
"[18] that (1) the size of the bottleneck of the method is twice as big as the input, hence the model could potentially copy the input when reconstructing and (2) the codebook could simply be ignored by the decoder as it has access to raw bottleneck features.",,,1,related
"The network architecture is based on U-Net [23], which has been successfully applied to the task of reconstruction and future frame prediction [18, 13].",,,0,not_related
"As more research efforts been invested into anomaly detection, recent studies combined the feature extraction step with the model training step and proposed deep learning methods in an end-to-end manner, such as VAE [50], Generative Adversarial Network (GAN) [51], Recurrent Neural Network (RNN) [7] and Long Short-Term Memory (LSTM) [52], etc.",,,0,not_related
The w/ Mem [67] predicts anomalies based on reconstruction error.,,,0,not_related
"The ten methods are Frame-Pred [1], MPED-RNN [2], w/Mem [67], ST-GCAE [68], Multi-",,,1,related
"Qualitative results for future frame prediction of (top to bottom): w/ Mem [67] model, MPED-RNN [2] model and our HSTGCNN model.",,,1,related
"The ten methods are Frame-Pred [1], MPED-RNN [2], w/Mem [67], ST-GCAE [68], Multitimescale [9], PoseCVAE [60], LSA [69], Ano-Graph [70],
AnomalyNet [71], and Normal Graph [5], some of them integrate a model focusing on appearance and motion with others dealing with the trajectories of human skeletons.",,,0,not_related
"Among them, MPPCA (hybrid of probabilistic principal component analyzer) + SF (social power) [17] and MDT (hybrid of dynamic texture) [18] are methods based on manual features; Conv-AE [8], 3D Conv [19], Stacked RNN [20] and ConvLSTM-AE [21], MemNormality [10], and ClusterAE [22] are all methods based on autoencoders; AbnormalGAN [7] and Pred +Recon [23] are based on generating the adversarial networks’ method.",,,0,not_related
"network (GAN) [14], U-Net with memory module [15], and convolutional neural network (CNN)-based data augmentation [16] have been developed to reconstruct normal data.",,,0,not_related
"and cannot be combined with the deep learning technique, leading to inferior performance compared with recent DNNbased approaches [20], [27], [29].",,,0,not_related
"based or prediction-based methods with distance-based methods, such as [16], [24], [27], [29], and [30].",,,0,not_related
The main drawback of these methods is that they do not consider the diversity of normal patterns explicitly [27].,,,0,not_related
"reaches the best performance among the frame-level methods, and outperforms recent memory-augmented prediction-based methods [16], [24], [27], [29] by 4%∼7%.",,,0,not_related
"To comprehensively compare the state-of-the-art methods on the Corridor dataset, we re-implement two outstanding approaches MNAD [27] and MPN [29] (marked with †) on Corridor along with other datasets using their official codes and settings.",,,1,related
"Quantitatively, under the circumstance (c), the average AUC of LLSH is 87.5%, surpassing MNAD (82.3",,,1,related
"Some methods [5], [27], [38], [39] directly use the simple nearest neighbor search to calculate the distances between the testing instance and its normal neighbors as the anomaly score.",,,0,not_related
"Quantitatively, the average decreased AUC of LLSH is only 1.3%, while it is 2.4% and 1.8% for MNAD
and MPN.",,,0,not_related
[27] design a memory module to record the features of normal data.,,,0,not_related
MNAD [27] and MPN [29] (marked with †) on Corridor,,,0,not_related
From the results in (a) and (b) we can see that LLSH outperforms MNAD and MPN in 8 scenes consistently.,,,1,related
"Several subsequent prediction-based methods [16], [24], [27], [29], [30] have shown encouraging performance.",,,0,not_related
"Reconstruction-based methods [3], [14], [15], [16], [17], [18], [19] and prediction-based methods [4], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33] train deep neural networks (DNNs), such as convolutional auto-encoders and variational auto-encoders, on normal data to reconstruct the current frame or to predict the next frame.",,,0,not_related
"We compare two frame-level competitive methods MNAD [27] and MPN [29] with our LLSH under three circumstances: (a) training and testing models in each single scene, (b) computing average result of the first N scenes obtained in (a), and (c) training and testing models in the first N scenes.",,,1,related
"To be specific, the average absolute AUC of LLSH is 86.1%, outperforming MNAD and MPN by 2.5% and 4.8%, respectively.",,,1,related
"Therefore, we compare the scalability of MNAD [27], MPN [29] and the proposed LLSH for newly added scenes.",,,1,related
"The external memory can be used to address various vision problems, including video object segmentation [31, 32], image generation [62], object tracking [59], and anomaly detection [10, 34].",,,0,not_related
[16] presented a new memory update scheme and used feature compactness and separateness losses to enable the sparse access of the memory.,,,0,not_related
"AE/VAE-based Models Apart from the standard combination of reconstruction-error and AE/VAE models [111], [112], other methods use more sophisticated strategies such as reconstructing by memorized normality [115], [116], adapting model architectures [117], and partial/conditional reconstruction [89], [118], [119].",,,0,not_related
"2: Reconstruction-Error [89], [111], [111], [112], [112], [113], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124]",,,0,not_related
Our approach is different as we do not utilize any additional component and solely rely on the reconstruction based AEs.,,,1,related
"Reconstruction Based Methods: A common way to tackle the one-class classification (OCC) problem is by utilizing autoencoders (AEs) which learn normal data representations by reconstructing the inputs [9, 10, 29, 30, 35, 58].",,,0,not_related
"This clearly demonstrates the superiority of our proposed approach, i.e., training AEs by encouraging only normal data reconstructions assisted by pseudo anomalies.",,,0,not_related
"By encouraging to reconstruct only normal data for any kind of input (i.e., normal or pseudo anomalous), AEs are specifically trained to limit their reconstruction boundaries around the normal data hence not affecting the normal reconstructions while distorting anomalies, as illustrated in Fig.",,,0,not_related
"In contrast, our approach encourages AEs to produce unconstrained reconstructions for normal inputs while limiting the reconstructions for anomalous inputs, thus producing more discriminative anomaly scores.",,,0,not_related
": LEARNING NOT TO RECONSTRUCT ANOMALIES 3 2 Related Works Reconstruction Based Methods: A common way to tackle the one-class classification (OCC) problem is by utilizing autoencoders (AEs) which learn normal data representations by reconstructing the inputs [9, 10, 29, 30, 35, 58].",,,0,not_related
"Typically, to tackle OCC problem, AEs are utilized to learn the normal representations by minimizing the reconstruction loss between the normal input XN and its reconstruction X̂N as follows:
LN = 1 T ×C×H×W ∥∥X̂N−XN∥∥2F , (2)
where ‖.",,,0,not_related
"Concurrent with other recent anomaly detection methods [5, 24, 35], we utilize Peak Signal to Noise Ratio (PSNR) Pt between an input frame and its reconstruction to compute the anomaly score as follows:",,,0,not_related
"In order to capture rich information from video data, AEs are often designed to take multiple frames as input [9, 10, 35, 58].",,,0,not_related
"Non-Reconstruction Methods: Several researchers adopt different schemes for OCC based anomaly detection: focusing only on objects by utilizing object detectors in the frameworks [6, 7, 8, 11, 12, 43, 52]; predicting future frames from the past few consecutive frames with the intuition that it is difficult to predict unseen anomalous data [5, 24, 27, 28, 35]; or incorporating adversarial components [14, 19, 20, 24, 39, 45].",,,0,not_related
"Finally, following [5, 24, 35], the anomaly score St is obtained using min-max normalization of Pt as: St = 1− Pt −mint(Pt) maxt(Pt)−mint(Pt) , (10)",,,1,related
"However, since AEs can also wellreconstruct anomalous data [9, 33, 54, 60], several researchers proposed memory based networks to limit reconstruction capability of AEs [9, 35].",,,0,not_related
"One way to tackle the OCC problem is by using a deep autoencoder (AE) trained to reconstruct normal data [9, 10, 29, 30, 35, 58].",,,1,related
"Methods Ped2 [22] Ave [26] Sh [30] M is ce lla ne ou s AbnormalGAN [39] 93.5% - - Smeureanu et al. [41] - 84.6% - AMDN [49, 50] 90.8% - - STAN [19] 96.5% 87.2% - MC2ST [25] 87.5% 84.4% - Ionescu et al. [13] - 88.9% - BMAN [20] 96.6% 90.0% 76.2% AMC [34] 96.2% 86.9% - Vu et al. [45] 99.21% 71.54% - DeepOC [47] - 86.6% - TAM-Net [14] 98.1% 78.3% - LSA [1] 95.4% - 72.5% Ramachandra et al. [38] 94.0% 87.2% - Tang et al. [44] 96.3% 85.1% 73.0% Wang et al. [46] - 87.0% 79.3% OGNet [54] 98.1% - - Conv-VRNN [27] 96.06% 85.78% - Chang et al. [3] 96.5% 86.0% 73.3%
O bj
ec t-
ce nt
ri c MT-FRCN [11] 92.2% - -
Ionescu et al. [12] 1 94.3% 87.4% 78.7% Doshi and Yilmaz [6, 7] 97.8% 86.4% 71.62% Sun et al. [43] - 89.6% 74.7% VEC [52] 97.3% 89.6% 74.8% Georgescu et al. [8] 98.7% 92.3% 82.7%
Methods Ped2 [22] Ave [26] Sh [30]
N on
de ep
le ar
ni ng MPPCA [15] 69.3% - - MPPC+SFA [15] 61.3% - - Mehran et al. [32] 55.6% - - MDT [31] 82.9% - - Lu et al. [26] - 80.9% - AMDN [50] 90.8% - - Del Giorno et al. [4] - 78.3% - LSHF [57] 91.0% - - Xu et al. [48] 88.2% - - Ramachandra and Jones [37] 88.3% 72.0% -
Pr ed
ic tio n Frame-Pred [24] 95.4% 85.1% 72.8% Dong et al. [5] 95.6% 84.9% 73.7% Lu et al. [28] 96.2% 85.8% 77.9% MNAD-Pred [35] 97.0% 88.5% 70.5%
R ec
on st
ru ct
io n AE-Conv2D [10] 90.0% 70.2% 60.85% AE-Conv3D [58] 91.2% 71.1% - AE-ConvLSTM [29] 88.10% 77.00% - TSC [30] 91.03% 80.56% 67.94% StackRNN [30] 92.21% 81.71% 68.00% MemAE [9] 94.1% 83.3% 71.2% MNAD-Recon [35] 90.2% 82.8% 69.8% Baseline 92.49% 81.47% 71.28% Ours: Patch based 94.77% 84.91% 72.46% Ours: Skip frame based 96.50% 84.67% 75.97%
1Micro AUC reported in [8]
Table 1: AUC performance comparison of our approach with several existing SOTA methods on Ped2, Avenue (Ave), and ShanghaiTech (Sh).",,,0,not_related
"To alleviate this problem, several researchers [9, 35] proposed employing a memory mechanism over the latent space between the encoder and the decoder of an AE to limit the reconstruction capability in the case of anomalous input.",,,0,not_related
The contributions of this work are threefold: 1) We propose a pseudo anomaly based novel method of encouraging only normal data reconstructions to train AEs in the OCC setting.,,,1,related
"Interestingly, both of our models trained with different kinds of pseudo anomalies achieve better performance than the memory based networks, such as MNAD-Recon [35] and MemAE [9], considering that we use a very similar network architecture with these approaches and a common goal of limiting the AE capability of reconstructing anomalies.",,,0,not_related
"Following [1], [3], [10], [13], [16], [17], we scale the anomaly score to fall within the range of [0, 1]:",,,0,not_related
"As for anomaly detection, previous works (Gong et al. 2019; Park, Noh, and Ham 2020) have utilized memory to prevent unexpected generalization on anomalous inputs.",,,0,not_related
[30] in both forms of 1) reconstruction-based: CUHK Avenue [21] by 4.,,,0,not_related
"[30], which produce spatial anomaly detection scores.",,,0,not_related
[30] incorporates additional memory modules for both prediction-based and reconstruction-based anomaly detection.,,,0,not_related
"The baseline architectures of [30, 19] are trained for 15 epochs each on Nvidia RTX 2080 Ti GPU on ShanghaiTech [19] dataset, which took ∼12hrs to complete.",,,0,not_related
"We performed micro-level evaluation, as done in [9, 30], where we concatenate all the sequence and learned the parameters for",,,1,related
"The memory-augmented autoencoders [30, 10] contain an extra memory module along with a prediction/reconstruction-based network.",,,0,not_related
[30] uses convolutional autoencoders for both reconstruction and prediction networks.,,,0,not_related
[30] contains an additional memory module which records prototypical pattern of normal data.,,,0,not_related
"come this drawback, memory-augmented auto-encoders [30, 10] are proposed.",,,1,related
"A common approach to aid in the learning is to use pre-trained systems to impose what is already known and learned, either in the form of optical flow [9], object detectors [32, 15, 36], skeletons [27], or memory augmentation [10, 30].",,,0,not_related
"Recently, it has also been utilized in anomaly detection [29, 18] where PSNR between an input frame and its reconstruction is used to calculate the anomaly score.",,,0,not_related
"2) Extensive experiments demonstrate the superiority of our method compared to a wide range of existing state-of-the-art (SOTA) works [12, 25, 22, 19, 44, 33, 8, 45, 20, 21, 34, 39, 1, 4, 18, 7, 29, 10] on three benchmark datasets.",,,0,not_related
"An AE is trained only on normal data for reconstruction [7, 29, 8, 45, 35, 39] or prediction [29, 18] tasks.",,,0,not_related
"With the recent popularity of deep learning, several researchers [8, 45, 39, 4, 28, 21, 29, 20, 35, 7] utilize autoencoder (AE) based networks to learn normal data representations.",,,0,not_related
"In order to capture robust representations, autoencoders (AE) are often designed to take multi-frame inputs [7, 29, 8, 45].",,,0,not_related
"Memory-based networks [7, 29] employ a memory mechanism over the latent space between the encoder and the decoder of an AE.",,,0,not_related
"Similar to the common practices in training a conventional AE [8, 29],",,,0,not_related
"One common way to tackle the OCC problem is by using a deep autoencoder (AE) [8, 45, 39, 4, 21, 29, 20, 35, 7].",,,1,related
"At test time, concurrent to the existing approaches [7, 29, 18, 8, 45, 39, 41], we predict anomaly scores at frame level.",,,0,not_related
"The read policy of memory bank is different from those used in memory networks [7, 8, 21, 25, 31, 32].",,,0,not_related
"Memory network usually reads and writes its item according to the similarity score [21, 25, 32].",,,0,not_related
"But for the memory network using the similarity scores as weights [21, 25, 32], gi is set to constant.",,,0,not_related
"Recently, memory augmented neural networks have been introduced in various computer vision fields [6, 10, 15, 18, 30, 31, 37, 43, 50, 52, 61].",,,0,not_related
"deep lear ing [60, 136]) have been provided.",,,0,not_related
"They also use deep architectures to learn a compressed representation for the training data, by reducing the number of hidden units [60].",,,0,not_related
"ey also se ee arc itect res to lear a co resse re rese tatio for t e trai i g ata, by re ci g t e ber of i e its [60].",,,1,related
"d ep learning [60, 136]) have b en provided.",,,0,not_related
"deep learning [60, 136]) have been provided.",,,0,not_related
"In early stages, the unsupervised video anomaly detection methods are dominating and they can be categorized into two categories, the reconstruction methods [1, 2, 3, 4, 5, 6, 7] and prediction methods [8, 9].",,,0,not_related
"Many deep learningbased methods have been proposed [1], with a wide range of input data, including sound [2], big data [3], signal data [4], natural language [5], image [6], and video [7].",,,0,not_related
"With the advent of deep learning, AE-based approaches are dominating this area and achieve state-of-the-art performance [1], [5], [7], [8], [23], [31], [32], [33], [34].",,,0,not_related
"This can be achieved by constraining the latent space to make the features closer to a prior distribution [7], [23].",,,0,not_related
"The first one is widely adopted in existing literature [37, 10, 22, 19, 23, 27], where only normal videos are available during training.",,,0,not_related
MemAE [10] and LMN [37] are most-related methods to our approach.,,,0,not_related
"Previous approaches [37, 10] proposed to explicitly model the shared normal patterns across normal training videos with a memory bank, for",,,0,not_related
"To this end, [37] defines a rule for updating items in the memory bank based on a threshold to record normal patterns and ignore abnormal ones.",,,0,not_related
[37] further expand the update rules of the memory bank by using a threshold to distinguish abnormal frames and record normal patterns.,,,0,not_related
(LMN) [37] introduce a memory bank into the AE for anomaly detecE Encoder DDecoder EnsembleOperation AggregationOperation,,,0,not_related
"Gong et al. (MemAE) [10] and Park et al. (LMN) [37] introduce a memory bank into the AE for anomaly detec-
tion.",,,0,not_related
"We adopt the same network architecture in [22, 37] as the backbone of AE to facilitate a fair comparison.",,,1,related
"The learning procedure is fully differentiable and the prototypes are dynamically learned with the benefits of adapting to the current scene spatially and temporally, compared with querying and updating the memory bank with pre-defined rules for recording rough patterns cross the training data in [10, 37].",,,0,not_related
"These models are prone to face the ‘overgeneralizing’ dilemma, where all video frames can be predicted well, no matter they are normal or abnormal, owing to the powerful representation capacity of convolutional neural networks (CNNs) [37, 10].",,,0,not_related
"Moreover, the prototypes are automatically derived based on the real-time video data during inference, without referencing to the memory items collected from the training phase [10, 37].",,,0,not_related
", one-shot learning [1], video object segmentation [26], domain adaptation [48], image colorization [42], and anomaly detection [6, 27].",,,0,not_related
"From top to bottom, we show the sampled video frames, ground-truth abnormal sections (green regions are abnormal), result of MNAD-R [38], result of MNAD-P [38], result of VEC-VAD [47] and result of HF(2)-VAD.",,,1,related
"We compare our proposed HF2-VAD with state-of-the-art methods, including (1) reconstruction-based methods: Conv-AE [11], ConvLSTMAE [31], GMFC-VAE [7], MemAE [8] and MNADR [38]; (2) prediction-based methods: Frame-Pred.",,,1,related
[38] follow this trend and present a more compact memory that can be updated during testing.,,,0,not_related
"[25], MNAD-P [38], VEC [47] and Conv-VRNN [30]; and (3) hybrid methods including ST-AE [50], AMC [36] and AnoPCN [46].",,,0,not_related
"Rather than traditional handcrafted feature based methods [1, 20, 2, 34], a lot of modern deep neural network based methods [11, 45, 50, 31, 25, 30, 8, 36, 38, 47, 43, 24] have been proposed for VAD.",,,0,not_related
"For saving space, we do not show predicted frames by VEC and MNAD-P, but instead show the difference maps
between the ground-truth and the predicted frames by HF2VAD, VEC and MNAD-P in the last three columns respectively.",,,1,related
"Reconstruction-based methods [11, 31, 8, 36, 7, 38] typically train autoencoders on normal data.",,,0,not_related
"Placing a memory module at the bottleneck of AE is a recent development in VAD community [8, 38].",,,0,not_related
"As can be seen, VEC and HF2-VAD perform much better than MNAD-R and MNAD-P in normal sections, producing lower and more stable anomaly scores.",,,0,not_related
"We compare our proposed HF(2)-VAD with state-of-the-art methods, including (1) reconstruction-based methods: Conv-AE [11], ConvLSTMAE [31], GMFC-VAE [7], MemAE [8] and MNADR [38]; (2) prediction-based methods: Frame-Pred.",,,1,related
"Examples in Figure 5 show anomaly curves of two testing videos compared among MNAD-R [38], MNAD-P [38], VEC [47] and HF2-VAD.",,,0,not_related
"Future frame prediction is another prevalent VAD paradigm, often obtaining better anomaly detection accuracy than reconstruction-based methods [47, 38].",,,0,not_related
"Inspired by [8, 38], we design a Multi-Level Memoryaugmented Autoencoder with Skip Connections (MLMemAE-SC) for optical flow reconstruction.",,,1,related
"flexibility, it is adapted to a variety of tasks, such as fewshot learning [41], [42], video summarization [43], image captioning [44], anomaly detection [45], etc.",,,0,not_related
"mance on image-to-image tasks such as segmentation and super-resolution, has been widely used in previous anomaly detection methods [14, 22, 24, 29].",,,0,not_related
"Some methods [8, 24] exploited memory networks to read and write normal patterns and made reconstructions biased to normal events.",,,0,not_related
"Although these methods achieved improved performance, most of them require heavy computations as they employ conventional U-Net for reconstruction or prediction [14, 22, 24, 29].",,,0,not_related
"Similarly, Park et al. [60] advocated a memory module with new update scheme to learn the feature representation of normal behaviors.",,,0,not_related
00 - - - - - - - - Park et al.[60]-2020 88.,,,0,not_related
[60] advocated a memory module with new update scheme to learn the feature representation of normal behaviors.,,,0,not_related
00 - - - Park et al.[60]-2020 - - - - - 97.,,,0,not_related
"[44] propose a new memory module to record normal prototypical patterns, which is deployed in the reconstruction and prediction networks respectively.",,,0,not_related
4.1.2 CUHK Avenue.,,,0,not_related
CUHK Avenue includes pedestrians and objects both moving parallel to or toward/away from the camera.,,,0,not_related
"Despite a substantial amount of research effort has been devoted to this problem [3, 8, 13, 14, 16, 19, 22, 31, 34], video anomaly detection, which aims to identify the activities that do not conform to regular patterns in a video sequence, is still a challenging task.",,,0,not_related
"proposed CT-D2GAN framework achieves the best performance on UCSD Ped2 and SH-Tech, and close to the best performance in CUHK [22].",,,0,not_related
"Recent memory or clustering enhanced methods [3, 6, 22] show good performance and is orthogonal to our proposed framework and can integrate with our proposed framework in future work to further improve performance.",,,0,not_related
"Thoroughly empirical studies on three public video anomaly detection datasets, i.e., UCSD Ped2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of the proposed framework and techniques.",,,0,not_related
"Among those, MPPCA (mixture of probabilistic principal component analyzers) + SF (social force) [19], MDT (mixture of dynamic textures) [13, 19] are handcrafted feature based methods; Conv-AE [8], 3D Conv [40], Stacked RNN [18], and ConvLSTM-AE [17] are encoder-decoder based approaches; MemAE [6], MemNormality [22] and ClusterAE [3] are recent encoder-decoder based methods enhanced with memory module or clustering; AbnormalGAN [25], Frame prediction [14], and Pred+Recon [31] are methods based on adversarial training.",,,0,not_related
"We evaluate our framework on threewidely used public video anomaly detection datasets, i.e., UCSD Ped2 dataset [13] 1, CUHK Avenue dataset [16] 2, and ShanghaiTech Campus (SH-Tech) dataset [18] 3.",,,1,related
"Finally, our
proposed CT-D2GAN framework achieves the best performance on UCSD Ped2 and SH-Tech, and close to the best performance in CUHK [22].",,,1,related
"It has also been applied for anomaly detection problems [18], [19].",,,0,not_related
"To test the effectiveness of our proposed cluster-based memory module, Table III lists the results of MemAE and AE+memory.",,,1,related
It should be noted that AE+Memory still achieves better results than MemAE on CIFAR10 when the training dataset is mixed with 10% outliers.,,,0,not_related
"One can observe that when the training dataset only has inliers, AE+Memory has comparable performance compared with MemAE on MNIST and significantly outperforms MemAE on CIFAR10.",,,0,not_related
"Besides, we compare our methods with another memory augmented AE MemAE [19].",,,1,related
"Some of techniques of contrast learning are just beginning to be used in anomaly detection [38, 14, 31].",,,0,not_related
"[12] used a memory module to record a prototypical pattern of normal data, while lessening the representation capacity of CNNs.",,,0,not_related
"introduced loss functions that, unlike memoryguided AE, guarantee intra-class compactness and inter-class separateness of “normal” instance patterns based on a 2D convolutional AE to increase the efficiency of the memory module [17].",,,0,not_related
"The autoencoders have many variations, such as residual learning model [19], self-attention model [20] and feature memory model [21], super-resolution model [22], temporally coherent sparse coding RNN [23], stacked RNN auto-encoder [24].",,,0,not_related
"The above methods learn normal feature pattern [23, 24], normal frame prediction [21, 26], and normal skeleton pattern [13, 14].",,,0,not_related
"recently, explicit characterization of the diversity of the normal data is explored in [73], self-supervised learning is proposed in [74], and normalizing flows for the task in [75].",,,0,not_related
"Since the first five frames of each clip cannot be predicted, they are ignored in the evaluation, following [22, 34, 43].",,,0,not_related
"Frame predicting AEs [22, 43] and frame reconstructing AEs [33, 13] have been proposed assuming that anomalies that are unseen in the training phase cannot be predicted or reconstructed when the model is trained only on normal frames.",,,0,not_related
"Additionally, many effective reconstructing AEs [33, 51, 34, 5] have been proposed.",,,0,not_related
"Our patch anomaly generation phase is computationally cheaper than the other methods that embed spatio-temporal feature extraction in networks, such as storing and updating memory items [12, 34], and estimating optical flow with pre-trained networks [22, 39, 49, 2].",,,0,not_related
"To the best of our knowledge, unlike [22, 12, 34, 39, 15, 38], our framework performs the fastest because there are no additional modules or pre-trained networks.",,,0,not_related
[34] suggested networks that employ memory modules to read and update memory items.,,,0,not_related
"We compare the frame-level AUC of our model with those of nonprediction-based methods [13, 28, 39, 42, 40, 33, 12, 15, 34, 49, 51, 10, 11] and prediction-based methods [22, 43, 32, 34].",,,1,related
"Focusing on the fact that abnormal events occur in small regions, patch-based AEs [51, 47, 33, 8], have been proposed.",,,0,not_related
"However, it has been observed that AEs tend to generalize well to generate abnormal events strongly, mainly due to the capacity of CNNs, which leads to missing out on anomalies during detection.",,,0,not_related
"Frame predicting and reconstructing AEs have been proposed under the assumption that models trained only on normal data are not capable of predicting or reconstructing abnormal frames, because these are unseen during training.",,,0,not_related
"Some studies [22, 43, 34, 26] trained AEs that predict a single future frame from several successive input frames.",,,0,not_related
[34] proposed memory-based methods to use only the most essential features of normal frames for the generation.,,,0,not_related
"This metric is used in most studies [4, 10, 11, 27, 28, 33, 34, 49, 51] on video anomaly detection.",,,0,not_related
"Following the method of many related studies [6, 12, 13, 21, 22, 28, 34, 40], we define the final normality score St by normalizing PSNR(Pt,Pt) of each video clip to the range [0, 1].",,,1,related
"A common approach to anomaly detection with deep learning is to predict future ‘normal’ video frames from sparse feature representations [3, 15, 26], sometimes augmented with memory modules [19], and/or optical-flow images [13, 20, 21].",,,0,not_related
al [39] propose to further enhance the performance by adopting a memory module to record representative normal patterns.,,,0,not_related
"[36] believe that limiting the generalization ability is vital in finding novel images, and hence propose a memory-based autoencoder to reconstruct images from features.",,,0,not_related
"They commonly utilize AutoEncoders [40, 11, 20, 50, 17, 14, 36, 32] or GANs [49, 46, 37, 39, 5] to learn the distribution underlying normal data and then make the decision based on whether a test sample can be well recovered or not.",,,0,not_related
"Anomaly detection has received broad attention in recent years due to its wide applications in industrial inspection [6, 7, 48, 11, 10, 32], medical diagnosis [51, 5, 46, 42], and surveillance [27, 30, 36].",,,0,not_related
"In the literature of anomaly detection [1–12], a common evaluation metric is frame-level AUC, which gradually uses the abnormal score as the threshold and does not rely on subjective thresholds.",,,0,not_related
"Based on autoencoders, the prior methods [10–12] detect abnormal events by reconstructing or predicting video frames.",,,0,not_related
"(15) D̂(Qt,Mp) = D(Qt,Mp)−mint(D(Qt,Mp)) maxt(D(Qt,Mp))−mint(D(Qt,Mp)) (16) In the RGB color space, according to [11, 12], we calculate the peak signal-to-noise ratio (PSNR) between the predicted frame Ît and its ground truth It, and normalize PSNR:",,,1,related
"To remedy these issues, some approaches [6, 8, 10–12] to anomaly detection are based",,,0,not_related
"Following [11, 12], we input a video clip with consecutive t − 1 frames to the encoder, and the decoder can predict the t-th frame.",,,1,related
"This is a significant and practical field on many occasions, especially video surveillance [1–12].",,,0,not_related
712 MNAD [11] 0.,,,0,not_related
"[11] claim to learn memory-guided normality for anomaly detection (MNAD), which can reconstruct and predict video frames.",,,0,not_related
"Similar to [11, 12], we use 2 norm to calculate the mean square error between the predicted frame Ît and its ground truth It.",,,1,related
MNAD and OGNet are latest unsupervised anomaly detection methods.,,,0,not_related
"The unsupervised anomaly detection methods contain GMM [12], Sparse [11], ConvAE [4], Stack RNN [13], U-Net [5], MNAD [7] and OGNet [8].",,,0,not_related
"The DNNbased anomaly detection technology has been applied to many fields (Liu et al. 2013; Zhao et al. 2017; Abati et al. 2019; Markovitz et al. 2020; Pang et al. 2020; Park, Noh, and Ham 2020).",,,0,not_related
"To show the effectiveness of our AMMC-Net, we compare our method with different prediction-based method (Liu et al. 2018), memory-based method (Gong et al. 2019; Park, Noh, and Ham 2020) and two-stream-based method (Prawiro et al. 2020).",,,1,related
"Many approaches have been proposed such as AE based methods[1, 13, 38], OSVM based methods[6, 42], and density based methods[49, 52].",,,0,not_related
[34] proposed a model that can be applied for reconstruction or prediction.,,,0,not_related
"It is worth pointing out that training only on normal data is often claimed as unsupervision like [12, 13, 34] recently.",,,0,not_related
"After that, we send the converted 2D feature map to the Memory [10] Module.",,,1,related
"Among them, Memory [10] is used to aggregate similar features, while separating different features, and all classes are weighted for feature integration.",,,0,not_related
"[10, 15] proposed to use 2D convolutions to construct an autoencoder, which performs well in small anomaly detection datasets while being faster.",,,0,not_related
"After completing the Memory [10] operation, we send the feature map to the 2D Decoder Module, The 2D decoder up-samples the obtained features, and combines the features directly output from the encoder through skip connections, and fmally generates the predicted value lt through three up­ sampling.",,,1,related
"represents the weight, Lpre , Lcompact, Lseparate represents the prediction loss, feature compactness loss and feature separateness loss [10], It represents the ground truth value, lt represents the predicted value, and T represents the total length of a video sequence.",,,0,not_related
"On the other hand, inspired by [10], We build a 2D convolutional decoder to predict the image while achieving faster speed.",,,1,related
"At present, methods based on 2D convolutional autoencoders [10] have good real-time performance.",,,0,not_related
"This architecture takes continuous video frames as input, the left side is the 3D encoder structure, and the three modules in the middle are the Skip Connections Module, Dimensional Alignment Module, and the Memory [10] Module.",,,0,not_related
"The comprehensive loss includes the prediction loss (as shown in formula (4)), the feature compactness loss and feature separateness loss proposed in [10].",,,0,not_related
"For the Memory [10], our hyperparameters are consistent with the settings in [10].",,,0,not_related
"Following the standard evaluation metric in the latest works [7], [6], [8], [9], we use the average frame-level area under curve (AUC) as the metric with varying threshold values for abnormality scores.",,,0,not_related
"Following [7], [6], [8], we use Peak Signal to Noise Ratio (PSNR) between the predicted frame and its ground truth to quantify the reconstruction.",,,0,not_related
"Prediction based anomaly detection [3], [7], [8], on the other hand, uses the consecutive t frames to predict the next frame and assumes anomaly will cause a large prediction error.",,,0,not_related
"Following the standard memory network [8], we use the compactness loss and separateness loss to conduct a sparse effect for both feature space as well.",,,1,related
(denoted by *) of Frame-Pred [6] and Mem-Guided [8] on new testing set.,,,1,related
"For one image of size 256x256, ours achieves 52 fps, while Mem-Guided [8] is 56 fps, MLEP [9] is 65 fps, Frame-pred [6] is 24 fps.",,,0,not_related
"2 is a memory-based [6], [8] frame prediction network.",,,0,not_related
The most related work to ours is Mem-Guided [8].,,,0,not_related
"A basic idea in memory-based network [6], [8] is to represent each kind of normal features using their nearest memory.",,,0,not_related
Mem-Guided [8] benefits from a specially designed memory structure for only normal data.,,,0,not_related
"In [50], the authors propose a memory network to memorize normal patterns for detecting anomalies in an video.",,,0,not_related
"Moreover, the memory network is further applied in video analysis [49, 50, 51] and image captioning [52].",,,0,not_related
"Driven by this observation, we propose a novel network, termed as prototype-based memory network (PM-Net), which is inspired by recent successes of memory networks in natural language processing (NLP) tasks [47, 48] and video analysis [49, 50, 51].",,,0,not_related
"As in previous methods [22, 10, 37], frame prediction error is also leveraged as an anomaly descriptor: Sfra = Lfra(ŷt, yt).",,,0,not_related
"The first one is widely adopted in existing literature [37, 10, 22, 19, 23, 27], where only normal videos are available during training.",,,0,not_related
MemAE [10] and LMN [37] are most-related methods to our approach.,,,0,not_related
"Previous approaches [37, 10] proposed to explicitly model the shared normal patterns across normal training videos with a memory bank, for",,,0,not_related
"To this end, [37] defines a rule for updating items in the memory bank based on a threshold to record normal patterns and ignore abnormal ones.",,,0,not_related
[37] further expand the update rules of the memory bank by using a threshold to distinguish abnormal frames and record normal patterns.,,,0,not_related
"We adopt the same network architecture in [22, 37] as the backbone of AE to facilitate a fair comparison.",,,1,related
Gong et al. (MemAE) [10] and Park et al. (LMN) [37] introduce a memory bank into the AE for anomaly detection.,,,0,not_related
"The learning procedure is fully differentiable and the prototypes are dynamically learned with the benefits of adapting to the current scene spatially and temporally, compared with querying and updating the memory bank with pre-defined rules for recording rough patterns cross the training data in [10, 37].",,,0,not_related
"These models are prone to face the ‘overgeneralizing’ dilemma, where all video frames can be predicted well, no matter they are normal or abnormal, owing to the powerful representation capacity of convolutional neural networks (CNNs) [37, 10].",,,0,not_related
(LMN) [37] introduce a memory bank into the AE for anomaly detection.,,,0,not_related
"Moreover, the prototypes are automatically derived based on the real-time video data during inference, without referencing to the memory items collected from the training phase [10, 37].",,,0,not_related
"This is conceptually similar to feature separateness loss in [33], which encourages the queries to be close to the nearest item and separates individual items in the memory.",,,0,not_related
"Such computer vision tasks include anomaly detection [9, 23], few-shot learning [3, 14, 46], image generation [47], and video summarization [20].",,,0,not_related
"A number of approaches have adopted the memory-augmented networks to solve various computer vision tasks including visual question answering [9, 17, 25], one-shot learning [2, 14, 32], anomaly detection [11, 27], and person recognition [41, 48].",,,0,not_related
"A common technique used by several recent works [22, 14, 31, 33] is to normalize the computed statistic for each test video independently, including the ShanghaiTech dataset.",,,0,not_related
"For example, even recent algorithms such as [48, 22, 33] cannot detect (or be modified to detect) anomalies pertaining to changes in human poses.",,,0,not_related
"Owing to the mentioned challenges and growing related critical applications such as detecting criminal events [70], road-traffic accidents [36] and, vehicle collisions for selfdriving cars [84], VAD has recently gained significant attention [13, 14, 15, 19, 25, 26, 34, 42, 48, 50, 51, 88, 80, 17].",,,0,not_related
"Therefore, recently, AE-based approaches [1, 64, 9, 51, 19, 61, 21, 91] have been used dominantly aiming to encode and decode every consecutive fixed T frames of normal training samples, supposedly failing to reconstruct anomalous ones.",,,0,not_related
"[19, 51] try to use memory-based AEs to learn different normal patterns for the normal representation space.",,,0,not_related
"Similar to previous methods [39, 64, 26, 25, 51, 19, 1] the frame-level area under the curve (AUC) is exploited for evaluation the performance of our method on Avenue, ShanghaiTech, UCSD-Ped2, ADOC, and Street Scene datasets.",,,1,related
"These methods [1]–[3], [5], [48]–[52], [63], [64] generally construct a normal pattern to encode normal samples.",,,0,not_related
"[Park et al., 2020; Gong et al., 2019] had a tendency to store several prototypes for normal samples by considering a memory module where individual items in it correspond to prototypical features of normal patterns.",,,0,not_related
Anomaly detection has been performed pixel-wise in video [29].,,,0,not_related
"Our method RTFM achieves superior performance when compared with previous SOTA unsupervised learning methods [15, 27, 30, 41, 70] and weaklysupervised approaches [62, 74, 78].",,,1,related
"Therefore, we don’t compare our method with other anomaly detection algorithms which are based on original data of Shanghai Tech [30], such as [20], [21], [30], [31].",,,1,related
"It has attracted increasing interest for solving problems like question-answering systems [14, 15], summarization [16],image generation[17], and anomaly detection[18].",,,0,not_related
he image- and video-based AUC AE-Conv2D [6]y 0.609 TSC [64]y 0.679 Stack RNN [64]y 0.680 AE-Conv3D [65]y 0.697 MemAE [20]y 0.712 LSA [39] 0.725 ITAE [41] 0.725 FFP+MC [66] 0.728 Mem-Guided (w/o Mem.) [67] 0.668 Mem-Guided (w/ Mem.) [67] 0.705 MemAE-nonSpar [20] 0.688 MemAE [20] 0.712 Clustering-Driven [68] 0.733 MOCCA (s) 0.730 MOCCA (h) 0.725 yValues reported in [41] TABLE VI AUC VALUES FOR THE SHANG,,,1,related
4 MemAE2020 [30] 97.,,,0,not_related
"Frame-level AUC scores (in %) of the state-of-the-art methods [7, 8, 9, 10, 13, 14, 16, 17, 18, 19, 20, 21, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 41, 47, 48, 49, 51, 53, 55, 57, 59, 60, 61, 62, 64] versus our deep+wide architecture trained on four proxy tasks at the object level, at the frame level or based on late fusion.",,,0,not_related
"Our approach outperforms the state-of-the-art methods [7, 8, 9, 10, 13, 14, 16, 17, 18, 19, 20, 21, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 41, 47, 48, 49, 51, 53, 55, 57, 59, 60, 61, 62, 64] on all three data sets, achieving frame-level AUC scores of 92.",,,0,not_related
"In recent years, a growing interest has been dedicated to the task of detecting anomalous events in video [8, 9, 10, 13, 17, 19, 20, 24, 30, 34, 35, 36, 37, 38, 39, 49, 51, 55, 57, 61, 62, 63].",,,0,not_related
"Employing generative networks for video anomaly detection [8, 36, 41] is another significant line of research that relies on the same principle, that is, synthesizing future frames will prove to be significantly more challenging when an anomalous event occurs than in a normal situation.",,,0,not_related
"Aside from the direction relying on reconstruction errors [14, 27, 29, 31, 34, 36, 41, 51, 53], other recent works, such as [9, 38], tackle the problem from completely different angles.",,,0,not_related
"Without being able to employ standard supervision, researchers have proposed alternative approaches ranging from distance-based [17, 19, 37, 38, 40, 44, 45, 46, 47, 50, 52, 59] and reconstruction-based strategies [5, 13, 14, 27, 29, 31, 34, 36, 41, 51, 53] to probabilistic [1, 2, 4, 12, 16, 21, 32, 33, 58] and change detection methods [7, 18, 28, 35].",,,0,not_related
"Without an external model, the memory-based approach [15, 42] that stores and updates normal query feaStatic Histogram Dynamic Histogram",,,0,not_related
"13 where norm(·) denotes normalization within a video clip as in some previous studies [29, 1, 42].",,,0,not_related
"Other algorithms have been proposed by learning reconstruction with other objectives [39], usingmemory modules [15, 42], reconstructing optical flows from frames [48], or encoding normal patterns with sparse dictionary learning [57, 35].",,,0,not_related
"As in a previous study [42], the error maps are visualized by marking the pixel that is larger than the average error value within the frame.",,,0,not_related
"Following (Park, Noh, and Ham 2020), the error maps are visualized by marking the pixel that is larger than the average error value within the frame.",,,0,not_related
"Other algorithms have been proposed via learning reconstruction with other objectives (Nguyen and Meunier 2019b), memory modules (Gong et al. 2019; Park, Noh, and Ham 2020), or reconstructing optical flows from frames (Ra-
vanbakhsh et al. 2017; Ganokratanaa, Aramvith, and Sebe 2020).",,,0,not_related
"In non-reconstruction method, good performance is achieved by the Mem-guided (Park, Noh, and Ham 2020), which is a prediction-based method that stores and updates normal query features by memory module.",,,0,not_related
"TABLE 1 Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores (in %) of our approach compared to the state-of-the-art methods [4], [5], [6], [7], [9], [11], [12], [13], [16], [18], [19], [24], [25], [31], [33], [34], [35], [38], [39], [40], [41], [45], [46], [47] on the Avenue data set.",,,0,not_related
"We first compare our approach with several state-of-theart methods [4], [5], [6], [7], [9], [11], [12], [13], [16], [18], [19], [24], [25], [31], [33], [34], [35], [38], [39], [40], [41], [45], [46], [47] reporting results on the Avenue data set.",,,0,not_related
"Most of the recent works treat abnormal event detection as an outlier detection task [2], [3], [4], [5], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [24], [25], [26], [27], [28], [30], [32], [51], learning a model using only normal data.",,,0,not_related
"Considering the prior work on video anomaly detection [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], we can devise a non-exhaustive taxonomy of abnormal events that includes: appearance anomalies (for example, a car or a truck in a pedestrian area), short-term motion anomalies (for example, a person running or a person throwing an object), long-term motion anomalies (for instance, loitering) and group anomalies (for example, several people running inside a public building).",,,0,not_related
"We further compare our method with the state-of-the-art approaches [4], [5], [6], [9], [11], [13], [16], [18], [37], [38], [39], TABLE 2 Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores (in %) of our approach compared to the state-of-the-art methods [4], [5], [6], [9], [11], [13], [16], [18], [37], [38], [39], [45], [46], [47] on the ShanghaiTech data set.",,,0,not_related
"We conduct experiments on four challenging benchmarks, namely Avenue [12], ShanghaiTech [13], Subway [23] and UCSD Ped2 [14], reporting favorable performance levels compared to the state-of-the-art methods [4], [5], [6], [7], [8], [9], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [24], [25], [26], [30], [31], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48].",,,0,not_related
"In general, existing abnormal event detection methods build the normality model using local features [7], [8], [12], [13], [14], [19], [22], [27], [28], [29], [30], global (frame-level) features [4], [9], [11], [15], [16], [17], [18], [24], [25], [26], [31], or both [3], [5], [32].",,,0,not_related
"There are at least two works [6], [16] that report the macroaveraged AUC.",,,0,not_related
"Existing abnormal event detection methods can be categorized into distance-based approaches [6], [7], [17], [24], [25], [27], [29], [31], [52], [53], [54], reconstructionbased models [5], [11], [12], [13], [16], [18], [26], [32], [41], [42], probabilistic models [2], [3], [8], [14], [15], [23], [55], [56], [57] and change detection methods [33], [34], [35], [36].",,,0,not_related
"Therefore, the majority of anomaly detection methods proposed so far are based on outlier detection [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [24], [25], [26], [27], learning normality models from training videos containing only normal events.",,,0,not_related
"Most of the recent works treat abnormal event detection as an outlier detection task [1], [2], [3], [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [28], [30], [43], [44], learning a model using only normal data.",,,0,not_related
"[28], global (frame-level) features [3], [8], [10], [14], [15], [16], [17], [18], [19], [21], [29], or both [2], [4], [30].",,,0,not_related
"TABLE 1 Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores (in %) of our approach compared to the state-of-the-art methods [3], [4], [5], [6], [8], [10], [11], [12], [15], [16], [17], [21], [22], [29], [31], [32], [33], [37], [41], [42] on the Avenue data set.",,,0,not_related
"There are at least two works [5], [15] that report the macro-averaged AUC.",,,0,not_related
"Several methods trained on the ShanghaiTech training videos [4], [5], [8], [10], [12], [15], [21], [35], [37] are included as reference.",,,0,not_related
"Therefore, the majority of anomaly detection methods proposed so far are based on outlier detection [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], learning normality models from training videos containing only normal events.",,,0,not_related
"We further compare our method with the state-of-the art approaches [3], [4], [5], [8], [10], [12], [15], [21], [35], [37] on the ShanghaiTech data set, presenting the corresponding results in Table 2.",,,1,related
"We first compare our approach with several state-of-the-art methods [3], [4], [5], [6], [8], [10], [11], [12], [15], [16], [17], [21], [22], [29], [31], [32], [33], [37], [41], [42] reporting results on the Avenue data set.",,,0,not_related
"TABLE 2 Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores (in %) of our approach compared to the state-of-the-art methods [3], [4], [5], [8], [10], [12], [15], [21], [35], [37] on the ShanghaiTech data set.",,,0,not_related
"We conduct experiments on three challenging benchmark data sets, namely Avenue [11], ShanghaiTech [12] and Subway [40], reporting favorable performance levels compared to the state-of-the-art methods [3], [4], [5], [6], [8], [10], [11], [12], [15], [16], [17], [21], [22], [29], [31], [32], [33], [34], [35], [37], [38], [41], [42].",,,0,not_related
"Video anomaly detectors are originally designed in an unsupervised manner [9], [17], [18], [19], [20] that only normal samples are available in the training phase without any labels.",,,0,not_related
[57] Recon.,,,0,not_related
8% Memory-guided normality [57] -/97.,,,0,not_related
"In [57], the authors contend that CNN-based reconstruction approaches suffer from reconstructing anomalous events well because of CNNs’ high representational capacity.",,,0,not_related
"Another problematic practice that has emerged is that of per-video normalization, such as that in [9], [10], [18], [46], [54], [56], [57].",,,0,not_related
"5%, compared with the algorithms MemAE [14] and P w/ MemAE [20] using memory modules, our results are also greatly improved, because there are more targets per frame in this data set, and the abnormal events are a small number of bicycles and cars, skateboards, and the normal mode can be well learned through spatial context information fusion to check out the abnormal target.",,,0,not_related
"Video anomaly detection uses unsupervised training, Only using the normal samples as training, and spatial context information of each frame represents a normal mode, so the normal mode from the training set is the key to our anomaly generalization ability, so that part of the exception context information can also be better used for reconstruction, we added a memory module [14] after the context encoding, aiming to learn a limited number of prototype features that can best characterize all normal context information, and save them in memory items, the structure is as shown in Figure 2, the memory block is a matrix of size N×C, M ∈ RN×C, storing N memory items, each memory item is mj, the dimension is C, update and read the memory item during training, Only memory entries are read during testing.",,,0,not_related
"[14] in the AE method added the Memory module, which reduces the generalization ability of the model and makes abnormal samples have larger reconstruction errors.",,,0,not_related
"Obviously, our method has achieved the best results on UCSD-ped2 dataset, compared with the same object detection algorithm FPN-AE-SVM [15], our results are improved by 1.5%, compared with the algorithms MemAE [14] and P w/ MemAE [20] using memory modules, our results are also greatly improved, because there are more targets per frame in this data set, and the abnormal events are a small number of bicycles and cars, skateboards, and the normal mode can be well learned through spatial context information fusion to check out the abnormal target.",,,1,related
"To evaluate the results of ABD, during inference, we use average area under curve (AUC) [10].",,,1,related
Detailed description can be found in [10].,,,1,related
"As a result, the AUC is boosted by 4.4%.",,,1,related
"During training, the final loss function is calculated as the linear combination of prediction, feature compactness, and feature separateness losses [10].",,,0,not_related
Experimental results show that our method achieves AUC=76.0% on Northking2022.,,,1,related
"Following [10], the min-max normalization of L2 distance and PSNR (peak signal-to-noise ratio) is adopted for anomaly score’s calculation, where the L2 distance measures the gap between the actual observed frame features and the pattern features memorized by the model, and the PSNR measures the pixel-level gap between the actual observed frame and the predicted one from the model.",,,0,not_related
"Similar to [10], the input of OLAD is t=4 adjacent frames in the video, and the output is the prediction result for the 5th frame.",,,0,not_related
"One of them employ traditional framelevel unsupervised ABD methods (such as MNAD algorithm [10]), and the other introduces videobased object-level image segmentation methods (for example Mask2Former algorithm [13,14]).",,,0,not_related
"An autoencoder may generalize and fit PAs well, especially when the PAs have structures that are similar to those of bona fide samples [14], [15].",,,0,not_related
"An autoencoder is inclined to reconstruct anomalies so well that it cannot distinguish them if the anomalies share common compositional patterns with normal training data, or the autoencoder learns general OCT image features rather than the particular features of a bona fide sample [14], [15], [18].",,,0,not_related
"ting [14], [15], [18], which enables them to fit presentation attack (PA) samples well, especially those that closely resemble bona fide samples; (2) OCT images contain severe noise, making image-based reconstruction methods vulnerable to noise interference [13]; and (3) existing unsupervised models have difficulty identifying more compact decision boundaries for differentiating between bona fide and PA samples.",,,0,not_related
"Since our approach is derived from an memory-based autoencoder inspired by anomaly detection, we also investigate other unsupervised models for anomaly detection, such as an autoencoder(AE) [49], a variational autoencoder(VAE) [50], a memory-augmented autoencoder(MemAE) [18] and two memory-based models (MAEP [43] and LMN [14]).",,,1,related
"In these experiments, the AE, VAE, MemAE, MAEP, and LMN use reconstructed images for discrimination.",,,0,not_related
proposed a new update strategy to compute queries and distinctions separately and proposed a separateness loss training model [14].,,,0,not_related
"a variational autoencoder(VAE) [50], a memory-augmented autoencoder(MemAE) [18] and two memory-based models (MAEP [43] and LMN [14]).",,,0,not_related
"The proposed model achieves outstanding performance compared to that of the state-of-the-art PAD methods (FFDM [13] and LMD [14]), as shown in Table IV.",,,0,not_related
"1) Read: Different from [5], [9], which use cosine similarity to retrieve the appropriate memory items, we distribute the input data to each memory item.",,,1,related
"3310882 training, and treats those that do not conform to the model as anomalies during testing [5].",,,0,not_related
"[5] proposed to integrate a memory module that includes an updating mechanism at the bottleneck of the network to record diverse prototypical normal patterns, thereby enhancing the model’s prediction ability for normal video frames while simultaneously suppressing it for abnormal events.",,,0,not_related
"To mitigate this drawback, some recent work [5], [9], [10] have proposed utilizing memory networks to preserve prototypical normal patterns, which can help reduce the representation capability of DNNs.",,,0,not_related
",n and use them as queries to read the scene features in M, as the reading operation [16].",,,1,related
"Therefore, existing methods [13], [14], [15], [16] typically formulate VAD as an unsupervised learning task, where only easily collected normal videos are used to train the model to learn prototypical features of normal events and discriminate",,,0,not_related
"The methods involved in the comparison include (a) traditional handicraft feature-based methods [32], [39], [40], [41], (b) single-stream networks [14], [15], [16], [42], (c) two-stream networks [20], [21], [22], [43], and (d) object-level methods [23], [24], [25].",,,0,not_related
"object-scene interactions, while limiting the model’s ability to generalize to abnormal events to prevent missed detections [16], [17], [18].",,,0,not_related
", models can reconstruct unseen anomalies very well [2], [10], [11], [12].",,,0,not_related
"ory [21], some methods intentionally constrain the representation ability of AE by inserting memory modules between the encoder and decoder [2], [10], [11].",,,0,not_related
"However, it has been widely observed that the AE trained on normal samples can also reconstruct unseen anomalies quite well [2], [10], [11], leading to missed detections of anomalies (as shown in Fig.",,,0,not_related
"On the one hand, some methods have been proposed that attempt to intentionally regulate AEs’ reconstruction ability by inserting memory modules into feature spaces [2], [10], [11].",,,0,not_related
"However, this assumption has been generally observed to be difficult to hold by many works—the trained model can also reconstruct unseen anomalies well [2], [10], [11], [12], [13], [14].",,,0,not_related
"However, it has been widely observed that the unsupervised trained models can also reconstruct unseen anomalies well, leading to missed detection [2], [11], [12], [14].",,,0,not_related
"Most of the previous VAD methods were devoted to unsupervised learning paradigms [4], [5], [6], [7], which train a model to memorize normal patterns by solely using normal samples.",,,0,not_related
"advantage of the memory module [22], [23], [24] as the second part of the multi-scale-memorizer, which was previously used in the frame-reconstruction-based method to alleviate the over-generalization issue of autoencoders [22].",,,0,not_related
"In [37], the authors augmented the output of an encoder in a variation of an auto-encoder CNN with a memory module that adaptively records prototypical patterns of normal data formore accurate detection of violent cases in a given database.",,,0,not_related
There were also approaches that augmented memory modules [37],,,0,not_related
"Authors in [37], [38], and [39] also incorporated auto-encoders to learn normal behaviors, but without",,,0,not_related
"ingly, memory mechanism [29], [30] and iteration mecha-",,,0,not_related
"vised learning [2], [3], [4] or weakly supervised methods [5],",,,0,not_related
"Thus, some methods [21], [22], [23] introduce the memory consisting of only normal sample features into anomaly detection to explicitly suppress the generalization ability of the reconstruction network.",,,0,not_related
"This module utilizes the Dynamic Memoryguided Normality for Anomaly Detection (DMNAD) method [17], [18] to differentiate between pedestrians and personal mobility users.",,,0,not_related
"Memory-based anomaly detection network used for mobility user recognition [17], [18].",,,0,not_related
"detection-based mobility user recognition [17], [18].",,,0,not_related
"into a hybrid paradigm [9], [63], [64], [65], [66].",,,0,not_related
"To mitigate this problem, some approaches [6], [8] propose to replace the features of the input with a combination of normal features, however, the combination sometimes is not still normal [6].",,,0,not_related
"To solve the anomaly escape problem, some approaches [6], [8] propose to replace the feature of the input with a combination of the normal features saved during training.",,,0,not_related
"The second is a memory-guided AE — MNAD (Park et al., 2020) that uses a concatenated latent space (of the naive latent space from the encoder output and
the typical features stored in a memory module constructed from training) to reconstruct the input.",,,0,not_related
"…inputs (Song et al., 2019), using both the memorized features of the training set and the input’s features to do reconstruction (Gong et al., 2019; Park et al., 2020), and so on, these methods are only studies on benchmark datasets — Avenue (Lu et al., 2013), ShanghaiTech (Luo et al., 2017),…",,,0,not_related
", 2019), using both the memorized features of the training set and the input’s features to do reconstruction (Gong et al., 2019; Park et al., 2020), and so on, these methods are only studies on benchmark datasets — Avenue (Lu et al.",,,0,not_related
"The second is a memory-guided AE — MNAD (Park et al., 2020) that uses a concatenated latent space (of the naive latent space from the encoder output and the typical features stored in a memory module constructed from training) to reconstruct the input.",,,0,not_related
"(1)
Meanwhile, inspired by (Park et al., 2020), to increase the diversity of embeddings for learning various patterns of normal samples, a separateness loss Lseparate is adopted to keep the distance of between ze(x) and its nearest embedding e1 smaller than the distance between ze(x) and its second closest embedding e2 with a margin m (set to 1), as shown in Eq.",,,0,not_related
"(1)
Meanwhile, inspired by (Park et al., 2020), to increase the diversity of embeddings for learning various patterns of normal samples, a separateness loss Lseparate is adopted to keep the distance of between ze(x) and its nearest embedding e1 smaller than the distance between ze(x) and its second…",,,0,not_related
"To solve this, Gong et al. (Gong et al., 2019) and Park et al. (Park et al., 2020) proposed memory-augmented autoencoder methods, which employ a memory book to learn only the key common features of the training dataset to ensure that the reconstructed image is generated from the latent…",,,0,not_related
"(Park et al., 2020) proposed memory-augmented autoencoder methods, which employ a memory book to learn only the key common features of the training dataset to ensure that the reconstructed image is generated from the latent representations of in-distribution data.",,,0,not_related
"Meanwhile, inspired by (Park et al., 2020), to increase the diversity of embeddings for learning various patterns of normal samples, a separateness loss Lseparate is adopted to keep the distance of between ze(x) and its nearest embedding e1 smaller than the distance between ze(x) and its second closest embedding e2 with a margin m (set to 1), as shown in Eq.",,,0,not_related
"To solve this, Gong et al. (Gong et al., 2019) and Park et al. (Park et al., 2020) proposed memory-augmented autoencoder methods, which employ a memory book to learn only the key common features of the training dataset to ensure that the reconstructed image is generated from the latent representations of in-distribution data.",,,0,not_related
"(1)
Meanwhile, inspired by (Park et al., 2020), to increase the diversity of embeddings for learning various patterns of normal samples, a separateness loss Lseparate is adopted to keep the distance of between ze(x) and its nearest embedding e1 smaller than the distance between ze(x) and its second closest embedding e2 with a margin m (set to 1), as shown in Eq.",,,0,not_related
"(1)
Meanwhile, inspired by (Park et al., 2020), to increase the diversity of embeddings for learning various patterns of normal samples, a separateness loss Lseparate is adopted to keep the distance of between ze(x) and its nearest embedding e1 smaller than the distance between ze(x) and its second…",,,0,not_related
"To solve this, Gong et al. (Gong et al., 2019) and Park et al. (Park et al., 2020) proposed memory-augmented autoencoder methods, which employ a memory book to learn only the key common features of the training dataset to ensure that the reconstructed image is generated from the latent…",,,0,not_related
"(Park et al., 2020) proposed memory-augmented autoencoder methods, which employ a memory book to learn only the key common features of the training dataset to ensure that the reconstructed image is generated from the latent representations of in-distribution data.",,,0,not_related
"Meanwhile, inspired by (Park et al., 2020), to increase the diversity of embeddings for learning various patterns of normal samples, a separateness loss Lseparate is adopted to keep the distance of between ze(x) and its nearest embedding e1 smaller than the distance between ze(x) and its second closest embedding e2 with a margin m (set to 1), as shown in Eq.",,,0,not_related
"To solve this, Gong et al. (Gong et al., 2019) and Park et al. (Park et al., 2020) proposed memory-augmented autoencoder methods, which employ a memory book to learn only the key common features of the training dataset to ensure that the reconstructed image is generated from the latent representations of in-distribution data.",,,0,not_related
"Moreover, (Park et al., 2020) uses a memory module with a different update design to record the prototypical patterns of normal data.",,,0,not_related
"Typical data sources in AD include images [34, 36, 43], video [30, 42], audio [29] and text [36].",,,0,not_related
"Also, in [34], they introduced a memory module with items that capture prototypical models of inlier class with a new update system.",,,0,not_related
"Inspired by [33, 35], we design the following composite operation (equation (11)).",,,1,related
"On this basis, reference [35] used U-Net and memory module; reference [36] used AE and DPU module.",,,0,not_related
"Some work built memory modules that learn only normal patterns from normal data and determine the presence of anomalies by computing reconstruction errors [33, 35].",,,0,not_related
"Therefore, some researches introduced more complex self-supervised tasks enabling the model to learn semantic reconstruction [15,32] or memory modules storing normal sample features while only reading normal features [16,19,27,43].",,,0,not_related
"The AE-based anomaly detection model depends on that the train AE to reconstruct only normal images [5,15,16,23, 27,32].",,,0,not_related
"The methods based on different modalities can be categorized into three classes: Unimodal Methods [3], [8], [9], Multimodal Mutual Loss Methods [10], [11] and Multimodal Fusion Methods [12], [13], [14], [15], as shown in Fig.",,,0,not_related
"Evaluation metrics: Following previous works [8], [9], [10], [11], [12], we use the area under the curve (AUC) of the framelevel receiver operating characteristics (ROC) as a primary metric.",,,0,not_related
"Relevant studies on VAD can be broadly classified into two categories: unsupervised learning methods [6, 17, 42, 45, 12, 16, 35, 36, 1, 38, 5, 14, 18, 23, 31, 15, 52, 27, 20, 25, 46] and weakly supervised learning methods [29, 51, 32, 4, 48, 53, 40, 21, 47, 39, 34, 41].",,,0,not_related
"Inspired by the memory network [45, 41, 28, 35], we adopt a similar memory update strategy for prototypes.",,,1,related
"Previous methods [4,7,12,13,20,21,22,23,27,30,31,33,35,40,43,44,48] deal with VAD in the one-class setup.",,,0,not_related
"There were several tasks to exploit the memory such as anomaly detection [18,44], few-shot learning [2,23,62], object tracking/detection [15, 25, 58], future prediction [29, 37], and representation learning [19, 26, 30].",,,0,not_related
"Recently, memory networks have been introduced in many computer vision tasks, such as anomaly detection [14, 47], few-shot learning [5, 20, 68], video captioning [48], video prediction [29], etc.",,,0,not_related
"We look at the typical reconstruction based comparison (MNAD_recon), as well as the prediction approach (MNAD_pred), using the preceding four consecutive frames to predict the future frame.",,,1,related
"The backbone consists of the U-Net structure, without skip-connections for the MNAD_recon variant.",,,0,not_related
Two versions of the anomaly detector method MNAD [57] are also tested.,,,0,not_related
The MNAD_pred is the only model keeping a consistent performance through the months without any noticeable drift.,,,0,not_related
"We can see that the MSE for the CAE, VQVAE2 and MNAD_recon increases the farther away the test data goes from the training data.",,,1,related
209 Two versions of the anomaly detector method MNAD [57] are also tested.,,,0,not_related
"Alternatively, this was also solved with generative models, such as the generative adversarial network (GAN) [53] and auto-encoders (AE) [12, 21, 30, 30, 39, 46, 48, 48, 48, 50, 58, 61, 62, 73, 82, 88].",,,0,not_related
"Initially, OCCs were explored with hand-crafted features [4, 45, 75, 83], but more recently, end-to-end deep learning models that learn both the feature extractor and classifier have been proposed [1, 6, 8, 13, 16, 20, 44, 46, 50, 50, 53, 60, 62, 69, 73, 74, 87].",,,0,not_related
"Our method MTN-KMIL achieves superior performance when compared with previous SOTA unsupervised learning methods [14, 25, 27, 37, 65] and weaklysupervised approaches [54, 66, 70].",,,1,related
"Alternatively, some approaches depend on data reconstruction using generative models to learn the representations of normal samples by (adversarially) minimising the reconstruction error [6,13,18,18,25,31,32,32,33,37,43,46,47,53,60,73].",,,0,not_related
"introduce the memory module with a new update scheme into the convolutional neural network for anomaly detection, and train the memory according to new feature compactness and separateness losses [15].",,,0,not_related
"trajectory Piciarelli et al. (2008), dynamic texture Mahadevan et al.",,,0,not_related
"…results to classic and state-of-the-art methods on both USCD datasets (Ped1 and Ped2) as in Table 7, we notice that both C3D and AE3D methods for learning representations achieved remarkable results on Ped2, specially regarding AUC comparisons even when compared to recent work (Park et al., 2020).",,,1,related
"When comparing C3D and AE3D results to classic and state-of-the-art methods on both USCD datasets (Ped1 and Ped2) as in Table 7, we notice that both C3D and AE3D methods for learning representations achieved remarkable results on Ped2, specially regarding AUC comparisons even when compared to recent work (Park et al., 2020).",,,1,related
[49] introduced a nonlearnable memory module that can be updated with inputs.,,,0,not_related
"[49] Hyunjong Park, Jongyoun Noh, and Bumsub Ham.",,,0,not_related
"AE [1-6] (Auto-Encoder), VAE [7, 8] (Variational Auto-Encoder), f-AnoGAN [9] and their variants attempt to model the distribution of normality with abnormal samples, by reducing reconstruction error between origin image and reconstructed image.",,,0,not_related
"The deep methods of abnormal event detection can be roughly divided into classificationbased methods [13], [24], [25] and reconstruction-based methods [12], [26], [27].",,,0,not_related
"We compare our method with multiple existing methods [1], [18], [42], [43], [45]–[47], [49]–[55].",,,1,related
Datasets for Anomaly Detection: Today vision algorithms are heavily data driven.,,,0,not_related
– Future Frame Prediction for Anomaly Detection –,,,0,not_related
"Anomaly Detection in Surveillance Cameras: Surveillance cameras are deployed in a variety of scenarios and are expected to function through varying global conditions like natural illumination changes such as day, night, dawn, etc. and weather changes such as rain, snow, and fog.",,,0,not_related
– Learning Memory-guided Normality for Anomaly Detection [29] (Mnad) (CVPR 2020): This method is similar to the MemAE [28].,,,0,not_related
"Recently, some deep learningbased methods have been proposed in various products, such as the detecting defects on fabric, metallic surfaces [19], [34], images and videos [4], [12], [23], [24], [30], [31], [37].",,,0,not_related
