text,label_score,label,target_predict,target_predict_label
"It is also a natural upper bound for certified individual fairness (Ruoss et al., 2020; Peychev et al., 2021)",,,0,not_related
"It is also a natural upper bound for certified individual fairness (Ruoss et al., 2020; Peychev et al., 2021)
2https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data
and prediction consistency (Yurochkin & Sun, 2020) which consider equal predictions across all…",,,1,related
"(Ruoss et al., 2020; Yeom & Fredrikson, 2020; Peychev et al., 2021) not only enforce, but also certify the adherence to individual fairness constraints expressed in logical formulas, weighted Lp metrics or similarity sets defined in the latent space of a generative model.",,,0,not_related
"(Ruoss et al., 2020; Yeom & Fredrikson, 2020; Peychev et al., 2021) not only enforce, but also certify the adherence to individual fairness constraints expressed in logical formulas, weighted L metrics or similarity sets defined in the latent space of a generative model.",,,0,not_related
"Namely, variants of RS have been proposed for various scenarios [9, 23, 43, 44, 72, 99, 104, 122].",,,0,not_related
"[99] combines RS with generative models to achieve provably fair representation learning, and Bojchevski et al.",,,0,not_related
"Several FRL methods have proposed approaches for learning individually fair representations (Lahoti et al., 2019; Ruoss et al., 2020; Peychev et al., 2021), a different notion of fairness than group fairness.",,,0,not_related
Peychev et al. (2021) address robustness from the perspective of individual fairness: they certify that samples close in a feature directions are close in representation space.,,,0,not_related
"Recent studies have explored the certified fair representation of ML [39, 4, 36].",,,0,not_related
"In the literature, the concepts of fairness are usually directly defined at the model prediction level, where the criterion is whether the model prediction is fair against individual attribute changes [39, 36, 50] or fair at population level [54].",,,0,not_related
"[36] Momchil Peychev, Anian Ruoss, Mislav Balunović, Maximilian Baader, and Martin Vechev.",,,0,not_related
"In addition, there is a line of work trying to certify the fair representation [39, 4, 36].",,,0,not_related
"Recent studies have explored the certified fair representation of ML [39, 4, 36].",,,0,not_related
"In the literature, the concepts of fairness are usually directly defined at the model prediction level, where the criterion is whether the model prediction is fair against individual attribute changes [39, 36, 50] or fair at population level [54].",,,0,not_related
"In addition, there is a line of work trying to certify the fair representation [39, 4, 36].",,,0,not_related
"First, several FRL methods have proposed approaches for learning individually fair representations (Lahoti et al., 2019; Ruoss et al., 2020; Peychev et al., 2021), a different notion of fairness than group fairness which we focus on.",,,0,not_related
"First, in the setting of FRL, several methods have proposed approaches for learning individually fair representations (Lahoti et al., 2019; Ruoss et al., 2020; Peychev et al., 2021), a different notion of fairness than group fairness which we focus on.",,,0,not_related
"(Ruoss et al., 2020; Yeom & Fredrikson, 2020; Peychev et al., 2021) not only enforce, but also certify the adherence to individual fairness constraints expressed in logical formulas, weighted Lp metrics or similarity sets defined in the latent space of a generative model.",,,0,not_related
"(Ruoss et al., 2020; Yeom & Fredrikson, 2020; Peychev et al., 2021) not only enforce, but also certify the adherence to individual fairness constraints expressed in logical formulas, weighted L metrics or similarity sets defined in the latent space of a generative model.",,,0,not_related
