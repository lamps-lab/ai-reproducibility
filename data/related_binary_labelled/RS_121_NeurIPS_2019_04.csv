text,label_score,label,target_predict,target_predict_label
of representing learning on the UTS and the MTS datasets Franceschi et al. (2019). D.,,,0,not_related
"Srlt Franceschi et al. (2019): The abbreviation Srlt comes from the paper title — Unsupervised Scalable Representation Learning for Multivariate Time Series. Inspired by how word2vec Mikolov et al. (2013) is trained, this method proposes a novel triplet loss for modeling the time series data.",,,1,related
"of representing learning on the UTS and the MTS datasets Franceschi et al. (2019). D.Conv consists of layers of dilated convolutions. Compared to the previous work Franceschi et al. (2019), we do not use causal convolutions, so as to incorporate information from both before and after time step i when conducting convolution operations. Time-Series Transformer (TsTransformer): The Time-Series Transformer has proved a success in representing the MTS data type Zerveas et al. (2021). It has the same structure as the original transformer encoder Vaswani et al. (2017), except that it replaces the Layer Normalization layer with the Batch Normalization layer and the embedding layer with the linear projection layer.",,,1,related
"of representing learning on the UTS and the MTS datasets Franceschi et al. (2019). D.Conv consists of layers of dilated convolutions. Compared to the previous work Franceschi et al. (2019), we do not use causal convolutions, so as to incorporate information from both before and after time step i when conducting convolution operations. Time-Series Transformer (TsTransformer): The Time-Series Transformer has proved a success in representing the MTS data type Zerveas et al. (2021). It has the same structure as the original transformer encoder Vaswani et al.",,,1,related
"Effects on optimization Non-convex optimization in high-dimensional space has been a major challenge in deep learning since the overall process is affected by many factors Goodfellow et al. (2016), such as the initial parameters, the choice of the optimizer, the model structure, etc.",,,0,not_related
"(2020); Franceschi et al. (2019). Second, under limited computational resources, we chose to do experiments on as many datasets as possible to make conclusions informative and credible.",,,1,related
Srlt Franceschi et al. (2019): The abbreviation Srlt comes from the paper title — Unsupervised Scalable Representation Learning for Multivariate Time Series.,,,1,related
"of representing learning on the UTS and the MTS datasets Franceschi et al. (2019). D.Conv consists of layers of dilated convolutions. Compared to the previous work Franceschi et al. (2019), we do not use causal convolutions, so as to incorporate information from both before and after time step i when conducting convolution operations.",,,1,related
"(2020); Franceschi et al. (2019). Second, under limited computational resources, we chose to do experiments on as many datasets as possible to make conclusions informative and credible. It is worth noting that the model structures presented below all model the time series in both directions. LSTM: Previous work Sagheer and Kotb (2019) shows the usefulness of unsupervised pretraining of LSTM-based autoencoder for MTS prediction tasks.",,,1,related
"(2020); Franceschi et al. (2019). Second, under limited computational resources, we chose to do experiments on as many datasets as possible to make conclusions informative and credible. It is worth noting that the model structures presented below all model the time series in both directions. LSTM: Previous work Sagheer and Kotb (2019) shows the usefulness of unsupervised pretraining of LSTM-based autoencoder for MTS prediction tasks. Here we have simplified the model by constructing a vanilla bidirectional LSTM of two layers. Dilated Convolutional Neural Network (D.Conv): The Convolutional Neural Network performs well on time series forecasting Yue et al. (2022) and demonstrates its strengths",,,1,related
"The evaluation results presented in the aforementioned paper demonstrate that TST outperforms other methods, such as ROCKET [18], dilation-CNN [24], XGBoost [10], and LSTM [39], on a range of datasets, achieving the best average rank across multiple dimensions.",,,1,related
"Using the time series embeddings from [24], it learns embeddings that seamlessly blend with the local context.",,,1,related
"Specifically, for DS and PS, we adopt two layers of LSTM; for C-FID, we adopt ts2vec [24] as the backbone.",,,1,related
"To overcome these limitations, methods such as TNC [56] and T-Loss [17] have been proposed.",,,0,not_related
"In the recent, deep neural networks begin to automatically learn complex features in TSC, including supervised feature mining [24], unsupervised feature learning [23], transformer methods [15, 48, 49, 68], etc.",,,0,not_related
[19] have lower performance on one or more datasets.,,,0,not_related
"e) T-Loss [50]: T-Loss learns scalable general-purpose representations by considering inherent characteristics of time series, including highly variable lengths and sparse labeling.",,,0,not_related
"Existing temporal-level contrastive learning models either consider the temporal dependencies by leveraging temporal contrasting modules [33, 50, 51], or focus on capturing multi-scale contextual information across different granularities [52, 61, 112, 113].",,,0,not_related
"The model is trained through a multinomial logistic regression classifier, which aims to accurately discriminate all time segments in a time series by utilizing the segment indices as labels for the data points.
e) T-Loss [50]: T-Loss learns scalable general-purpose representations by considering inherent characteristics of time series, including highly variable lengths and sparse labeling.",,,0,not_related
"inherent in time series data, researchers have explored the feasibility of distinguishing contextual information at a finegrained temporal level [50, 51, 52].",,,0,not_related
"advantages in representation learning for various types of data, including image [43, 44, 45, 46], video [47, 48, 49] and time series [50, 51, 52, 53].",,,0,not_related
T-Loss [50] T-Loss learns scalable representations by taking highly variable lengths and sparse labeling properties of time series data into account.,,,0,not_related
Temporal-Level TS2Vec [52] TS-TCC [51] T-Loss [50],,,0,not_related
"From the results, it can be observed that contrastive learning models such as TS2Vec [52], TSTCC [51] and T-Loss [50], which emphasize the impact at the temporal level, achieve better results compared to methods that focus on instance-level or prototype-level contrast.",,,0,not_related
Approaches such as T-Loss [50] and TNC [33] utilize the information from the neighborhood to construct positive and negative samples for contrastive learning.,,,0,not_related
"Finally, we follow the same protocol as [10], where a decoder is trained on top of the",,,1,related
"[10] encouraged representations that closely resemble sampled subseries, while Tonekaboni et al.",,,0,not_related
"We perform comprehensive experiments on time series classification to assess the classification performance of our approach, in comparison to other unsupervised time series representation models, namely T-Loss [10], TS-TCC [9], TST [49], and TNC [32].",,,1,related
"[56], is a deep learning architecture developed to model causal relationships in sequential data.",,,0,not_related
"To overcome the limitations of RNNs and CNNs and their variants, there have been roughly twomain streams of research: Transformer-based models [13, 17, 19, 23] and time-series representation learning models [4, 5, 21].",,,0,not_related
The work in [25] adopts subsequences to generate positive and negative pairs.,,,0,not_related
"Similar to the augmentation network, we utilize a neural network with two components, a fully connected layer, and a multi-layer dilated CNN module [25], [61] as the feature extraction encoder.",,,1,related
"Compared to other forms of data, the time series domain has seen less research on contrastive learning [22], [23], [25], [51].",,,0,not_related
"During our experiments, we utilize a grid search algorithm to explore the search space of [1, 2, 3, 4, 5, 10, 15, 20, 25, 30] for optimizing the anomaly threshold proportion r.",,,1,related
"Recently, efforts have been made to utilize self-supervised learning in the context of time series data [22], [23], [25], [39], [51], [61].",,,0,not_related
"Scalable Representation Learning (SRL) (Franceschi, Dieuleveut, and Jaggi 2019) employs negative sampling techniques with an encoder-based architecture to learn the representation.",,,0,not_related
"We encode each sequence with unsupervised representations learned by (Franceschi, Dieuleveut, and Jaggi 2019).",,,1,related
"a) Encoder: Although the instance-level representation learning models like TNC [18] and T-Loss [19] have shown great success in clustering and classification tasks, the methods fail to capture multi-scale features that provide different levels of semantics and improve the generalization capability of learned representations [23] .",,,0,not_related
"The encoder maps the input time series X to its representation R, and the decoder maps R to the output text sequence.
a) Encoder: Although the instance-level representation learning models like TNC [18] and T-Loss [19] have shown great success in clustering and classification tasks, the methods fail to capture multi-scale features that provide different levels of semantics and improve the generalization capability of learned representations [23] .",,,0,not_related
"Then, we use the triplet loss that is used in [31] as the loss function of the similarity prediction task:",,,1,related
"As mentioned in [102], suppose one anchor x , one positive sample x, and K negative samples x k , k∈1,2,··· ,K are chosen, we expect to assimilate x ref and x and to distinguish between x and x k , i.",,,1,related
"We compare MBrain with state-of-the-art models including one supervised classification model MiniRocket [12] and several self-supervised and unsupervised models: CPC [28], SimCLR [7], Triplet-Loss (T-Loss) [17], Time Series Transformer (TST) [45], GTS [36], TS-TCC [16] and TS2Vec [43].",,,1,related
"Thirdly, all the contrast-based methods [13]–[16] construct positive pairs or negative samples based on prior knowledge of the time series dataset or make strong assumptions of the data distribution of time series.",,,0,not_related
"Firstly, most recent studies [12], [13] only learn instancelevel representations, which is unsuitable for point-wise tasks, e.",,,0,not_related
"One of the earliest works, T-Loss [13], following Word2Vec [23], attempts to learn scalable representations by randomly selecting time segments via triplet loss.",,,0,not_related
"For the classification task, we follow the settings in [18] and train an RBF SVM classifier on segment-level representations generated by our baselines.",,,1,related
"Causal Convolution [3, 59, 18] or Dilated Convolution [18, 65]).",,,0,not_related
"Similar to [65], we incorporate both instance-wise loss [18] (Lins) and temporal loss [55] (Ltemp) to model the distance within a couplet.",,,1,related
"Recognizing the significance of the task and the existing gaps in current literature [18, 55, 17, 64, 65], this study dedicates to addressing the time series representation learning problem.",,,0,not_related
"The existing sampling strategies mainly center around the time series’ invariance characteristics, such as temporal invariance [26, 55, 18], transformation and augmentation invariance [54, 64, 68], and contextual invariance [17, 65].",,,0,not_related
"In this way, our CoInception framework can be seen as a set of multiple dilated convolution experts, with much shallower depth and equivalent receptive fields compared with ordinary stacked Dilated Convolution networks [18, 65].",,,0,not_related
"Although ensuring contextual consistency has been demonstrated to be more robust than previous consistencies [17, 18, 55], we recognize",,,0,not_related
"To further strengthen our empirical evidence, we additionally implement a K-nearest neighbor classifier equipped with DTW [9] metric, along with T-Loss [18] and TST [66] beside the aforementioned SOTA approaches.",,,1,related
"Prior research on representation learning in time series data has predominantly focused on employing the self-supervised contrastive learning technique [18, 55, 17, 64, 65], which consists of two main components: sampling strategy and encoder architecture.",,,0,not_related
"To evaluate the effectiveness of the proposed loss function, we compare it to state-of-the-art baselines, which are Triplet-Loss [13], Temporal Neighborhood Coding (TNC) [33], Contrastive Predictive Coding (CPC) [21, 34] and TS2Vec [40].",,,0,not_related
"For the choice of the encoder, we directly adopt the encoder network used in [13].",,,1,related
"Among those methods that consider segmentation as a downstream task, Triplet-Loss [13] and TNC [33] are two state-of-the-art methods.",,,0,not_related
"As stated in the original paper [13, 33], all of these baselines are independent of the architecture of the encoder.",,,1,related
"There are dozens of time series representation learning methods [10, 12, 13, 22, 31, 33, 40], but very few consider segmentation as a downstream task.",,,0,not_related
"4 Effectiveness of LSE-Loss To evaluate the effectiveness of the proposed loss function, we compare it to state-of-the-art baselines, which are Triplet-Loss [13], Temporal Neighborhood Coding (TNC) [33], Contrastive Predictive Coding (CPC) [21, 34] and TS2Vec [40].",,,0,not_related
"For a fair comparison and to ensure that the difference in performance is not caused by the differences in the encoders’ architecture, we followed the convention in [13, 33] and used the same encoder network across all compared baselines.",,,1,related
"fθ (xi )⊺ fθ (x j ) is the similarity, which uses the widely-adopted dot product to measure the similarity [13, 17, 24, 35].",,,0,not_related
"In this paper, we implement the encoder as a causal convolution network [13] because it has been proved to be efficient for time series data, and can alleviate the disadvantages of recurrent neural networks (e.",,,1,related
"For a detailed introduction of these loss functions, refer to the original paper [13, 21, 33, 34, 40].",,,1,related
Triplet-Loss trains the encoder by maximizing the distance between an anchor sample and negative samples and minimizing the distance between the anchor samples and positive samples.,,,0,not_related
"Although several studies have attempted to fill this gap by considering some of the key characteristics of time series, such as the temporal dynamics [44] and the multi-scale semantics [9, 56], existing approaches can still be weak in learning well-performed representations partly due to the following reasons.",,,0,not_related
"Second, some existing approaches rely on domain-specific assumptions, such as the neighbor similarity [9, 44] and the contextual consistency [56], thus are difficult to generalize to various scenarios.",,,0,not_related
[9] adapts the triplet loss to time series to achieve URL.,,,0,not_related
"We compare our CSL with 5 URL baselines specially designed for time series, including TS2Vec [56], T-Loss [9], Table 2: Statistics of used anomaly detection datasets.",,,1,related
"Specifically, convolutional neural network (CNN) [14, 47] and Transformer [48] are commonly-used encoders in recent studies [8, 9, 44, 53, 56, 57].",,,0,not_related
"self-supervised) representation learning (URL) for MTS [8, 9, 37, 44, 53, 56, 57].",,,0,not_related
"We believe that our method is more general as it does not depend on task-specific assumptions like [9, 44, 56].",,,1,related
"In recent years, various deep learning models, including RNNs [55, 43, 45], multi-layer perceptrons (MLP) [62, 63], CNNs [24], and Temporal convolution networks (TCN) [11] are utilized to perform time series forecasting [14, 41, 4, 21].",,,0,not_related
"[11] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi.",,,0,not_related
"To address this problem, T-Loss [26] employs time-based negative sampling and a triplet loss to learn scalable representations for multivariate time series.",,,0,not_related
"address this problem, T-Loss [26] employs time-based negative",,,0,not_related
Experimental results on UCR and UEA time series archives indicated that the representations obtained by T-Loss can be beneficial for the downstream classification task.,,,0,not_related
first [134] employed a sufficiently long and non-stationary subseries in a time series sample as the context.,,,0,not_related
T-Loss: Franceschi et al. [134] proposed an unsupervised time series representation learning method using TCN and a novel Triplet Loss (T-Loss).,,,0,not_related
Experimental results on multiple time series datasets demonstrate that TNC performs better on downstream classification and clustering tasks compared with the T-Loss [134].,,,0,not_related
"Further, the authors [134] employed the Triplet Loss (T-Loss) to keep the context and positive subsequences close, while making the context and negative subsequences far away for representation learning of time series.",,,0,not_related
"Then, we analyze the classification performance of T-Loss, SelfTime, TS-TCC, TST, and TS2Vec after pre-training on 128 UCR and 30 UEA time series datasets.",,,1,related
"T-loss [134], Selftime [135], TS-TCC [38], and TS2Vec [68] are consistency-based PTMs.",,,0,not_related
"For the time-series classification task, we select T-Loss [134], SelfTime [135], TS-TCC [38], TST [29], and TS2Vec [68] to analyze the performance of TS-PTMs and compare them with the supervised FCN [65] model.",,,1,related
(All samples are selected following the negative sampling approach in [9]),,,0,not_related
"Since the focus of our work is on the novel negative sampling technique introduced above, we rely on an existing proven encoder architecture as described in [9,2] based on exponential dilated convolutional neural networks.",,,1,related
"3: (3a) shows the idea of an exponentially dilated causal convolution, repainted from [9].",,,0,not_related
"Triplet networks have been successful in computer vision [5,21] and natural language processing [14] but were only recently introduced for time-series clustering and classification [9].",,,0,not_related
"For more details on the architecture, we refer to [9].",,,1,related
This is similar to [9] who applied a classifier on the learned representation for classification purposes.,,,0,not_related
Previous work for contrastive learning in time-series [9] assumes sufficient variety in the data such that x i will be substantially different from x i and x pos i by random sampling (similar to word2vec [14]).,,,0,not_related
"Formally, this results in algorithm 1 (for one epoch), inspired by [9].",,,1,related
"[9], we normalize the data by substracting the mean and dividing by the variance of the entire dataset.",,,1,related
"We use these as input to the triplet network with the following objective loss function [9,14]:",,,1,related
"Many recently emerged time series embeddings transform one time series into one vector, cf. Kazemi et al. (2019); Franceschi, Dieuleveut, and Jaggi (2019); Nalmpantis and Vrakas (2019); Kim, Hong, and Cha (2020); Tonekaboni, Eytan, and Goldenberg (2021); Yue et al. (2022).",,,0,not_related
Triplet Loss: the triplet loss [31] as a concept was first adopted and introduced in [13] in MTS modeling and soon adopted in [32].,,,0,not_related
"In this study, we evaluate three state-of-the-art unsupervised representation learning techniques, namely Temporal Neighborhood Coding (TNC) [12], Triplet Loss [13], and Contrastive Predictive Coding (CPC) [14], [15].",,,0,not_related
Triplet Loss [13] learns scalable MTS representations with a time-based negative sampling and triplet loss.,,,0,not_related
"Each of our baselines selections ( [12], [13], and [14]) offer a unique approach to generalization, resulting in different methods for extracting latent states in MTS problems.",,,0,not_related
"Recently, self-supervised learning (SSL) techniques have been developed that can generalize to various tasks, data domains, and input structures [10], [21], [24].",,,0,not_related
"RNNs model successive time points based on the Markov assumption [5, 16, 32], while CNNs extract variation information along the temporal dimension using techniques such as temporal convolutional networks (TCNs) [2, 12].",,,0,not_related
"(1) Feature Extraction [28], [35], [79], [87]: Deep learning models are designed to extract latent features from input data.",,,0,not_related
"In recent years, due to the prevalence of deep neural networks, many unsupervised deep learning-based methods have been proposed for anomaly detection [3], [9], [26], [28], [35], [49], [60], [77], [79], [86], [87].",,,0,not_related
"For example, (Franceschi et al., 2019) learns scalable representations for various time series lengths using contrasting positive, negative, and reference pairs with an innovative triplet loss.",,,0,not_related
"the whole-series level[10], the sub-sequence level [2], and the timestamp level [8].",,,0,not_related
"Although some recent works [15, 56] train supervised classifiers using these learned features on temporal data as input, to the best of our knowledge, no method designed for time series performs classification in a fully unsupervised manner.",,,0,not_related
"time series either use pseudo-labels to train neural networks in a supervised fashion [19, 21] or focus on learning deep representations on which clustering can be performed with standards algorithms [15, 56].",,,0,not_related
"Following the previous setting, we evaluate the quality of representations on time series classification in a standard supervised manner (Franceschi, Dieuleveut, and Jaggi 2019; Yue et al. 2022).",,,1,related
"Despite being effective and prevalent, contrastive learning has been less explored in the time series domain (Eldele et al. 2021; Franceschi, Dieuleveut, and Jaggi 2019; Fan, Zhang, and Gao 2020; Tonekaboni, Eytan, and Goldenberg 2021).",,,0,not_related
"Meta-learner Network Previous time series contrastive learning methods (Franceschi, Dieuleveut, and Jaggi 2019; Fan, Zhang, and Gao 2020; Eldele et al. 2021; Tonekaboni, Eytan, and Goldenberg 2021) generate augmentations with either rule of thumb guided by prefabricated human priors or tedious…",,,0,not_related
"Recently, some efforts have been devoted to applying contrastive learning to the time series domain (Oord, Li, and Vinyals 2018; Franceschi, Dieuleveut, and Jaggi 2019; Fan, Zhang, and Gao 2020; Eldele et al. 2021; Tonekaboni, Eytan, and Goldenberg 2021; Yue et al. 2022).",,,0,not_related
"We compare InfoTS with baselines including TS2Vec (Yue et al. 2022), T-Loss (Franceschi, Dieuleveut, and Jaggi 2019), TSTCC (Eldele et al. 2021), TST (Zerveas et al. 2021), and DTW (Franceschi, Dieuleveut, and Jaggi 2019).",,,1,related
"Franceschi et.al. propose to extract subsequences for data augmentation (Franceschi, Dieuleveut, and Jaggi 2019).",,,0,not_related
"In (Franceschi, Dieuleveut, and Jaggi 2019), Franceschi et.al. generate positive and negative pairs based on subsequences.",,,0,not_related
"Architecture The adopted encoder fθ(x) : RT×F → RD consists of two components, a fully connected layer, and a 10-layer dilated CNN module (Franceschi, Dieuleveut, and Jaggi 2019; Yue et al. 2022).",,,1,related
"Each dataset is normalized such that it has zero mean and unit variance, following (Franceschi et al., 2019).",,,0,not_related
"A long-standing line of research has thus focused on efforts in learning informative time series representations, such as simple vectors, that are capable of capturing local and global structure in such data (Franceschi et al., 2019; Gu et al., 2020).",,,0,not_related
T-Loss [13] proposed an unsupervised triplet loss employing time-based negative sampling.,,,0,not_related
"However, only a few researches have been proposed for time-series analysis [13, 14, 15, 16, 17].",,,0,not_related
"In this study, we compared our method with the benchmark models, including DTW [45], T-Loss [13], TNC[14], TS-TCC[16], TST [15], and TS2Vec [17] for 30 classification datasets in the UEA archive.",,,1,related
T-Loss [13] mainly pursued the subseries consistency that encourages representations of the input time segment and its sampled sub-series to be close to each other.,,,0,not_related
"For instance in T-Loss [42], time-based negative sampling with triple loss is utilized, simultaneously.",,,0,not_related
"We follow (Zerveas et al., 2021) to use these baselines for comparison: DTWD which stands for dimension-Dependent DTW combined with dilationCNN (Franceschi et al., 2019), LSTM (Graves, 2012), XGBoost (Chen & Guestrin, 2016), Rocket (Dempster et al., 2020), and a transformer-based TST (Zerveas et…",,,1,related
", 2021) to use these baselines for comparison: DTWD which stands for dimension-Dependent DTW combined with dilationCNN (Franceschi et al., 2019), LSTM (Graves, 2012), XGBoost (Chen & Guestrin, 2016), Rocket (Dempster et al.",,,1,related
applies triplet loss to unsupervised learning through specially designed sampling method [7].,,,1,related
The exponential dilated causal convolution inspired by WaveNet is also used for universal time-series data [7].,,,0,not_related
", state-space models, stochastic recurrent neural networks, etc [37, 38, 21].",,,0,not_related
"Graphs in Figure 6 show that despite the influence of ShapeWord Length LSW , the overlapping optimal interval of three datasets for NSW is approximately [5, 12].",,,0,not_related
"In our scheme, we choose the deep dilated causal convolutional neural network [12] as the encoder backbone given its high efficiency and outstanding excellence in capturing long-range dependencies [5].",,,1,related
"Recently, following previous contrastive learning and masked modeling paradigms, some self-supervised pre-training methods for time series have been proposed (Franceschi et al., 2019; Sarkar & Etemad, 2020; Rebjock et al., 2021; Sun et al., 2021; Yang & Hong, 2022).",,,0,not_related
"work, takes the hierarchical and sequential structure of time series data into account [62], but otherwise does not specifically exploit potential skew product structure of the underlying time series.",,,0,not_related
"We also consider artificial neural networks in the form of a causal convolutional network (cCNN), an established architecture for time series representation learning [62]; and Latent Factor Analysis via Dynamical Systems (LFADS), a recurrent variational autoencoder [63].",,,0,not_related
Lei et al. (2019); Franceschi et al. (2019) used loss function of metric learning to preserve pairwise similarities in the time domain.,,,0,not_related
"To evaluate the performance of models on classification, we follow the same protocol Franceschi et al. (2019), where an SVM classifier with RBF kernel is trained on obtained instance-level representations.",,,1,related
"For time series classification tasks, we include more competitive unsupervised representation learning methods: TS2Vec, T-Loss (Franceschi et al., 2019), TS-TCC (Eldele et al., 2021), TST (Zerveas et al., 2021), TNC (Tonekaboni et al., 2021) and DTW (Chen et al., 2013).",,,0,not_related
"For time series classification tasks, we include more competitive unsupervised representation learning methods: TS2Vec, T-Loss (Franceschi et al., 2019), TS-TCC (Eldele et al.",,,1,related
"• Deep-learning based methods USRL-FordA [7] is an unsupervised method to learn
universal embeddings of time series and achieves stateof-the-art performance in time series classification.",,,0,not_related
• Deep-learning based methods USRL-FordA [7] is an unsupervised method to learn universal embeddings of time series and achieves stateof-the-art performance in time series classification.,,,0,not_related
"The results show that our mthod is superior to RISE, SAXVFSEQL, FS, LRS and USRL-FordA because the 1-to-1 wins numbers for these methods are all more than half of all the datasets.",,,1,related
"Thus, we can conclude the performance of our method on classification accuracy is at the same level with COTE and significantly better than RISE, SAX-VFSEQL, FS, LRS and USRL-FordA.",,,1,related
"to construct different views of the input data, then the feature representation is learned by maximizing the similarity between different views of the same sample and minimizing the similarity of different sample views [31,32].",,,0,not_related
", [15, 23, 34]) have shown that different states of a system (measured by a multivariate time series) can be learned for each time step in a self-",,,0,not_related
", [15, 34]), we first encode MOTS in short time windows with a neural network to a spatio-temporal voxel embedding.",,,1,related
"[17] presented an unsupervised learning model with convolutional kernels for time series feature transformation, in which the dilation factors of kernels increased exponentially layer by layer.",,,0,not_related
"In particular, the work in [11] constructs an encoder-only architecture using TCN with triple loss and negative sampling to generate representation",,,0,not_related
5) TCN [11] constructs an encoder-only architecture using temporal convolutional networks with triple loss and negative sampling to generate representation embeddings.,,,0,not_related
"GRU [22], LSTM [23], [24], dilated convolutions [25], [26] and ResNet [27].",,,0,not_related
"Most existing methods are based on some comparison functions [33] or contrasting learning [26], [34], [35].",,,0,not_related
implementation of [26] and ResNet follows [77].,,,1,related
T-Loss [26] adopts a triplet loss on randomly cropped subseries to enhance,,,0,not_related
"Following previous works [16], [26], [36], we evaluate the",,,0,not_related
"The 1D-convolution module denotes a multi-layer dilated convolution [26], where the first layer has a dilation s = 1; the second layer has a dilation s = 2, and the n-th layer has a dilation s = 2.",,,0,not_related
"We compare MHCCL with eight state-of-the-art approaches in two categories: 1) instance-wise approaches including SimCLR (Chen et al. 2020), BYOL (Grill et al. 2020), TLoss (Franceschi, Dieuleveut, and Jaggi 2019), TSTCC (Eldele et al. 2021) and TS2Vec (Yue et al. 2022), and 2) cluster-wise approaches including SwAV (Caron et al. 2020), PCL (Li et al. 2021a) and CCL (Sharma et al. 2020).",,,1,related
"T-Loss (Franceschi, Dieuleveut, and Jaggi 2019) employs an efficient triplet loss that uses timebased negative sampling to distinguish anchors from negative instances, and assimilate anchors and positive instances.",,,0,not_related
"…has achieved remarkable advantages in diverse applications such as image (Chen et al. 2020; Grill et al. 2020; He et al. 2020; Chen et al. 2021; Dave et al. 2022) and time series classification (Franceschi, Dieuleveut, and Jaggi 2019; Eldele et al. 2021; Yue et al. 2022; Bagnall et al. 2017).",,,0,not_related
"…state-of-the-art approaches in two categories: 1) instance-wise approaches including SimCLR (Chen et al. 2020), BYOL (Grill et al. 2020), TLoss (Franceschi, Dieuleveut, and Jaggi 2019), TSTCC (Eldele et al. 2021) and TS2Vec (Yue et al. 2022), and 2) cluster-wise approaches including SwAV…",,,0,not_related
"There are many non-Transformer-based models proposed in recent years to learn representations in time series (Franceschi et al., 2019; Tonekaboni et al., 2021; Yang & Hong, 2022; Yue et al., 2022).",,,0,not_related
"It can be seen that our model performed best on 3 out of 4 datasets, achieving an average rank of 1.5, followed by Causal CNN.",,,1,related
There are two sequentially stacked sub-layers in an MCAT layer: (1) a multi-head self-attention and (2) a multi-scale CNN.,,,0,not_related
Causal CNN [27]: Causal Convolutional Neural Network (Causal CNN) combines an encoder based on causal dilated convolutions with a triplet loss.,,,0,not_related
Both Rocket and Causal CNN achieve SOTA performance on other multivariate time series datasets.,,,0,not_related
"[15] create an unsupervised machine learning task to generate a generic representation vector for time series data and improve the representation quality, portability, and practicability.",,,0,not_related
"Following the success of unsupervised learning in computer vision [23], [24] and natural language processing [25], different unsupervised learning frameworks for time series data have been proposed that try to make efficient use of large amounts of unlabeled time series data and produce powerful representations [14], [26], [27].",,,0,not_related
In one of the earliest applications of unsupervised learning [26] propose a scalable representation learning framework for time series by applying a triplet loss to positive samples from a timeseries’ subseries and negative samples from other instances.,,,0,not_related
Extracting features by deep learning techniques has been practiced (Franceschi et al. 2019) and applied to tasks like classification (Ismail Fawaz et al. 2020).,,,0,not_related
FDJNet (Franceschi et al. 2019) combines an encoder based on causal dilated convolutions with triplet loss to embed variable-length and multivariate time series.,,,1,related
Extracting features by deep learning techniques has been practiced (Franceschi et al. 2019) and applied to tasks like classification (Ismail Fawaz et al.,,,0,not_related
"Time-series Meta Features: There are prior works that generated standard time-series features [17], tsfresh [12] (that we used for generating part of our meta-features).",,,1,related
"Unfortunately, they are all designed to model one trajectory independently without considering the spatial distance between objects, leading to the overfitting problem[8, 37].",,,0,not_related
The diverse learning tasks that can be defined in a supervised manner directly from the samples only have been proposed and validated their efficacy to learn general-purpose representations [6].,,,0,not_related
"applied to human activity recognition (HAR) data [13], electroencephalography (EEG) data [15], or household consumption data [14], among others.",,,0,not_related
", [14], [15]) motivate to research the applicability of contrastive SSL for vehicle CAN-data.",,,0,not_related
"As contrastive learning methods, we consider Temporal Neighborhood Coding (TNC) [15] and a triplet loss approach (T-Loss) [14].",,,0,not_related
", HAR data [15], T-Loss for Household Consumption data [14], and an AE in [17] for CAN-data.",,,0,not_related
"Regarding the neural networks, we follow [14] and do not perform any hyperparameter optimization on the network parameters.",,,1,related
"In recent years, many deep models have been proposed for temporal modeling, such as MLP, TCN, RNN-based models (Hochreiter & Schmidhuber, 1997; Lai et al., 2018; Franceschi et al., 2019).",,,0,not_related
"to extract the variation information (Franceschi et al., 2019; He & Zhao, 2019).",,,1,related
", 2021) and classification of trajectories for action recognition (Franceschi et al., 2019).",,,0,not_related
"For representation function, a neural network with exponentially dilated causal convolutions [7] with a depth of 10, 40 channels, and embedding size of 320 is used.",,,0,not_related
"(1) Temporal contrastive loss: Similar to [7], we employ triplet loss as the temporal contrastive loss for the ith sample, which is formulated as",,,1,related
"employ a triplet loss in [3], which strives to ensure that a reference time series has a representation that is close to any one of its subseries (a positive sample) but far from negative series (chosen at random).",,,0,not_related
"[28], for example, proposed measuring the similarity in time for time-series data in an unsupervised setup.",,,0,not_related
[31] also applied contrastive learning successfully to time series data.,,,0,not_related
"Existing works for multivariate time series clustering (MTC) can be roughly divided into three groups, dimension reduction-based methods [22], [23], classical distance-based methods [24], and deep learning-based methods [25], [26].",,,0,not_related
"Moreover, the deep learning-based method is an important technique for multivariate time series clustering, such as USRL [25] and DeTSEC [26].",,,0,not_related
"We compare MUSLA with ten representative multivariate time series clustering methods: 1) dimension reductionbased methods, including MC2PCA, SWMDFC, and TCK; 2) classical distance-based methods, including m-kAVG +ED, m-kDBA, m-kShape, and m-KSC; 3) deep learningbased methods, such as USRL and DeTSEC; 4) multiview learning methods, such as multi-view spectral clustering via integrating Nonnegative Embedding and Spectral Embedding (NESE) [53].",,,1,related
", USRL) on Epilepsy and PenDigits, where the results of USRL on RI are reported according to the previous work [25].",,,0,not_related
"USRL developed an encoder-only architecture and a triplet loss to train the model, which could admit variable-length inputs and obtain stable and high-quality representation.",,,0,not_related
"3) MUSLA is only inferior to the deep learning-based methods (e.g., USRL) on Epilepsy and PenDigits, where the results of USRL on RI are reported according to the previous work [25].",,,0,not_related
"In addition, the performance of MUSLA only on
TABLE 3 Performance Comparisons of MUSLA and Contrast Algorithms in Terms of RI
Data sets n Algorithms MC2PCA SWMDFC TCK m-kAVG+ED m-kDBA m-kShape m-kSC USRL DeTSEC NESE MUSLA ArticularyWordRecognition 0.9891 0.8939 0.9734 0.9522 0.9336 0.7582 0.9510 0.9730 0.9718 0.9756 0.9768 AtrialFibrilation 0.5143 0.7429 0.5524 0.7048 0.6857 0.3810 0.6571 0.2000 0.6286 0.6190 0.7238 BasicMotions 0.7910 0.7013 0.8679 0.7718 0.7487 0.5244 0.7718 1.0000 0.7165 0.7449 1.0000 Epilepsy 0.6126 0.6666 0.7856 0.7684 0.7771 0.5136 0.6044 0.9710 0.8397 0.8897 0.8157 ERing 0.7563 0.7724 0.7724 0.8046 0.7747 0.7701 0.7494 0.1330 0.7701 0.7540 0.8414 HandMovementDirection 0.6272 0.6527 0.6353 0.6968 0.6853 0.5728 0.6920 0.3510 0.6275 0.5920 0.7194 Libras 0.8920 0.8611 0.9171 0.9111 0.9133 0.6605 0.9227 0.8830 0.9070 0.9087 0.9412 NATOPS 0.8818 0.7610 0.8334 0.8525 0.8755 0.6534 0.8348 0.9170 0.7143 0.7637 0.9760 PEMS-SF 0.4239 0.7814 0.1909 0.8172 0.7546 0.7303 0.8039 0.6880 0.8058 0.7842 0.8920 PenDigits 0.9288 0.9110 0.9219 0.9345 0.8807 0.8655 0.9208 0.9850 0.8850 0.9064 0.9455 StandWalkJump 0.5905 0.7238 0.7619 0.7333 0.6952 0.3485 0.6571 0.4020 0.7333 0.6476 0.7714 UWaveGestureLibrary 0.8828 0.8246 0.9130 0.9204 0.8934 0.8015 0.9259 0.8840 0.8790 0.8553 0.9129
Arithmetic Mean "" 0.7409 0.7744 0.7604 0.8223 0.8015 0.6316 0.7909 0.6989 0.7899 0.7868 0.8763 Geometric Mean "" 0.7183 0.7702 0.7115 0.8177 0.7966 0.6092 0.7822 0.5883 0.7827 0.7775 0.8709 Absolute Wins "" 1.00 1.00 0.00 0.00 0.00 0.00 1.00 2.50 0.00 0.00 6.50 MUSLA 1-to-1a 11/0/1 11/0/1 11/1/0 11/0/1 12/0/0 12/0/0 11/0/1 9/1/2 11/0/1 11/0/1 - Wilcoxon-Holmb # 6.6667 7.1250 5.1250 3.9167 5.7500 10.2083 5.6250 6.4583 6.5000 6.8333 1.7917 Rank Mean Rank Std # 6.67 2.78 7.17 2.73 5.08 2.47 4.00 1.87 5.75 2.0910.25 1.305.58 2.756.50 3.976.42 2.336.83 2.271.75 1.09 a “MUSLA 1-to-1” indicates the number of MUSLA 1-versus-1 wins/draws/losses. b “Wilcoxon-Holm” indicates the Wilcoxon rank test and Holm’s alpha correction.",,,1,related
"As for the patient-level epileptic wave detection task, we use the following multivariate time series classification models as baselines: EEGNet [23], TapNet [40], MLSTM-FCN [20] and NS [15].",,,1,related
"Unsupervised representation learning for time-series uses triplet loss with negative sampling [14], hierarchical contrastive loss [36], temporal and contextual contrasting [11], local smoothness to define neighborhoods in time [28], and reprogramming acoustic models [32].",,,0,not_related
(3) Negative samples (NS) [14] generates negative samples and trains a dilated causal convolution encoder with triplet loss.,,,0,not_related
"First, unlike images, where its features are mostly spatial, we find time-series data are mainly characterised by the temporal dependencies [8].",,,0,not_related
"Contrastive self-supervised learning techniques have been successfully applied as pretraining tasks in numerous deep learning field, from speech [15], time series [16], [17], structured language models [18]",,,0,not_related
"Authors of [24] investigate representation learning on multivariate time-series data [24], However, their method depends on triplet losse and require explicit mining of negative pairs to train the model, which is quite challenging in practice.",,,0,not_related
"Contrastive learning methods mostly have been proposed for single modality data across a variety of applications including computer vision [10, 12, 32, 37], audio processing [32, 62, 90], natural language processing [23, 28], sensor data analytics [13, 18, 22, 24, 47, 64, 65, 78].",,,0,not_related
"Baselines we select contain end-to-end models, contrastive learning based models (CoST [18], TS2Vec [22], TNC [24], MoCo [51], Triplet [52], CPC [53], TST [54], TCC [55]) and a feature engineered model (TSFresh package).",,,1,related
"The first stage of model is designed to map input sequences into a latent space and extract representations of the input sequences through contrastive loss function [14], [18], [24], [52].",,,0,not_related
"Starting from RNN [3], [4], [5], [24], [57], [68], [69], popular networks which are successful in other research fields are successively applied to time series forecasting, like CNN [1], [2], [18], [22], [52], GNN [9], [10], [70] and Transformer [6], [7], [31], [36], [37], [41], [71].",,,0,not_related
"A wide range of time-series contrastive learing methods crudely choose positive samples from temporal neighbors of the given anchor sample, which includes some false positive samples when temporally neighboring samples are of different semantic information [12], [26], [27].",,,0,not_related
"Multiple pretext losses exist in the literature, such as the reconstruction loss for autoencoders [6] or its variation for the denoising autoencoders, or the triplet loss proposed for times series in [7].",,,0,not_related
"8 pretext losses : the classical reconstruction loss (rec) - the ELBO loss for Variational AutoEncoders (vae) - the triplet loss [7] with K equals to 1,2,5, 10 and combined (tripletKxxx)- a joint reconstruction loss [9] (multi rec).",,,0,not_related
"SSL has been widely exploited in different domains, including computer vision [21–24], audio/speech processing [25], and time-series analysis [26,27].",,,0,not_related
"Moreover, there is convincing evidence that dilated convolutional operations can further enhance the performance of sequence modeling such as forecasting, generation, and representation learning, even outperforming sequence-tosequence models [21, 22, 23].",,,0,not_related
"• The SSL-based methods include SleepDPC [Xiao et al., 2021], CoSleep [Ye et al., 2021], Time-Series representation learning via Temporal and Contextual Contrasting (TS-TCC) [Eldele et al., 2021b], Relative Positioning (RP) [Banville et al., 2021], Temporal Shuffling (TS) [Banville et al., 2021], Momentum Contrast (MoCo) [He et al., 2020], Contrastive Predictive Coding (CPC) [Oord et al., 2018], Dense Predictive Coding (DPC) [Han et al., 2019] and Triplet Loss [Franceschi et al., 2019].",,,0,not_related
"…Relative Positioning (RP) [Banville et al., 2021], Temporal Shuffling (TS) [Banville et al., 2021], Momentum Contrast (MoCo) [He et al., 2020], Contrastive Predictive Coding (CPC) [Oord et al., 2018], Dense Predictive Coding (DPC) [Han et al., 2019] and Triplet Loss [Franceschi et al., 2019].",,,0,not_related
"Similar to anomaly detection for time series, representation learning for time series has a rich body of literature, e.g., Franceschi et al. (2019); Zerveas et al. (2021); Lubba et al. (2019); Christ et al. (2017).",,,0,not_related
"Time series representation learning techniques such as (Franceschi et al., 2019) aim to provide generalpurpose representations that are independent of the downstream task.",,,0,not_related
"1 Contrastive Losses and Anomaly Detection Time series representation learning techniques such as (Franceschi et al., 2019) aim to provide generalpurpose representations that are independent of the downstream task.",,,0,not_related
"In particular, we propose a novel adaptation of unsupervised time series representations (Franceschi et al., 2019) yielding environment-invariant embeddings.",,,1,related
"While we extend Franceschi et al. (2019) for practical reasons such as speed of experimentation and general robustness, we remark our approach carries over also to other architectures, such as Zerveas et al. (2021).",,,1,related
"The basic building block of our embedding network architecture consists of stacked temporal dilated causal convolutions (Bai et al., 2018) following (Franceschi et al., 2019).",,,1,related
"Specifically, the actor πϕ(st) adopts the dilated causal convolutions (Franceschi, Dieuleveut, and Jaggi 2019) as the basic encoder structure to extract latent time series features, and uses a rank embedding table to extract the base model features.",,,1,related
"More recent works try to combine classical CL approaches with time-series specific training objectives and augmentations such as slicing (Tonekaboni et al., 2020; Franceschi et al., 2019; Zheng et al., 2021), forecasting (Eldele et al., 2021) and neural processes (Kallidromitis et al., 2021).",,,0,not_related
"More recent works try to combine classical CL approaches with time-series specific training objectives and augmentations such as slicing (Tonekaboni et al., 2020; Franceschi et al., 2019; Zheng et al., 2021), forecasting (Eldele et al.",,,0,not_related
"…et al., 2020b; Dwibedi et al., 2021; He et al., 2020), and several of which are tailored to unsupervised representation learning of time series (Franceschi et al., 2019; Yèche et al., 2021; Yue et al., 2022; Tonekaboni et al., 2021; Kiyasseh et al., 2021; Eldele et al., 2021; Yang & Hong,…",,,0,not_related
"As a result, several methods emerged: Scalable representation learning (SRL) (Franceschi et al., 2019), neighborhood contrastive learning (NCL) (Yèche et al., 2021), TS2Vec (Yue et al., 2022), and temporal neighborhood coding (TNC) (Tonekaboni et al., 2021) treat the neighboring windows of the time series as positive pairs and use other windows to construct negative pairs.",,,0,not_related
"For this, SRL, NCL, and TS2Vec minimize the triplet loss, contrastive loss, and hierarchical contrastive loss, respectively, while TNC trains a discriminator network to predict neighborhood information.",,,0,not_related
"As a result, several methods emerged: Scalable representation learning (SRL) (Franceschi et al., 2019), neighborhood contrastive learning (NCL) (Yèche et al.",,,0,not_related
", 2020), and several of which are tailored to unsupervised representation learning of time series (Franceschi et al., 2019; Yèche et al., 2021; Yue et al., 2022; Tonekaboni et al., 2021; Kiyasseh et al., 2021; Eldele et al., 2021; Yang & Hong, 2022; Zhang et al., 2022).",,,0,not_related
"As a result, several methods emerged: Scalable representation learning (SRL) (Franceschi et al., 2019), neighborhood contrastive learning (NCL) (Yèche et al., 2021), TS2Vec (Yue et al., 2022), and temporal neighborhood coding (TNC) (Tonekaboni et al., 2021) treat the neighboring windows of the time…",,,0,not_related
"Another research stream has developed time series methods for transfer learning from the source domain to the target domain (Eldele et al., 2021; Franceschi et al., 2019; Kiyasseh et al., 2021; Tonekaboni et al., 2021; Yang & Hong, 2022; Yèche et al., 2021; Yue et al., 2022).",,,0,not_related
"Although there are several approaches to SSRL from multivariate sensor data [41, 55], they are not listed here as they do not consider",,,0,not_related
"Popular methods are normally discriminative approaches that first extract useful temporal representations followed by clustering in the embedding space (Franceschi et al., 2019; Ma et al., 2019).",,,0,not_related
Contrast-based methods [12]–[14] are the main-stream approach of self-supervised representation learning for time series.,,,0,not_related
"One baseline named Scalable Representation Learning [12] is not included in our results, as it requires a much longer running time and we failed to produce its results in several days.",,,1,related
"named Scalable Representation Learning [12] is not included in our results, as it requires a much longer running time and we failed to produce its results in several days.",,,1,related
"Unsupervised Scalable Representation Learning [12] introduces a novel unsupervised loss with timebased negative sampling to train a scalable encoder, shaped as a deep convolutional neural network with dilated convolutions [43].",,,0,not_related
"Classical representation learning methods for time series data[1][2], which focuses on learning a function to automatically transform the raw time series into feature representations, has received more attention.",,,0,not_related
"Clustering in latent space using variational autoencoders and contrastive learning [8, 2, 16] will also be evaluated in future work.",,,0,not_related
"However, the previous works usually focus on time-wise features and need to continuously obtain the features of several time steps [15,12,34], which makes them difficult to be applied to the novel MTS representation.",,,0,not_related
(4) TS-TCC+: It applies TaT as the encoder like W2V+ while training with the pretext task of TS-TCC.,,,1,related
"W2V and our framework are suitable for both short and long time length datasets, and our performances can significantly surpass W2V.",,,1,related
"W2V is a method which pays more attention on the local information, so the causal dilated convolutions which only focuses on previous information is
more suitable than W2V+ which encodes the global information.",,,0,not_related
"To name a few, [15] employs the idea of word2vec [23] which regards part of the time series as word, the rest as context, and part of other time series as negative samples for training.",,,0,not_related
"Many research efforts have been devoted to the self-supervised representation learning of time series [15,12,34] and promising results have been achieved.",,,0,not_related
(2) W2V+: It applies our proposed two tower Transformer-based model as the encoder while training with the pretext task of W2V.,,,1,related
The detailed information and the reason why we choose these methods are as followed: (1) W2V [15]: This method employs the idea of word2vec.,,,1,related
"(6) TST+: It applies TaT as the encoder like W2V+ while training with the
pretext task of TST. (7) NVP+CS: To compare with the regression, in our framework we replace Next Trend Prediction with Next Value Predict (NVP) and regard it as a new strong baseline.",,,1,related
"Inspired by Word2Vec (Mikolov et al., 2013), Scalable Representation Learning (SRL) (Franceschi et al., 2019) proposes a novel triplet loss and tries to learn scalable representations via randomly sampling time segments.",,,0,not_related
"Data Preprocessing Following Franceschi et al. (2019); Zhou et al. (2021), for univariate time series classification task, we normalize datasets using z-score so that the set of observations for each dataset has zero mean and unit variance.",,,1,related
"…Learning (TCL) (Hyvarinen & Morioka, 2016), Contrastive Predictive Coding (CPC) (Oord et al., 2018), Scalable RepresentationLearning (SRL) (Franceschi et al., 2019), Temporal and Contextual Contrasting (TS-TCC) (Eldele et al., 2021b) and Temporal Neighborhood Coding(TNC) (Tonekaboni et…",,,0,not_related
", 2018), Scalable RepresentationLearning (SRL) (Franceschi et al., 2019), Temporal and Contextual Contrasting (TS-TCC) (Eldele et al.",,,0,not_related
", 2013), Scalable Representation Learning (SRL) (Franceschi et al., 2019) proposes a novel triplet loss and tries to learn scalable representations via randomly sampling time segments.",,,0,not_related
"…on reconstruction (Yuan et al., 2019; Fortuin et al., 2018, 2020; Chorowski et al., 2019), clustering (Ma et al., 2019; Lei et al., 2019), contrastive objectives (Oord et al., 2018; Franceschi et al., 2019; Hyvarinen et al., 2019; Tonekaboni et al., 2020; Hyvarinen and Morioka, 2016), and others.",,,0,not_related
"This approach is commonly used for evaluating the quality of representations (Oord et al., 2018; Franceschi et al., 2019; Fortuin et al., 2020).",,,0,not_related
", 2019), contrastive objectives (Oord et al., 2018; Franceschi et al., 2019; Hyvarinen et al., 2019; Tonekaboni et al., 2020; Hyvarinen and Morioka, 2016), and others.",,,0,not_related
"Data augmentations used are described in Appendix C.
Triplet (Franceschi et al., 2020) Triplet proposes a time series self-supervised learning approach by taking positive samples to be substrings of the anchor, and negative samples to be randomly sampled from the dataset.",,,1,related
"While recent work in time series representation learning focused on various aspects of representation learning such how to sample contrastive pairs (Franceschi et al., 2020; Tonekaboni et al., 2021), taking a Transformer based approach (Zerveas et al.",,,0,not_related
"While recent work in time series representation learning focused on various aspects of representation learning such how to sample contrastive pairs (Franceschi et al., 2020; Tonekaboni et al., 2021), taking a Transformer based approach (Zerveas et al., 2021), exploring complex contrastive learning…",,,0,not_related
"on the foundation of random triplet selection in [7] and applied the same network architecture, which consists of layers of dilated causal convolutions.",,,0,not_related
We use a loss function that is close to [7] while allowing forK positive and negative samples for each anchor.,,,1,related
"With respect to time series representation learning, recent work has considered sequential techniques like Long Short Term Memory (LSTM) autoencoders for time series classification [15], similarity preserving low-rank factorization for time series clustering [13], as well as contrastive learning [7] that uses a triplet loss along with a dilational temporal convolutional network architecture [3, 19].",,,0,not_related
Software implementation was extended from code published by [7] with Python 3.,,,1,related
"These issues haven’t received much attention for multivariate time series, where the state of the art [7] simply uses plain random sampling.",,,0,not_related
"In a special case which we generalize in this paper, xpos is a subinterval within xref [7].",,,1,related
"combining anchor selection with similarity based triplet mining outperforms the random triplet selection [7], which itself was shown to be better or comparable against the best non-representation learning based time series classification methods including HIVE-COTE [14], RWS [21], ResNet [20], as well as TimeNet [15] among others.",,,0,not_related
"1) Distance-based methods include derivative DTW(DDDT W) [12] and derivative transform distance (DTDC) [13]; 2) Feature-based methods include bag of SFA symbols(BOSS) [28], time series fores (TSF) [8], time series bag of features (TSBF) [5] and learned pattern similarity(LPS) [4]; 3) Ensemble-based methods include elastic ensembles (EE) [21] and collection of transformation ensembles (COTE) [2]; 4) Deep learning methods include multilayer perceptrons (MLP) [32] and unsupervised scalable representation (FordA) [11].",,,0,not_related
"We reported the performance of the baseline results from the original papers [8], [9], [10], [21], [22], [24], [44] and [11], respectively.",,,1,related
"In the UEA archive experiment, we compared our proposed method with eight different methods, including three benchmarks [44], a bag-of-pattern based approach [8], and deep learning-based methods [9], [10], [11], [21], [22], [24].",,,1,related
Dataset EDI DTWI DTWD MLSTM WEASEL NS TapNet ShapeNet TST TS2Vec ODE-RGRU [44] [44] [44] -FCNs [21] +MUSE [8] [22] [9] [10] [24] [11] (Ours),,,1,related
"Also, compared with distance-based methods such as NS [22], TapNet [9], and ShapeNet [10], it showed sufficiently high performance even on small-sized datasets (e.",,,0,not_related
Negative samples (NS) [22] is an unsupervised learning method with triplet loss through several negative samples.,,,0,not_related
"These approaches use convolutional neural networks (CNNs) or recurrent neural networks (RNNs) to capture spatial or temporal features of time-series data [9], [10], [20], [21], [22].",,,0,not_related
"In [22], an unsupervised method is proposed to solve the variable lengths and sparse labeling problems of time-series data.",,,0,not_related
"Unlike the usual supervision-based triplet selection [16,7], we propose a novel unsupervised triplet selection for 2-D time-series images.",,,1,related
"estimation of the extent of resemblance between the samples utilizing characteristics such as their shape [14], shapelets [7], alignment [3] and structure [13].",,,0,not_related
"Once trained, a model is used as a feature extractor [7] for the end task.",,,0,not_related
"It facilitates representing time-series from a visual perspective inspired by human visual cognition, involving 2-D convolutions, unlike the 1-D convolutional approaches popular in time-series domain [7].",,,0,not_related
Unsupervised triplet loss training has been proposed for time-series [7] where representations are learned and evaluated for time-series classification using 1-D dilated causal convolutions.,,,0,not_related
"[25] proposed the time-contrastive learning framework, which combines triplet loss, causal convolution, and hole convolution network through positive and negative sampling of time series data.",,,0,not_related
"While image [31, 32], video [33], language [34, 35], and speech [36] representations have benefited from contrastive learning, research on learning physiological signals has been limited [37, 38].",,,0,not_related
"The T-loss model (Franceschi et al., 2019) uses a triplet loss, which maximizes the distance between",,,0,not_related
"Recent self-supervised methods for time series data (Franceschi et al., 2019; van den Oord et al., 2018; Tonekaboni et al., 2021) learn representations using forecasting or distance-based metrics in order to employ conventional contrastive loss functions.",,,0,not_related
"The T-loss model (Franceschi et al., 2019) uses a triplet loss, which maximizes the distance between
negative examples that are chosen independently at random while minimizing the distance between the reference sequence and its subsets.",,,0,not_related
"We make the logical assumption similar to other time series approaches (Franceschi et al., 2019; Tonekaboni et al., 2021) that in the input D the smaller the distance (in time) is between two points Dt and Dt′ , the more related they are.",,,1,related
art in a variety of diverse benchmarks a suite of benchmarks in diverse application areas [86].,,,0,not_related
24): – Triplet loss (triplet): This is the loss proposed in Franceschi et al. (2019) that aims to obtain similar representation between a time subseries and its neighborhood (see Eq.,,,0,not_related
– Dilated-CNN (DCNN): This architecture was proposed in Franceschi et al. (2019).,,,0,not_related
"Another loss has been proposed to train an encoder specific to time series, called triplet loss (Franceschi et al. 2019).",,,0,not_related
"Besides offering a variety of datasets, these two archives have been subject to numerous use (Fawaz et al. 2019; Franceschi et al. 2019; Ma et al. 2019; Madiraju et al. 2018; Xiao et al. 2020; Zhang et al. 2018).",,,0,not_related
The authors in Franceschi et al. (2019) proposed to solve this problem by using a time-based sampling strategy.,,,0,not_related
"For all configurations, we use the Adam optimizer with a learning rate of 0.001 (Bo et al. 2020; Franceschi et al. 2019; Madiraju et al. 2018; Mukherjee et al. 2019) at the exception of theDRNN architecture wherewe use the SGDoptimizer with exponential decay, a learning rate of 0.1, and a decay…",,,1,related
"• T-Loss (Franceschi, Dieuleveut, and Jaggi 2019): Unsupervised scalable representation learning for multivariate time series.",,,0,not_related
"Recent progress of self-supervised learning (SSL) (Jing and Tian 2019; He et al. 2020; Chen et al. 2020) has gained promising performance for physiological time series, with competitive performance compared with supervised methods (Franceschi, Dieuleveut, and Jaggi 2019; Xiao et al. 2021).",,,0,not_related
"For time series, a line of SSL methods sample positives from temporally neighbors (Oord, Li, and Vinyals 2018; Franceschi, Dieuleveut, and Jaggi 2019; Tonekaboni, Eytan, and Goldenberg 2021; Eldele et al. 2021).",,,0,not_related
"On certain datasets, our results approach other recent unsupervised approaches in which a simple linear classifier is trained on top of a complex unsupervised feature extractor [43, 70, 71].",,,1,related
"Our encoder is a one layer causal dilated encoder with skip connections, an architecture recently shown to provide strong time series classification performance [71].",,,1,related
"’s [42] unsupervised representation learning algorithm, hereby noted as “RL”, learns representations of time-series elements using an encoder architecture based on causal dilated convolutions with a triplet",,,0,not_related
"This conclusion is also demonstrated in EEG/MEG applications (SelfRegulationSCP1), with 10% labeled samples, SMATE obtained a higher accuracy (0.781) than fully supervised 1NNDTW-D (0.775), USRL (0.771), Semi-TapNet (0.739) and MTL (0.730).",,,1,related
", from 10% labeled training set to fully labeled one, the accuracy of SMATE varies only by 0.046, compared to INN-DTW-D (0.264), USRL (0.286), Semi-TapNet (0.151) and MTL(0.225), showing that SMATE is capable of learning a class-separable representation under weak supervision.",,,1,related
"For comparison, we applied one classic model 1NN-DTW-D [3] and three recently proposed semi-supervised deep learning models: USRL [15], Semi-TapNet [20] and MTL [21].",,,1,related
"USRL [15]; TapNet [20]; MLSTM-FCN [12]; CA-SFCN [6]; SMATENR: SMATE without supervised Regularization, instead, a Softmax layer is applied on the embedding.",,,1,related
"1We use the term spatial in this paper for the variable axis The recent research turns to Representation Learning [14] when handling weakly labeled MTS, which allows learning low dimensional embeddings in an unsupervised manner, such as using triplet loss [15] to form the embedding space, then even an SVM classifier is powerful enough on the learned representation [15].",,,0,not_related
"USRL [15]; TapNet
[20]; MLSTM-FCN [12]; CA-SFCN [6]; SMATENR: SMATE without supervised Regularization, instead, a Softmax layer is applied on the embedding.",,,1,related
Unsupervised Scalable Representation Learning (USRL) described in [15] combines causal dilated convolutions with triplet loss for contrastive learning.,,,0,not_related
"Besides, USRL and SMATENR perform much worse than SMATE with the same SVM classifier, confirming the reliability of our supervised regularization on the embedding space.",,,1,related
"As no label information is utilized to learn the representation [15], there is a risk that it deviates from the true features, thus affecting the classifier performance.",,,1,related
"Some works have studied self-supervision in time series data (Yue et al. 2021; Mehari and Strodthoff 2021; Spathis et al. 2020; Banville et al. 2019; Ma et al. 2019; Franceschi, Dieuleveut, and Jaggi 2019; Hyvarinen and Morioka 2016).",,,0,not_related
(b) composition of the i-th layer of the chosen architecture [10],,,0,not_related
"We use an algorithm [10] that outperforms the previously mentioned methods, also tested on many standard datasets for unsupervised representation learning for time series.",,,1,related
"Overall, the training procedure consists of traveling through the training dataset for several epochs (possibly using mini-batches), picking tuples (x, x,(xk neg )k) at random as detailed in Algorithm 1 of franceschi [10], and performing a minimization step on the",,,1,related
Franceschi Algorithm 1- for time series embedding [10],,,0,not_related
"Negative sampling and triplet loss are frequently used in various tasks [15, 16, 17, 18].",,,0,not_related
"These methods were also applied for multivariate time series representation learning and image similarity learning [17, 18].",,,0,not_related
"[21] proposed an unsupervised scalable representation learning model (USRL) for multivariate time series, which utilized a deep encoder network formed by dilated convolutions to generate informative features.",,,0,not_related
"It is known that there are many works that can reduce dimensionality of multivariate time series, such as CPCA [20], VPCA [17], and deep encoder networks [21].",,,0,not_related
"For example, two-dimensional singular value decomposition [15], variable-based principal component analysis (VPCA) [17], common principal component analysis (CPCA) [18–20], and deep encoder networks [21, 22] are used to reduce dimensionality of multivariate time series and learn informative features for clustering.",,,0,not_related
[25] proposed a metric-learning-based pre-text task for time series.,,,0,not_related
"Comprehensive experiments verify that, compared to PAA and DEA generated by other SOTA architectures, including FDJNet [17], TimeNet [31] and InceptionTime [15], theDEA generated by SEAnet is more effective in preserving the original pairwise distances in the lower-dimensional summarized space.",,,0,not_related
"Unlike most existing encoders with linear final layers [17], the SEAnet encoder is finalized by LayerNorm2, which is specifically designed using the SoS preservation principle.",,,0,not_related
TimeNet [31] and FDJNet [17] are two SOTA architectures for data series representation learning.,,,0,not_related
"[Methods] We evaluated the SEAnet-generated DEA and its applications in data series similarity search against PAA and DEA generated by SEAnet-nD (a simplified version of SEAnet), and our adaptations of FDJNet [17], TimeNet [31], and InceptionTime [15].",,,1,related
"On the other hand, few recent works [17, 31] focus on data series representation learning, none of which targets similarity search.",,,0,not_related
"Unlike FDJNet [17] and other SOTA convolutional architectures for series embedding, SEAnet is composed of both an encoder and a decoder.",,,0,not_related
"Although encoder-only architectures is the popular choice [17], we argue (and experimentally verify) that the decoder is necessary in similarity search applications in order to regularize the DEAs, so that they are distinguishable among each other.",,,1,related
"Its success in data series has also been reported for speech recognition [27], data series classification [17] and many other applications.",,,0,not_related
"In contrast to existing convolutional autoencoders for series embedding [17], SEAnet comprises both an encoder and a decoder.",,,1,related
"In the past years, methods such as (Salinas et al., 2020; Franceschi et al., 2019; Kurle et al., 2020; de Bézenac et al., 2020; Oreshkin et al., 2020a; Rasul et al., 2021; Cui et al., 2016; Wang et al., 2017) have consistently showcased the effectiveness of deep learning in time series analysis tasks.",,,0,not_related
", 2016), ContextFID, leveraging unsupervised time series embeddings (Franceschi et al., 2019).",,,0,not_related
"Additionally, we suggest a Frechet Inception distance-like score that is based on unsupervised time series embeddings (Franceschi et al., 2019).",,,1,related
"Convolutional architectures are able to learn relevant features from the raw time series data (van den Oord et al., 2016; Bai et al., 2018; Franceschi et al., 2019), but are ultimately limited to local receptive fields and can only capture long-range dependencies via many stacks of convolutional layers.",,,0,not_related
"One way to achieve longer realistic synthetic time series is by employing convolutional (van den Oord et al., 2016; Bai et al., 2018; Franceschi et al., 2019) and self-attention architectures (Vaswani et al.",,,0,not_related
"Similarly, the work [7] builds a temporal convolutional network based encoder-only structure using negative sampling and triple loss for representation learning.",,,0,not_related
"TCN is orginally proposed in work [15] and popularly applied in various sequence modeling tasks in works [3], [7].",,,0,not_related
"1) Our model is different from traditional TCN in works [3], [7] in two aspects.",,,1,related
"Different from works [3], [7] that take the whole MTS X(∗, t1 : tn−1) ∈ Rm×(n−1) as input, our TCN takes X(i, t1 : tn−1) ∈ R1×(n−1) for i = 1, 2, .",,,1,related
"Recent deep learning methods, such as [7], [11], explore similarities on low-dimensional embeddings through autoencoder structures.",,,0,not_related
5) TCN [7] constructs an encoder-only architecture using TCN with triple loss and negative sampling to generate representation embeddings.,,,0,not_related
sequence to a sequence of the same length such that the i output sequence is calculated using the values up till i element of the input [2].,,,0,not_related
"Recent work proposes different methods to learn useful representations of the time series data in an unsupervised way that can be leveraged to perform well on downstream tasks such as classification [2], [3].",,,0,not_related
"Recently, ANNs such as the RNN and CNN, have been used to learn supervised [17] or unsupervised representation [18] for time series analysis.",,,0,not_related
"For example, T-Loss (Franceschi, Dieuleveut, and Jaggi 2019) performs instance-wise contrasting only at the instance level; TS-TCC (Eldele et al. 2021) applies instance-wise contrasting only at the timestamp level; TNC (Tonekaboni, Eytan, and Goldenberg 2021) encourages temporal local smoothness in…",,,0,not_related
"As mentioned in section and , T-Loss, TS-TCC and TNC perform contrastive learning at only a certain level and impose strong inductive bias, such as transformation-invariance, to select positive pairs.",,,0,not_related
"TLoss (Franceschi, Dieuleveut, and Jaggi 2019) uses random sub-series from the original time series as positive samples.",,,0,not_related
"For example, T-Loss (Franceschi, Dieuleveut, and Jaggi 2019) performs instance-wise contrasting only at the instance level; TS-TCC (Eldele et al. 2021) applies instance-wise contrasting only at the timestamp level; TNC (Tonekaboni, Eytan, and Goldenberg 2021) encourages temporal local smoothness in a specific level of granularity.",,,0,not_related
"We replace our proposed contextual consistency, including the timestamp masking
and random cropping, into temporal consistency (Tonekaboni, Eytan, and Goldenberg 2021) and subseries consistency (Franceschi, Dieuleveut, and Jaggi 2019).",,,1,related
"Previous works have adopted various selection strategies (Figure 2), which are summarized as follows:
• Subseries consistency (Franceschi, Dieuleveut, and Jaggi 2019) encourages the representation of a time series to be closer to its sampled subseries.",,,0,not_related
"We then follow the same protocol
as T-Loss (Franceschi, Dieuleveut, and Jaggi 2019) where an SVM classifier with RBF kernel is trained on top of the instance-level representations to make predictions.",,,1,related
"In addition, recent works (Eldele et al. 2021; Franceschi, Dieuleveut, and Jaggi 2019) employed the contrastive loss to learn the inherent structure of time series.",,,0,not_related
"Many studies (Tonekaboni, Eytan, and Goldenberg 2021; Franceschi, Dieuleveut, and Jaggi 2019; Wu et al. 2018) focused on learning instance-level representations, which described the whole segment of the input time series and have showed great success in tasks like clustering and classification.",,,0,not_related
"We conduct extensive experiments on time series classification to evaluate the instance-level representations, compared with other SOTAs of unsupervised time series representation, including T-Loss, TS-TCC (Eldele et al. 2021), TST (Zerveas et al. 2021) and TNC (Tonekaboni, Eytan, and Goldenberg 2021).",,,1,related
Two particularly notable models here are Triplet loss [15] and Temporal Neighborhood Coding [34].,,,0,not_related
"Three were unsupervised time series models: Triplet Loss (tloss) [15], Temporal Neighborhood coding (tnc) [34], and Contrastive Predictive Coding (cpc) [36].",,,0,not_related
"We chose to consider as neighbors samples from a patient that are close in time motivated by (Banville et al., 2020) and (Franceschi et al., 2019) works.",,,1,related
"Both Banville et al. (2020) and Franceschi et al. (2019) approach this problem by enforcing temporal smoothness between contiguous samples, similar in spirit to Mikolov et al. (2013).",,,0,not_related
Franceschi et al. (2019) employs time-based negative sampling and a triplet loss to learn scalable representations for multivariate time series.,,,0,not_related
"Triplet-Loss (T-Loss), introduced in (Franceschi et al., 2019), which employs time-based negative sampling and a triplet loss to learn representations for time series windows.",,,0,not_related
"Table 2 shows the results of our proposed method Pattern Discovery with Byte Pair Encoding (PD-BPE), time series embedding proposed in [10] shown here as TS-Embed, and Rocket [7], for IGTB constructs.",,,1,related
Another notable work is an unsupervised embedding approach proposed in [10].,,,0,not_related
"…θ, θ̃) 12: θ ′ ← θ − η∇θJ (θ) 13: end while
lize an original sampling strategy combined with the consistency training [Bachman et al., 2014; Laine and Aila, 2017; Franceschi et al., 2019; Xie et al., 2020] on numerous unlabeled time series to constrain model predictions to be invariant to…",,,1,related
"We recall that Franceschi et al. (Franceschi, Dieuleveut, and Jaggi 2019) followed the principle from word2vec (Mikolov et al. 2013), which makes the assumption that the representation of a word should meet two requirements: (i) the representation should be close to those near its context (Goldberg and Levy 2014), and (ii) it should be distant from those in a randomly chosen context, since they are probably different from the original word’s context.",,,1,related
"Figure 4 shows the loss in using the original triplet loss (Franceschi, Dieuleveut, and Jaggi 2019) to learn shapelet representation.",,,0,not_related
"In (Chechik et al. 2010) and (Schroff, Kalenichenko, and Philbin 2015), only one positive sample and one negative
sample are considered, whereas, in (Franceschi, Dieuleveut, and Jaggi 2019) and (Mikolov et al. 2013), one positive and several negative samples are considered.",,,0,not_related
"Comparison with other methods The experimental accuracies of the baseline results are all taken from the original papers (Bagnall et al. 2018), (Franceschi, Dieuleveut, and Jaggi 2019) and (Zhang et al. 2020), respectively.",,,1,related
"We recall that Franceschi et al. (Franceschi, Dieuleveut, and Jaggi 2019) followed the principle from word2vec (Mikolov et al. 2013), which makes the assumption that the representation of a word should meet two requirements: (i) the representation should be close to those near its context (Goldberg…",,,1,related
"(Franceschi, Dieuleveut, and Jaggi 2019) applies one positive sample and several negative samples when training their neural network, then SVM is utilized to do the final classification.",,,0,not_related
"WEASEL-MUSE is a bag-of-pattern based approach with statistical feature selection, variable window lengths and SAX for MTSC.
• Negative samples (NS) (Franceschi, Dieuleveut, and Jaggi 2019).",,,0,not_related
applied contrastive learning also successfully to timeseries data [14].,,,0,not_related
"Exponentially-dilated convolutions were first proposed for semantic segmentation of images in [33] and have since seen success in other domains like audio synthesis [34] and time series classification [35, 36].",,,0,not_related
[3] use a pure encoderbased network in combination with a so-called triplet loss function for classification on various reference time series.,,,0,not_related
"Methods of representation learning have recently emerged in the field of computer vision and speech recognition, enabling unsupervised feature extraction that outperforms the prediction performance of common manual and automated feature extraction methods in the case of only a small amount of existing labeled data [3].",,,0,not_related
"In order to learn these representations for driving time series, we have calculated a triplet loss function inspired by [21], [22] for driving time series, which is fully unsupervised.",,,1,related
"There is much interest in effective methods for classification of MTS data [3] across a broad range of application domains including finance [58], metereology [8], graph mining [55, 60], audio representation learning [17, 54], healthcare [13, 34], human activity recognition [38, 57], among others.",,,0,not_related
"Besides, feature subset selected by AgnoS impaired the results on all six models, which suggested that AgnoS might not work well on high-dimensional MTS dataset (Face Detection is 144-dimensional).",,,0,not_related
"Noticeably, combination of NFS and MCDCNN achieved an accuracy of 0.636 on the Face Detection dataset, exceeding the most recent work by more than 10%.",,,0,not_related
"Note that MCNN didn’t converge on Face Detection dataset, thus it should be ignored.",,,0,not_related
"After ignoring MCNN on Face Detection dataset, the average Accuracy score of NFS is 0.595, exceeding that of AgnoS by 0.05.",,,1,related
"As for Face Detection, the highest accuracy of 0.636 was obtained by NFS+MCDCNN model trained with NFS feature subset.",,,0,not_related
Face Detection This dataset is from the UEA MTS classification archive4.,,,0,not_related
"Besides the above supervised MTS classification/regression methods, unsupervised MTS representation learning has also been explored [20].",,,0,not_related
"2) Experimental results on four publicly-available datasets (OhioT1DM [5], Favorita [2], PhysioNet 2012 [3] and Face Detection [20] ) show that NFS can boost the performance of state-of-the-art MTS models including Resnet, MCNN, MCDCNN, T-leNet and Transformer by joint training and selecting discriminative features.",,,0,not_related
Table IV summarizes the results obtained on PhysioNet 2012 and Face Detection.,,,0,not_related
"To validate the general learning capability of NSF, we consider four public MTS datasets, namely OhioT1DM, Favorita, PhysioNet 2012 and Face Detection, from varying domains.",,,1,related
Causal convolutions map a sequence to a sequence of the same length such that the i output sequence is calculated using the values up till i element of the input [2].,,,0,not_related
"Recent work proposes different methods to learn useful representations of the time series data in an unsupervised way that can be leveraged to perform well on downstream tasks such as classification [2], [3].",,,0,not_related
"An overall visualization of important ECG segments was constructed with the following steps: (1) all median ECG beats and their corresponding per-patient normalized Guided Grad-CAM maps were aligned temporally by normalizing the PQ and QT intervals, (2) the mean and standard deviations of the ECG signal were derived within each group, (3) the proportion of the per-patient Guided Grad-CAM",,,0,not_related
", 2020) and time-series (Franceschi et al., 2019).",,,0,not_related
"…methods show also strong success in natural language processing (Logeswaran & Lee, 2018; Mikolov et al., 2013; Devlin et al., 2018; van den Oord et al., 2018), video classification (Sun et al., 2019), reinforcement learning (Srinivas et al., 2020) and time-series (Franceschi et al., 2019).",,,0,not_related
"In [9], an unsupervised scalable time series representation (USTR) is proposed using the notion of triplet loss.",,,0,not_related
We do not find consistent representations with USTR.,,,1,related
"In all experiments, we compare Seq2Graph to three time series representation methods: a sequential autoencoder (SAE) [17], to the unsupervised scalable time series representation (USTR) [9] and to the sequence-to-VAR (Seq2VAR) [22].",,,1,related
"In particular, SAE and USTR completely miss the consistent ageing information, as expected from pattern-based approaches.",,,0,not_related
"From top to bottom: USTR [9], SAE [17], Seq2VAR [22], Seq2Graph.",,,1,related
"We see that Seq2Graph outperforms both SAE, USTR and Seq2VAR for unsupervised representation learning, when meaningful information is fully contained in the causality.",,,1,related
"In the current paper, we use SAE, USTR and Seq2VAR as comparative models in the experiment part.",,,1,related
"In [9], an unsupervised scalable time series",,,0,not_related
"sentation Learning In recent years, self-supervised representation learning has been used to capture informative and compact representations of video [1, 33], image [8, 21], text[35], and time series [17, 31, 39, 40] data. 2.2.1 Contrastive Learning. Contrastive learning is an approach used to formulate what makes the samples in a dataset similar or dissimilar using a set of training instances composed of positiv",,,0,not_related
s the first time contrastive learning has been used for change point detection. There is a few works that investigates the use of representation learning with multivariate time series. The authors of [17] proposed a general-purpose approach to learn representations of variable length time series using a deep dilated convolutional network (WaveNet [34]) and an unsupervised triplet loss function based o,,,0,not_related
"For time series data such as audio and ECG, the metric learning based methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al., 2019; Saeed et al., 2020), or multi-task learning based methods that predict different handcrafted features such as MFCCs, prosody,…",,,0,not_related
"Also, in Triplet Loss, a time-based negative sampling is used to capture the inter-sample temporal relation among time pieces sampled from the different time series, which is cannot directly and efficiently
capture the intra-sample temporal pattern of time series.",,,0,not_related
"More recently, in the time series analysis domain, some metric learning based self-supervised methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al., 2019; Saeed et al., 2020), or multi-task learning based self-supervised methods that predict different…",,,0,not_related
"More recently, in the time series analysis domain, some metric learning based self-supervised methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al.",,,0,not_related
"Among those baselines, either global features (Deep InfoMax, Transformation, SimCLR, Relation) or local features (Triplet Loss, Deep InfoMax, Forecast) are considered during representation learning, they neglect the essential temporal information of time series except Triplet Loss and Forecast.",,,0,not_related
"Triplet Loss4 (Franceschi et al., 2019) We download the authors’ official source code and use the same backbone as SelfTime, and set the number of negative samples as 10.",,,1,related
"For time series data such as audio and ECG, the metric learning based methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al.",,,0,not_related
"• Triplet Loss (Franceschi et al., 2019) is an unsupervised time series representation learning model that uses triplet loss to push a subsequence of time series close to its context and distant from a randomly chosen time series.",,,0,not_related
"Dataset Existing SOTA USRLFordA[13] InceptionTime[12] Combined (1NN)[13] OSCNN[53] Best: fcn-lstm[19] Vanilla:RNTransformer [18, 55] ResNetTransformer1 [18] ResNetTransformer2 [18] ResNetTransformer3 [18] Ours",,,1,related
"Dataset Classes SeriesLength Existing SOTA USRLFordA[13] InceptionTime[12] Combined (1NN)[13] OSCNN[53] Best: lstm-fcn [19] Vanilla:RNTransformer [18, 55] ResNetTransformer1 [18] ResNetTransformer2 [18] ResNetTransformer3 [18] Ours",,,1,related
"There is much interest in effective methods for classification of MTS data [3] across a broad range of application domains including finance [60], metereology [8], graph mining [57, 62], audio representation learning [17, 56], healthcare [13, 33, 35], human activity recognition [28, 38, 59], among others.",,,0,not_related
"A multi-layer perceptron (MLP) with two hidden layers as well as temporal convolutional network (TCN) blocks, which have been shown to outperform RNNbased models in sequence modeling tasks (Bai, Kolter, and Koltun 2018; Franceschi, Dieuleveut, and Jaggi 2019).",,,0,not_related
"We can see the results in a comparison against two baselines and a novel SOTA deep-learning architecture by Franceschi, [8].",,,1,related
"Finally, Jansen et al. (2018) rely on a triplet loss and the idea of temporal proximity (the loss rewards similarity of representations between proximal segments and penalizes similarity between distal segments of the time series) for unsupervised representation learning of non-speech audio data.",,,0,not_related
"The dilation-CNN (Franceschi et al., 2019) and XGBoost, which performed best on the remaining 1 dataset, tied and on average ranked 3.",,,0,not_related
", 2019), USRLT (Franceschi et al., 2019) and BeatGAN (Zhou et al.",,,0,not_related
"…of comparisons with related work We next provide a comprehensive comparison between the proposed framework and other state-of-the-art methods, including (WaRTEm (Mathew et al., 2019), DTCR (Ma et al., 2019), USRLT (Franceschi et al., 2019) and BeatGAN (Zhou et al., 2019)), as shown in Table 1.",,,1,related
"Summary of comparisons with related work We next provide a comprehensive comparison between the proposed framework and other state-of-the-art methods, including (WaRTEm (Mathew et al., 2019), DTCR (Ma et al., 2019), USRLT (Franceschi et al., 2019) and BeatGAN (Zhou et al., 2019)), as shown in Table 1.",,,1,related
"Classical approaches in time series forecasting can roughly be split into three main categories: short-term predictors, often relying on historical tools [2], mid-term predictors exploiting deep learning to extract automatically robust features from a large period of time [8] and, finally, longterm predictors that correspond mainly to seasonality [9], [10].",,,0,not_related
"[8], we built an architecture composed of several causal convolutional blocks, transforming the 12× L-sized ECG data to 216 L-dimensional feature maps, where each point in a feature map is based on a history of 383 sample points, including itself.",,,1,related
"However, the application of exponentially dilated causal convolutions is more sensible for time series, such as ECGs, as they take the temporal nature of the data into account and use increasing receptive fields from which ECG features could be extracted [8].",,,0,not_related
"TS shares similarities with the unsupervised metric learning approach of [40], however the sampling procedure and loss function both differ.",,,0,not_related
"In order to overcome the shortcomings of conventional autoregressive methods about the limitation of numbers to be simultaneously forecasted, matrix factorizationbased method [17], unsupervised scalable representation learning [18], DeepGLO that combines a global matrix factorization and a local temporal network [19] have been proposed.",,,0,not_related
"Convolutional architectures are able to learn relevant features from the raw time series data [11, 62, 183], but are ultimately limited to local receptive fields and can only capture long-range dependencies via many stacks of convolutional layers.",,,0,not_related
"One way to achieve longer realistic synthetic time series is by employing convolutional [11, 62, 183] and self-attention architectures [184].",,,0,not_related
"To this end, we investigated the utility of an unsupervised representation learning model proposed by (Franceschi, Dieuleveut, and Jaggi 2019), which can be trained on a large amount of unlabeled data to learn potentially useful feature representations.",,,1,related
"Very few studies have focused on unsupervised representation learning for time series and (Franceschi, Dieuleveut, and Jaggi 2019) is amongst the few general-purpose representation learning algorithms for time series without any structural assumptions on nontemporal data.",,,0,not_related
"To this end, we investigated the utility of an unsupervised representation learning model proposed by (Franceschi, Dieuleveut, and Jaggi 2019), which can be trained on a large amount of unlabeled data to learn potentially useful feature representations.",,,1,related
"time series analysis [30], we reduced each feature of a clip",,,1,related
The encoder is able to obtain meaningful embeddings that perform well on time series classification and regression tasks and trains significantly faster than a traditional RNN encoder-decoder model [11].,,,0,not_related
"In this paper, we propose an embedding algorithm using a state-of-the-art deep unsupervised dilated, causal convolutional encoder model [11] to find informative embeddings from continuous vital sign time series hemorrhage data of six vital signs.",,,1,related
"Two other recent techniques, [13] and [14], use convolutional neural",,,0,not_related
"…use of dilation in convolutional neural networks, where dilation increases exponentially with depth (e.g., Yu and Koltun 2016; Bai et al. 2018; Franceschi et al. 2019), we sample dilation randomly for each kernel, producing a huge variety of kernel dilation, capturing patterns at different…",,,0,not_related
"Franceschi et al. (2019) present a method for unsupervised learning of convolutional kernels for a feature transform for time series input, based on a multilayer convolutional architecture with dilation increasing exponentially in each successive layer.",,,0,not_related
"In contrast to the typical use of dilation in convolutional neural networks, where dilation increases exponentially with depth (e.g., Yu and Koltun 2016; Bai et al. 2018; Franceschi et al. 2019), we sample dilation randomly for each kernel, producing a huge variety of kernel dilation, capturing patterns at different frequencies and scales, which is critical to the performance of the method (see section 4.",,,0,not_related
"Although there exist several methods tailored to representation learning on time series [8, 10, 11], only few models present extensions of the SOM optimized for temporal data.",,,0,not_related
"There exist several methods tailored to representation learning on time series, among them [7, 8, 9], which are however not based on SOMs.",,,0,not_related
CNNs trained using triplet loss for TSC have been very recently proposed for unsupervised learning in [13] and for supervised learning in [3].,,,0,not_related
"Another recent work (Franceschi et al., 2019) makes use of convolutional neural networks in a framework heavily inspired by word2vec (Mikolov et al.",,,0,not_related
"Another recent work (Franceschi et al., 2019) makes use of convolutional neural networks in a framework heavily inspired by word2vec (Mikolov et al., 2013).",,,0,not_related
", 2022], T-Loss [Franceschi et al., 2019], TNC [Tonekaboni et al.",,,0,not_related
"Compared to other forms of data, the time series domain has seen less research on contrastive learning [Eldele et al., 2021a; Franceschi et al., 2019; Fan et al., 2020; Tonekaboni et al., 2021].",,,0,not_related
"Recently, there have been efforts to apply contrastive learning to the time series domain [Oord et al., 2018; Franceschi et al., 2019; Fan et al., 2020; Eldele et al., 2021a; Tonekaboni et al., 2021; Yue et al., 2021].",,,0,not_related
"We first utilized the same input layer and a stacked dilated CNN module [Franceschi et al., 2019; Yue et al., 2021] for both g(x) and h(x), respectively.",,,1,related
"[Franceschi et al., 2019] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi.",,,0,not_related
"In [Franceschi et al., 2019], Franceschi et.al. utilize subsequences to generate positive and negative pairs.",,,0,not_related
"We compare our method with 8 state-of-the-art baselines, including TS2Vec [Yue et al., 2022], T-Loss [Franceschi et al., 2019], TNC [Tonekaboni et al., 2021], TS-TCC [Eldele et al., 2021b], TST [Zerveas et al., 2021], DTW [Chen et al., 2013], TF-C [Zhang et al., 2022] and InfoTS [Luo et al., 2023].",,,1,related
"Recently, some representation methods of learning time series based on deep learning have been proposed, such as DTCR [28] and USRLTS [12].",,,0,not_related
USRLTS constructs a scalable network to learn the representation of time series with variable lengths.,,,0,not_related
"Unsupervised time-series embedders have been proposed [29, 30] to deal with label scarcity.",,,1,related
We used [29] for our stream data embedder since it is more flexible to different time-series lengths and generates good representations for anomaly data compared to [30].,,,1,related
[29] use the triplet loss function to learn a representation of multidimensional time-series data.,,,0,not_related
"We follow (Zerveas et al., 2021) to use these baselines for comparison: DTWD which stands for dimension-Dependent DTW combined with dilation-CNN (Franceschi et al., 2019), LSTM (Graves, 2012), XGBoost (Chen & Guestrin, 2016), Rocket (Dempster et al., 2020), and a transformer-based TST (Zerveas et…",,,1,related
", 2021) to use these baselines for comparison: DTWD which stands for dimension-Dependent DTW combined with dilation-CNN (Franceschi et al., 2019), LSTM (Graves, 2012), XGBoost (Chen & Guestrin, 2016), Rocket (Dempster et al.",,,1,related
[32] [C2] Unsupervised representation for times series Neurips 2019,,,0,not_related
Franceschi et al.[5] introduces an unsupervised contrastive learning framework by introducing a novel triplet selection approach based on segment’s context.,,,0,not_related
"Causal convolutions have been shown to be effective in creating embeddings[16] for time series data, allowing for an easier interpretation of long sequences.",,,0,not_related
"Despite the success of supervised learning, in particular that of recurrent architectures in the field of time-series forecasting, other machine learning branches have also proposed various models for time-series forecasting, such as, state spate models in (Franceschi et al., 2020), representation learning based models in (Rangapuram et al.",,,0,not_related
"rithms [29], [30], [31], [72], DCRLS-based classification uses a two-step method.",,,0,not_related
"Following the previous work in [72], we embed a one-nearestneighbor (1-NN) classifier into the DCRLS-based classifica-",,,1,related
• T-Loss: a method based on random sub-series technique ResNet as its feature extractor [72].,,,0,not_related
"Inspired by Word2Vec [Mikolov et al., 2013], Scalable Representation Learning (SRL) [Franceschi et al., 2019] proposes a novel triplet loss and tries to learn scalable representations via randomly sampling time segments.",,,0,not_related
"…Learning (TCL) [Hyvarinen and Morioka, 2016], Contrastive Predictive Coding (CPC) [Oord et al., 2018], Scalable RepresentationLearning (SRL) [Franceschi et al., 2019], Temporal and Contextual Contrasting (TS-TCC) [Eldele et al., 2021a] and Temporal Neighborhood Coding(TNC) [Tonekaboni et…",,,0,not_related
", 2013], Scalable Representation Learning (SRL) [Franceschi et al., 2019] proposes a novel triplet loss and tries to learn scalable representations via randomly sampling time segments.",,,0,not_related
"Time-Contrastive Learning (TCL) [Hyvarinen and Morioka, 2016], Contrastive Predictive Coding (CPC) [Oord et al., 2018], Scalable RepresentationLearning (SRL) [Franceschi et al., 2019], Temporal and Contextual Contrasting (TS-TCC) [Eldele et al., 2021a] and Temporal Neighborhood Coding(TNC) [Tonekaboni et al., 2021] are all segment-level methods which sample contrastive pairs along temporal axis.",,,0,not_related
"We compare our performances with stateof-the-art approaches CPC, SRL, TS-TCC and TNC.",,,1,related
", 2018], Scalable RepresentationLearning (SRL) [Franceschi et al., 2019], Temporal and Contextual Contrasting (TS-TCC) [Eldele et al.",,,0,not_related
"Following [14, 30], we fit an SVM classifier on the learned representations for classification.",,,1,related
"Previous studies in [14, 30] show that the dilated CNN-based networks, such as the temporal convolutional network (TCN) [5], are better at capturing the long-term dependencies in time series than the Recurrent Networks or Transformer based networks.",,,0,not_related
"1, 3, 4, 8, 10, 13, 15 [14] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi.",,,0,not_related
"2 Positive and Negative Selection for Triplet Learning For any time series x, T-Loss [14] selects positive subseries xpos such that the positive subseries itself is part of the anchor subseries, i.",,,0,not_related
"Recent works [14, 28, 13, 30] employ self-supervised contrastive learning to learn time series representations.",,,0,not_related
"[14] presented T-Loss, a time series representation learning method based on triplet loss and time-based negative sampling.",,,0,not_related
3 Triplet loss Our triplet loss formulation is based on word2vec [19] and T-Loss [14].,,,1,related
"We compare TS-Rep with the self-supervised methods [14, 13, 30], and a VAE-based method Incr-VAE [4].",,,1,related
"In recent years, self-supervised contrastive learning-based methods have gained attention for generalised time series representation learning [28, 13, 14, 30].",,,0,not_related
"1 Network Architecture Our network is identical to T-Loss [14], and further details about the network are given in Appendix D.",,,1,related
"…methods use measure of similarity to train the encoders (Lei et al., 2017; Ma et al., 2019; Madiraju et al., 2018), and more recent methods use different types of contrastive objectives for training (Oord et al., 2016; Franceschi et al., 2019; Tonekaboni et al., 2020; Hyvarinen and Morioka, 2016).",,,0,not_related
", 2018), and more recent methods use different types of contrastive objectives for training (Oord et al., 2016; Franceschi et al., 2019; Tonekaboni et al., 2020; Hyvarinen and Morioka, 2016).",,,0,not_related
"One baseline named Scalable Representation Learning [15] is not included in our results, as it requires much longer running time and we failed to produce its results in several days.",,,1,related
"Contrast based methods [15], [16] are mainly to apply a segment-level sampling policy to conduct the contrastive learning.",,,0,not_related
"Unsupervised Scalable Representation Learning [15] introduces a novel unsupervised loss with time-based negative sampling to train a scalable encoder, shaped as a deep convolutional neural network with dilated convolutions [30].",,,0,not_related
"In summary, the results demonstrate that the SEAnet DEA is robust across various dataset properties and outperforms its competitors by better preserving original pairwise distances and nearest neighborhood structure, leading to better approximate similarity search results than traditional (PAA-based) and alternative deep learning (DEA-based using FDJNet [23], TimeNet [26], and InceptionTime [27]) approaches.",,,0,not_related
"Unlike most existing encoders with linear final layers [23], the SEAnet encoder is finalized by LayerNorm2, which is specifically designed using the SoS preservation principle.",,,0,not_related
T-Loss [6] uses random sub-series from the original time series as positive samples.,,,0,not_related
"We compare TS2Vec with other SOTAs of unsupervised time series representation, including T-Loss [6] and TNC [5], on time series classification tasks.",,,1,related
"We adopt the same evaluation protocol as [6], which trains a SVM with RBF kernel on top of the instance-level representations to predict the label of an instance.",,,1,related
"The representation dimensions of TS2Vec, T-Loss and TNC are all set to 320 and under SVM evaluation protocol [6] for fair comparison.",,,0,not_related
"To address this problem, T-Loss [6] employs time-based negative sampling and a triplet loss to learn scalable representations for multivariate time series.",,,0,not_related
"Many studies [4, 5, 6, 7] focused on learning instance-level representations, which described the whole segment of the input time series and have showed great success in downstream tasks like clustering and classification.",,,0,not_related
", TLoss [6]) employed the contrastive loss to learn the inherent structure of time series.",,,0,not_related
"Following [6], the representation dimension is set to 320.",,,0,not_related
[1] ein rein Encoder-basiertes Netzwerk in Kombination mit einer sogenannten Triplet-Loss-Funktion für die Klassifikation auf diversen Referenzzeitreihen nutzen.,,,1,related
"Methoden des Representation Learning haben sich in jüngster Zeit im Bereich der Bild- und Sprachverarbeitung etabliert und ermöglichen eine unüberwachte Featureextraktion, welche die Prädiktionsgüte gängiger Verfahren der manuellen und automatisierten Featureextraktion im Fall von nur wenigen vorhandenen gelabelten Daten übertrifft [1].",,,1,related
"Predicting neighboring time series segments, have also resulted in similarly powerful embeddings for time series clustering tasks(Franceschi et al., 2019).",,,0,not_related
"Negative sampling techniques have been used in the past in self-supervised learning for time series, in changepoint detection(Deldari et al., 2020) and clustering(Franceschi et al., 2019).",,,0,not_related
"[13] combined an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining low-dimensional features for multivariate time series of variable length.",,,0,not_related
"For time series data such as audio and ECG, the metric learning based methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al., 2019; Saeed et al., 2020), or multi-task learning based methods that predict different handcrafted features such as MFCCs, prosody,…",,,0,not_related
"Triplet Loss5 (Franceschi et al., 2019) We download the authors’ official source code and use the same backbone as SelfTime, and set the number of negative samples as 10.",,,1,related
"More recently, in the time series analysis domain, some metric learning based self-supervised methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al., 2019; Saeed et al., 2020), or multi-task learning based self-supervised methods that predict different…",,,0,not_related
"More recently, in the time series analysis domain, some metric learning based self-supervised methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al.",,,0,not_related
"Among those baselines, either global features (Deep InfoMax, Transformation, SimCLR, Relation) or local features (Triplet Loss, Deep InfoMax, Forecast) are considered during representation learning, they neglect the essential temporal information of time series except Triplet Loss and Forecast.",,,0,not_related
"For time series data such as audio and ECG, the metric learning based methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al.",,,0,not_related
"Also, in Triplet Loss, a time-based negative sampling is used to capture the inter-sample temporal relation among time pieces sampled from the different time series, which is cannot directly and efficiently capture the intra-sample temporal pattern of time series.",,,0,not_related
"• Triplet Loss (Franceschi et al., 2019) is an unsupervised time series representation learning model that uses triplet loss to push a subsequence of time series close to its context and distant from a randomly chosen time series.",,,0,not_related
"To this end, we use the triplet loss introduced in [2] that follows the word2vec’s intuition.",,,1,related
[2] aims to classify time series using an unsupervised representation learning with causal Convolutional Neural Network (CNN).,,,0,not_related
We will pre-train our encoder in a self-supervised manner using the so-called triplet loss defined in [2].,,,1,related
"Our results are consistent across all datasets that we trained on: while the benchmark performed just under the state-of-the-art results of Franceschi et al [2], the Transformer approach failed to leverage the representation learned in the pre-training and is overfitting early in the training process.",,,1,related
"Based on the recent success of Transformers [4] on machine translation, we try to combine the unsupervised representation learning of [2] with a Transformer to improve the accuracy on the standardized datasets of the UCR archive1.",,,1,related
The work of Franceschi et al. [2] aims to classify time series using an unsupervised representation learning with causal Convolutional Neural Network (CNN).,,,0,not_related
"We observe that while our benchmark EmbConv performs just under the state-of-the art results of Franceschi et al. [2], the simplified BERT BenchBert fails to produce viable results and is prone to over-fitting, even when performing a pre-training.",,,1,related
"Notice that EmbConv reaches an accuracy of 69% on the testing set, where the state-ofthe-art method presented in [2] reaches an accuracy of 74%.",,,1,related
"[2], the simplified BERT BenchBert fails to produce viable results and is prone to over-fitting, even when performing a pre-training.",,,0,not_related
"All rights reserved.
by the application of deep neural networks, which learn appropriate representations of time series within their hidden layers (Franceschi, Dieuleveut, and Jaggi 2019).",,,0,not_related
We also experiment with a recent general-purpose unsupervised timeseries representation proposed in [5].,,,1,related
"Though USRL demonstrates encouraging performance on timeseries classification [5], it does not perform well on timeseries anomaly detection.",,,0,not_related
The paper Unsupervised Scalable Representation Learning for Multivariate Time Series by [1] presents an unsupervised approach to learning representations for time series that can be used for subsequent classification.,,,0,not_related
This is the case for these experiments since subseries are explicitly sampled at different lengths from the dataset (see Algorithm 1 in [1]).,,,0,not_related
"In this work, we have presented a replication study of the work by [1] and found that most of the results are replicable.",,,1,related
