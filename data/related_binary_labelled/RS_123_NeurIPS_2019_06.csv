text,label_score,label,target_predict,target_predict_label
Dynamical systems have been treated by learning the Lagrangian or Hamiltonian with correspondingly Lagrangian NNs [321–323] and Hamiltonian NNs [324].,,,0,not_related
"Given the positions and velocities as a function of time of a two-body system interacting with gravitational force, we trained Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019) to match the data.",,,1,related
"Note that this set up is different from the original HNN paper where they train the network to match the velocity + acceleration, given position + velocity at every point in time.",,,1,related
"For the model, we are using Hamiltonian neural network (Greydanus et al., 2019) that consists of 6 linear layers with softplus activation except on the last linear layer.",,,1,related
"For example, Lagrangian [9]–[13] and Hamiltonian formulations [14]–[20] have been used to design neural network models to approximate dynamics of mechanical systems, where the equations of motions are enforced in the neural network architecture.",,,0,not_related
"[2019a], Hamiltonian neural nets Greydanus et al. [2019]) have been successfully deployed in this domain. In these approaches, the spatial and temporal updates of a PDE model are matched with ground truth experimental observations. A loss function is dened based on the mismatch between the simulation and the ground truth, and then the physics model is learned by back-propagation of error gradients of the loss function Xue et al. [2021a], Raissi et al.",,,0,not_related
"Physics model learning from data have been explored in Greydanus et al. [2019], Cranmer et al.",,,0,not_related
"[2019a], Hamiltonian neural nets Greydanus et al. [2019]) have been successfully deployed in this domain. In these approaches, the spatial and temporal updates of a PDE model are matched with ground truth experimental observations. A loss function is dened based on the mismatch between the simulation and the ground truth, and then the physics model is learned by back-propagation of error gradients of the loss function Xue et al. [2021a], Raissi et al. [2019a]. Nevertheless, such learning processes are expensive because of the need to back-propagate gradients over spatial and temporal simulations involving millions of mutually interacting elements. One line of successful approaches to accelerate the learning of PDE models exploits the sparsity nature of system changes over time. For example, during the microstructure evolution of many engineering materials, only the boundary of the microstructure changes while large portion of the system remains unchanged. It is also assumed that the corresponding PDE models can be decomposed into afne function of parameter functions and feature functions Nasim et al. [2022], Sima and Xue [2021]. The combination of decomposablity of the PDE model and sparse changes/updates over time together create opportunities for efcient algorithms which handle learning in compressed spaces using random projections and/or locality sensitive hashing.",,,0,not_related
"Physics model learning from data have been explored in Greydanus et al. [2019], Cranmer et al. [2020b], Lutter et al. [2018], Niu et al.",,,0,not_related
"Physics model learning from data have been explored in Greydanus et al. [2019], Cranmer et al. [2020b], Lutter et al.",,,0,not_related
"[2019a], Hamiltonian neural nets Greydanus et al. [2019]) have been successfully deployed in this domain.",,,0,not_related
"[2019a], Hamiltonian neural nets Greydanus et al. [2019]) have been successfully deployed in this domain. In these approaches, the spatial and temporal updates of a PDE model are matched with ground truth experimental observations. A loss function is dened based on the mismatch between the simulation and the ground truth, and then the physics model is learned by back-propagation of error gradients of the loss function Xue et al. [2021a], Raissi et al. [2019a]. Nevertheless, such learning processes are expensive because of the need to back-propagate gradients over spatial and temporal simulations involving millions of mutually interacting elements.",,,0,not_related
"[2019a], Hamiltonian neural nets Greydanus et al. [2019]) have been successfully deployed in this domain. In these approaches, the spatial and temporal updates of a PDE model are matched with ground truth experimental observations. A loss function is dened based on the mismatch between the simulation and the ground truth, and then the physics model is learned by back-propagation of error gradients of the loss function Xue et al. [2021a], Raissi et al. [2019a]. Nevertheless, such learning processes are expensive because of the need to back-propagate gradients over spatial and temporal simulations involving millions of mutually interacting elements. One line of successful approaches to accelerate the learning of PDE models exploits the sparsity nature of system changes over time. For example, during the microstructure evolution of many engineering materials, only the boundary of the microstructure changes while large portion of the system remains unchanged. It is also assumed that the corresponding PDE models can be decomposed into afne function of parameter functions and feature functions Nasim et al. [2022], Sima and Xue [2021]. The combination of decomposablity of the PDE model and sparse changes/updates over time together create opportunities for efcient algorithms which handle learning in compressed spaces using random projections and/or locality sensitive hashing. Nevertheless, such decomposablity structure applies to a limited class of PDEs and sparsity structures may change with varying initial and boundary conditions (BC/IC). This paper propose a more general approach for efciently learning PDE models via random projection, by exploiting sparsity in both value domain and frequency domain, and also approximating nondecomposable functions with decomposable polynomials. We observe that, systems modeled by PDEs often have slow and gradual updates across wide regions in addition to a few rapid changes concentrated in small “interfacial” regions. Such systems are frequently found in the real world. For example, during manufacturing processes such as laser sintering of powder materials into dense solids, grain boundary changes sharply at the interface area (sparse local change), while temperature rises gradually around the whole material (dense global change). Systems with dense global change and sharp interface change limit the application of existing approaches Nasim et al. [2022], Sima and Xue [2021] for efciently learning relevant PDE models.",,,0,not_related
"[2019a], Hamiltonian neural nets Greydanus et al. [2019]) have been successfully deployed in this domain. In these approaches, the spatial and temporal updates of a PDE model are matched with ground truth experimental observations. A loss function is dened based on the mismatch between the simulation and the ground truth, and then the physics model is learned by back-propagation of error gradients of the loss function Xue et al. [2021a], Raissi et al. [2019a]. Nevertheless, such learning processes are expensive because of the need to back-propagate gradients over spatial and temporal simulations involving millions of mutually interacting elements. One line of successful approaches to accelerate the learning of PDE models exploits the sparsity nature of system changes over time. For example, during the microstructure evolution of many engineering materials, only the boundary of the microstructure changes while large portion of the system remains unchanged. It is also assumed that the corresponding PDE models can be decomposed into afne function of parameter functions and feature functions Nasim et al. [2022], Sima and Xue [2021].",,,0,not_related
"Recent advances have sought to alleviate these constraints by drawing inspiration from Hamiltonian systems [4, 5], a class of dynamical systems governed by Hamilton’s equations.",,,0,not_related
"Researchers have employed machine learning models to discover Green’s functions [11] [18], symmetries [11], Hamiltonian’s [19], Dynamical Systems [13], Delay Dynamical Systems [31], and invariant quantities [40] directly from scientific data.",,,0,not_related
"For instance, Hamiltonian NN [47] draws inspiration from Hamiltonian mechanics and trains models to respect exact conservation laws, resulting in better inductive biases.",,,0,not_related
"Motivated by prior research [5, 6], this study aims to develop a method that captures the inductive bias of energy changes in rigid bodies as external conditions vary while preserving the high-precision modeling of 6-DoF equations for rigid bodies and the high-precision forward and backward sliding along the temporal dimension.",,,0,not_related
"Building upon, the authors in [5, 6] proposed a deep generative model termed the Hamiltonian generative network (HGN), which can learn the Hamiltonian dynamics of continuous-time evolution systems, exhibiting features such as time reversibility and smooth temporal interpolation.",,,0,not_related
"For instance, the authors in [5] devised a robust induction bias for energy conservation, yielding an intriguing byproduct: time reversibility.",,,0,not_related
"We adapt Hamiltonian neural networks (HNNs) [2], [3] (leftmost, Figure 2) for the task of regressing the Hamiltonian and vector field of a Hamiltonian system from random samples of the vector field.",,,1,related
"Hamiltonian neural networks leverage learning biases and use Hamilton’s equations as soft constraints in the loss function of the neural network to favour convergence toward the Hamiltonian [2], [3].",,,0,not_related
"They use a learning bias [4] based on physics information regarding Hamilton’s equations [2], [3] to aid the neural network in converging towards solutions that adhere to physics laws [4].",,,0,not_related
"Our work emulates theirs, by embedding Hamilton’s equations within the loss function of a neural network to regress the Hamiltonian [3] and using automatic differentiation of the regressed Hamiltonian to yield the regressed vector field [3].",,,1,related
[3] independently use physics-informed machine learning methods to regress the value of the Hamiltonian from multiple evenly-spaced samples along multiple Hamiltonian trajectories.,,,0,not_related
"A recent advancement in the modelling of dynamical systems is Hamiltonian neural networks [2], [3], which are physics-informed neural networks with learning biases given by Hamilton’s equations and their corollaries [4].",,,0,not_related
"To that end, we begin by highlighting the Hamiltonian neural network (HNN) framework [13], in which the central idea is energy-based modeling.",,,1,related
"Representation models include Lagrangian Neural Networks (LNN) [8, 23], Hamiltonian neural networks (HNN) [12], and Neural ODE [4, 13].",,,0,not_related
"Previous efforts in learning dynamics from images [23, 47, 4, 54] consider only 2D planar systems (e.",,,0,not_related
[23] predict symplectic gradients of a Hamiltonian system using a Hamiltonian parameterized by a neural network.,,,0,not_related
"This is a generalization of the existing literature where dynamics of canonical Hamiltonian systems are learned with the canonical symplectic form as the physics prior [23, 16, 13, 47].",,,0,not_related
"This formulation has been used by several authors to learn unknown dynamics: the Hamiltonian structure (canonical symplectic form) is used as a physics prior and the unknown dynamics are uncovered by learning the Hamiltonian [23, 56, 47].",,,0,not_related
"A growing body of work incorporates Hamiltonian and Lagrangian formalisms to improve the accuracy and interpretability of learned representations in neural network based dynamical systems forecasting [23, 13, 16].",,,0,not_related
[13] improve the long-term prediction performance of [23] by minimizing the MSE over predicted state trajectories rather than one-step symplectic gradients.,,,0,not_related
"The latter in turn may be done by the combination of raw data and physical equations [27, 28], by enforcing a metriplectic structure to the model, related with the fulfillment of thermodynamic laws [29] or by defining the specific structure of the model [30, 31].",,,0,not_related
8 Objective and Design of the Experiments We have chosen two Hamiltonian neural networks (HNNs)[19] from ”Hamiltonian Neural Networks for Solving Equations of Motion” by M.,,,0,not_related
"1 Introduction to Hamiltonian Neural Networks Hamiltonian neural networks (HNNs), as proposed in the literature [28, 19, 8, 7, 11], introduce a novel approach to solving differential equations that describe dynamical systems.",,,0,not_related
in [42] introduce Hamiltonian Neural Networks (HNNs).,,,0,not_related
"[42] Hamiltonian mechanics neural network self-constructed trajectory data, [133], pixel observations modeling of problems where conservation of energy is important loss functions, MSE PGML",,,0,not_related
"By integrating knowledge from analytical mechanics, neural networks can learn the dynamics that adheres to physical laws, such as the conservation of energy, and even uncover these laws from data [8, 10, 28].",,,0,not_related
"Another work in the same vein as our method is Hamiltonian Neural Networks [6], which parameterises a vector field which conserves energy by formulating it as the symplectic gradient of an energy function.",,,0,not_related
"The latter’s automatic encoding of priors enables dynamical behaviors to be learned by neural networks [6,7].",,,0,not_related
"These approaches are known as Hamiltonian (HNN) [20, 21, 22, 14], and Lagrangian neural networks (LNN) [15, 16, 17], and Graph Neural ODEs [23, 18, 24].",,,0,not_related
We observe that HGNN outperforms both HNN and HGN on both spring and pendulum systems.,,,1,related
Note that the decoupling of kinetic and potential energies is implemented in HNN.,,,1,related
"5 shows the performance of HNN, HGN, and HGNN for spring and pendulum systems.",,,0,not_related
"While the performance of HNN has been demonstrated on several spring and pendulum systems, HGN [20] has been evaluated only on spring systems.",,,0,not_related
"Second, HGN [20] is a graph-based version of HNN, albeit without decoupling the kinetic and potential energies.",,,0,not_related
"The first, HNN [21], is a simple MLP that directly predicts the Hamiltonian of the system.",,,0,not_related
"Additional evaluation of the HGNN architecture is performed by comparing it with two baselines, namely, HNN (which is a physics-enforced MLP) and HGN, which does not decouple potential and kinetic energies (see Supplementary Materials) and on additional metrics such as energy and momentum error.",,,0,not_related
"[21] Samuel Greydanus, Misko Dzamba, and Jason Yosinski.",,,0,not_related
"Note that HNN is trained and evaluated on each of these systems separately, while HGN and HGNN are trained in only one system and inferred for all other systems by performing the forward simulation.",,,1,related
We observe that HGNN significantly outperforms HGN and HNN in terms of rollout and energy error (see Supplementary Materials).,,,1,related
"•HGNN
Parameter Value Node embedding dimension 5 Edge embedding dimension 5 Hidden layer neurons (MLP) 5
Number of hidden layers (MLP) 2 Activation function squareplus
Number of layers of message passing(pendulum) 2 Number of layers of message passing(spring) 1
Optimizer ADAM Learning rate 1.0e−3
Batch size 100
•HNN
Parameter Value Hidden layer neurons (MLP) 256
Number of hidden layers (MLP) 2 Activation function squareplus
Optimizer ADAM Learning rate 1.0e−3
Batch size 100
•HGN
Parameter Value Node embedding dimension 8 Edge embedding dimension 8 Hidden layer neurons (MLP) 16
Number of hidden layers (MLP) 2 Activation function squareplus
Number of layers of message passing 1 Optimizer ADAM
Learning rate 1.0e−3 Batch size 100",,,0,not_related
"Despite best efforts, the HGN and HNN was unable to provide a forward trajectory for the hybrid system.",,,0,not_related
"Work on discovering Lagrangian [10, 11] and Hamiltonian [12, 13] can also be found in the literature.",,,0,not_related
"In a recent study [31], the idea of parameterization of a scalar function that represents the Hamiltonian in a conservative dynamic system by a neural network was explored.",,,0,not_related
"Other approaches [1, 11, 17, 19, 33, 47] induce Hamiltonian and Lagrangian priors into neural networks exploiting the reformulations of Newton’s equations of motion in energy-conservative dynamics.",,,0,not_related
"Learning the dynamics of physical systems directly from their trajectory is an active area of research due to their potential applications in materials modeling [28], drug discovery [33], motion planning [25], robotics [29, 16], and even astrophysics [30].",,,0,not_related
"Among these, a family of models, such as Lagrangian or Hamiltonian neural networks [2, 1, 23, 29, 16] and Neural ODEs [3, 35, 17, 7], enforces the physics-based inductive biases in a strong sense.",,,0,not_related
", particle-based systems [3, 32], atomistic dynamics [28, 19], physical systems [2, 29, 16], and articulated systems [1].",,,0,not_related
"For example, data-driven machine learning algorithms that can preserve either the Hamiltonian structure [10; 11] or the Lagrangian structure [12], have been applied to a number of cases with success.",,,0,not_related
"As baselines for comparison, we train a neural ODE (which has the same architecture as the SDE model but excludes the diffusion term), and an ensemble of 5 probabilistic (Gaussian) models [54].",,,1,related
"However, near the dataset, there is no clear pattern as to when it will produce stochastic versus deterministic predictions.
slightly more accurate than those of the neural ODE.",,,0,not_related
"For example, many works use the Lagrangian, Hamiltonian, or Port-Hamiltonian formulation of dynamics to inform the structure of a neural ODE [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",,,0,not_related
"While the neural ODE has no notion of prediction uncertainty, the proposed neural SDE yields uncertainty estimates that agree with the availability of the training data.",,,0,not_related
The first row shows that neural SDE and ODE models have high prediction accuracy.,,,0,not_related
"We evaluate our approach by comparing its performance for modeling and model-based control tasks against state-of-the-art techniques such as probabilistic ensembles [45, 3, 54], system identificationbased algorithms [55, 56, 57], and neural ODEs [6, 1].",,,1,related
"Neural ordinary differential equations (ODEs) [6], which parametrize the right-hand side of a differential equation using a neural network, are a class of models that provide a natural mechanism for incorporating existing physics and engineering knowledge into neural networks [1, 7].",,,0,not_related
"Learning Fm [60] from few data is impractical [61], (II) precludes deriving ite on first principles, and using traditional image features would limit discriminating power for unknown patterns.",,,0,not_related
"There are also numerous pieces of research [22, 45, 8, 43] focusing on recovering the Hamiltonian, and predicting the dynamics of certain physical systems based on observed trajectories.",,,0,not_related
"Related approaches include energy-based model architectures such as Hamiltonian [36, 37] or Lagrangian [38, 39] neural networks, and their various extensions such as graph Hamiltonian neural networks (HNN) [40], Hamiltonian dynamics with dissipative forces [41], HNN with explicit constraints [42], or noncanonical Hamiltonian systems [43].",,,0,not_related
"Related approaches include energy-based model architectures such as Hamiltonian [36, 37] or Lagrangian [38, 39] neural networks, and their various extensions such as graph Hamiltonian neural networks (HNN) [40], Hamiltonian dynamics with dissipative forces [41], HNN with explicit constraints [42], or non-",,,0,not_related
"Therefore, a compact and precise governing equation of action-value function is not available for regulating the training loss function, which thus hinders the applications of current frameworks of physics-enhanced DNN here [48, 51, 21, 20, 49, 31, 6, 45, 52, 23, 47, 11, 9, 14, 17, 36, 33, 35, 18, 46, 28].",,,0,not_related
"[17] Samuel Greydanus, Misko Dzamba, and Jason Yosinski.",,,0,not_related
"Current frameworks include physicsinformed NN [48, 51, 21, 20, 49, 31, 6, 45, 52, 23, 47, 11, 9, 14, 17], physics-guided NN architectures [36, 33, 35, 18, 46, 28] and physics-inspired neural operators [32, 29].",,,0,not_related
"[19] Samuel Greydanus, Misko Dzamba, and Jason Yosinski.",,,0,not_related
"Structure-preserving dense networks: For dense networks, it is relatively straightforward to parameterize reversible dynamics, see for example: Hamiltonian neural networks [19, 20, 21, 22], Hamiltonian generative networks [23], Hamiltonian with Control (SymODEN) [24], Deep Lagrangian networks [25] and Lagrangian neural networks [26].",,,0,not_related
"(2020b;a) encode Hamiltonian dynamics and dissipative Hamiltonian dynamics into the structure of the neural ODE using Hamiltonian neural networks (Greydanus et al., 2019).",,,0,not_related
Neural ODEs With Structure Greydanus et al. (2019) introduce the idea of adding a Hamiltonian structure to a neural network.,,,0,not_related
"For example, Zhong et al. (2020b;a) encode Hamiltonian dynamics and dissipative Hamiltonian dynamics into the structure of the neural ODE using Hamiltonian neural networks (Greydanus et al., 2019).",,,0,not_related
"It has applications in learning dynamics [52, 53, 54, 55], control [56, 57], generative modeling [51, 58], and joint shape encoding, reconstruction, and registration [59, 19, 29].",,,0,not_related
HOGN [11] imports the Hamiltonian mechanics [21] as physics informed inductive biases into INs for more accurate particle system simulation.,,,0,not_related
"However, since many physical systems are dissipative, the assumption of HNNs limits its applicability to real-world systems.",,,0,not_related
"Efforts to build models that conserve the total energy of a system led to a body of work on Hamiltonian neural networks (HNN) (Greydanus et al., 2019).",,,0,not_related
"[23, 16, 34], and a more general pseudo-Hamiltonian formulation than the port-Hamiltonian formulation of e.",,,0,not_related
"Much of the literature has focused on Hamiltonian or Lagrangian formulations of (1), beginning with [23, 8, 10].",,,0,not_related
"Hamiltonian neural networks (HNN) have received considerable attention since their introduction in [23], resulting in several extensions and generalizations [8, 22, 27].",,,0,not_related
"However, in the system we will consider, S is either of canonical form, as is assumed in the many recent works on Hamiltonian neural networks [23], or a non-canonical form that can be obtained from engineering knowledge.",,,0,not_related
"Various strategies have been proposed such as the direct approximation of the infinitesimal dynamic [2], fix point methods mimicking infinitedepth networks [3], variational principles and conservative systems [4], the direct minimization of the differential equation residue [5] or also more generic data-driven frameworks to approximate mappings between Hilbert spaces [6, 7].",,,0,not_related
", 2021), physics-inspired inductive biases (Jonschkowski and Brock, 2015; Cranmer et al., 2020; Greydanus et al., 2019), unsupervised",,,0,not_related
"…et al., 2016; Liu et al., 2019; Lyle et al., 2021), physics-inspired inductive biases (Jonschkowski and Brock, 2015; Cranmer et al., 2020; Greydanus et al., 2019), unsupervised
1. https://github.com/sahandrez/homomorphic policy gradient
Policy Gradient Methods in the Presence of…",,,0,not_related
"Learning the symplectic character (if it exists) of a physical system (including particles in potential fields, pendulums of various complexities) can be done utilizing neural networks, see, for example, [15, 16].",,,0,not_related
"Hamiltonian neural networks (HNNs) (Greydanus et al., 2019) are a state-of-the-art grey-box modeling tool in which Hamiltonian mechanics is embedded as prior knowledge in the neural network.",,,0,not_related
"Greydanus et al. (2019) proposed to learn the Hamiltonian, as an energy-like scalar value in an unsupervised manner.",,,0,not_related
"After the introduction of HNNs by Greydanus et al. (2019), many researchers from different fields studied its application and proposed extensions to the original idea.",,,0,not_related
"Networks [5, 20] where a structural feature of the dynamics (symplecticity, or their Hamiltonian nature) is imposed in addition to the demand for accurate prediction.",,,0,not_related
"Many of the examples cited above involve conservation of quantities based on the symmetry of the equations of motion, such as conservation of energy and momentum [13], or the inclusion of previously derived differential equations [11] as components of the ML training.",,,0,not_related
"[13, 49]) proposed that including conserved quantities such as energy/momentum may help to improve the application of neural networks to physical systems.",,,0,not_related
"Enforcing conservation of quantities such as momentum, mass, or energy [11, 13, 22, 23] for dissipative systems in isolation may not be",,,0,not_related
"Using neural networks embedded with physical information will significantly alleviate the above three problems, such as Hamiltonian neural networks [19], Lagrangian neural networks [20], Physics-informed neural networks [21], Neural ODEs [22], etc.",,,0,not_related
"To promote physical feasibility of the solution, many works impose equality or inequality system constraints by i) encoding hard constraints inside NN layers (e.g. using sigmoid layer to encode technical limits of upper and lower bounds), ii) applying prior on the NN architecture (e.g., Hamiltonian [25] and Lagrangian neural networks [26]), iii) augmenting the objective function with penalty terms in a supervised [13] or unsupervised [17] [14] way, iv) projecting outputs [16] to the feasible domain, or v) combining many different strategies.",,,0,not_related
", Hamiltonian [25] and Lagrangian neural networks [26]), iii) augmenting the objective function with penalty terms in a supervised [13] or unsupervised [17] [14] way, iv) projecting outputs [16] to the feasible domain, or v) combining many different strategies.",,,0,not_related
More work is required to implement such physics-aware reasoning layers [220]–[224] into onboard neural-inertial navigation models.,,,0,not_related
"[6, 22]) or higher-order explicit Runge-Kutta methods (e.",,,0,not_related
"Recently, researchers have focused on leveraging a continuous-time representation to incorporate physical inductive biases such as symplectic structure [6, 22, 50], the Onsager principle [53], the GENERIC formalism [54] and time-reversal symmetry [29], to name a few, into the learning model.",,,0,not_related
"To the best of our knowledge, this is the first demonstration of the remarkable capacity of numerical integrators of order p > 4 to facilitate the training of Hamiltonian neural networks [9] from sparse datasets, to do accurate interpolation and extrapolation in time.",,,1,related
"We follow the idea of Hamiltonian neural networks [9] aiming at approximating the Hamiltonian, H : R → R, such that Hθ is a neural network and f is approximated by fθ(y) := J∇Hθ(y).",,,1,related
Hamiltonian neural networks [9] aim at learning energy-preserving dynamical systems from data by approximating the Hamiltonian using neural networks.,,,0,not_related
"Recently, authors have proposed neural network models that use physics-based inductive biases, also known as grey-box models (Lutter et al., 2019a;b; Greydanus et al., 2019).",,,0,not_related
"…2017a; Raissi, 2018; Jiang et al., 2019) or with missing terms (Yin et al., 2021), and (c) different domain-specific physical constraints such as energy conservation (Greydanus et al., 2019; Cranmer et al., 2020a), symmetries (Wang et al., 2020b; Finzi et al., 2021; Brandstetter et al., 2022a).",,,0,not_related
", 2021), and (c) different domain-specific physical constraints such as energy conservation (Greydanus et al., 2019; Cranmer et al., 2020a), symmetries (Wang et al.",,,0,not_related
"Greydanus et al. (2019) introduce the Hamiltonian NN (HNN), which trains an NN to predict the dynamics with an auxiliary loss based on Hamilton’s equations.",,,0,not_related
"Recently, numerous data-driven approaches have been introduced to learn Hamiltonian systems with neural networks (Greydanus et al., 2019; Cranmer et al., 2020; Zhong et al., 2019; Finzi et al., 2020).",,,0,not_related
[144] can learn an arbitrary conservation law based on Hamiltonian mechanics.,,,0,not_related
"Finally, Greydanus & Sosanya (2022) extended Hamiltonian Neural Networks (Greydanus et al., 2019) to model both curl- and divergence-free dynamics simultaneously.",,,0,not_related
"Finally, a recent work from the deep learning community (Greydanus & Sosanya, 2022) extended Hamiltonian Neural Networks (Greydanus et al., 2019) in such a way that, the authors suggest, allows one to model both curl- and divergence-free dynamics simultaneously, for example for reconstructing…",,,0,not_related
"Finally, a recent work from the deep learning community (Greydanus & Sosanya, 2022) extended Hamiltonian Neural Networks (Greydanus et al., 2019) in such a way that, the authors suggest, allows one to model both curl- and divergence-free dynamics simultaneously, for example for reconstructing surface flows from a noisy ocean current dataset.",,,0,not_related
"Finally, Greydanus & Sosanya
(2022) extended Hamiltonian Neural Networks (Greydanus et al., 2019) to model both curl- and divergence-free dynamics simultaneously.",,,0,not_related
"The quality of the learned model can greatly be improved when prior geometric knowledge about the dynamical system is taken into account such as conservation laws [16,14,5,9,2], symmetries [10,8,7], equilibrium points [19], or asymptotic behaviour of its motions.",,,0,not_related
"2021; Pathak et al., 2022; Bi et al., 2022; Lam et al., 2022; Nguyen et al., 2023), molecular dynamics (Mardt et al., 2018; Zhong et al., 2019; Greydanus et al., 2019; Mattheakis et al., 2019; Li et al., 2020a), or astrophysics (Tamayo et al., 2016; Cranmer et al., 2021).",,,0,not_related
"Table 6 and Table 7 show the result comparison between our proposed method and another DNN-based method HNN (Greydanus et al., 2019) on the two examples.",,,1,related
"On the other hand, the approach of data-driven dynamical modeling tries to learn a dynamical system from data, which often generate models that are prone to violation of physics laws as demonstrated in (Greydanus et al., 2019).",,,0,not_related
"…(2019) learns the dynamical system and a Lyapunov function to ensure the exponential stability of predicted system; Hamiltonian neural network (HNN, Greydanus et al. (2019)) targets at the Hamiltonian mechanics, directly learns the Hamiltonian and uses the symplectic vector field to approximate…",,,0,not_related
"For contrastive learning with a single conservation value (e.g. ideal spring mass system), HNN performs slightly better than ConCerNet.",,,0,not_related
"Average R(2) comparison with prior work (HNN (Greydanus et al., 2019)) in conservation property learning Task Conservation ConCerNet HNN Ideal spring mass system x[1](2) + x[2](2) = C 0.",,,0,not_related
"HNN is not applicable to the other two experiments, because they are not Hamiltonian systems.",,,0,not_related
ConCerNet empirically learns the Angular momentum function and HNN learns the Hamiltonian value.,,,0,not_related
"Simulation error comparison with DNN-based prior work (HNN (Greydanus et al., 2019)) Task Mean square error Violation of conservation laws Baseline NN ConCerNet HNN Baseline NN ConCerNet HNN Ideal spring mass 0.",,,1,related
"Existing work includes: Kolter & Manek (2019) learns the dynamical system and a Lyapunov function to ensure the exponential stability of predicted system; Hamiltonian neural network (HNN, Greydanus et al. (2019)) targets at the Hamiltonian mechanics, directly learns the Hamiltonian and uses the symplectic vector field to approximate the dynamics; Lagrangian neural network (LNN, Cranmer et al. (2020)) extends the work of HNN to Lagrangian mechanics.",,,0,not_related
"For example, Greydanus et al. (2019) enforces the Hamiltonian to be conserved in Hamiltonian systems, Cranmer et al. (2020) further extends it to Lagrangian dynamics.",,,0,not_related
"Besides the abovementioned HNN and LNN, a few recent works (Zhang et al., 2018; Liu & Tegmark, 2021; Ha & Jeong, 2021; Liu et al., 2022; Udrescu & Tegmark, 2020) have explored automated approaches to extract the conservation laws from data.",,,0,not_related
", 2016)) and a DNN-based prior work (HNN, (Greydanus et al., 2019)) and delay the results to Appendix B.",,,1,related
"We also compare ConCerNet with one classical modeling method (SINDy, (Brunton et al., 2016)) and a DNN-based prior work (HNN, (Greydanus et al., 2019)) and delay the results to Appendix B.2, where ConCerNet shows similar performance but ConCerNet is more generally applicable.",,,1,related
"Initial works related to the discovery of Lagrangian can be linked to Hamiltonian Neural Networks (HNN) [12, 13].",,,0,not_related
"For learning the Hamiltonian and Lagrangian directly in Cartesian coordinates, Constrained Hamiltonian Neural Networks (CHNNs) and Constrained Lagrangian Neural Networks (CLNNs) were proposed in [16].",,,0,not_related
"Specifically, some differential equations are chaotic (Greydanus et al., 2019), i.e. sensitive to the initial values of inputs, which is hard for PDDM to fundamentally model the ubiquitous and elusive randomness of chaos with only finite data, leading to poor generalization (Abu-Mostafa et al.,…",,,0,not_related
"Specifically, some differential equations are chaotic (Greydanus et al., 2019), i.",,,0,not_related
"Subsequently, extensive work has been conducted on parametrizing the continuous dynamics of hidden states using an ODE (Greydanus et al., 2019; Lu et al., 2019b; Liu et al., 2021).",,,0,not_related
"Furthermore, classical mechanics is already used as a framework to encode conserved quantities in continuous-valued data [65, 66].",,,0,not_related
"as a differentiable computational graph [11], [15], [16], [17] or used separately in an error-learning scheme [3], [11], [18].",,,0,not_related
"N-Body Trajectory We test our model as well as the baselines, Augerino and SymmetryGAN, on the simulated n-body trajectory dataset from Hamiltonian NN (Greydanus et al., 2019).",,,1,related
"Recent approaches have been interested in combining deep learning algorithms with physical knowledge (Greydanus et al., 2019; Brunton et al., 2020; Willard et al., 2020).",,,0,not_related
"…Lagrangian and Hamiltonian neural networks (Cranmer et al., 2020; Lutter et al., 2019; Zhong and Leonard, 2020; Allen-Blanchette et al., 2020; Greydanus et al., 2019; Matsubara et al., 2020; Toth et al., 2020; Finzi et al., 2020), and it also enables learning control-oriented dynamics models…",,,0,not_related
"This approach allows for the inclusion of general forms physics knowledge into data-driven models , such as for so-called Lagrangian and Hamiltonian neural networks (Cranmer et al., 2020; Lutter et al., 2019; Zhong and Leonard, 2020; Allen-Blanchette et al., 2020; Greydanus et al., 2019; Matsubara et al., 2020; Toth et al., 2020; Finzi et al., 2020), and it also enables learning control-oriented dynamics models (Zhong et al.",,,0,not_related
"The main assumption that we make is that the energy of the system is conserved for short periods of time thanks to the energy injected by the motor, which allows us to use the HNN [Greydanus et al., 2019].",,,1,related
"This approach in developing arbitrary coordinates has been proposed in the original HNN paper [Greydanus et al., 2019].",,,0,not_related
"used by Hamiltonian neural networks is to learn a parametric function in the form of a neural network for the Hamiltonian itself [Greydanus et al., 2019].",,,0,not_related
"This work leverages the Hamiltonian neural network (HNN) [Greydanus et al., 2019] to learn the Hamiltonian equations of energyconserving dynamical systems from noisy data.",,,0,not_related
"number of systems [Greydanus et al., 2019], including an ideal mass-spring system.",,,0,not_related
"Examples are hamiltonian, symplectic, and lagrangian neural networks [31, 32, 33] and the physics-informed neural networks [34], aiming at exploiting the best of both worlds, namely the expressive power of nonlinear function approximators with grounded physical knowledge.",,,0,not_related
"For example, Hamiltonian neural networks can model dynamics that obey exact conservation laws [14, 9], and monotonic neural networks can model monotonically increasing dynamics [1, 45, 56].",,,0,not_related
"HNN is Hamiltonian neural networks [14], which models the Hamiltonian by a neural network.",,,0,not_related
"Hamiltonian and generalized Hamiltonian neural networks can place physics-inspired priors on systems [14, 9].",,,0,not_related
"Neural network-based methods for modeling continuous-time ODEs require derivative regression [14] or computationally expensive numerical integration to solve the ODEs [7, 40, 51, 30, 8].",,,0,not_related
"[11] assume the underlying system is measure-preserving, and their network learns the Hamiltonian during training.",,,0,not_related
The propagator objective is used within algorithms such as dynamic mode decomposition (DMD) [61] and sparse identification of nonlinear dynamics (SINDy) [10] and for training certain neural networks such as Hamiltonian neural networks [29].,,,0,not_related
"Section 7.2 of Finzi et al. (2021) points to the paper by Greydanus et al. (2019), where the authors look to learn the Hamiltonian of a system coming from Hamiltonian mechanics.",,,0,not_related
"In addition to physical inductive biases encodable via differentiable equations, there have also been recently developed methods to effectively impose energy conservation on learnt representations (Greydanus et al., 2019; Cranmer et al., 2020).",,,0,not_related
"…in machine learning, e.g. ranging from convolutional neural networks (LeCun et al., 1998) and geometrically-invariant networks (Giles & Maxwell, 1987) through to recent examples such as Hamiltonian neural networks (Greydanus et al., 2019) and physics-informed neural networks (Raissi et al., 2019).",,,0,not_related
"On the contrary, several existing works [1, 5, 6, 8, 24] embed specialized physical laws as hard constraints into neural networks, and enforce the model to must satisfy physical constraints.",,,0,not_related
"Results for the data-driven learning of Hamiltonian systems with neural networks have been established in [9], [10], and extended to PHS, see [11], [12].",,,0,not_related
One typical example is Hamiltonian neural networks (HNNs) [2] which aim at training models that respect exact conservation laws.,,,0,not_related
Authors in [2] proposed to learn Hamiltonians from data using artificial neural networks (ANNs).,,,0,not_related
"…been growing interest in developing better deep neural network based dynamics models for physical systems through better inductive biases (Lutter et al., 2019a; Greydanus et al., 2019; Lutter et al., 2019b; Zhong et al., 2019, 2020; Cranmer et al., 2020; Finzi et al., 2020; Zhong et al., 2021a).",,,0,not_related
"Recently there has been growing interest in developing better deep neural network based dynamics models for physical systems through better inductive biases (Lutter et al., 2019a; Greydanus et al., 2019; Lutter et al., 2019b; Zhong et al., 2019, 2020; Cranmer et al., 2020; Finzi et al., 2020; Zhong et al., 2021a).",,,0,not_related
"Physics informed neural networks for dynamics models: Although our focus is on enforcing constraints, we also briefly discuss related ideas in physics-informed neural networks (Raissi et al., 2019; Márquez-Neila et al., 2017; Lu et al., 2021a; Lutter et al., 2019; Cranmer et al., 2020; Greydanus et al., 2019).",,,0,not_related
"…neural networks for dynamics models: Although our focus is on enforcing constraints, we also briefly discuss related ideas in physics-informed neural networks (Raissi et al., 2019; Márquez-Neila et al., 2017; Lu et al., 2021a; Lutter et al., 2019; Cranmer et al., 2020; Greydanus et al., 2019).",,,0,not_related
"It is worth noting that the meta-trained model requires significantly fewer adaptation steps compared to the randomly initialized model, which corresponds to the vanilla HNN model [8].",,,0,not_related
"Prior works [8, 9, 23] have utilized tanh or softplus activations, hypothesizing that the relu activation may hinder parameter optimization due to its piecewise linear nature, as the HNN loss defined in Equation.",,,0,not_related
"1, HNN predicts the dynamics of the system incorporating the symplectic gradient inside the loss function as follows [8].",,,0,not_related
"…bias have recently shown promise in learning dynamical system models that respect physical laws and that generalize beyond the training dataset (Djeumou et al., 2022a; Menda et al., 2019; Gupta et al., 2020; Cranmer et al., 2020; Greydanus et al., 2019; Finzi et al., 2020; Zhong et al., 2021a).",,,0,not_related
"Note that in this section, similar to all existing works involving port-Hamiltonian neural networks (Greydanus et al., 2019; Zhong et al., 2020; Desai et al., 2021; Eidnes et al., 2023), we assume that the interaction term J(x) is known a priori.",,,0,not_related
"Deep learning methods that use physics-based knowledge as inductive bias have recently shown promise in learning dynamical system models that respect physical laws and that generalize beyond the training dataset (Djeumou et al., 2022a; Menda et al., 2019; Gupta et al., 2020; Cranmer et al., 2020; Greydanus et al., 2019; Finzi et al., 2020; Zhong et al., 2021a).",,,0,not_related
"Of particular relevance to our work, Hamiltonian neural networks use the Hamiltonian formulation of dynamics to inform the structure of a neural ODE (Greydanus et al., 2019; Matsubara et al., 2020; Toth et al., 2020; Finzi et al., 2020).",,,0,not_related
Scientific Knowledge Mathematical Equations [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50]–[52] [53] [54] [55],,,0,not_related
"Hamiltonian functionality that enforces energy conservation has attracted much attention [24], [25], [30], [31].",,,0,not_related
"Many physics-informed approaches have recently been proposed to learn Hamiltonian dynamics and symplectic maps (Lutter et al., 2019b; Greydanus et al., 2019; Bertalan et al., 2019; Jin et al., 2020; Burby et al., 2020; Chen et al., 2020; Cranmer et al., 2020; Zhong et al., 2020a,b, 2021; Marco and…",,,0,not_related
"Many physics-informed approaches have recently been proposed to learn Hamiltonian dynamics and symplectic maps (Lutter et al., 2019b; Greydanus et al., 2019; Bertalan et al., 2019; Jin et al., 2020; Burby et al., 2020; Chen et al., 2020; Cranmer et al., 2020; Zhong et al., 2020a,b, 2021; Marco and Méhats, 2021; Rath et al., 2021; Chen et al., 2021; Offen and Ober-Blöbaum, 2022; Santos et al., 2022; Valperga et al., 2022; Mathiesen et al., 2022; Duruisseaux et al., 2023a).",,,0,not_related
"For instance regularization terms can be introduced into the loss function to penalize the NN that would otherwise not satisfy physical constraints [6, 10].",,,0,not_related
"A typical approach to embed the conservation property into the network model is to add a regularization term to the loss function [6, 10, 22].",,,0,not_related
"They usually use a higher-order numerical integration scheme to update the latent state:
û(t)← Encode(u(t)) û(t+ ∆t)← û(t) + ∫ t+∆t t F(û(s))ds u(t+ ∆t)← Decode(û(t+ ∆t))
Hamiltonian Neural Network: Hamiltonian Neural Networks (HNNs) [Greydanus et al., 2019] models the Hamiltonian in a latent space explicitly and updates the latent coordinates via Hamilton’s equations.
q̂(t), p̂(t)← Encode(u(t)) q̂(t+ ∆t)← q̂(t) + ∫ t+∆t t ∂ ∂p̂ H(q̂(s), p̂(s))ds
p̂(t+ ∆t)← p̂(t)− ∫ t+∆t t ∂ ∂q̂ H(q̂(s), p̂(s))ds u(t+ ∆t)← Decode(q̂(t+ ∆t), p̂(t+ ∆t))
Wherever applicable, we used the Dormand-Prince 5(4) solver, a 5th order Runge-Kutta method for numerical integration.",,,1,related
"(1) Previous efforts towards modelling Hamiltonian systems [Greydanus et al., 2019, Cranmer et al., 2020, Jin et al., 2020, Chen et al., 2018, Kidger, 2022] have seen success with neural networks trained via backpropagation on the trajectory prediction objective:
φ∗ = arg min φ ∑ t ‖Mφ(u(t),∆t)−…",,,0,not_related
", 2018], and the physicsinspired Hamiltonian Neural Networks (HNN) [Greydanus et al., 2019].",,,0,not_related
"We compare the Action-Angle Network to three strong baseline models: the Euler Update Network (EUN), the Neural Ordinary Differential Equations (Neural ODE) [Chen et al., 2018], and the physicsinspired Hamiltonian Neural Networks (HNN) [Greydanus et al., 2019].",,,0,not_related
"…state:
û(t)← Encode(u(t)) û(t+ ∆t)← û(t) + ∫ t+∆t t F(û(s))ds u(t+ ∆t)← Decode(û(t+ ∆t))
Hamiltonian Neural Network: Hamiltonian Neural Networks (HNNs) [Greydanus et al., 2019] models the Hamiltonian in a latent space explicitly and updates the latent coordinates via Hamilton’s…",,,1,related
"Figure 3a shows that the Action-Angle Network can be queried much faster than the Neural ODE and the HNN, with an inference time that is independent of ∆t. Figure 3b depicts the prediction error as a function of ∆t, showing that the Action-Angle Network also scales much better with the jump size ∆t, even for jump sizes larger than those seen during training ∆tmax = 10.",,,0,not_related
"as a differentiable computational graph [11], [15]–[17] or used separately in an error-learning scheme [3], [11], [18].",,,0,not_related
"Using the latter approach Greydanus et al. (2019) proposed the Hamiltonian Neural Network (HNN), closely followed by the Lagrangian Neural Networks (LNN) by Cranmer et al. (2020) leading to a rapid growth of research interest in this topic.",,,0,not_related
"Moreover, NLD can be viewed in a line of physics inspired neural network models such as (Cranmer et al., 2020; Greydanus et al., 2019; Toth et al., 2020; Botev et al., 2021).",,,0,not_related
"There is a great deal of work designing specific neural architectures that naturally obey Hamiltonian equations and Lagrangian equations [67], [68], [134], [135].",,,0,not_related
"For PDEs with Dirichlet conditions, they sample a dataset of collocation points from Ω and ∂Ω, i.e.{xi} ⊂ Ω
7 Neural Solver Method Description Representatives Loss Reweighting Grad Norm GradientPathologiesPINNs [43] NTK Reweighting PINNsNTK [44] Variance Reweighting Inverse-Dirichlet PINNs [45] Novel Optimization Targets Numerical Differentiation DGM [46], CAN-PINN [47], cvPINNs [48] Variantional Formulation vPINN [49], hp-PINN [50], VarNet [51], WAN [52] Regularization gPINNs [53], Sobolev Training [54]
Novel Architectures
Adaptive Activation LAAF-PINNs [55], [56], SReLU [57] Feature Preprocessing Fourier Embedding [58], Prior Dictionary Embedding [59] Boundary Encoding TFC-based [60], CENN [61], PFNN [62], HCNet [63]
Sequential Architecture PhyCRNet [64], PhyLSTM [65] AR-DenseED [66], HNN [67], HGN [68] Convolutional Architecture PhyGeoNet [69], PhyCRNet [64], PPNN [70]
Domain Decomposition XPINNs [71], cPINNs [72], FBPINNs [73], Shukla et al. [74]
Other Learning Paradigms Transfer Learning Desai et al. [75], MF-PIDNN [76]Meta-Learning Psaros et al. [77], NRPINNs [78]
TABLE 2: An overview of variants of PINNs.",,,1,related
"Hamiltonian neural networks (HNN) [67], [136] represent the Hamiltonian with a neural network Hw(q,p).",,,0,not_related
"Novel Architectures Adaptive Activation LAAF-PINNs [55], [56], SReLU [57] Feature Preprocessing Fourier Embedding [58], Prior Dictionary Embedding [59] Boundary Encoding TFC-based [60], CENN [61], PFNN [62], HCNet [63] Sequential Architecture PhyCRNet [64], PhyLSTM [65] AR-DenseED [66], HNN [67], HGN [68] Convolutional Architecture PhyGeoNet [69], PhyCRNet [64], PPNN [70] Domain Decomposition XPINNs [71], cPINNs [72], FBPINNs [73], Shukla et al.",,,0,not_related
"HNN
(Greydanus et. al.)",,,0,not_related
"For time series modelling, recurrent architectures [13, 3] or physically inspired models [7] are often considered with success.",,,0,not_related
"See for instance [13, 1, 7, 3, 6] just to name a few recent work.",,,0,not_related
"As a countermeasure to the brittleness of NN-based models, there has been increasing interest in incorporating prior knowledge – also known as inductive bias – into NNs to ensure physical consistency, leading to Hamiltonian NNs (Chen et al., 2019; Greydanus et al., 2019; Finzi et al., 2020), Lagrangian NNs (Cranmer et al., 2020), or Poisson NNs (Jin et al., 2022), amongst others.",,,0,not_related
"As a countermeasure to the brittleness of NN-based models, there has been increasing interest in incorporating prior knowledge – also known as inductive bias – into NNs to ensure physical consistency, leading to Hamiltonian NNs (Chen et al., 2019; Greydanus et al., 2019; Finzi et al., 2020), Lagrangian NNs (Cranmer et al.",,,0,not_related
"In other words, NODEs learn the parameters of an ODE to fit data, making them particularly suitable to model complex dynamical systems (Greydanus et al., 2019; Rubanova et al., 2019).",,,0,not_related
"…interest in incorporating prior knowledge – also known as inductive bias – into NNs to ensure physical consistency, leading to Hamiltonian NNs (Chen et al., 2019; Greydanus et al., 2019; Finzi et al., 2020), Lagrangian NNs (Cranmer et al., 2020), or Poisson NNs (Jin et al., 2022), amongst others.",,,0,not_related
"(2019); Greydanus et al. (2019). Further, the potential energy is predicted using the GNN and the diagonal mass matrix is trained as a learnable parameter.",,,0,not_related
"…Chen et al. (2018); Gruver et al. (2021); Bishnoi et al. (2022),Lagrangian (LNNs) Cranmer et al. (2020a); Finzi et al. (2020); Lutter et al. (2019); Bhattoo et al. (2022), and Hamiltonian neural networks (HNNs) Sanchez-Gonzalez et al. (2019); Greydanus et al. (2019); Zhong et al. (2020, 2019).",,,0,not_related
"Specifically, these biases allow the MLP to preserve the characteristics of physical systems, such as energy and momentum conservation, and thus lead to a realistic realization of a trajectory of the system Greydanus et al. (2019); Cranmer et al. (2020a); Zhong and Leonard (2020).",,,0,not_related
"•HGNN: HGNN refers to Hamiltonian graph neural network, where the structure of the Hamiltonian of a system is exploited to decouple it into potential and kinetic energies Bhattoo et al. (2022); Sanchez-Gonzalez et al. (2019); Greydanus et al. (2019).",,,0,not_related
"Tasks For physical simulation tasks, we use the setting of pixel pendulum and real pendulum in the paper of HNN (Greydanus et al., 2019).",,,1,related
"In the experimental part, we verify that NODA provides a more efficient modeling than HNN, and we can use prior knowledge or transfer learning to further boost its training.",,,1,related
"For physical simulation tasks, we choose AE and Hamiltonian neural network (HNN) (Greydanus et al., 2019) as baseline methods.",,,1,related
"The testing loss curves of HNN, AE and NODA over two physical environments are shown in Figure 2 (a) and (b).",,,0,not_related
"For example, Lagrangian neural networks (Lutter et al., 2019; Cranmer et al., 2020) and Hamiltonian neural networks (Greydanus et al., 2019) can be used to simulate dynamic systems.",,,0,not_related
", 2020) and Hamiltonian neural networks (Greydanus et al., 2019) can be used to simulate dynamic systems.",,,0,not_related
"In each experiment, the number of NODA’s parameters equals to that of AE’s parameters, and it is no more than that of HNN’s parameters.",,,0,not_related
"Compared to traditional discrete layers, Neural ODEs [24] demonstrate that continuous modeling of neural network can better learn the continuous structures [22, 56] with infinite depth [44] and constant memory cost [69].",,,0,not_related
"INTRODUCTION Physics-informed machine learning is applied to numerous highly complex problems, such as turbulence and climate modeling [7], [44], [45], model predictive control [3], [33], and Hamiltonian system dynamics [16].",,,0,not_related
", 2021), and endowing NeuralODEs with mathematical structures that the system must satisfy (Greydanus et al., 2019; Finzi et al., 2020).",,,0,not_related
"…on this idea, including blending NeuralODEs with partial information on the form of the governing equation to produce ”grey-box” dynamics model (Rackauckas et al., 2021), and endowing NeuralODEs with mathematical structures that the system must satisfy (Greydanus et al., 2019; Finzi et al., 2020).",,,0,not_related
"HNN (Greydanus et al., 2019) assumes the target system to be a Hamiltonian system in the canonical form, thereby guaranteeing various properties of Hamiltonian systems by definition, including the conservation of energy and preservation of the symplectic structure in continuous time (Hairer et al.",,,0,not_related
"We implemented all codes by modifying the officially released codes of HNN (Greydanus et al., 2019) 1 and DGNet (Matsubara et al., 2020)2.",,,1,related
"Following HNN (Greydanus et al., 2019) and DGNet (Matsubara et al.",,,0,not_related
"We implemented all codes by modifying the officially released codes of HNN (Greydanus et al., 2019) 1 and DGNet (Matsubara et al.",,,1,related
"2-pend 2-body
Model 1-step↓ VPT↑ 1-step↓ VPT↑
NODE 0.82 ±0.020 0.110 ±0.035 144.21 ±12.65 0.134 ±0.014 HNN (Greydanus et al., 2019) 6220.26 ±91.57 0.002 ±0.000 5.17 ±0.570 0.362 ±0.026 CHNN (Finzi et al., 2020b) 0.07 ±0.000 0.928 ±0.036 (not working)
NODE+cFINDE 0.71 ±0.040 0.461 ±0.071 163.64…",,,1,related
"Greydanus et al. (2019) proposed the Hamiltonian neural network (HNN), which employs a neural network to approximate Hamilton’s equation, thereby conserving the system energy called the Hamiltonian.",,,0,not_related
"HNN (Greydanus et al., 2019) assumes the target system to be a Hamiltonian system in the canonical form, thereby guaranteeing various properties of Hamiltonian systems by definition, including the conservation of energy and preservation of the symplectic structure in continuous time (Hairer et al.,…",,,0,not_related
"The HNN was developed to model Hamiltonian systems in the canonical forms (Greydanus et al., 2019).",,,0,not_related
"Following HNN (Greydanus et al., 2019) and DGNet (Matsubara et al., 2020), we used fullyconnected neural networks with two hidden layers.",,,1,related
", 2018) HNN (Greydanus et al., 2019) X LieConv (Finzi et al.",,,0,not_related
"[1], Greydanus and Sosanya [37] can be interpreted as an application of the explicit Euler integrator David and Méhats [10].",,,0,not_related
"Popular integrators such as forward Euler, Runge-Kutta 4 (RK4) and Leapfrog [1, 6, 36] unfortunately do no maintain the manifold structure.",,,0,not_related
", [1, 3, 6, 15]); however, the Newtonian (point-mass) gravity considered is already well understood.",,,0,not_related
"One important class of systems to be learned have dynamics described by physical laws, whose structure can be exploited by learning the Hamiltonian of the system instead of the vector field [1, 2].",,,0,not_related
"Note that we do not assume access to the true derivatives q̇k,j and ṗk,j used in the loss function of some works [1, 37, 38].",,,1,related
"[1] Samuel Greydanus, Misko Dzamba, and Jason Yosinski.",,,0,not_related
"Seminal work initially approximated the time derivative via finite differences and then matched it with a learned (Hamiltonian) vector field[1, 2].",,,0,not_related
"In addition, such learning models have been further extended to incorporate the physical inductive bias of the underlying problems [3, 5, 19, 28, 36, 54, 55, 56].",,,0,not_related
"The layer weights are initialised with a random normal distribution similarly to [13]: the standard deviation of the normal distribution is set to 2/nh for the first hidden layer D1, 1/ √ nh for the second hidden layer D2 and √ nh for the output layer D3; the biases of all layers are initialised to zero.",,,0,not_related
"In [30] the GNN architecture is combined with the Hamiltonian neural network approach [13], and it is demonstrated that this can lead to more accurate predictions.",,,0,not_related
"In contrast to our work, they use the Hamiltonian formulation in [13].",,,1,related
"1 Hamiltonian- and Lagrangian neural networks For Hamiltonian systems a completely different approach is pursued in [13]: a neural network is trained to learn a scalar valued function HNN(q, p) which approximates the true Hamiltonian H(q, p) as a function of the generalised coordinates q ∈ R and conjugate momenta p ∈ R; here and in the following d is the dimension of the dynamical system.",,,0,not_related
"While the authors on [13] do not exploit this since they use a non-symplectic fourth order Runge Kutta integrator, in [14] it is argued that symplectic Neural Networks (SRNNs) show superior performance.",,,0,not_related
"To achieve this, we use the Lagrangian formalism and represent the Lagrangian LNN by a neural network as in [13].",,,1,related
Removing the symmetry-enforcing layer S results in the standard architecture already introduced in [13].,,,0,not_related
"To this extent, three broad approaches have been proposed, namely, Lagrangian neural networks (Lnn) [9, 5, 7], Hamiltonian neural networks (Hnn) [10, 11, 3, 12], and neural ODE (Node) [13, 14].",,,0,not_related
"Although the idea of graph-based modeling have been suggested for physical systems [16, 11], the inductive biases induced due to different graph structures and their consequences on the dynamics remain poorly explored.",,,0,not_related
This setting is similar to the approach considered in [6] or [32] where the authors use nonsymplectic integrators for training structure-preserving neural networks and then integrate the learned Hamiltonian models using symplectic integrators.,,,0,not_related
"From black-box architectures using simple multilayer perceptrons [15] to grey-box architectures that aim to preserve physical invariants [16], [17], these approaches vary by the level of inductive bias they introduce.",,,0,not_related
"Structure-preserving algorithms have a rich history in geometric integration [30] and have recently come to the fold in data-driven problems [33, 19, 40, 28, 32].",,,0,not_related
Table 1 Difference between HNN [2] and SymODEN [3] and the proposed method.,,,0,not_related
Table 1 shows the differences between the conventional method (HNN [2] and SymODEN [3]) and the proposed method.,,,1,related
"In such methods, some have been proposed to use energy conservation laws for model estimation: Greydanus et al. [2] proposed Hamiltonian Neural Networks (HNN), which estimates the Hamiltonian of a conservative system by utilizing the law of conservation of energy.",,,0,not_related
"Note that, HNN is not included in the comparison because it cannot handle inputs.",,,1,related
"[2] proposed Hamiltonian Neural Networks (HNN), which estimates the Hamiltonian of a conservative system by utilizing the law of conservation of energy.",,,0,not_related
"For trajectory data, there exist many methods developed such as Hamiltonian neural networks [18], Hidden Markov Model (HMM) [19], Kalman Filter (KF) [20], Particle Filter (PF) [21] and related works [16, 22].",,,0,not_related
It has also been shown that deep learning is also capable of approximating invariant quantities from dynamical systems such as the Hamiltonian [3] and the Lagrangian [4].,,,0,not_related
"Much of this work has focused on specific formalisms of governing equations, such as Hamiltonians, which provide a consistent approach for many physical dynamical systems and the easy incorporation of priors to shrink the solution space (Greydanus et al., 2019; DiPietro et al., 2020; Chen et al., 2019).",,,0,not_related
"…of this work has focused on specific formalisms of governing equations, such as Hamiltonians, which provide a consistent approach for many physical dynamical systems and the easy incorporation of priors to shrink the solution space (Greydanus et al., 2019; DiPietro et al., 2020; Chen et al., 2019).",,,0,not_related
"Greydanus et al. (2019) propose Hamiltonian Neural Networks, which parameterize Hamiltonians using black-box deep neural networks.",,,0,not_related
"The first approach emphasizes prediction, relying upon black-box machine learning techniques that can time-evolve the dynamical system with very high accuracy but offer no insight into its underlying governing equations (Greydanus et al., 2019; Chen et al., 2019; Raissi et al., 2020; Breen et al., 2020; Cranmer et al., 2020a).",,,0,not_related
"…prediction, relying upon black-box machine learning techniques that can time-evolve the dynamical system with very high accuracy but offer no insight into its underlying governing equations (Greydanus et al., 2019; Chen et al., 2019; Raissi et al., 2020; Breen et al., 2020; Cranmer et al., 2020a).",,,0,not_related
"LGNN for n-pendulum and n-spring systems In order to evaluate the performance of LGNN, we first consider two standard systems, that have been widely studied in the literature, namely, n-pendulum and n-spring systems [4, 6, 8, 9], with n= (3,4,5).",,,1,related
GNS uses the position and velocity of the particles to directly predict their updates for a future timestep [4].,,,0,not_related
"deep learning have seen neural network designs that can better model the physical world (Greydanus et al., 2019), measure uncertainties in their predictions (Louart & Couillet, 2018), and mitigate the risks attached with their tendency to be overconfident regarding a predicted confidence interval (Pereira & Thomas, 2020).",,,0,not_related
"For each case, we compared the performance of COMET with other methods: (1) simple neural ODE (NODE) [10], (2) Hamiltonian neural network (HNN) [6] with the coordinates given in each case below, (3) neural symplectic form (NSF) [7], and (4) Lagrangian neural network (LNN) [8].",,,1,related
"In contrast to Hamiltonian-based networks [6, 7, 8, 9], COMET is not constrained to Hamiltonian systems and its coordinate choice, making it generally applicable to a wider range of systems as shown in Table 1.",,,0,not_related
NODE [10] HNN [6] NSF [7] LNN [8] COMET,,,0,not_related
Case NODE [10] HNN [6] NSF [7] LNN [8] COMET,,,0,not_related
"With the recent emergence of employing neural networks for scientific discovery and learning systems’ behaviour from observational data [3, 4, 5, 6], naturally it raises a question, ""can we find constants of motion of dynamical systems from their data and exploit them to make a better prediction?""",,,0,not_related
Hamiltonian neural network (HNN) [6] is an attempt to solve this conservation problem by learning the Hamiltonian and calculate the state dynamics from the Hamiltonian.,,,0,not_related
"A large body of literature has been moving into this direction by learning the Hamiltonian [6, 7] or its variations [8, 9] of a system.",,,0,not_related
"Some approaches incorporate the neural network with Hamiltonian mechanics [1], Lagrangian mechanics [2], and solid mechanics [3].",,,0,not_related
"If the dynamics are learned using Hamiltonian neural network [2], the dynamics will be equal to equation 3.",,,0,not_related
"A popular workaround is to learn the Hamiltonian [2, 6, 7] or Lagrangian [3] of the energy-conserving system with a neural network, then get the dynamics as the derivatives of the learned quantity.",,,0,not_related
"Similar to [2] and [13], we tested our model with pixel data.",,,1,related
"The states’ dynamics of Hamiltonian systems can be written as [2, 13]",,,0,not_related
"Motivated by Hamiltonian mechanics, Hamiltonian Neural Network (HNN) was proposed in [2].",,,0,not_related
"introduced the so-called Hamiltonian Neural Networks for canonical, discrete systems, in which the loss term is of the form [72]",,,0,not_related
"We can see from Figure 50 that Adam and the BrAVO algorithms can achieve good training and test losses on this system identification problem using the Hamiltonian-based neural ODE network from [27] (with 231,310 parameters), inspired by [40; 91].",,,1,related
"Our approach leverages known equations for computing a coarse-grid prior; which is complementary to using known equations as soft [109, 74, 142, 137, 144, 139] or hard constraints [50, 89, 8, 39, 7, 61] as these methods can still be used to constrain the learned parametrization.",,,0,not_related
", 2020; 2021) or data-driven modeling of physical quantities (Greydanus et al., 2019; Cranmer et al., 2020; Lee et al., 2021).",,,0,not_related
"Followup work used those models and has shown success in diverse tasks, such as image classification (Zhuang et al., 2020; 2021) or data-driven modeling of physical quantities (Greydanus et al., 2019; Cranmer et al., 2020; Lee et al., 2021).",,,0,not_related
Using neural networks to learn Hamiltonian dynamics is an earlier idea that was proposed in Greydanus et al. (2019).,,,0,not_related
"To enforce invertibility, we express the flow fθ as a conservative operator using Hamiltonian neural networks inspired from Greydanus et al. (2019).",,,1,related
"Many ideas were explored, such as imposing soft physical constraints [43, 48, 34, 28, 56] in the training loss function or hard constraints in the network architectures [6, 12, 27, 16, 36, 41].",,,0,not_related
"Some works aimed at directly learning the system’s underlying Hamiltonian (Chen et al., 2020; Greydanus et al., 2019).",,,0,not_related
"Systems identification of frictionless dynamical systems such as pendulums and molecular dynamics through deep learning has seen a surge in recent years, particularly Hamiltonian Neural Networks (Greydanus, Dzamba, and Yosinski 2019).",,,0,not_related
"We test the performance on the two classical systems, undamped spring-mass and pendulum (Greydanus, Dzamba, and Yosinski 2019).",,,1,related
"We include the two most representative methods for comparison, HNN and LNN (Greydanus et al., 2019; Cranmer et al., 2020).",,,1,related
"To reduce the difficulty in training, researchers started to design physics-informed neural networks such as DeLaN, HNN, and LNN (Lutter et al., 2018; Greydanus et al., 2019; Cranmer et al., 2020).",,,0,not_related
"The baseline model is a three-layer MLP directly modelling the evolution function f in Equation (1) following HNN and LNN (Greydanus et al., 2019; Cranmer et al., 2020).",,,0,not_related
"The fusion of physical knowledge and neural networks casts a light on this problem, which leads to physics-informed neural networks such as Hamiltonian Neural Networks (HNNs, Greydanus et al., 2019) or Lagrangian Neural Networks (LNNs, Cranmer et al., 2020).",,,0,not_related
"The majority of related work [1, 8, 9, 10, 11] use a variational autoencoder (VAE) framework to represent the state in a latent space embedding.",,,0,not_related
"The Hamiltonian expresses the total energy of the system H(q,p) = T (q,p) + V (q) [1, 8].",,,1,related
[1] introduced Hamiltonian neural networks.,,,0,not_related
"References [1] Samuel Greydanus, Misko Dzamba, and Jason Yosinski.",,,0,not_related
"This is similar to other reported results in literature [1, 9, 8, 10].",,,0,not_related
"Learning Hamiltonian Systems Hamiltonian system is an important category in ordinary differential equations and there have been satisfactory works on learning Hamiltonian systems (Bertalan et al., 2019; Chen & Tao, 2021; Greydanus et al., 2019; Jin et al., 2020).",,,0,not_related
"In order to substantiate this claim, we use Neural ODE to learn several benchmark problems that are widely investigated for the discovery of hidden dynamics (Du et al., 2021; Greydanus et al., 2019; Keller & Du, 2021; Yu et al., 2021).",,,1,related
Greydanus et al. (2019) observed drifting of the predicted trajectory when learning a Hamiltonian system using Neural ODE.,,,0,not_related
"Since both true f and the IMDE fh are inaccessible in practice, we consider several benchmark problems that are widely investigated for the discovery of hidden dynamics (Du et al., 2021; Greydanus et al., 2019; Keller & Du, 2021; Yu et al., 2021).",,,1,related
"Experimental Details Since both true f and the IMDE fh are inaccessible in practice, we consider several benchmark problems that are widely investigated for the discovery of hidden dynamics (Du et al., 2021; Greydanus et al., 2019; Keller & Du, 2021; Yu et al., 2021).",,,1,related
"Hamiltonian system is an important category in ordinary differential equations and there have been satisfactory works on learning Hamiltonian systems (Bertalan et al., 2019; Chen & Tao, 2021; Greydanus et al., 2019; Jin et al., 2020).",,,0,not_related
"We generated the non-chaotic two-body systems and chaotic three-body systems in a relatively stable near-circular way [52], where the trajectories are obtained with the Explicit Runge-Kutta method [53].",,,1,related
"In [15], without the knowledge about the Hamiltonian parameters, unsupervised learning has been applied to learn a parametric function according to the symplectic gradient of the Hamiltonian function.",,,0,not_related
"In [5], a framework for discovering the Hamiltonian dynamics from time derivatives of the observed system coordinates is proposed.",,,0,not_related
"[19], via the SymODEN [39, 38] and port-Hamiltonian neural network [14] frameworks, and is defined for systems on any manifold.",,,0,not_related
Hamiltonian neural network (HNN) is a hybrid machine learning framework imposing hard constraints on a data-driven model [19].,,,0,not_related
"Following [19, 8, 27], we use fully connected neural networks with two hidden layers of 100 neurons each to estimate the Hamiltonian and the external force.",,,0,not_related
"Furthermore, we use the hyperbolic tangent (tanh) and Rectified Linear Unit (ReLU) as activation functions for the first and second hidden layer, respectively, while [19, 8, 27] use tanh for both.",,,1,related
"The HNNs of [19] use a neural network Ĥθ with weights θ to approximate the Hamiltonian H(q, p) of a system.",,,0,not_related
"4 Implementation and hyperparameters Following [19, 8, 27], we use fully connected neural networks with two hidden layers of 100 neurons each to estimate the Hamiltonian and the external force.",,,0,not_related
"2 Choice of discretization method in training Instead of training on the integration scheme as we do, works like [19, 8, 14] either assume that derivatives of the state variables are known or perform one or more integration steps at each training step.",,,1,related
"reasoning [233] [234], signal temporal logic [232], and physics-aware embeddings [235] [236] [237] [238] for robust complex event processing within the laws and bounds of physics.",,,0,not_related
"Experimentaly, Neural ODEs have been successful in a various range of applications, among which physical modelling [Greydanus et al., 2019, Cranmer et al., 2019] and generative modeling [Chen et al., 2018, Grathwohl et al., 2018].",,,0,not_related
"Experimentaly, Neural ODEs have been successful in a various range of applications, among which physical modelling (Greydanus et al., 2019; Cranmer et al., 2019) and generative modeling (Chen et al.",,,0,not_related
"Physics-inspired neural networks such as Deep Lagriangen Networks (DeLaN) [13] or Hamiltonian networks [5] guarantee physically plausible dynamic models, which conserve energy.",,,0,not_related
"This line of work originates from Lutter et al. (2019); Greydanus et al. (2019); Cranmer et al. (2020) (see Zhong et al. (2021) for an overview) and has been extended to NODEs for Hamiltonian and port-Hamiltonian systems (Zhong et al., 2020; Massaroli et al., 2020a; Zakwan et al., 2022), but also…",,,0,not_related
"It also does not conserve energy, which is not surprising when no structure is imposed, as discussed e.g., in Greydanus et al. (2019).",,,0,not_related
"Hamiltonian dynamics (Greydanus et al., 2019) describes a system’s total energyH(q,p) as a function of its canonical coordinates q and momenta p, e.",,,0,not_related
"Hamiltonian dynamics (Greydanus et al., 2019) describes a system’s total energyH(q,p) as a function of its canonical coordinates q and momenta p, e.g., each particles’ position and momentum.",,,0,not_related
"To promote physical feasibility of the solution, many works impose equality or inequality system constraints by i) encoding hard constraints inside layers (e.g. sigmoid layer to encode technical limits of upper and lower bounds), ii) applying prior on the NN architecture (e.g., Hamiltonian [24] and Lagrangian neural networks [25]), iii) augmenting the objective function with penalty terms (supervised [4] or unsupervised [8] [5]), iv) projecting outputs [7] to the feasible domain, or v) combining many different strategies.",,,0,not_related
", Hamiltonian [24] and Lagrangian neural networks [25]), iii) augmenting the objective function with penalty terms (supervised [4] or unsupervised [8] [5]), iv) projecting outputs [7] to the feasible domain, or v) combining many different strategies.",,,0,not_related
"Other research focused on efficient simulations by learning conservation laws (Cranmer et al., 2020; Greydanus et al., 2019), or aimed at correcting iterative solvers (Hsieh et al., 2019).",,,0,not_related
"For example, [16, 10] and [52] use a neural network to parameterize the Hamiltonian of a system, which relates the total energy to the change of the state.",,,0,not_related
"Several of the previously mentioned works model physical systems using Lagrangian or Hamiltonian energy formulations [30, 16, 11, 10, 52, 58, 28, 59], or other general physics models [26].",,,0,not_related
"While earlier works [30, 16, 42, 11, 58, 10, 20, 43] require coordinate data, i.",,,0,not_related
"Recent works [18, 19, 20, 21, 22, 23, 24] mainly focus on approximating Hamiltonian vector field from phase space data by means of using numerical integration to reconstruct symplectic map.",,,0,not_related
"(Greydanus et al., 2019; Desai et al., 2021) learn a Hamiltonian function using a neural network.",,,0,not_related
"Moreover, we attain additional novelty as these results are obtained without needing to impose a specific structure on the state-space (such as in Greydanus et al. (2019); Cranmer et al. (2020)) obtaining a practically widely applicable method.",,,1,related
"An adjacent research direction is the method/models which consider CT dynamics and directly use the state derivatives and even often the noiseless states to formulate structured and interpretable models such as Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Lagrangian Neural Networks (LNN) (Cranmer et al., 2020) and Sparse Identification of Nonlinear Dynamics (SINDy) (Brunton et al., 2016).",,,0,not_related
"…state derivatives and even often the noiseless states to formulate structured and interpretable models such as Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Lagrangian Neural Networks (LNN) (Cranmer et al., 2020) and Sparse Identification of Nonlinear Dynamics (SINDy) (Brunton et…",,,0,not_related
"An adjacent research direction is the method/models which consider CT dynamics and directly use the state derivatives and even often the noiseless states to formulate structured and interpretable models such as Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Lagrangian Neural Networks (LNN) (Cranmer et al.",,,0,not_related
"1, we compare the trajectories for the mass-spring system estimated with the proposed DHH approach and with the HNN from [11] in which the derivatives are estimated using finite differences.",,,1,related
"The modeling assumption of Hamiltonian neural networks [11] is that the observed state s = (q,p) of a dynamical system evolves according to Hamilton’s equations:",,,0,not_related
One prominent research direction that emerged recently in the literature is modeling Hamiltonian systems with neural networks [11].,,,0,not_related
"Next, we quantatively compare the proposed approach against the following baselines: 1) HNN [11] with derivatives calculated as finite differences, 2) HNN [11] with derivatives provided by the simulator, 3) NSSNN [28], 4) Neural ODE [3] and 5) DHPMs [23].",,,1,related
The original HNN model [11] had the limitation of assuming the knowledge of the state derivatives with respect to time or approximating those using finite differences.,,,0,not_related
We test our method on the following four physical systems from [11]:,,,1,related
The original HNN model [11] was trained by minimizing the loss,,,0,not_related
", [7, 11, 14, 15, 19]) is therefore a promising line of research with many potential applications.",,,0,not_related
We implement Hamiltonian neural networks (HNNs; Greydanus et al. 2019; SanchezGonzalez et al. 2019) with scalar-based MLPs for this learning task.,,,1,related
"Examples of real-world object-centric data include trajectory data from multi-agent systems [109, 110, 111], position and velocity data from networks of motion sensors [110, 112], and molecule data [53].",,,0,not_related
"For example, DeLaN [117] and HNN [109] contain modules estimating the Lagrangian/Hamiltonian function of the system and following modules deriving the prediction.",,,0,not_related
"Similarly, [109] (HNN) models the Hamiltonian function with a neural network.",,,0,not_related
"Second, existing works such as [117, 109, 123, 60, 140, 43, 34] heavily rely on heterogeneous domain-specific datasets, which greatly increases the difficulty of fairly comparing different PIML methods.",,,0,not_related
Energy Conservation Law [117][118][109][154][134],,,0,not_related
[109] designs models for forecasting n-body systems that learn and respect exact conservation laws - Hamiltonian mechanics - in an unsupervised manner.,,,0,not_related
"The continuous-time nature of NODEs makes them particularly suitable for learning complex dynamical systems [9], [10] and allows borrowing tools from the system theory to analyze NN properties.",,,0,not_related
"In recent years, the use of physical loss functions has proven beneficial for the training procedure, yielding substantial improvements over purely supervised training approaches (Tompson et al., 2017; Wu & Tegmark, 2019; Greydanus et al., 2019).",,,0,not_related
"Additional works have shown the advantages of physical loss formulations (Greydanus et al., 2019; Cranmer et al., 2020).",,,0,not_related
"Neural ODEs allow training continuous deep neural networks in an end-to-end manner, which have been successfully applied in numerous tasks, such as density estimation [2, 3], time-series modeling [4, 5], physics-based models [6, 7] and some others [8, 9, 10].",,,0,not_related
"The idea of learning Hamiltonian dynamics by machine learning models dates back to the 1990s [Howse et al., 1995; Seung et al., 1997], recently a ground-breaking study of learning a deep neural network for Hamiltonian, Hamiltonian Neural Networks (HNN) [Greydanus et al., 2019] emerges.",,,0,not_related
"Method Hamiltonian Type Loss Form Input Form Separablity Assumption Integration HNN [Greydanus et al., 2019] Standard Pointwise Canonical/Pixel No Euler DHNN [Greydanus and Sosanya, 2022] Generalized Pointwise Canonical No Euler GHNN [Course et al.",,,0,not_related
"…[Zhong et al., 2021] is perhaps the most relevant to our work, as it benchmarks ten energy-conserving neural network models, including HNN [Greydanus et al., 2019], SymODEN [Zhong et al., 2019] and CHNN [Celledoni et al., 2022], and also Lagrangian models like Deep Lagrange Network…",,,0,not_related
", 2021] is perhaps the most relevant to our work, as it benchmarks ten energy-conserving neural network models, including HNN [Greydanus et al., 2019], SymODEN [Zhong et al.",,,0,not_related
", 1997], recently a ground-breaking study of learning a deep neural network for Hamiltonian, Hamiltonian Neural Networks (HNN) [Greydanus et al., 2019] emerges.",,,0,not_related
"Recently NODEs are beginning to be deployed in system identification tasks [19], [20], [21], [22].",,,0,not_related
"Several architectures and methods regarding graph networks were proposed for leaning and inference of dynamics of physical systems [Battaglia et al., 2016, Greydanus et al., 2019, Sanchez-Gonzalez et al., 2018, Chang et al., 2016].",,,0,not_related
"One can model the driving force of the system directly [11, 13], focus on the Hamiltonian [14, 15], or the Lagrangian [6, 16].",,,0,not_related
"At the same time, the deep learning community has developed powerful tools by adapting physical modeling concepts to deep learning, for instance the Neural ODE approach was proposed for the modeling of continuous transformations ([42]), stable neural architectures were also developed by embedding neural networks with invariant structures ([36, 50, 48]).",,,0,not_related
"As an example, inspired by HNN [11], HOGN [25] models the evolution of interacting systems by Hamiltonian equations to obtain energy conservation.",,,0,not_related
"3 can automatically capture continuous dynamics [39, 40, 41, 42, 43].",,,0,not_related
"More recent neural models learn Hamiltonians directly (Greydanus et al., 2019) or indirectly (Sanchez-Gonzalez et al.",,,0,not_related
"More recent neural models learn Hamiltonians directly (Greydanus et al., 2019) or indirectly (Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020; Finzi et al., 2020b).",,,0,not_related
"This has been exploited by the ML4Physics community with Lagrangian Neural Networks (Greydanus et al., 2019), and with classical solvers this is the basis of the finite volume method and symplectic integrators.",,,0,not_related
This work is generalized to include all forms of Lagrangians in Cranmer et al. (2020) and extended to Hamiltonian mechanics in Greydanus et al. (2019).,,,0,not_related
"We can make further progress if we take advantage of the knowledge about the underlying data generation process and encode such information in the neural network architectures and the loss functions [49, 88, 40, 70].",,,1,related
"For example, recent works have used NODEs as a means to incorporate physics-based knowledge into the learning of dynamical systems [Djeumou et al., 2022; Menda et al., 2019; Gupta et al., 2020; Cranmer et al., 2020; Greydanus et al., 2019; Finzi et al., 2020; Zhong et al., 2021].",,,0,not_related
"For example, recent works have used NODEs as a means to incorporate physics-based knowledge into the learning of dynamical systems [Djeumou et al., 2022a; Menda et al., 2019; Gupta et al., 2020; Cranmer et al., 2020; Greydanus et al., 2019; Finzi et al., 2020; Zhong et al., 2021].",,,0,not_related
"In contrast to Lagrangian Shadow Integration, these approaches typically assume that the symplectic structure of the system is known and assume that observations of momentum data is available in addition to position data: Hamiltonian Neural Networks [12] learn the Hamiltonian of a system from position and momentum data of trajectories.",,,0,not_related
"Takens theorem has been fundamental to several system identification methods for general dynamical systems already [24], recently also involvingmachine learning [25– 28], with a focus on PDE [29, 30] or special structure such as (classical) Hamiltonian dynamics [31, 32].",,,0,not_related
Zhong et al. (2021) further introduce an explicit module to handle contacts inside a Lagrangian or Hamiltonian system.,,,0,not_related
"Imposing Hamiltonian (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2019) and Lagrangian (Lutter et al., 2019; Cranmer et al., 2020; Finzi et al., 2020) mechanics in learned simulators offers unique speed/accuracy tradeoffs and can preserve symmetries more effectively.",,,0,not_related
"Imposing Hamiltonian (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2019) and Lagrangian (Lutter et al.",,,0,not_related
"In contrast to [6], [8], [11], [12] we take the prior physical knowledge as external input into account which rather guides the PGNN than forces it to strictly obey a priori presumed dynamics.",,,0,not_related
"Similar to the notion of incorporating physical relations as an additional loss term, there are also approaches that exclusively rely on a physical loss function as error function evaluated on estimated and targeted outputs, such as the Hamiltonian function [12], the Lagrangian formulation of a system [11], [15] or the governing equations [6].",,,0,not_related
"Neural networks can parametrize the equations of motion for physical systems, which can have conserved quantities resulting from symmetries [4, 5].",,,0,not_related
"[4] Sam Greydanus, Misko Dzamba, and Jason Yosinski, “Hamiltonian neural networks,” (2019), arXiv:1906.",,,0,not_related
"This trend already began several years ago with the emergence of physics-guided machine learning [26] and the creation of specific network structures that represent known physical systems [18, 27, 28].",,,0,not_related
"In recent years, a new field emerged in the Machine Learning community to tackle the generalization issue of neural networks and create new NN architectures bound to follow given physical laws, such as Hamiltonian NNs [28] or Lagrangian NNs [27], later generalized by Djeumou et al.",,,0,not_related
"In recent years, a new field emerged in the Machine Learning community to tackle the generalization issue of neural networks and create new NN architectures bound to follow given physical laws, such as Hamiltonian NNs [28] or Lagrangian NNs [27], later generalized by Djeumou et al. [18].",,,0,not_related
"Recent works have encoded Hamiltonian and Lagrangian mechanics into neural models [22, 30, 11, 19], with gains in data-efficiency in physical and robotics systems, including some modeling controlled or dissipative systems [60, 15].",,,0,not_related
"Given the parameters in [22] the energy is H = 3(1 − cos q) + p(2), with p(2) − 3 · cos q being a simpler equivalent.",,,1,related
"[22] propose the setting of an ideal spring and ideal pendulum, which will allow us to understand the behavior of Noether Networks for scientific data where we know a useful conserved quantity: the energy.",,,0,not_related
"Greydanus et al. (2019) use NNs to predict Hamiltonian from phase-space coordinates s = (p,q) and their derivatives.",,,0,not_related
Hamiltonian neural network [1] Hamiltonian neural networks learn the Hamiltonian instead of the ode.,,,0,not_related
"Thereafter, Greydanus [23] built a neural network to parameterize the Hamiltonian and learn it directly from data.",,,0,not_related
"Another type of models including HNN [10], SRNN [11], GFNN [12] and others [15–19], manged to incorporate physical knowledge and solve mechanical problems by learning Hamiltonian or other physical quantities.",,,0,not_related
"To compare with existing works, we use the same initial condition as Greydanus et al. [2019]. We carry out the simulation for 1,000 time steps, and WH-NIH yields a relative energy error of dE/E0 ∼ 10−9 by the end of the simulation. This result is about 7 orders of magnitude more accurate comparing to Greydanus et al. [2019].",,,1,related
"Following Greydanus et al. [2019], we use a simple multi-layer perceptron (MLP) backbone network Hinter,θ to serve as a function approximator of Hinter.",,,1,related
"To compare with existing works, we use the same initial condition as Greydanus et al. [2019]. We carry out the simulation for 1,000 time steps, and WH-NIH yields a relative energy error of dE/E0 ∼ 10−9 by the end of the simulation.",,,1,related
"are often governed by latent differential equations that can be discovered with data driven methods [7, 14, 49, 52].",,,0,not_related
"This work is similar to [27], where inductive biases based on the underlying physics laws are coded directly into the network.",,,0,not_related
"Physics-informed deep learning models: Deep learning models can enforce physical constraints partially through the loss function [28, 39, 40] or changes in neural network architecture [41].",,,0,not_related
"To overcome these problems, we therefore combine latest advances in physics-enhanced Neural Networks based on dynamic invariants such as the Hamiltonian [1, 2, 3] or Lagrangian [4, 5] NNs with additional inductive bias to gain better predictive capabilities and to reduce the requisite training data.",,,0,not_related
"Greydanus [1] proposed to learn the Hamiltonian on the basis of N observations of the system’s state variables z = (q,p) as well as their time derivatives ż = (q̇, ṗ), i = 1, .",,,0,not_related
"The results of a black-box MLP model [1, 3] serve as baseline in Tables 1, 2.",,,0,not_related
"Recently, a body of work has emerged that brings these well-established principles of modelling dynamics from physics – such as the conservation of energy, and numerical formulations from the theory of differential equations – to neural network architectures [49, 20, 9, 3, 58, 8, 4, 31, 50, 43, 56, 12, 17, 24, 14, 32, 11, 45, 57, 54, 48, 21].",,,0,not_related
"Besides analytical methods, many authors introduced provably stable neural architectures [Haber et al., 2019, Greydanus et al., 2019, Cranmer et al., 2020] or stability constraints [John et al., 2017].",,,0,not_related
"First, Section 4.4.1 describes how energy-conserving dynamics can be enforced by encoding the problem using Hamiltonian or Lagrangian mechanics.",,,0,not_related
"For instance, we relate energy-conserving numerical solvers to Hamiltonian NNs, whose goal is to encode energy conservation, and we discuss concepts such as numerical stability and solver convergence, which are crucial in long-term prediction using NNs.",,,1,related
"The main advantage of Hamiltonian [41, 124] NNs and the closely related",,,0,not_related
"In recent years, the research community turned its attention to deriving these types of scalar valued energy functions by means of data-driven methods [41, 77, 142].",,,0,not_related
"We start with the Hamiltonian defined as
H (x ) = T (x ) −V (x ), (14)
where x = [q,p] represents the concatenated state vector of generalized coordinates q and generalized momentap.",,,1,related
"Despite their mathematical elegance, deriving analytical Hamiltonian and Lagrangian functions for complex dynamical systems is a grueling task.",,,0,not_related
"The main advantage of Hamiltonian [41, 124] NNs and the closely related Lagrangian [21, 77] NNs is that they naturally incorporate the preservation of energy into the network structure itself.",,,0,not_related
"A similar concept to that of Hamiltonian and Lagrangian NNs involves learning neural surrogates for potential energy functionsV (x ) of a dynamical system, where the primary difference with Hamiltonians and Lagrangians is that the kinetic terms are
ACM Computing Surveys, Vol. 55, No. 11, Article 236.",,,0,not_related
"Specifically, the goal is to train an NN to approximate the Hamiltonian/Lagrangian of the system, as shown in Figure 18.",,,0,not_related
"In physics, a special class of closely related functions, called Hamiltonian and Lagrangian functions, has been developed for describing the total energy of a system.",,,0,not_related
4.4.1 Hamiltonian and Lagrangian Networks.,,,0,not_related
"To improve the performance, others have introduced various inductive biases such as Hamiltonian NODE architecture [142] or penalizing higher-order derivatives of the NODEs in the
ACM Computing Surveys, Vol. 55, No. 11, Article 236.",,,0,not_related
Both Hamiltonian H and Lagrangian L are defined as a sum of total kineticT and potential energy V of the system.,,,0,not_related
"Finally, we implemented and compared with the equivariant version of Hamiltonian neural network of Greydanus et al. (2019) (see EqHNN in table.",,,1,related
"We can for example restrict the policy to obey stable system dynamics derived from first principles (Greydanus et al., 2019; Lutter et al., 2019).",,,1,related
"[5, 17, 46] introduce non-regression loss functions inspired by Hamiltonian mechanics [19].",,,0,not_related
"For example, numerous works show that energy conserving trajectories can be effectively learnt from data by enforcing known energy constraints such as Hamiltonians (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019), Lagrangians (Cranmer et al., 2020) and variational integrators (Saemundsson et al., 2020; Desai et al., 2021b) into networks.",,,0,not_related
"…works show that energy conserving trajectories can be effectively learnt from data by enforcing known energy constraints such as Hamiltonians (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019), Lagrangians (Cranmer et al., 2020) and variational integrators (Saemundsson et al., 2020;…",,,0,not_related
"For example physical conservation laws can be learned [1, 2].",,,0,not_related
A different way of formulating rigid body dynamics has been investigated in Greydanus et al. (2019) using energy conservation laws.,,,0,not_related
"Also, [Greydanus et al., 2019] used a Hamiltonian prior on a NN and studied conservative systems such as a mass-spring system and a pendulum (ideal and real data).",,,0,not_related
"Recent studies on Lagrangian (LNN) and Hamiltonian neural networks show that one of these invariant quantities, energy, can be learned directly from the data enabling realistic simulation of systems [6, 7, 4, 8, 9, 10, 11].",,,0,not_related
"For the HNN case, we parameterize the Hamiltonian function of the dynamical system with a neural network, and we use a Hamiltonian integrator to predict the dynamics [10].",,,1,related
"Inspired by [7] we consider two approaches to model f in equation (6): Neural ordinary differential equations (N-ODEs) [3], and Hamiltonian neural networks (HNNs) [10, 15].",,,0,not_related
"Methods for incorporating prior knowledge also exist for neural networks, such as [15], inspired by Hamiltonian mechanics.",,,0,not_related
"In contrast to learning the flow of the Hamiltonian dynamics, alternatively, the Hamiltonian itself can be learned, see [8], or the framework of Neural ODEs [15] can be adapted for large-scale nonseparable Hamiltonian systems [13].",,,0,not_related
"In addition to that, structure-preserving neural networks have been shown to generalize better than regular neural networks and produce qualitatively better predictions [8, 9, 10, 11, 12, 13].",,,0,not_related
"Hamiltonian and Lagrangian Networks: Hamiltonian Neural Networks (Greydanus, Dzamba, and Yosinski 2019)
Intelligence (www.aaai.org).",,,0,not_related
"…works either use the Lagrangian or the Hamiltonian formulation of dynamics to inform the structure of a neural ODE, as in (Cranmer et al. 2020; Lutter, Ritter, and Peters 2019; Roehrl et al. 2020) vs. (Greydanus, Dzamba, and Yosinski 2019; Matsubara, Ishikawa, and Yaguchi 2020; Toth et al. 2020).",,,0,not_related
"Unsupervised system identification from vision is a recent area of research that removes the requirements for trajectory data, with approaches including unsupervised physical parameter estimation [24, 31, 40], structured latent space learning [19, 25, 32], and Hamiltonian/Lagrangian learning [18, 51, 58].",,,0,not_related
"For generating data, we base our implementation on the code from Greydanus et al. (2019). We generate 800 training trajectories, 160 validation trajectories, and 160 test trajectories for varying initial conditions.",,,1,related
"For generating data, we base our implementation on the code from (Greydanus, Dzamba, and Yosinski 2019).",,,1,related
"Hamiltonian structure-preserving parameterization As a special case of the GENERIC formalism, we consider the Hamiltonian structure-preserving parameterization technique, which is originally proposed in Hamiltonian neural networks (HNNs) Greydanus et al. (2019): parameterizing the Hamiltonian function H(q, p) as HΘ(q, p) such that HΘ = (φ(q, p)Ξ).",,,0,not_related
"Parameterization techniques that preserve physical
structure include Hamiltonian neural networks (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019), Lagrangian neural networks (Cranmer et al. 2020; Lutter, Ritter, and Peters 2018), port-Hamiltonian neural networks (Desai et al. 2021), and…",,,0,not_related
"In the following, we consider the Hamiltonian structurepreserving parameterization technique proposed in Hamiltonian neural networks (HNNs) (Greydanus, Dzamba, and Yosinski 2019): parameterizing the Hamiltonian function H(q, p) asHΘ(q, p) such that
HΘ = (φ(q, p)TΞ)T. (5)
With the above…",,,1,related
"Some approaches have focused on embedding specialized physical constraints into NNs, such as conservation of energy or momentum [4, 12] or multiscale features [34].",,,0,not_related
"In addition, Greydanus et al. (2019) have used Hamiltonian Neural Networks to define reduced-order models based on learned conservation laws.",,,0,not_related
"(1) One line of work aims to understand how neural networks can be structured and trained to reproduce known physical system behavior, with the goal of designing general methods applicable in a variety of settings [8, 42, 4, 41, 37, 38, 26, 10, 50, 51].",,,0,not_related
"Past works have chosen systems of the types featured here: simple oscillators (both spring and pendulum [8]), particle systems with various interaction laws (gravity, spring forces, charges, cloth simulations, etc.",,,0,not_related
We mention here related work that embeds symmetries and invariances into neural network architectures: [41] embedded even/odd symmetry of a function and energy conservation into a neural network by adding special hub layers; [42] propose gauge equivariant CNN layers to capture rotational symmetry; [43] structures their networks following a Hamiltonian in order to learn physically conserved quantities and symmetries.,,,0,not_related
"However, in contrast we do not propagate the system by Hamiltonian dynamics, but learn a (deterministic) flow, as in Hamiltonian flows [23, 24].",,,1,related
"[23] Samuel Greydanus, Misko Dzamba, and Jason Yosinski.",,,0,not_related
"For this pendulum system, we have dp dt = −2mglsin(q), wherem = 1 (mass), g = 3 (gravity constant), l = 1 (length of the pendulum) (as in [Greydanus et al., 2019]).",,,1,related
"Robust Mimicking from Domain Knowledge Model We investigate a pendulum system [Greydanus et al., 2019] with mass m and length l, and collect the data and model
fitting results during the training process.",,,1,related
"• Spring [Greydanus et al., 2019].",,,0,not_related
"4 RQ2: How the Domain Knowledge Helps Robust Mimicking from Domain Knowledge Model We investigate a pendulum system [Greydanus et al., 2019] with mass m and length l, and collect the data and model fitting results during the training process.",,,1,related
"• Pend [Greydanus et al., 2019].",,,0,not_related
"For instance, an ideal pendulum system can be described by the following equation [Greydanus et al., 2019].",,,0,not_related
"In [9] and [6] it has been shown that by learning the Hamiltonian or the Lagrangian of a system, it is possible to accurately predict the temporal dynamics and conserve energy.",,,0,not_related
"Using the Hamiltonian formalism, [6] showed that a NN with parameters θ can be used to learn a Hamiltonian Hθ(q,p) given q and p as inputs to the network.",,,0,not_related
"1(c) [6] take q and p as inputs and are trained to yield the time derivatives of the input, with the HNN learning an intermediate Hamiltonian and employing backpropagation to compute the final output.",,,1,related
"However, their performance in learning and generalizing the long-term behaviour of dynamic systems governed by known physical laws from state data has often been limited [6, 7].",,,0,not_related
"Recently, the authors of [6] demonstrated that the dynamics of an energy conserving autonomous system can be accurately learned by guiding a neural network to prear X iv :2 10 7.",,,0,not_related
"Concretely, it has been shown that physicallyinformed learning biases embedded in networks, such as Hamiltonian mechanics [6, 8], Lagrangians [9, 10], Ordinary Differential Equations (ODEs) [11], physicsinformed networks [12, 13], generative networks [14], and Graph Neural Networks [15, 16] can significantly improve learning and generalization over vanilla neural networks in complex physical domains.",,,0,not_related
", bioinformatics [1], computer visions [2,3], finance [4,5], and physics [6, 7].",,,0,not_related
", using a prior distribution [9], a graph-network forward kinematic model [34], or a network architecture reflecting the structure of Lagrangian [22] or Hamiltonian [13] mechanical systems.",,,0,not_related
"Models designed with structure respecting kinematic constraints [34], symmetry [33, 39], Lagrangian mechanics [32, 22, 14, 8, 21] or Hamiltonian mechanics [13, 3, 5, 11, 43, 41] guarantee that the laws of physics are satisfied by construction, regardless of the training data.",,,0,not_related
"[13] model the Hamiltonian as a neural network and update its parameters by minimizing the discrepancy between its symplectic gradients and the time derivatives of the states (q, p).",,,0,not_related
"Hamiltonian-based methods [13, 3, 5, 11, 43, 41] use a Hamiltonian formulation [20, 15] of the system dynamics, instead, in terms of generalized coordinates q, generalized momenta p, and a Hamiltonian function, H(q, p), representing the total energy of the system.",,,0,not_related
"Lagrangian and Hamiltonian mechanics [20, 15] provide physical system descriptions that can be integrated into the structure of a neural network [13, 3, 5, 11, 43, 41].",,,0,not_related
"Recent works [22, 14, 8, 13, 5, 32] have considered a hybrid approach to this problem, where prior knowledge of the physics, governing the system dynamics, is used to assist the learning process.",,,0,not_related
"Many recent approaches have turned to structure preserving models of reversible dynamics to obtain an inductive bias that lies in between [4, 5, 6, 7, 8].",,,0,not_related
"Structure preserving neural networks A thorough accounting of works embedding structurepreservation into neural networks include pioneering works for Hamiltonian neural networks [4, 40], followed by development of Lagragian neural networks [41, 5] and neural networks that mimic the action of symplectic integrators [6, 7, 8].",,,0,not_related
"1 Hamiltonian Neural Networks In contrast to obtaining trajectories from a known Hamiltonian, the purpose of Hamiltonian Neural Networks (HNNs) [13] is to learn a Hamiltonian from data, composed of observed trajectories y(t) which solve Hamilton’s equation (1).",,,0,not_related
[13] have recently proposed a clever class of Hamiltonian Neural Networks (HNNs) whose architecture (see Figure 1) engraves the mathematical properties of Hamilton’s equations (notably their symplecticity).,,,0,not_related
"[13], HNNs have generated much scientific interest.",,,0,not_related
"[13], its loss function1 for one data point (y0, y1) is LHNN = ∥∥∥y1 − y0 h − J∇Ĥ(y0) ∥∥∥2 L2 (5) (1)Note that Greydanus et al.",,,1,related
The forward Euler scheme s = y0 replicates HNNs [13] trained with discretized data and represents our baseline.,,,1,related
"[13] used the analytic gradient of the true Hamiltonian as the target for most tasks, which yields a different mathematical problem, i.",,,0,not_related
"…are derived from first principles, e.g., rigid body dynamics (Wittenburg, 2013), mass action kinetics (Ingalls, 2013), or Hamiltonian dynamics (Greydanus et al., 2019), or chosen for computational convenience (e.g., linear systems (Ljung, 1998)) or parametrized to facilitate system…",,,0,not_related
", rigid body dynamics (Wittenburg, 2013), mass action kinetics (Ingalls, 2013), or Hamiltonian dynamics (Greydanus et al., 2019), or chosen for computational convenience (e.",,,0,not_related
"…paradigm has successfully guided the discovery of novel deep learning models, with applications in prediction (Rubanova et al., 2019; Greydanus et al., 2019), control (Du et al., 2020), density estimation (Grathwohl et al., 2018; Lou et al., 2020; Mathieu and Nickel, 2020), time…",,,0,not_related
"Inspired by this view, numerous ODE and PDE-based network architectures [1, 8, 9, 15, 32, 42, 48, 49], and continuous-time recurrent units [4, 29, 30, 40, 41] have been proposed.",,,0,not_related
"Besides, it has been shown 84 that neural networks often struggle to learn [25] invariant properties of physical systems and other 85 qualitative properties.",,,0,not_related
"Another interesting direction to pursue would be to apply novel physics-informed neural network architectures[63, 64] to resolve Hamiltonians of systems with many degrees of freedom (such as molecules) using time-resolved HHG spectra, such as those obtained from solids driven by mid-IR fields [24].",,,0,not_related
"For the rule encoder (φr), data encoder (φd), and decision block (φ), we use MLPs with ReLU activation at intermediate layers, similarly to [5, 16].",,,1,related
"More broadly, Hamiltonian networks have been shown to improve physical characteristics, such as better conservation of energy, and to better generalize [35, 80, 103], and Lagrangian neural networks can also enforce conservation laws [63, 20].",,,0,not_related
"We build upon the Hamiltonian Neural Network (HNN) model by [18] and run experiments on a physics-inspired setting, the ideal spring, which is represented by the Hamiltonian",,,1,related
"Examples include Hamiltonian neural networks [31, 8, 94], which reflect the conservation of energy.",,,0,not_related
"about the underlying physical system, such as conservation of energy [8, 31, 94], independence of mechanism [59], monotonicity [55], or linearity [33].",,,0,not_related
"Greydanus et al. (2019) highlights these issues and provides a method
ar X
iv :2
10 6.",,,0,not_related
The work of [16] proposes a similar approach to the previous one but using the Hamiltonian instead.,,,0,not_related
"While theoretically well-motivated learning methods for Hamiltonian [25]–[28] and Lagrangian mechanics [29], [30] have recently been proposed, the practical relevance of these methods remains limited due to restrictive assumptions [29].",,,0,not_related
"In the application of neural networks to model physical systems, several authors have also constructed equivariant (or invariant) models by incorporating equations of motion - in either the Hamiltonian or Lagrangian formulation of classical mechanics - to accommodate the learning of system dynamics and conservation laws [71, 72, 73].",,,0,not_related
"We also highlight that H-DNNs are fundamentally different from the neural networks proposed in [30], which have the same name but are designed to learn the Hamiltonian functions of mechanical systems.",,,0,not_related
"Here we will focus our discussion around the classic HNN architecture from [11] illustrated in Figure 4(b), that receives only a system’s canonical coordinates as input.",,,1,related
As in [11] the HNN model learned to respect the conservation of energy constraint and returned physically consistent predictions.,,,0,not_related
As in [11] we implemented two networks in PyTorch [22] a Baseline MLP Figure 4(a) and a HNN Figure 4(b).,,,1,related
"Some of these efforts have resulted in neural alternatives for modeling and simulation of simple dynamical systems [10, 11] and for solving ordinary and partial differential equations [12, 6, 3, 13].",,,0,not_related
Hamiltonian neural networks (HNNs) are a neural architecture that learns conservation laws in an unsupervised manner [11].,,,0,not_related
"Hamiltonian mechanics was first utilized in neural network design nearly three decades ago [18] and in recent years this area has seen renewed interest [11, 19, 20, 21, 14].",,,0,not_related
"[11] generally reported results over a short time interval, t-span = [0,20] and reported results from HNNs using Tanh activation functions in their architecture.",,,0,not_related
As a motivating example in embedding conservation of energy constraint through a HNN we will replicate the results [11] for a noisy ideal mass-spring system and discuss architectural choices.,,,1,related
We replicated the results of [11] for a noisy ideal mass-spring system.,,,1,related
"physical equations [17, 18, 31], sequence modeling processes [6], and games [41].",,,0,not_related
"Gradient-constrained Optimization: Recent work has studied directly optimizing gradients to enforce theoretical properties while training neural nets (Greydanus, Dzamba, and Yosinski 2019).",,,0,not_related
"Similarly, authors in (Greydanus, Dzamba, and Yosinski 2019) draw inspiration from Hamiltonian mechanics to train models that learn exact conservation laws in an unsupervised manner by endowing the models with better inductive biases.",,,0,not_related
"Another related field of HamNet is neural physics engines (Sanchez-Gonzalez et al., 2018; 2019; Greydanus et al., 2019), which learn to conduct simulations that conform to physical laws.",,,0,not_related
"2017; Lu et al., 2017) to physics (Greydanus et al., 2019).",,,0,not_related
", 2019), the learned representation (Lusch et al., 2018; Greydanus et al., 2019; Bau et al., 2020), hard output constraints (Mohan et al.",,,0,not_related
"…to incorporate physics as: inputs (Reichstein et al., 2019), training loss (Raissi et al., 2019), the learned representation (Lusch et al., 2018; Greydanus et al., 2019; Bau et al., 2020), hard output constraints (Mohan et al., 2020), or evaluation function (Lütjens* et al., 2021; Lesort et…",,,0,not_related
"An example for such a bias is to learn a motion which is conserving energy as performed in the context of Hamiltonian Neural Networks (HNNs) (Greydanus et al., 2019).",,,0,not_related
This is the same loss as introduced in Greydanus et al. (2019).,,,0,not_related
"As datasets we use the nearly circular orbits constructed by Greydanus et al. (2019), but give the whole system a boost in a random direction sampled from N (0, 0.1)2.",,,1,related
For integrating the solutions in time from our respective symmetry control network we use a fourth order Runge-Kutta integrator as in Greydanus et al. (2019) which unlike symplectic integrators allows for a comparison with neural network approaches directly predicting the dynamics of a…,,,1,related
"engineering, such as intuitive physics [24], [25], computational fluid dynamics [26], [27], Hamiltonian mechanics [28], and simulations of Symplectic integration and Lagrangians [29],",,,0,not_related
"Recent work has also focused on embedding specialized kinds of constraints into neural networks, such as conservation of energy (see, e.g., Greydanus et al. (2019) and Beucler et al. (2019)), and homogeneous linear inequality constraints (Frerix et al., 2020).",,,0,not_related
"This concern has led to the development of HNN and Lagrangian neural networks (LNN) [24, 30], with which the system energy is well conserved in the long-term evolution.",,,0,not_related
"In machine learning of Hamiltonian systems, a major concern is whether the system energy will be conserved in the long-term evolution [24, 30].",,,0,not_related
"As shown in Greydanus et al. (2019) with Hamiltonian Neural Networks (HNNs), one can exploit this Hamiltonian structure by parametrizing Ĥθ(z) with a neural network, and then taking derivatives to find the implied Hamiltonian dynamics.",,,0,not_related
"Making use of such priors allows models in this class to exhibit desirable properties by construction, such as being strictly Hamiltonian [16, 17, 21] or globally stable [15].",,,0,not_related
"To achieve physical-consistency, one could adapt the neural network architecture to incorporate physics as: inputs [31], training loss [32], the learned representation [33], [34], [22], hard output constraints [35],",,,0,not_related
"An important component for sample-based learning of dynamical models is the choice of priors, which usually comes from our understanding of the underlying physical laws (Greydanus et al., 2019; Miles et al., 2020).",,,0,not_related
"This generic approach manages to achieve decent results,
and it is often adopted as the comparative baseline when proposing other novel approaches (Greydanus et al., 2019; Miles et al., 2020; Sanchez-Gonzalez et al., 2018; Lutter et al., 2019).",,,0,not_related
"Long horizon prediction is a commonly used task to test the quality of learned dynamical models (Sanchez-Gonzalez et al., 2018; Lutter et al., 2019; Greydanus et al., 2019; Miles et al., 2020; Janner et al., 2019).",,,0,not_related
"One line of research takes advantage of the properties of the Hamiltonian and the Lagrangian function of dynamical systems (Greydanus et al., 2019; Miles et al., 2020; Lutter et al., 2019).",,,0,not_related
"Specifically, Greydanus et al. (2019) propose Hamiltonian Neural Networks (HNN) to explicitly use the Hamiltonian formalism and the canonical coordinates.",,,0,not_related
", 2016) or due to the ambiguities in unsupervised learning (Greydanus et al., 2019; Cranmer et al., 2020b).",,,0,not_related
"These methods require few prior assumptions about the system itself, but lack interpretability due to entangled variational factors (Chen et al., 2016) or due to the ambiguities in unsupervised learning (Greydanus et al., 2019; Cranmer et al., 2020b).",,,0,not_related
"Recent work on physics-based deep learning injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al.,
2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly…",,,0,not_related
"…in the context of differentiable physics simulation Degrave et al. (2016); de Avila Belbute-Peres et al. (2018); Cranmer et al. (2020b); Toth et al. (2020); Greydanus et al. (2019); Sanchez-Gonzalez et al. (2019), we focus on more challenging systems which have not been studied in prior art.",,,0,not_related
"We used hidden layer sizes [16,16] for ICNN, [8,8] for each of the FCNN damping modules and [16,16] for ANN-PPO.",,,1,related
"Many recent methods have used the Lagrangian formulation [14], [15] and also the closely related Hamiltonian formulation [16], [17], as physics-based priors for learning dynamics models.",,,0,not_related
"We used hidden layer sizes [16,16] for ICNN, [8,8] for each of the FCNN damping modules and [16,16] for ANN-PPO.",,,1,related
"Many recent methods have used the Lagrangian formulation [14], [15] and also the closely related Hamiltonian formulation [16], [17], as physics-based priors for learning dynamics models.",,,0,not_related
The ConSciNet model’s performance and parameter generalization ability was assessed on two simple dynamic systems from [8]: an ideal pendulum and ideal mass-spring system.,,,0,not_related
"In recent years this area has seen renewed interest [8, 10, 17, 18].",,,0,not_related
Here we will focus our discussion around [8]’s Hamiltonian Neural Network architecture for learning exactly conserved quantities from data in an unsupervised manner.,,,1,related
The individual results here for each value of l∗ are similar to [8]’s results for a system with fixed parameters suggesting the ConSciNet approach successfully generalizes an HNN with regards to a system’s parameters.,,,1,related
"To address this issue neural architectures have been designed to explicitly embed physical constraints and symmetries [8, 9, 10, 11, 12] in the description of classical dynamical systems.",,,0,not_related
In [8] the system’s physical parameters are held constant and absorbed into the canonical coordinates.,,,0,not_related
"The modelling of dynamical systems with neural networks is discussed in many papers [15, 16, 17, 18].",,,0,not_related
"For example, the network may incorporate inductive bias corresponding to the physical laws of interaction and motion [25, 6, 107, 16, 92, 32].",,,0,not_related
"Among other, networks covering lagrangians [12] and hamiltonian dynamics [13] have been introduced.",,,0,not_related
"In terms of mechanical problems modeled by Hamiltonian systems, seminal progress include HNN (Greydanus et al., 2019) and an independent work (Bertalan et al., 2019), SRNN (Chen et al., 2020), SympNets (Jin et al., 2020), and (Lutter et al., 2019; Toth et al., 2020; Zhong et al., 2020; Wu et al.,…",,,0,not_related
"In particular, both HNN and SRNN are based on the great idea of learning (using a neural network) the Hamiltonian that generates the vector field (VF), instead of learning the VF itself; this improves accuracy as the Hamiltonian structure of the VF will not be lost due to approximation.",,,0,not_related
"HNN, on the other hand, has exponentially growing error which quickly saturates to maximum values (due to boundedness of trajectories).",,,0,not_related
"HNN learns the Hamiltonian by matching its induced VF with the latent VF (when such information is unavailable, for example in a purely data driven context, data-based approximation such as finite-difference is needed).",,,0,not_related
"The time derivative data of the vector field based methods (VFNN, HNN) are generated using (1st-order) finite difference.",,,0,not_related
"HNN, SRNN, SympNets are trained by their provided codes.",,,1,related
"VFNN, HNN, SRNN (seq len=2), and GFNN are trained with the same data set D2, while SRNN (seq len=5) is trained with D5.",,,1,related
"One can still apply these methods regardless, for example by using finite differences to construct a fictitious vector field for VFNN and HNN to learn, or just use SRNN without realizing that no Hamiltonian will be able to produce the training data.",,,0,not_related
"HNN, SRNN are trained under default training setups and SympNets is trained using LA-SympNets with 30 layers and 10 sublayers (deeper than their default setups for improved performance).",,,0,not_related
"In terms of mechanical problems modeled by Hamiltonian systems, seminal progress include HNN (Greydanus et al., 2019) and an independent work (Bertalan et al., 2019), SRNN (Chen et al., 2020), SympNets (Jin et al., 2020), and (Lutter et al., 2019; Toth et al., 2020; Zhong et al., 2020; Wu et al., 2020; Xiong et al., 2021), all of which, except SympNets, are related to learning some quantity that produces the Hamiltonian vector field.",,,0,not_related
"The standard map seems to be a challenging problem; HNN and SRNN did not manage to reproduce any chaotic motion, and VFNN completely distorted the chaotic sea.",,,0,not_related
"Note the seminal work of HNN used RK45 which is not symplectic, however with small error tolerance (thus good precision but high computation cost).
nonseparable3.",,,0,not_related
"Methods based on vector fields (e.g., VFNN) or Hamiltonian (e.g., HNN, SRNN) are not very suitable for this prediction task because there is no latent continuous (Hamiltonian) dynamics.",,,0,not_related
"HNN, SRNN are trained by their provided codes under default training setups.",,,1,related
"The time derivative data of the vector field based methods (VFNN, HNN) are generated using (1st-order) finite difference (with ∆t = 1).",,,0,not_related
"Finally, we would like to point out that discrete Hamiltonian systems have already been used to design RNNs, for instance in (Greydanus et al., 2019) and also in (Chen et al.",,,1,related
"Finally, we would like to point out that discrete Hamiltonian systems have already been used to design RNNs, for instance in (Greydanus et al., 2019) and also in (Chen et al.)",,,1,related
"…al., 2020; Muralidhar et al., 2020; Zhang et al., 2021; BelbutePeres et al., 2020);
• fB works as an observation function that changes signal’s modality (Greydanus et al., 2019; Lutter et al., 2019; Yıldız et al., 2019; Linial et al., 2020; Toth et al., 2020; Cranmer et al., 2020; Saemundsson et…",,,0,not_related
"Experiments on Human Locomotion
Physics model We modeled fP with a trainable Hamilton’s equation as in Toth et al. (2020); Greydanus et al. (2019):
fP
([ pT qT ]T , zP ) = [ −∂H∂q T ∂H ∂p T ]T , (26)
where p ∈ Rdy is a generalized position, q ∈ Rdy is a generalized momentum, andH : Rdy ×Rdy → R is…",,,1,related
"We did not choose a specific model but let fP be a trainable Hamilton’s equation as in [39, 11].",,,1,related
"We did not choose a specific model but let fP be a trainable Hamilton’s equation as in Toth et al. (2020); Greydanus et al. (2019):
fP
([ pT qT ]T) = [ −∂H∂q T ∂H ∂p T ]T , (24)
where p ∈ Rdy is a generalized position, q ∈ Rdy is a generalized momentum, andH : Rdy ×Rdy → R is a Hamiltonian.",,,1,related
"The first condition has been mitigated by explicitly incorporating symplectic structure or conservation laws on neural networks, called Hamiltonian neural networks (HNN) (Greydanus et al., 2019) for learning Hamiltonian dynamics.",,,0,not_related
"The most related to our work, (Greydanus et al., 2019) introduced Hamiltonian Neural Networks (HNNs) to learn the dynamics of Hamiltonian systems by parameterizing the Hamiltonian with neural networks.",,,0,not_related
"Following (Greydanus et al., 2019), we evaluate the MSEs of the predicted trajectories and energies from their corresponding ground truth at each time step.",,,1,related
"In (Greydanus et al., 2019), the Hamiltonian function can be approximated by neural networks, Hθ, called HNN.",,,0,not_related
"At least hundreds of states should be given as train set, and thousands of gradient steps are required for training HNN for learning a system (Greydanus et al., 2019).",,,0,not_related
"…physical phenomena by learning the energy functionH in such equations with a neural network HNN (e.g., Chen et al. (2020); Cranmer et al. (2020); Greydanus, Dzamba, and Yosinski (2019); Matsubara, Ishikawa, and Yaguchi (2020); Zhong, Dey, and Chakraborty (2020)); however, to the best of our…",,,0,not_related
"Among them, the most basic studies are neural ordinary differential equations (Chen et al. 2018) and HNNs (Greydanus, Dzamba, and Yosinski 2019).",,,0,not_related
"In this paper, we focus on theories of the properties of the most fundamental model, comprising Hamiltonian neural networks (HNNs) (Greydanus, Dzamba, and Yosinski 2019)
du dt = S ∂HNN ∂u
(2)
and their extensions in practical situations, where the learning error is not completely zero.",,,0,not_related
"A rapidlygrowing line of work (Raissi, 2018; Greydanus et al., 2019; Lutter et al., 2020; Cranmer et al., 2020; Sæmundsson et al., 2020) has thus focused on how to introduce inductive biases into these networks to enable them to learn more accurate models from less data.",,,0,not_related
"Though neural networks are good at approximating general classes of functions, they often struggle to learn invariant properties of physical systems, such as the conservation of energy or momentum (Greydanus et al., 2019) and other qualitative properties.",,,0,not_related
"…differential equations (E, 2017; Haber and Ruthotto, 2017; Chen et al., 2018; Ruthotto and Haber, 2018) and physicallyinspired neural networks (Raissi, 2018; Greydanus et al., 2019; Lutter et al., 2020; Sæmundsson et al., 2020), we study the problem of learning unknown contact dynamics from data.",,,0,not_related
"L G
] 1
5 A
models (Deisenroth et al., 2015; Greydanus et al., 2019; Lutter et al., 2020; Cranmer et al., 2020; Sæmundsson et al., 2020).",,,0,not_related
"These inductive biases have been shown to improve data efficiency and facilitate accurate long-term forecasting (Raissi, 2018; Greydanus et al., 2019; Lutter et al., 2020; Cranmer et al., 2020; Sæmundsson et al., 2020).",,,0,not_related
"This task has been studied previously by a number of authors, such as Greydanus et al. (2019) and Sæmundsson et al. (2020), who propose to use a network constructed from a variational velocity Verlet integrator in order to learn in a data-efficient manner.",,,0,not_related
"Building on these ideas, a number of recent works have proposed replacing the black-box system of ODEs with other systems that are more structured (Raissi, 2018; Greydanus et al., 2019; Lutter et al., 2020; Cranmer et al., 2020; Sæmundsson et al., 2020).",,,0,not_related
"…from mechanics, such as the Euler–Lagrange equations or Hamilton’s equations, with structure-preserving integrators, such as the Störmer–Verlet method, giving rise to physically structured neural networks (Greydanus et al., 2019; Lutter et al., 2020; Cranmer et al., 2020; Sæmundsson et al., 2020).",,,0,not_related
"Neural ODE is available for modeling a continuous-time dynamics; irregularly sampled time series [21], stable dynamical systems [30, 35], and physical phenomena associated with geometric structures [2, 12, 26].",,,0,not_related
"Experimental Settings: We evaluated the proposed symplectic adjoint method on learning continuous-time dynamical systems [12, 26, 33].",,,1,related
"One example involves the learning of invariant quantities via their Hamiltonian or Lagrangian representations [13, 14, 15, 16, 17].",,,0,not_related
"A particular interest was given to the automatic learning of equivariances in dynamical systems through their Hamiltonian [14, 15, 16, 17] or Lagrangian [13] formulations.",,,0,not_related
"As example applications, to solve the dynamics modeling problems, some works have introduced Hamiltonian dynamics (Greydanus et al., 2019; Toth et al., 2019; Zhong et al., 2019).",,,0,not_related
"In some works, in order to solve machine learning problems such as sequence prediction or reinforcement learning, neural networks attempt to learn a data symmetry of physical systems from noisy observations directly (Greydanus et al., 2019; Toth et al., 2019; Zhong et al., 2019; Sanchez-Gonzalez et al., 2019).",,,0,not_related
"…in order to solve machine learning problems such as sequence prediction or reinforcement learning, neural networks attempt to learn a data symmetry of physical systems from noisy observations directly (Greydanus et al., 2019; Toth et al., 2019; Zhong et al., 2019; Sanchez-Gonzalez et al., 2019).",,,0,not_related
"Energy: The prior of Lagrangian/Hamiltonian dynamics conserve energy along each collision-free trajectory, which is one of the reason that Lagrangian/Hamiltonian-based neural network models perform better in prediction and generalization [7, 13, 14, 10].",,,0,not_related
"A large body of work (Cohen et al., 2020; 2018; Esteves et al., 2018; Greydanus et al., 2019; Romero et al., 2020; Finzi et al., 2020; Tai et al., 2019) proposes to hard-code equivariances in the neural architecture, which requires a priori knowledge of the transformations present in the data.",,,0,not_related
"NODE framework also allows encoding prior knowledge about the observed phenomena on the network topology, which leads to Hamiltonian and Lagrangian neural networks that are capable of long-term extrapolations, even when trained from images (Greydanus et al., 2019; Cranmer et al., 2020).",,,0,not_related
"Other examples of dynamical systems inspired models include the learning of invariant quantities via their Hamiltonian or Lagrangian representations [37, 22, 10, 71, 58].",,,0,not_related
"In contrast to our method the HNN requires additional derivative information, either analytical or as finite differences.",,,1,related
"The dynamics f̂ is again trained with independent sparse GPs, where the third
Structure-preserving Gaussian Process Dynamics
0 5 10 15 20 25 30 35 40 time t
0
1
2
3
4
5
L 2 -e
rr or
SGPD Euler HNN
0 5 10 15 20 25 30 time t
0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
L 2 -e
rr or
SGPD Euler HNN
0 5 10 15 20 25 30 35 40 time t
0.00
0.01
0.02
0.03
0.04
0.05
0.06
L 2 -e
rr or
SGPD Euler HNN
Figure 3: L2-errors of averaged state trajectories for all three methods.",,,0,not_related
We compare with the following methods: Hamiltonian neural network (HNN) [11]: Deep learning approach that is tailored to respect Hamiltonian structure.,,,1,related
SGPD 9 · 10−4 ·10−3 Euler 10−3 4 · 10−3 HNN 10−3 2 · 10−3,,,0,not_related
"Since the HNN is designed for non-constrained Hamiltonians it requires pairs of p and q and is, thus, not applicable.",,,0,not_related
"Inspired by [11], we average the approximated energy along 5 independent trajectories Hn = ∑5 i=1 H n 5 and compute the average total energy Ĥ = 1 n ∑ nHn.",,,1,related
This was addressed by applying a NN [11].,,,1,related
"Table 1: Shown are the total L2-errors in 1a and an analysis of the total energy for the non-separable system 1b.
(a) total L2-errors (mean (std) over 5 indep. runs)
task SGPD Euler HNN (i) 0.421 (0.1) 0.459 (0.12) 4.69 (0.02) (ii) 0.056 (0.01) 0.057 (0.009) 0.12 (0.009) (iii) 0.033 (0.01) 0.034 (0.021) 0.035 (0.007) (iv) 0.046 (0.014) 0.073 (0.02) -
(b) Energy for non-separable Hamiltonian
method energy err. std. dev.",,,1,related
"We compare to the following state-of-theart approaches:
Hamiltonian neural network (HNN) [14]: Deep learning approach that is tailored to respect Hamiltonian structure.",,,1,related
Separable Hamiltonians: the systems i) and ii) are both separable Hamiltonians that are also considered as baseline problems in [11].,,,0,not_related
"This paradigm, including one of its core model classes, the Neural Ordinary Differential Equation (Neural ODE) [17], [18] has been successfully employed in combination to energy based models [19], [20], [21], [22], offering a grounded way to introduce first–principles into neural networks.",,,0,not_related
"More general formulations such as Hamiltonian (Greydanus et al., 2019; Toth et al., 2019) and Lagrangian",,,0,not_related
"More general formulations such as Hamiltonian (Greydanus et al., 2019; Toth et al., 2019) and Lagrangian
Dynamics (Lutter et al., 2019) are currently restricted in terms of the dimension of the dynamical system.",,,0,not_related
"This type of behavior presents a difficulty for machine learning and is impossible for methods that assume the pendulum to be a closed system, such as HNNs (Greydanus et al., 2019) (see Appendix B.",,,0,not_related
"Hamiltonian Neural Networks (HNNs) (Greydanus et al., 2019) and Symplectic Recurrent Neural Networks (Chen et al., 2019) make energy conserving predictions by using the Hamiltonian, a function that maps the inputs to the quantity that needs to be conserved.",,,0,not_related
"However, standard deep learning approaches struggle at conserving quantities across layers or timesteps (Beucler et al., 2019b; Greydanus et al., 2019; Song & Hopke, 1996; Yitian & Gu, 2003), and often solve a task by exploiting spurious correlations (Szegedy et al.",,,0,not_related
"To this end, we use the data generation process by (Greydanus et al., 2019).",,,1,related
"This type of behavior presents a difficulty for machine learning and is impossible for methods that assume the pendulum to be a closed system, such as HNNs (Greydanus et al., 2019) (see Appendix B.3.2).",,,0,not_related
"This type of behavior presents a difficulty for machine learning and is impossible for methods that assume the pendulum to be closed systems, such as HNNs (Greydanus et al., 2019).",,,0,not_related
"However, standard deep learning approaches struggle at conserving quantities across layers or timesteps (Beucler et al., 2019b; Greydanus et al., 2019; Song & Hopke, 1996; Yitian & Gu, 2003), and often solve a task by exploiting spurious correlations (Szegedy et al., 2014; Lapuschkin et al., 2019).",,,0,not_related
"Hamiltonian Neural Networks (HNNs) (Greydanus et al., 2019) and Symplectic Recurrent Neural Networks (Chen et al.",,,0,not_related
"In addition, deep learning tools are used to solve various practical problems related to PDE recently [5, 14, 26].",,,0,not_related
"posed neural network is inspired by Hamiltonian mechanics [20, 35].",,,0,not_related
"This directly contrasts with similar approaches [21], [6], [7] that only focus on learning a Hamiltonian but do not actually account for data generated by a conservative process.",,,0,not_related
"A wide variety of recent work has focused on the same problem where a physics-informed identification of dynamical systems is sought by prioritizing adherence to physical laws and/or phenomena such as stability [1], [2], [3], [4], conservation of energy [5], [6], [7], the principle of least action [8], [9], [10], [11], and symmetries in the dynamics [12], [10], [11].",,,0,not_related
"For example in [6], the authors learn a Hamiltonian system (denoted a Hamiltonian neural network) by minimizing the squared difference between the derivative of the parameterized Hamiltonian and the numerical derivatives of the states",,,0,not_related
"This approach is akin to the derivative reconstruction approaches used by, for example, Hamiltonian neural networks [6].",,,0,not_related
"Additional details on ARMs with linear transformed NNs is found in [22, 68, 23, 72, 67, 71] (cf.",,,1,related
[67] 2019 DD NN  PyTorch (Python) Lutter et al.,,,0,not_related
"In [25] the models of HNNs and LNNs were generalized to Constrained Hamiltonian Neural Networks (CHNNs) and Constrained Lagrangian Neural Networks (CLNNs), enabling them to learn constrained mechanical systems written in Cartesian coordinates.",,,0,not_related
"Moreover, INN-based PNNs are able to learn multiple trajectories of a Poisson system on the whole data space simultaneously, while AE-HNNs, HGNs, CHNNs and CLNNs are only designed to work on low-dimensional submanifolds of Rn.",,,0,not_related
"For example, HNNs [15] use a neural network H̃ to approximate the Hamiltonian H in (1), then learn H̃ by reformulating the loss function.",,,0,not_related
"In other developments, autoencoder-based HNNs (AE-HNNs) [15] and Hamiltonian Generative Networks (HGNs) [19] were proposed to learn and predict the images of mechanical systems, which can be seen as Hamiltonian systems on manifolds embedded in high-dimensional spaces.",,,0,not_related
"Based on HNNs, other models were proposed to tackle problems in generative modeling [16], [19] and continuous control [20].",,,0,not_related
"…shared network weights can be learned via forming a loss function consisting
1We also note that there are other studies (e.g., (Cranmer et al. 2020; Greydanus, Dzamba, and Yosinski 2019)) using the idea of parameterizing the governing equations, where derivatives are also computed using…",,,0,not_related
"Works such as [12, 18] propose networks constrained through physical properties such as Hamiltonian co-ordinates or Lagrangian Dynamics.",,,0,not_related
"In recent years, a growing number of neural network models have been proposed to solve the problem of learning dynamics, expressed via a set of differential equations, from data (Lutter et al., 2019b; Greydanus et al., 2019; Zhong et al., 2020a; Chen et al., 2020; Roehrl et al., 2020; Cranmer et al., 2020; Finzi et al., 2020).",,,0,not_related
"…of neural network models have been proposed to solve the problem of learning dynamics, expressed via a set of differential equations, from data (Lutter et al., 2019b; Greydanus et al., 2019; Zhong et al., 2020a; Chen et al., 2020; Roehrl et al., 2020; Cranmer et al., 2020; Finzi et al., 2020).",,,0,not_related
HNN: Greydanus et al. (2019) introduces Hamiltonian Neural Networks (HNN) to learn Hamiltonian dynamics (6) from data by parametrizing the Hamiltonian H using a neural network.,,,0,not_related
"such as Hamiltonian flows [7, 21] and Augmented Normalizing Flows (ANFs) [8].",,,0,not_related
"A host of works have begun to view neural networks from a dynamical systems perspective leading to new regularizations [23, 24], architectures [25, 26, 27, 28, 29], analysis methods [30, 31, 20, 32], and stability guarantees for some architectures [33].",,,0,not_related
"Most of them are done in a supervised way [1, 2, 3, 4].",,,0,not_related
"such as polynomial systems, Sums-of-Squares (SOS) method leads to a bilinear optimization that is solved by some form of alternation (Jarvis-Wloszek et al., 2003; Majumdar et al., 2013). As a dual to Lyapunov-based methods, Majumdar et al. (2014) uses the notion of occupation measure to optimize a feedback controller for a polynomial system, but it has scalability issues due to its reliance on SDP optimization toolbox.",,,0,not_related
Greydanus et al. (2019); Zhong et al. (2020) presented a dynamics model that conformed to the Hamiltonian equation and demonstrated that their model conserves a quantity analogous to the total energy.,,,0,not_related
"The same technique has been used to analyze the training of neural networks in other contexts (Saxe et al., 2014; Greydanus et al., 2019).",,,0,not_related
"Prior work has investigated this kind of temporal extrapolation in recurrent networks, but solutions usually required baking in a conservation law of some sort [26, 27].",,,0,not_related
"More generally, QDF can be viewed as one of the approaches such as the physics-informed, Hamiltonian, Fermionic, and Pauli neural networks [32, 33, 34, 35, 36, 37, 38]; these solve the physical problems and equations using physically meaningful modeling.",,,0,not_related
"rporating strong inductive biases inspired by physics, for instance, architectures respecting energy conservation laws. Examples of such architecture are linear operator constraints [52], Hamiltonian [53] or Lagrangian [54] neural networks. From an architecture perspective, the work we present here is inspired by a family of neural state-space models (SSM) [55, 56, 57, 58, 59], representing structural",,,0,not_related
"Other work linking conservation law and machine learning [9, 19–21] focus on embedding physical inductive biases into machine learning, but not the other way around to automate physical discoveries with machine leaning.",,,0,not_related
[20] S.,,,0,not_related
A Hamiltonian network achieves better conservation of an energy-like quantity without damping [24].,,,0,not_related
A Hamiltonian network achieves better conservation of an energy-like quantity without damping [24].,,,0,not_related
This would allow us to use Hamiltonian neural network approaches to learn the gravitational potential [6].,,,0,not_related
"(Greydanus, Dzamba, and Yosinski 2019) and (Cranmer et al. 2020) directly encode symmetries in neural networks using respectively the Hamiltonian and Lagrangian framework.",,,0,not_related
"An alternative to penalty and barrier methods are neural network architectures imposing hard constraints, such as linear operator constraints [59], or architectures with Hamiltonian [60] and Lagrangian [61] structural priors for enforcing energy conservation laws.",,,0,not_related
"ods are neural network architectures imposing hard constraints, such as linear operator constraints [45], IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 3 or architectures with Hamiltonian [46] and Lagrangian [47] structural priors for introducing physics-based constraints such as energy conservation laws. II. PRELIMINARIES A. System Dynamics We assume unknown partially observable nonlinear",,,0,not_related
"[11] introduced Hamiltonian neural networks (HHN), they did apply HNN to noncanonical coordinates of the simple pendulum, but their loss function assumed the conjugate momentum equalled the velocity, which is not generally true.",,,0,not_related
[11] in their “Hamiltonian neural networks” introduced a new physics-aware neural network that outputs an energylike scalar and then optimises its gradient to respect theHamiltonianflow.,,,0,not_related
[11] Introduces Hamiltonian neural network (HNN) [12] Improves to Hamiltonian generative network [13] Symplectic neural network [15] Symmetries neural network [14] Nonlinear pendulum application [16] HNN learns order-to-chaos transition [17] Quantifies HNN advantage with training [18] HNN for both libration and rotation,,,0,not_related
(2) We show how to learn Hamiltonians and Lagrangians in Cartesian coordinates via explicit constraints using networks that we term Constrained Hamiltonian Neural Networks (CHNNs) and Constrained Lagrangian Neural Networks (CLNNs).,,,1,related
"We compare our Constrained Hamiltonian and Lagrangian Neural Networks (CHNNs, CLNNs) against NeuralODEs [1], Hamiltonian Neural Networks (HNNs) [9], and Deep Lagrangian Networks (DeLaNs) [14] on the systems described above.",,,0,not_related
"Zhong et al. [21] showed how to extend HNNs to dissapative systems, and Cranmer et al. [3] with LNNs showed how DeLaNs could be generalized to systems without quadratic kinetic energies.",,,0,not_related
"Previous work has considered relatively simple systems such as the 1 and 2-pendulum [3, 9], Cartpole, and Acrobot [2].",,,0,not_related
[9] proposed Hamiltonian Neural Networks (HNNs) which parametrize H with a neural network.,,,0,not_related
Greydanus et al. [9] proposed Hamiltonian Neural Networks (HNNs) which parametrize H with a neural network.,,,0,not_related
"On these systems, our explicitly-constrained CHNNs and CLNNs are 10 to 100 times more accurate than HNNs [9] and DeLaNs [14], which are implicitlyconstrained models.",,,0,not_related
"Recent work has shown that we can model physical systems by learning their Hamiltonians and Lagrangians from data [9, 14, 20].",,,0,not_related
"in [14], the Hamiltonian conservation is encoded in the loss function.",,,0,not_related
We show a motivational example in Figure 1 by comparing our approach with a traditional HNN method [14] regarding their structural designs and predicting abilities.,,,1,related
", by enforcing incompressibility [24, 2], the Galilean invariance [22], quasistatic equilibrium [12], and the invariant quantities in Lagrangian systems [10, 23] and Hamiltonian systems [16, 14, 18, 37].",,,0,not_related
[14] introduced Hamiltonian neural networks (HNNs) to conserve the Hamiltonian energy of the system by reformulating the loss function.,,,0,not_related
"Models that learn to respect conservative laws, through the use of Hamiltonian dynamics have been recently developed [26].",,,0,not_related
"3 [36, 8, 56, 7, 26] O(2)",,,0,not_related
"Without attempting to be exhaustive, recent domains of application include; the development of Hamiltonian Monte Carlo techniques [28], applications of symplectic integration to optimization [32], inference of symbolic models from data [29], and the development of Hamiltonian Neural Networks [26, 57].",,,0,not_related
"While alternative methods for modelling, such as Lagrangian and Hamiltonian Mechanics, may be appreciated for their mathematical elegance and convenience; the Newton Euler approach is based on global and interpretable physical quantities enabling robust out-of-sample generalization.",,,0,not_related
"To overcome this shortcoming, ‘grey-box’ models that combine deep networks with physical insights have been recently proposed, e.g., incorporating Lagrangian (Lutter et al., 2019; Lutter & Peters, 2019; Gupta et al., 2019) and Hamiltonian Mechanics (Greydanus et al., 2019) for energy-conserving models.",,,0,not_related
", 2019) and Hamiltonian Mechanics (Greydanus et al., 2019) for energy-conserving models.",,,0,not_related
"…this shortcoming, ‘grey-box’ models that combine deep networks with physical insights have been recently proposed, e.g., incorporating Lagrangian (Lutter et al., 2019; Lutter & Peters, 2019; Gupta et al., 2019) and Hamiltonian Mechanics (Greydanus et al., 2019) for energy-conserving models.",,,0,not_related
"There have been several recent articles that focus on dynamical systems, only subject to conservative forces, in the context of machine learning [1, 2, 3, 4, 5].",,,0,not_related
"…design of novel architectures (Chang et al. 2017; Zhu, Chang, and Fu 2018; Demeester 2019; Chang et al. 2019; Cranmer et al. 2020; Massaroli et al. 2020a) as well as guiding the injection of physics–inspired inductive biases (Greydanus, Dzamba, and Yosinski 2019; Köhler, Klein, and Noé 2019).",,,0,not_related
"This complexity can be seen in the original work on Hamiltonian Neural Networks (Greydanus, Dzamba, and Yosinski 2019) where the performance on the 2-body problem was good, but significantly deteriorated on the 3-body problem.",,,0,not_related
"Most of the existing work focuses on introducing better physical biases such as a relational model (Battaglia et al. 2016), conservation law bias (Greydanus, Dzamba, and Yosinski 2019), combining
these with an ODE bias (Sanchez-Gonzalez et al. 2019) and various similar refinements (Chen et al.…",,,0,not_related
"Recent studies show that neural networks can successfully learn to simulate complex physical processes (Battaglia et al. 2016; Sanchez-Gonzalez et al. 2018; Mrowca et al. 2018; Li et al. 2018; Greydanus, Dzamba, and Yosinski 2019; Sanchez-Gonzalez et al. 2019, 2020).",,,0,not_related
"To address this, various datadriven methods for learning system dynamics have been investigated (Battaglia et al. 2016; Mrowca et al. 2018; Li et al. 2018; Greydanus, Dzamba, and Yosinski 2019; SanchezGonzalez et al. 2019, 2020; Finzi et al. 2020).",,,0,not_related
"The Hamiltonian (Greydanus et al., 2019; Toth et al., 2020) is implemented by a MLP that takes the state Xt and outputs a scalar estimation of the Hamiltonian H of the system: the derivative is then computed by an in-graph gradient of H with respect to the input: F (Xt) = ( ∂H ∂(dθ/ dt) ,− ∂H dθ ) .",,,0,not_related
"For the pendulum, we compare to Hamiltonian neural networks (Greydanus et al., 2019; Toth et al., 2020) and to the the deep Galerkin method (DGM, Sirignano & Spiliopoulos, 2018).",,,0,not_related
"Our physical models are: • Hamiltonian (Greydanus et al., 2019), a conservative approximation, with Fp = {FH p : (u, v) 7→ (∂yH(u, v),−∂xH(u, v)) | H ∈ H(1)(R(2))}, H(1)(R(2)) is the first order Sobolev space.",,,1,related
"Incomplete physics Hamiltonian (Greydanus et al., 2019) -1.",,,0,not_related
"We also mention hybrid methods, which use a discretization of an ODE (in particular a Hamiltonian system) in order to learn the continuous representation of the data, see for instance [15, 9].",,,1,related
", enforcing conservation laws in subdomains [40], hyperbolic conservation laws [57], Hamiltonian mechanics [25, 67], symplectic structures [11, 32], Lagrangian mechanics [14] and metriplectic structure [29]) and we believe that adapting/extending ideas of these approaches potentially mitigates the limitation of data-driven ROM approaches.",,,0,not_related
"More works on learning Hamiltonian systems can be found in [11, 34, 43] and references cited therein.",,,0,not_related
"More works on learning Hamiltonian systems can be found in [13, 37, 46] and references cited therein.",,,0,not_related
"A popular way of encoding inductive biases is with clever network design to make predictions translation equivariant (CNNs), permutation equivariant (GNNs), or conserve energy [23].",,,0,not_related
We showcase this in the real pendulum experiment used by Hamiltonian Neural Networks (HNNs) [23].,,,1,related
"Through TorchDyn neural differential equations and derivative models, e.g (Greydanus et al., 2019; Toth et al., 2019; Massaroli et al., 2020b; Lutter et al., 2019; Cranmer et al., 2020; Massaroli et al., 2020a; Li et al., 2020), including yet–to–be–published combinations, can effortlessly be…",,,0,not_related
"…(Massaroli et al., 2020b)
Galërkin Neural ODEs (Massaroli et al., 2020b) Stacked Neural ODEs (Massaroli et al., 2020b)
Hamiltonian (Greydanus et al., 2019)
Lagrangian (Lutter et al., 2019; Cranmer et al., 2020)
Stable Neural Flows (Massaroli et al., 2020a)
Graph Neural ODEs (Poli…",,,0,not_related
"g (Greydanus et al., 2019; Toth et al., 2019; Massaroli et al., 2020b; Lutter et al., 2019; Cranmer et al., 2020; Massaroli et al., 2020a; Li et al., 2020), including yet–to–be–published combinations, can effortlessly be obtained by ad hoc primitives in combination with the rich PyTorch (Paszke et al.",,,0,not_related
"Energy models There exists a whole line of work of physics–inspired Neural ODE variants such as Hamiltonian Neural Networks (Greydanus et al., 2019), Lagrangian Neural Networks (Lutter et al.",,,0,not_related
"Some examples include a complete cookbook for neural ordinary differential equation variants, and tutorials for Hamiltonian Neural Networks (Greydanus et al., 2019), FFJORD (Grathwohl et al.",,,0,not_related
"Energy models There exists a whole line of work of physics–inspired Neural ODE variants such as Hamiltonian Neural Networks (Greydanus et al., 2019), Lagrangian Neural Networks (Lutter et al., 2019; Cranmer et al., 2020) or general energy–based models (Massaroli et al., 2020a).",,,0,not_related
"Some examples include a complete cookbook for neural ordinary differential equation variants, and tutorials for Hamiltonian Neural Networks (Greydanus et al., 2019), FFJORD (Grathwohl et al., 2018), Neural Graph Ordinary Differential Equations (Poli et al., 2019) and more.",,,0,not_related
", 2020b) Hamiltonian (Greydanus et al., 2019) Lagrangian (Lutter et al.",,,0,not_related
A different way of formulating rigid body dynamics has been investigated in [11] using energy conservation laws.,,,0,not_related
"In this paper, we will investigate Hamiltonian neural networks (HNN) [6], [22], in which the unknown Hamiltonian function H instead of the total vector field f is parameterized.",,,1,related
"This statement was discussed in [22], while IMDE reveal this problem theoretically.",,,0,not_related
"DO IDEAS HAVE SHAPE? 33 learn the laws of physics, and instead of crafting the Hamiltonian by hand, [39] proposes parameterizing it with a neural network and then learning it directly from data.",,,0,not_related
[39] draws inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner: the purpose is to,,,0,not_related
"Due to their popularity, there is a rich body of work that aims to explain “why” shortcut connections between layers enables the training of very deep NNs [73, 4, 29, 48, 51, 44].",,,0,not_related
"They enable the training of very deep neural networks (NNs), and they have emerged as the state-of-the-art architecture for a large number of tasks in computer vision, natural language processing, and related areas.",,,0,not_related
"Others have rediscovered integration schemes such as the implicit trapezoidal rule, which coincides with the scaled Cayley transform in the context of recurrent neural networks (RNNs) [33].",,,0,not_related
"Of course, specific architectures (i.e., specific designs of NNs for a given problem) often correspond to a family of graphs with some hyperparameters that generate a particular computational graph.",,,0,not_related
"• Is there a more appropriate interpretation for this class of NNs that stems from dynamical systems and numerical
integration theory?",,,0,not_related
Before we develop the foundations for continuous-in-depth NNs (in Sec.,,,1,related
"Importantly, this approach
enables us to decouple the computational graph from the model parameters, a property which does not hold for traditional discrete NNs but which is crucial for being a numerical integrator in a meaningful sense.",,,0,not_related
"These tools are also useful for analyzing ML and optimization algorithms [75, 70, 58, 59], as well as for improving our understanding of NNs [35, 55, 46, 45, 7, 50, 68].",,,0,not_related
"Along these lines, several physics-based models [66, 53, 63, 54, 17, 2] and continuous analogues to NNs [23, 14, 3] have recently been proposed.",,,0,not_related
"The idea of Neural ODEs is to use NNs to parameterize a differential equation that governs the hidden states x(t) ∈ Rd with respect to time t,
ẋ = F (x(t), t; θ̂) = σ(Ŵ . . . σ(Ŵx+ Ât+ b̂) · · ·+ b̂), (4)
where F : Rd → Rd denotes the network that is parameterized by the learnable parameters θ̂ = {Â, Ŵ , b̂, . . . }, and where ẋ = dx/dt is the time derivative, and σ is any nonlinearity.",,,0,not_related
ResNets and other traditional discrete NNs can be described by a discrete computational graph that represents the stages of computation and data flow during a forward evaluation.,,,0,not_related
"Continuous-in-depth NNs are independent of any particular choice of discrete computational graph or parameter representation, yielding the property of manifestation invariance.",,,0,not_related
"Other theoretical research fronts include interfacing Neural ODEs with normalizing flows [22, 79, 40, 56], graph NNs [61, 67], stochastic differential equations [52, 37, 24, 47] and RNNs [10, 65, 27, 18, 49].",,,0,not_related
"The interest of the machine learning community in physical phenomena has substantially grown for the last few years (Shi et al., 2015; Long et al., 2018; Greydanus et al., 2019).",,,0,not_related
"Greydanus et al. (2019), Chen et al. (2020) and Toth et al. (2020) introduce non-regression losses by taking advantage of Hamiltonian mechanics (Hamilton, 1835), while Tompson et al. (2017) and Raissi et al. (2020) combine physically inspired constraints and structural priors for fluid dynamic…",,,0,not_related
"Neural ODE and its applications [6, 5, 15, 42, 32, 7, 40], alias ODE networks (ODENs), tackle these issues by learning the governing equations, rather than the state transitions directly.",,,0,not_related
"Furthermore, they can lean the underlying law of conservation of energy automatically, because they fully exploit the nature of the Hamiltonian mechanics [15].",,,0,not_related
"Recent works [15, 42, 32, 7, 40] apply the Hamiltonian mechanics to ODE networks, and succeed in enforcing the energy conservation as well as the accurate time evolution of classical conservative systems.",,,0,not_related
"Moreover, some of them use special ODE functions such as Hamilton’s equations to incorporate physical properties to neural network structurally [15, 42, 32, 7, 40].",,,0,not_related
", only for suitable explicitly conservative systems [15].",,,0,not_related
"However, these Hamiltonian ODE networks have inherent limitations that they cannot be applied to non-conservative systems, since the Hamiltonian structures require to strictly conserve the total energy [15].",,,0,not_related
", evolutionary algorithms [35, 28], sparse optimizations [33, 4], Gaussian process regressions [38, 8], and neural networks [18, 1, 15, 42, 32].",,,0,not_related
"They are shown to be able to represent the vast majority of dynamical systems with higher precision over vanilla recurrent neural networks and their variants [6, 5], but are still unable to learn underlying physics such as the law of conservation [15].",,,0,not_related
"Consequently, they often overfit to short-term training trajectories and fail to predict the long-term behaviors of complex dynamical systems [15, 42].",,,0,not_related
"It is also not straightforward to predict the continuous-time dynamics, because neural network models typically assume the discrete time-step between states [15].",,,0,not_related
", 2014), offering new system–theoretic perspectives on neural network architecture design (Greydanus et al., 2019; Bai et al., 2019; Poli et al., 2019; Cranmer et al., 2020) and generative modeling (Grathwohl et al.",,,0,not_related
"…research in continuous deep learning (Zhang et al., 2014), offering new system–theoretic perspectives on neural network architecture design (Greydanus et al., 2019; Bai et al., 2019; Poli et al., 2019; Cranmer et al., 2020) and generative modeling (Grathwohl et al., 2018; Yang et al.,…",,,0,not_related
"Among those, interpretation of DNNs as discrete-time nonlinear dynamical systems, by viewing each layer as a distinct time step, has received tremendous focus as it enables rich analysis ranging from numerical equations [2], mean-field theory [3], to physics [4, 5, 6].",,,0,not_related
[7] S.,,,0,not_related
By physics-inspired neural networks [4] authors generally mean either incorporating domain knowledge in the traditional NN or providing additional loss functions to ensure physically consistent predictions [5–7].,,,0,not_related
"Prior work on automatically learning symmetries is more sparse, and includes works that focus on Gaussian processes [49] and symmetries of physical systems [20, 8].",,,0,not_related
"Some methods bypass this problem by learning energetic invariants of the system [16, 17, 18] or exploiting the symplectic structure of the problem [19, 20], reporting promising and interpretable results for Hamiltonian dynamics.",,,0,not_related
"Hamiltonian Neural Network (HNN) [3] provides a single experiment with image observations, which requires a modification in the model.",,,0,not_related
All of these works (except one particular experiment in HNN [3]) require direct observation of low dimensional position and velocity data.,,,0,not_related
Baselines We set up two baseline models: HGN [6] and PixelHNN [3].,,,1,related
We implemented HGN based on the architecture described in the paper and used the official code for PixelHNN.,,,1,related
"The control input to the simulator is u(q, q̇) = β(q) + v(q̇) which is designed as in Section 2.2 with the learned potential energy, input matrix, coordinates encoded from the output images, and q?.
Baselines We set up two baseline models: HGN [6] and PixelHNN [3].",,,1,related
"Moreover, both HNN and HGN focus on prediction and have no design of control.",,,0,not_related
"In the original implementation of PixelHNN [3], the angle of the pendulum is constrained to be from −π/6 to π/6, where a linear approximation of the nonlinear dynamics is learned, which makes the learned coordinates easy to interpret.",,,0,not_related
PixelHNN does not use an integrator and requires a special term in the loss function.,,,0,not_related
"Hamiltonian Neural Networks [3] learn Hamiltonian dynamics from position, velocity and acceleration data.",,,0,not_related
"PixelHNN does not account for the rotational nature of coordinate q, so the reconstruction images around the unstable equilibrium point are blurry and the learned coordinates are not easy to interpret (see Supplementary Material).",,,0,not_related
"Recently, an increasing number of works [1, 2, 3, 4, 5] have incorporated Lagrangian/Hamiltonian dynamics into learning dynamical systems from coordinate data, to improve prediction and generalization.",,,0,not_related
"Other studies have investigated the similarities of dynamical systems and deep learning methods [65] and employed conservation laws to learn systems described by Hamiltonian mechanics [18, 12].",,,0,not_related
"In this context, deep learning methods are receiving strongly growing attention [40, 4, 18] and show promise to account for those components of the solutions that are difficult to resolve or are not well captured by our physical models.",,,0,not_related
"Recent articles have tried to address these limitations [26] by constructing physics-aware networks that learn conservation laws, but their extension to complicated systems with incomplete observations remains unclear.",,,0,not_related
"…there is a considerable amount of literature where authors have endowed neural networks with classical Hamiltonian mechanics (Toth et al. 2019; Greydanus, Dzamba, and Yosinski 2019; Tong et al. 2020; Iten et al. 2020; Bondesan and Lamacraft 2019; Zhong, Dey, and Chakraborty 2019; Chmiela et…",,,0,not_related
"In particular, the amount of literature where authors have endowed neural networks with classical Hamiltonian mechanics has increased considerably [7, 8, 10, 30, 31, 32, 33].",,,0,not_related
"In the case of natural sciences, applying ML to physics is not new, several works have been reported (Toth et al. 2019; Greydanus, Dzamba, and Yosinski 2019; Cranmer et al. 2020; Tong et al. 2020) where different authors have combined",,,0,not_related
"In this respect, several works have been reported [7, 8, 9, 10] where different authors have used Hamilton’s equations of motion to generate trajectories that obey the energy conservation and the laws of classical physics.",,,0,not_related
"The link between dynamical systems and models for forecasting sequential data also provides the opportunity to incorporate physical knowledge into the learning process which improves the generalization performance, robustness, and ability to learn with limited data [36, 37, 38, 39, 40, 41, 42, 43].",,,0,not_related
"This model is a combination of a Hamiltonian Neural Network [45, 46] and GN.",,,0,not_related
"Another class of object-based models explicitly represents the Hamiltonian or Lagrangian of the physical system [7, 9, 11].",,,0,not_related
"Another class of object-based models explicitly represents the Hamiltonian or Lagrangian of the physical system (Chen et al., 2019; Cranmer et al., 2020; Greydanus et al., 2019).",,,0,not_related
"Some forward-prediction models may exhibit better generalizing properties by making additional assumptions: for example, conservation of energy or point mass objects [7, 9, 11].",,,0,not_related
"Learning Physically Meaningful Systems Another related thread of studies is to learn physical systems, such as Lagrangian (Lutter, Ritter, and Peters 2019; Cranmer et al. 2020) and Hamiltonian (Greydanus, Dzamba, and Yosinski 2019; Matsubara, Ishikawa, and Yaguchi 2020) mechanics using neural networks.",,,0,not_related
"…Physically Meaningful Systems Another related thread of studies is to learn physical systems, such as Lagrangian (Lutter, Ritter, and Peters 2019; Cranmer et al. 2020) and Hamiltonian (Greydanus, Dzamba, and Yosinski 2019; Matsubara, Ishikawa, and Yaguchi 2020) mechanics using neural networks.",,,0,not_related
"…in dynamics models includes the Gaussian process dynamics models (Wang, Fleet, and Hertzmann 2006) and models based on deep neural networks (e.g., Takeishi, Kawahara, and Yairi 2017; Lusch, Kutz, and Brunton 2018; Chen et al. 2018; Manek and Kolter 2019; Greydanus, Dzamba, and Yosinski 2019).",,,0,not_related
"Learning physically meaningful systems Another related thread of studies is to learn physical models, such as Lagrangian [25, 9] and Hamiltonian [14] neural networks.",,,0,not_related
"Extension to port-Hamiltonian systems (Zhong, Dey, and Chakraborty 2020) is also considered.",,,0,not_related
"[Lutter et al., 2019; Greydanus et al., 2019] show that Lagrangian/Hamiltonian mechanics can be imposed to learn the equations of motion of a mechanical system and [Seo and Liu, 2019] regularize a graph neural network with a specific physics equation.",,,0,not_related
"Although there have been some works [de Bezenac et al., 2018; Greydanus et al., 2019] improving data efficiency via explicitly incorporating PDEs as neural network layers when modeling spatiotemporal dynamics, it is hard to generalize for modeling different or unknown dynamics, which is ubiquitous in real-world scenario.",,,0,not_related
"Although there have been some works [de Bezenac et al., 2018; Greydanus et al., 2019] improving data efficiency via explicitly incorporating PDEs as neural network layers when modeling spatiotemporal dynamics, it is hard to generalize for modeling different or unknown dynamics, which is ubiquitous…",,,0,not_related
introduced Hamiltonian Neural Networks (HNNs) to endow machine learning models for dynamical systems with better physical inductive biases [10].,,,0,not_related
"There is a growing body of research in this domain, but much of it relies upon black-box machine learning techniques that solve the prediction problem while neglecting the governing equations problem [10, 5].",,,0,not_related
"To this end, there has been significant research interest in applying machine learning to physical dynamical systems [10, 5, 21, 22, 29, 3, 32, 4].",,,0,not_related
"For related work on learning Hamiltonian systems, see [3, 16].",,,0,not_related
", also tried to combine an autoencoder with an Hamiltonian Neural Networks (HNN) to model the dynamics of pixel observations of a pendulum [10].",,,0,not_related
"Machine learning in fluid systems The rapid advent of machine learning techniques is opening up new possibilities to solve the physical system’s identification problems by statistically exploring the underlying structure of a variety of physical systems, encompassing applications in quantum physics [35], thermodynamics [14], material science [36], rigid body control [9], Lagrangian systems [8], and Hamiltonian systems [10, 19, 37].",,,0,not_related
The development of automatic differentiation packages for machine learning has opened up a new approach to stability analysis and estimation of functions of interest in control theory such as Hamiltonian [38] or Lyapunov function [39].,,,0,not_related
"It is interesting to note the similarity between this computational graph and the one of the residual network structure He et al. (2016). Note that number of repeated blocks in the computational graph (thus, the depth of the architecture) corresponds to the length of the training sequence.",,,0,not_related
"In these cases, a physics-based neural network may be used to learn the system’s Hamiltonian or Lagrangian function, instead of the individual components its ODE representation as independent terms (Greydanus et al., 2019; Lutter et al., 2019).",,,0,not_related
"In addition to the works discussed above, a research area in the unsupervised learning literature of particular interest is that of learning physically plausible representations (from video) by enforcing temporal evolution according to explicit or implicit physical dynamics [4, 19, 25, 45].",,,0,not_related
", see [6], [31], [32], [33], [34], and [35].",,,0,not_related
", the sum of potential and kinematic energies of a pendulum [6]).",,,0,not_related
"Two recent works aim to uncover physical laws from data in a general manner (Iten et al., 2020; Greydanus et al., 2019).",,,0,not_related
"g an ODE solver into the network structure. Chen et al. (2018) propose a general method to parameterize the derivative of the hidden state and then apply an arbitrary ODE solver. Gupta et al. (2019), Greydanus et al. (2019) and Zhong et al. (2020) take such a perspective for mechanical systems and model the derivative of the desired state. An earlier example for this idea, applied in another domain, given by Al Seyab an",,,0,not_related
"Gupta et al. (2019), Greydanus et al. (2019) and Zhong et al. (2020) take such a perspective for mechanical systems and model the derivative of the desired state.",,,0,not_related
"One recent development is physicsinformed neural networks (Raissi et al., 2019; Lutter et al., 2019; Greydanus et al., 2019; Zhong et al., 2020; Gupta et al., 2019; Rackauckas et al., 2020), which use mechanistic equations to endow neural networks with better prior.",,,0,not_related
"nt subsystems or multi-delity modeling (Fernandez-Godino et al., 2016; von Stosch et al., 2014). One recent development is physicsinformed neural networks (Raissi et al., 2019; Lutter et al., 2019; Greydanus et al., 2019; Zhong et al., 2020; Gupta et al., 2019; Rackauckas et al., 2020), which use mechanistic equations to endow neural networks with better prior. We follow this line and propose physics-informed neural ",,,0,not_related
"ilar motivation to ours and also achieves improved data and computational eciency in diverse examples. Two recent works aim to uncover physical laws from data in a general manner (Iten et al., 2020; Greydanus et al., 2019). More specic for mechanical systems, physics-informed neural networks were demonstrated with simulated time series data for the forward model (1) of a pendulum, double pendulum, and a cart pole syst",,,0,not_related
"The alternative tabula rasa approach assumes no physics whatsoever and attempts to learn physical object properties [60], object positions [61, 62], object relations [63] and time evolution [64–66] by learning a lowdimensional representation or latent space which is unfortunately too complex or inscrutable to allow discovery of exact equations of motion.",,,0,not_related
"[66] Samuel Greydanus, Misko Dzamba, and Jason Yosinski.",,,0,not_related
", embedding the notion of an incompressible fluid [29, 48], the Galilean invariance [28], a quasistatic physics simulation [13], and the invariant quantities in Lagrangian systems [7] and Hamiltonian systems [21, 14, 24, 51, 8, 49].",,,0,not_related
"In particular, we demonstrate that the training period of our model can be around 6000 times shorter than its predicting period (other methods have the training period 1–25 times shorter than the predicting period [5, 14, 24]), and the number of training samples is around 5 times smaller (meaning we use 5 times fewer time-sequences as in the training process) than that used by other methods.",,,0,not_related
"ODE-net [5] and HNN [14], have to rely on intermediate data in their training data to train the model.",,,0,not_related
"first tried to enforce conservative features of the Hamiltonian system by reformulating the loss function using Hamilton’s equations, known as Hamiltonian neural networks (HNNs) [14].",,,0,not_related
"Numerous approaches exist to model time dynamics but the most straightforward methods involve learning functions which map inputs directly to their time derivatives [6] or to the next state [1, 3].",,,0,not_related
Another key attribute of the Hamiltonian is that the vector field S is a symplectic gradient meaningH remains constant as long as state vectors are integrated along S [6].,,,0,not_related
"Extensive research has shown that enriching models with well-chosen inductive biases such as Hamiltonian Neural Networks (HNNs) [6], ordinary differential equations (ODEs) [4, 5] and more recently, graphs [1, 14, 13] can significantly improve learning.",,,0,not_related
"Recently, [6] demonstrated that dynamic predictions through time can be improved using Hamiltonian Neural Networks (HNNs) which endow models with a Hamiltonian constraint.",,,0,not_related
"Inspired by this work, [6] and [15] show that one can take a physical system (position and momentum values, or e.",,,0,not_related
"To build the noisy data, we follow the approach of [6] and [11] by sampling from a Gaussian N (0, 0.",,,1,related
"dq dt = ∂H ∂p , dp dt = −∂H ∂q (1) As a consequence, it is noted in [6] that by accurately learning a Hamiltonian, the system’s dynamics can be naturally extracted through backpropagation.",,,1,related
"However, [6] show that using this simple approach is not enough to accurately predict energy conserving phase space trajectories.",,,0,not_related
"(1997); Hansen and Ostermeier (2001) for system identification is to collect ground truth data and subsequently optimize the dynamics model over certain parameters of a physics simulation engine, so as to minimize the difference between the predicted trajectory and the ground truth.",,,0,not_related
"Recently, Li et al. (2019b) proposed the multi-step propagation network and Greydanus et al. (2019) applied a Hamiltonian network to conserve an
1. https://rutgers.box.com/shared/static/i9vvxpc8152i5e0zg897fj47sanen7nj.mp4
energy-like quantity without damping.",,,0,not_related
"Further, (Greydanus et al., 2019) combined Hamiltonian mechanics with neural networks to predict the forward dynamics of conservative mechanical systems.",,,0,not_related
"Multistep neural network (MNN) [33], and Hamiltonian neural network (HNet) [17], are two examples of non-zero target errors.",,,0,not_related
"In order to learn Hamiltonian systems, [17] proposes the HNet to learn a parametric function for H(y).",,,0,not_related
This often causes them to drift away from the true dynamics of the system as errors accumulate [17].,,,0,not_related
"osed method are inspired by interpretations of deep neural networks through the optics of differential equations [33, 17, 49, 30], and neural architectures based on physics priors such as Hamiltonian [27] and Lagrangian [50] networks. System identiﬁcation with neural networks: The natural idea of using RNN-based state-space models is motivated by previous works [45, 44, 57, 31]. A related idea of usin",,,0,not_related
"Hamiltonian neural networks [18], Lagrangian neural networks [19], and symplectic RNNs [15] are examples of these DL approaches that have been shown to learn the physics of",,,0,not_related
"[18] Sam Greydanus, Misko Dzamba, and Jason Yosinski, “Hamiltonian neural networks,” arXiv preprint arXiv:1906.",,,0,not_related
"put, in some cases leveraging the information that the partial derivatives of the output with respect to inputs are the time derivatives of the inputs [18].",,,0,not_related
"Also, we plan to explore the “recurrent” versions of physics-informed neural network architectures [33, 34] where in addition to the use of sequence-to-sequence mapping, the infusion of conservation laws in the design of loss functions of",,,0,not_related
"[33] Sam Greydanus, Misko Dzamba, and Jason Yosinski, “Hamiltonian neural networks,” arXiv preprint arXiv:1906.",,,0,not_related
"Here, instead of performing explicit time evolution, neural networks observe the positions and momenta and produce the Hamiltonian as output [33].",,,0,not_related
"3 Hamiltonian neural networks We present here results on the pendulum problem also considered in [22, 24].",,,0,not_related
"Furthermore, building on the recently introduced Hamiltonian neural networks [22, 23], Matsubara et al.",,,0,not_related
"Neural ODEs have also been shown to improve upon previous learning methods in the approximation of physical systems [10], [11].",,,0,not_related
"The Hamiltonian operator in physics is the primary tool for modeling the time evolution of systems with conserved quantities, but until recently the formalism had not been integrated with NNs. Greydanus et al. [112] design a NN architecture that naturally learns and respects energy conservation and other invariance laws in simple mass-spring or pendulum systems.",,,0,not_related
"For example, in NNs, weights are often initialized according to a random distribution prior to training.",,,0,not_related
"For example, RNNs encode temporal invariance and CNNs can implicitly encode spatial translation, rotation, and scale invariance.",,,0,not_related
"This is taken a step further in Toth et al. [268], where they show that not only can NNs learn the Hamiltonian, but also the abstract phase space (assumed to be known in Greydanus et al. [112]) to more effectively model expressive densities in similar physical systems and also extend more generally to other problems in physics.",,,0,not_related
"There is a vast amount of other work using physics-guided architecture towards solving PDEs and other PDE-related applications as well which are not included in this survey (e.g. see ICLR workshop on deep learning for differential equations ([5]))
A recent direction also relating to conserved or invariant quantities is the incorporation of the Hamiltonian operator into NNs [64, 112, 268, 317].",,,0,not_related
"A similar transfer and adapt approach is seen in Lu et al. [180], but for an ensemble of NNs transferred from related tasks.",,,0,not_related
"More specifically, they demonstrated the encoding of translational symmetries, rotational symmetries, scale invariances, and uniform motion into NNs using customized convolutional layers in CNNs that enforce desired invariance properties.",,,0,not_related
A recent approach is seen in geophysics where researchers use NNs for the waveform inversion modeling to find subsurface parameters from seismic wave data.,,,0,not_related
"Another common technique in inverse modeling of images (e.g. medical imaging, particle physics imaging), is the use of CNNs as deep image priors [271].",,,0,not_related
"Recently, the Hamiltonian-parameterized NNs above have also been expanded into NN architectures that perform additional differential equation-based integration steps based on the derivatives approximated by the Hamiltonian network [61].",,,0,not_related
"This can take place in many ways, including using domain-informed convolutions for CNNs, additional domain-informed discriminators in GANs, or structures informed by the physical characteristics of the problem.",,,0,not_related
"In a general setting, Wang et al. [281] show how spatiotemporal models can be made more generalizable by incorporating symmetries into deep NNs.",,,0,not_related
"They define a parabolic CNN inspired by anisotropic filtering, a hyperbolic CNN based on Hamiltonian systems, and a second order hyperbolic CNN. Hyperbolic CNNs were found to preserve the energy in the system as intended, which set them apart from parabolic CNNs that
, Vol. 1, No. 1, Article .",,,0,not_related
", see ICLR workshop on deep learning for differential equations ([5])) A recent direction also relating to conserved or invariant quantities is the incorporation of the Hamiltonian operator into NNs [64, 112, 268, 317].",,,0,not_related
"Their idea is to use NNs to discover hidden signs of ""simplicity"", such as symmetry or separability in the training data, which enables breaking the massive search space into smaller ones with fewer variables to be determined.",,,0,not_related
"In a pioneering work by Ruthotto et al [236], three variations of CNNs are proposed to improve classifiers for images.",,,0,not_related
[112]) to more effectively model expressive densities in similar physical systems and also extend more generally to other problems in physics.,,,0,not_related
"To do this, they substitute NN layers into an unrolled version of an existing solution framework which drastically reduced the overall computational cost due to the fast forward evaluation property of NNs, but kept information of the underlying physical models of power grids and of physical constraints.",,,0,not_related
"Schutt et al. [245] proposes continuous-filter convolutional (cfconv) layers for CNNs to allow for modeling objects with arbitrary positions such as atoms in molecules, in contrast to objects described by Cartesian-gridded data such as images.",,,0,not_related
The modular and flexible nature of NNs in particular makes them prime candidates for architecture modification.,,,0,not_related
"In particular, NN solvers can reduce the high computational demands of traditional numerical methods into a single forward-pass of a NN. Notably, solutions obtained via NNs are also naturally differentiable and have a closed analytic form that can be transferred to any subsequent calculations, a feature not found in more traditional solving methods [159].",,,0,not_related
"This is similar to the common application of pre-training in computer vision, where CNNs are often pre-trained with very large image datasets before being fine-tuned on images from the task at hand [259].",,,0,not_related
"Though these loss functions are mostly seen in common variants of NNs, they are also be seen in architectures such as echo state networks.",,,0,not_related
[112] designed a NN architecture that naturally learns and respects energy conservation and other invariance laws in simple mass-spring or pendulum systems.,,,0,not_related
Lagergren et al. [160] expand on this by using ANNs to construct the dictionary of functions.,,,0,not_related
"In this paper, we follow the same approach as Greydanus et al. (2019), but with the objective of learning a Lagrangian rather than a Hamiltonian so not to restrict the learned kinetic energy.",,,1,related
"In the same manner as Greydanus et al. (2019), we can also write a loss function in terms of the discrepancy between ẍLt and ẍtruet .",,,1,related
"Recent work by Greydanus et al. (2019), Toth et al. (2019), and Chen et al. (2019) built on previous approaches of endowing neural networks with physical priors by demonstrating how to learn invariant quantities by approximating a Hamiltonian with a neural network.",,,0,not_related
"In the same manner as Greydanus et al. (2019), we can also write a loss function in terms of the discrepancy between ẍt and ẍtrue t .",,,1,related
This was the core motivation behind Hamiltonian Neural Networks by Greydanus et al. (2019) and Hamiltonian Generative Networks by Toth et al. (2019).,,,0,not_related
". Integrating Physics-Based Modeling With Machine Learning: A Survey 15 A recent direction also relating to conserved or invariant quantities is the incorporation of the Hamiltonian operator into NNs [54, 98, 249, 299]. The Hamiltonian operator in physics is the primarytoolformodelingthetimeevolutionofsystemswithconservedquantities,butuntilrecently the formalism had not been integrated with NNs. Greydanus et al. [9",,,0,not_related
" themselves. This is taken a step further in Toth et al. [249], where they show that not only can NNs learn the Hamiltonian, but also the abstract phase space (assumed to be known in Greydanus et al. [98]) to more effectively model expressive densities in similar physical systems and also extend more generally to other problems in physics. Recently, the Hamiltonian-parameterized NNs above have also be",,,0,not_related
"This point of view allows one to construct models which enjoy highlevel properties such as time invertibility via Hamiltonian (Greydanus et al., 2019) or Symplectic (Chen et al.",,,0,not_related
"The nonlinear (undamped) pendulum (Hirsch et al., 1974) is a classic textbook example for dynamical systems, which is also used for benchmarking deep models (e.g., Greydanus et al., 2019; Bertalan et al., 2019; Chen et al., 2020).",,,0,not_related
"To this end, a few approaches were recently proposed (Greydanus et al., 2019; Chen et al., 2020) where the obtained dynamics are reversible by construction due to the leapfrog integration.",,,0,not_related
"Other methods attempt to learn conservation laws from data and their associated Hamiltonian representation, leading to exact preservation of energy (Greydanus et al., 2019) and better handling of stiff problems (Chen et al.",,,0,not_related
"This point of view allows one to construct models which enjoy highlevel properties such as time invertibility via Hamiltonian (Greydanus et al., 2019) or Symplectic (Chen et al., 2020; Zhong et al., 2020) networks.",,,0,not_related
"We also outperform the Hamiltonian NN (Greydanus et al., 2019) in all settings.",,,1,related
"Other methods attempt to learn conservation laws from data and their associated Hamiltonian representation, leading to exact preservation of energy (Greydanus et al., 2019) and better handling of stiff problems (Chen et al., 2020).",,,0,not_related
", 2018] as well as graph neural networks coupled with differentiable ODE solvers which have been used to learn the Hamiltonian dynamics of physical systems given their interactions modelled as a dynamic graph [Greydanus et al., 2019].",,,0,not_related
"GNNs coupled with differentiable ODE solvers have been used to learn the Hamiltonian dynamics of physical systems given their interactions modelled as a dynamic graph [Greydanus et al., 2019].",,,0,not_related
"In this section we analyze how GOKU-net can be used for observed signal extrapolation and ODE parameter identification in three domains: the classic Lotka-Volterra system (Lotka, 1910) with added non-linear emission function; an OpenAI Gym video simulator of a pendulum (Brockman et al., 2016; Greydanus et al., 2019); and a model of the cardiovascular system based on Zenker et al.",,,1,related
"We generated the data as in Greydanus et al. (2019), with one important change: the ODE parameter l was uniformly sampled, l ∼ U [1, 2] instead of being constant, making the task much harder.",,,1,related
Machine learning with mechanistic components Closer in spirit to our work is the work by Greydanus et al. (2019) on Hamiltonian neural networks.,,,0,not_related
"As in Greydanus et al. (2019), we pre-processed the observed data such that each frame is of size 28 × 28.",,,1,related
"…parameter identification in three domains: the classic Lotka-Volterra system (Lotka, 1910) with added non-linear emission function; an OpenAI Gym video simulator of a pendulum (Brockman et al., 2016; Greydanus et al., 2019); and a model of the cardiovascular system based on Zenker et al. (2007).",,,0,not_related
", 2018) not required learned 7 3 HNN (Greydanus et al., 2019) can be used learned 7 3 DSSM (Miladinović et al.",,,1,related
"We followed Greydanus et al. (2019) and used the Pendulum-v0 environment from OpenAI Gym (Brockman et al., 2016).",,,1,related
"The function fθf is an ODE model for these physiological variables, such as those presented by Guyton et al. (1972); Smith et al.",,,0,not_related
"HNN by (Greydanus et al., 2019).",,,0,not_related
We followed Greydanus et al. (2019) and used the Pendulum-v0 environment from OpenAI Gym (Brockman et al.,,,1,related
"This includes HNN (Greydanus et al., 2019) which has difficulty with the fact that the ODE parameter is not constant.",,,0,not_related
", 2016; Greydanus et al., 2019); and a model of the cardiovascular system based on Zenker et al. (2007). In each case we train the model on a set of sequences with varying ODE parameters (θf ) and initial conditions (z0), and test on unseen sequences with parameters and initial conditions sampled from the same distribution as the train.",,,1,related
"As shown in Greydanus et al. (2019), a neural network parametrizing Ĥθ(z, t) can be learned directly from trajectory data, providing substantial benefits in generalization over directly modeling F (z, t).",,,0,not_related
", 2017) and discrete mechanism (Greydanus et al., 2019; Zhong et al., 2019) when learning problems inherit ar X iv :2 00 2.",,,0,not_related
"It also enables principled architecture design by bringing rich analysis from numerical differential equations (Lu et al., 2017; Long et al., 2017) and discrete mechanism (Greydanus et al., 2019; Zhong et al., 2019) when learning problems inherit
ar X
iv :2
00 2.",,,0,not_related
"On the other hand, [16] and [17] have utilized Hamiltonian mechanics for learning dynamics from data.",,,0,not_related
"We show how this can be achieved without introducing additional inductive biases such as (Greydanus et al., 2019) through a synergistic combination of a two–layer Galërkin Neural ODEs and the generalized adjoint with integral loss l(s) := ‖β(s)− z(s)‖2.",,,1,related
We consider a simple environment similar to that studied in Higgins et al. (2018) and Caselles-Dupré et al.,,,1,related
"Another requirement we wish to make concerning the representation to be learned is that it is disentangled in the sense of Higgins et al. (2018). Formally, if there exists a subgroup decomposition of G such that G = G1 × G2.",,,1,related
"…parallel, physics and machine learning have been forging strong ties mostly based upon the Hamiltonian theory and the integration of ordinary differential equations in the latent space to describe the evolution of dynamical systems (Chen et al. (2018), Toth et al. (2019), Greydanus et al. (2019)).",,,0,not_related
"Typical ones include Hamiltonian neural networks (Greydanus et al., 2019), Hidden Markov Model (HMM) (Alshamaa et al., 2019; Eddy, 1996), Kalman Filter (KF) (Farahi & Yazdi, 2020; Harvey, 1990; Kalman, 1960) and Particle Filter (PF) (Santos et al., 2019; Djuric et al., 2003), as well as the models…",,,0,not_related
"There are a number of examples of using neural networks to learn the Hamiltonian or Lagrangian of a dynamic system and training this model using the derivatives of the neural network (Greydanus et al., 2019; Lutter et al., 2019; Zhong et al., 2019; Gupta et al., 2019).",,,0,not_related
"Similar to DeLaN, Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019) predict the Hamiltonian (instead of Lagrangian) of a dynamical system.",,,0,not_related
Deep Lagrangian Networks [31] and Hamiltonian Networks [32] represent functions in the respective classical mechanics frameworks using deep neural networks.,,,0,not_related
"[14] Sam Greydanus, Misko Dzamba, and Jason Yosinski.",,,0,not_related
"The data efficiency of PINNs has been shown to further improve when encoding structural assumptions like energy conservation by directly modeling the Hamiltonian [14, 15].",,,0,not_related
"Learning physically relevant concepts of the two body problem was recently solved both by Hamiltonian Neural Networks [12, 13] and by very problem specific VAE architectures [14].",,,0,not_related
"We replicate the setup from Greydanus et al. (2019) with one key difference: we introduce noise in all observations, rather than only introducing it in (qt,pt) and observing (q̇t, ṗt) noise free.",,,1,related
"For predictions with the baseline NN and HNN, we use the procedure of Greydanus et al. (2019), which uses fourth order Runga-Kutta with an error tolerance of 10−9, implemented in scipy.integrate.solve ivp.",,,1,related
"We compare our proposed VINs with Hamiltonian neural networks (HNNs) (Greydanus et al., 2019) and standard feed-forward neural networks (NNs) without additional structure that would explicitly incorporate physical or mechanical constraints.",,,1,related
"For the mass-spring system, we set the spring constant and mass to k = m = 1, as was done by Greydanus et al. (2019).",,,1,related
"As a workaround, Greydanus et al. (2019) proposed to parameterize the dynamical system’s Hamiltonian using a neural network, and to learn it directly from data.",,,0,not_related
"Owing to the precise form of their equations, such networks generally do not conserve these quantities exactly (Greydanus et al., 2019).",,,0,not_related
Greydanus et al. (2019) demonstrated that this flaw harms the networks’ capacity for accurate long-term prediction.,,,0,not_related
"C.1 Noisy System Observations
The setup resembles the one of Greydanus et al. (2019) closely.",,,0,not_related
"We replicate the setup from Greydanus et al. (2019) with one key difference: we introduce noise in all observations, rather than only introducing it in (qt,pt) and observing (q̇t, ṗt) noise free.",,,1,related
"Following Greydanus et al. (2019), we examine two scenarios: (a) a moderate-data regime, where models are trained using 25 training trajectories with a total of 750 data points, (b) a low-data regime using 5 training trajectories with a total of 150 data points.",,,1,related
"We represent an embedding in the network by qt = qθ(q1, q2, h, t), (10) which denotes layer t in the VIN, see Figure 1 for an illustration of a VIN.",,,1,related
"For predictions with the baseline NN and HNN, we use the procedure of Greydanus et al. (2019), which uses fourth order Runga-Kutta with an error tolerance of 10−9, implemented in scipy.integrate.solve ivp.",,,1,related
"We
compare our proposed VINs with Hamiltonian neural networks (HNNs) (Greydanus et al. 2019) and standard feed-forward neural networks (NNs) without additional structure that would explicitly incorporate physical or mechanical constraints.",,,1,related
"For the mass-spring system, we set the spring constant and mass to k = m = 1, as was done by Greydanus et al. (2019).",,,1,related
"As a workaround, Greydanus et al. (2019) proposed to parameterize the dynamical system’s Hamiltonian using a neural network, and to learn it directly from data.",,,0,not_related
"D.1 Noisy system observations
The setup resembles the one of Greydanus et al. (2019) closely.",,,0,not_related
"Owing to the precise form of their equations, such networks generally cannot learn to conserve these quantities exactly (Greydanus et al. 2019).",,,0,not_related
It has been recently demonstrated by Greydanus et al. (2019) that this flaw harms the networks’ capacity for accurate long-term prediction.,,,0,not_related
" 3:55 9:8 3:72 Table1: AveragepixelMSEovera30stepunrollonthetrainandtestdataonfourphysicalsystems. All values are multiplied by 1e+4. We evaluate two versions of the Hamiltonian Neural Network (HNN) (Greydanus et al., 2019): the original architecture and a convolutional version closely matched to the architecture of HGN. We also compare four versions of our proposed Hamiltonian Generative Network (HGN): the full version",,,1,related
"In the experiments presented here, we reimplemented the PixelHNN architecture as described in Greydanus et al. (2019) and trained it using the full loss (15).",,,1,related
"energy of the system denoted as a radius rin the phase space,beforesamplingtheinitialstate(q;p)uniformlyonthecircleofradiusr. Notethatourpendulum dataset is more challenging than the one described in Greydanus et al. (2019), where the pendulum had a ﬁxed radius and was initialized at a maximum angle of 30from the central axis. Mass-spring. The dynamics of a frictionless mass-spring system are modeled by the Hamiltonian",,,0,not_related
" in classical mechanics but which can still affect their behavior or function, like the colour or shape of an object. The ﬁrst question was recently addressed by the Hamiltonian Neural Network (HNN) (Greydanus et al., 2019) approach, which was able to learn the Hamiltonian of three simple physical systems from noisy phase space observations. However, to address the second question, HNN makes assumptions that restrict it",,,0,not_related
"One of the most comparable approaches to ours is the Hamiltonian Neural Network (HNN) (Greydanus et al., 2019).",,,0,not_related
"To ensure that our re-implementation of HNN was correct, we replicated all the results presented in the original paper (Greydanus et al., 2019) by verifying that it could learn the dynamics of the mass-spring, pendulum and two-body systems well from the ground truth state, and the dynamics of a…",,,1,related
"A.4 DATASETS
The datasets for the experiments described in 4 were generated in a similar manner to Greydanus et al. (2019) for comparative purposes.",,,0,not_related
"The first question was recently addressed by the Hamiltonian Neural Network (HNN) (Greydanus et al., 2019) approach, which was able to learn the Hamiltonian of three simple physical systems from noisy phase space observations.",,,0,not_related
"…phase space, before sampling the initial state (q,p) uniformly on the circle of radius r. Note that our pendulum dataset is more challenging than the one described in Greydanus et al. (2019), where the pendulum had a fixed radius and was initialized at a maximum angle of 30◦ from the central axis.",,,1,related
"A.3 HAMILTONIAN NEURAL NETWORK
The Hamiltonian Neural Network (HNN) (Greydanus et al., 2019) learns a differentiable function H(q,p) that maps a system’s state in phase space (its position q and momentum p) to a scalar quantity interpreted as the system’s Hamiltonian.",,,0,not_related
"In order to directly compare the performance of HGN to that of its closest baseline, HNN, we generated four datasets analogous to the data used in Greydanus et al. (2019).",,,1,related
"Most closely related to ours, Greydanus et al. (2019) use a neural network to learn the Hamiltonian of the dynamical system in such a way that its partial derivatives match the time derivatives of the position and momentum variables, which are both assumed to be observed.",,,0,not_related
"A recent paper (Greydanus et al., 2019), Hamiltonian Neural Networks (HNN), proposes to do so representing the Hamiltonian function H(q, p) as a multilayer neural network.",,,0,not_related
"For instance, with our terminology, the HNN model of Greydanus et al. (2019) is a “single-step E-E H-NET” with the additional subtlety that they supervise the training with actual derivatives instead of relying on finite differences between successive steps of the observed trajectories.",,,1,related
"1 Introduction Can machines learn physical laws from data? A recent paper (Greydanus et al., 2019), Hamiltonian Neural Networks (HNN), proposes to do so representing the Hamiltonian function H(q, p) as a multilayer neural network.",,,0,not_related
"Moreover, Greydanus et al. (2019) mentions that HNN does not outperform a baseline method using O-NET in learning the three-body system’s evolution.",,,0,not_related
"Here, we test our SRNN together with other baselines on the noiseless three-body system with the same configurations as Greydanus et al. (2019).",,,1,related
"For instance, Greydanus et al. (2019) trains H-NET in a fully supervised manner using the observed tuples (p, q, ṗ, q̇).",,,0,not_related
"A more recent (concurrent) work by Greydanus et al. (2019) uses Hamiltonian mechanics to learn the dynamics of autonomous, energy-conserved mechanical systems from time-series data of position, momentum, and their derivatives.",,,0,not_related
"In the HNN paper Greydanus et al. (2019), the initial conditions of the trajectories are generated randomly in an annulus, whereas in this paper, we generate the initial state conditions uniformly in a reasonable range in each state dimension.",,,1,related
"In Hamiltonian Neural Networks (HNN), Greydanus et al. (2019) incorporate the Hamiltonian structure into learning by minimizing the difference between the symplectic gradients and the true gradients.",,,0,not_related
"Previous approaches (Lutter et al., 2019; Greydanus et al., 2019) require data in the form of generalized coordinates and their derivatives up to the second order.",,,0,not_related
"…networks that can learn arbitrary conservation laws (hyperbolic conservation laws (Raissi, Perdikaris, and Karniadakis 2019), Hamiltonian dynamics (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019; Chen et al. 2019), Lagrangian dynamics (Cranmer et al. 2020)), or (2) designing a loss…",,,0,not_related
"Note added: Following submission of this work, the closely related preprint (Greydanus et al., 2019) appeared.",,,1,related
"Most previous studies focused on these two types of systems [8, 19, 38, 44, 45].",,,0,not_related
"The algorithm is applicable to any computational graph such as convolutional neural network [19] and graph neural network [12], and thereby one can handle extended tasks or further improve the modeling accuracy.",,,0,not_related
"Meanwhile, most previous studies aimed to model continuous-time differential equations and employed numerical integrators (typically, an explicit Runge–Kutta method) to integrate the neural network models for learning and computing the dynamics [7, 8, 19, 45].",,,0,not_related
"The HNN is a neural network where the output represents the system energy H , and its gradient with respect to the input state ~u is used for the time-derivative [19].",,,0,not_related
"Most studies have focused on one of the first two systems [19, 44, 45] under special conditions [8, 38, 41], or they are too general to model the conservation and dissipation laws [7, 34].",,,0,not_related
"Previous models interpolate the discrete-time data using numerical integrators for learning and computing [8, 19, 38, 41, 44, 45].",,,0,not_related
The Hamiltonian neural network (HNN) [19] implements the Hamiltonian structure on a neural network and thereby produces the energy conservation law in physics.,,,0,not_related
"The HNN approximates an energy function H from the data using a neural network, and thereby, builds a Hamiltonian system [19].",,,0,not_related
"Continuous Calculus Discrete Calculus approximation by numerical integrators [8,19,38,41,44,45] discrete approximation [5,6,15-18,24,32,33] physical phenomena discrete-time modeling",,,0,not_related
"HNN SymODEN Dissipative SRNN/VIN DGNet [19] [45] [44] [8, 38, 41] (this paper) Hamiltonian system yes yes yes yes yes Dissipative ODE yes yes Hamiltonian PDE yes Dissipative PDE yes Learning from finite difference approx.",,,1,related
"The use of ordinary differential equation (ODE) solvers within deep learning frameworks has allowed end-to-end training of Neural ODEs (Chen et al., 2018) in a variety of settings.",,,0,not_related
"We consider parametrizing event functions with neural networks in the context of solving ODEs, extending Neural ODEs to implicitly defined termination times.",,,1,related
"…2020), generative modeling (Grathwohl et al., 2018; Zhang et al., 2018; Chen & Duvenaud, 2019; Onken et al., 2020), time series modeling (Rubanova et al., 2019; De Brouwer et al., 2019; Jia & Benson, 2019; Kidger et al., 2020), and physics-based models (Zhong et al., 2019; Greydanus et al., 2019).",,,0,not_related
"Though there exist physics-motivated parameterizations of Neural ODEs (e.g. Zhong et al., 2019; Greydanus et al., 2019; Finzi et al., 2020), these cannot model collision effects.",,,0,not_related
", 2020), and physics-based models (Zhong et al., 2019; Greydanus et al., 2019).",,,0,not_related
"By introducing differentiable termination criteria in Neural ODEs, our approach allows the model to efficiently and automatically handle state discontinuities.",,,1,related
"To further expand the applications of Neural ODEs, we investigate the parameterization and learning of a termination criteria, such that the termination time is only implicitly defined and will depend on changes in the continuous-time state.",,,1,related
", 2019), Hamiltonian neural networks (HNN) (Sanchez-Gonzalez et al., 2019; Greydanus et al., 2019; Zhong et al., 2020; 2021), and neural ODE (NODE) (Chen et al.",,,0,not_related
"To this extent, three broad approaches have been proposed, namely, Lagrangian neural networks (LNN) (Cranmer et al., 2020a; Finzi et al., 2020; Lutter et al., 2019), Hamiltonian neural networks (HNN) (Sanchez-Gonzalez et al., 2019; Greydanus et al., 2019; Zhong et al., 2020; 2021), and neural ODE (NODE) (Chen et al., 2018; Gruver et al., 2021).",,,0,not_related
"Thus, a NODE with a second-order bias can give similar performances to
∗SB: School of Interdisciplinary Research, SR: Department of Computer Science, NMAK and RB: Department of Civil Engineering, J: Department of Electrical Engineering, SR and NMAK: Yardi School of Artificial Intelligence (joint appointment).
that of an HNN (Gruver et al., 2021).",,,0,not_related
"…proposed, namely, Lagrangian neural networks (LNN) (Cranmer et al., 2020a; Finzi et al., 2020; Lutter et al., 2019), Hamiltonian neural networks (HNN) (Sanchez-Gonzalez et al., 2019; Greydanus et al., 2019; Zhong et al., 2020; 2021), and neural ODE (NODE) (Chen et al., 2018; Gruver et al., 2021).",,,0,not_related
"The learning efficiency of LNNs and HNNs is shown to enhance significantly by employing explicit constraints (Finzi et al., 2020) and their inherent structure Zhong et al. (2019).",,,0,not_related
"Although the idea of graph-based modeling has been suggested for physical systems (Cranmer et al., 2020b; Greydanus et al., 2019), the inductive biases induced due to different graph structures and their consequences on the dynamics remain poorly explored.",,,0,not_related
"More specifically, an HNN with separable potential (V (q)) and kinetic (T (q, q̇)) energies is equivalent to a second order NODE of the form q̈ = F̂ (q, q̇).",,,0,not_related
We also show that the final model with the appropriate inductive biases significantly outperforms the simple version of GNODE with no additional inductive biases and even the graph versions of LNN and HNN.,,,1,related
"In addition, it has been shown that the superior performance of HNNs and LNNs is mainly due to their second-order bias, and not due to their symplectic or energy conserving bias (Gruver et al., 2021).",,,0,not_related
"In addition to physical inductive biases encodable via differentiable equations, there have also been recently developed methods to effectively impose energy conservation on learnt representations (Greydanus et al., 2019; Cranmer et al., 2020).",,,0,not_related
"Furthermore, we use hyperbolic tangent (tanh) and Rectified Linear Unit (ReLU) as activation function for the first and second hidden layer, respectively, while [16, 7, 24] use tanh for both.",,,1,related
", is a hybrid machine learning framework imposing hard constraints on a data-driven model [16].",,,0,not_related
"In contrast, works like [16, 7, 11] either assume derivatives are known or perform one or more integration steps at each training step.",,,0,not_related
"Following [16, 7, 24], we use fully connected neural networks with two hidden layers of 100 neurons each.",,,0,not_related
"[16], via the SymODEN [36, 35] and portHamiltonian neural network (PHNN) [11] frameworks, and is defined for systems on any manifold.",,,0,not_related
"The HNNs of [16] use a neural network Ĥθ with weights θ to approximate the Hamiltonian H(q, p) of a system.",,,0,not_related
"3 Implementation and hyperparameters Following [16, 7, 24], we use fully connected neural networks with two hidden layers of 100 neurons each.",,,0,not_related
suggest that the derivatives can be approximated by finite differences when unknown [16].,,,0,not_related
"Physics Learning There have been a significant research effort in learning physics models from data such as Hamiltonian neural networks (Greydanus et al. [2019]), Lagrangian neural networks (Cranmer et al. [2020]), Deep lagrangian neural networks (Lutter et al. [2018]).",,,0,not_related
"Physics Learning There have been a significant research effort in learning physics models from data such as Hamiltonian neural networks (Greydanus et al. [2019]), Lagrangian neural networks (Cranmer et al. [2020]), Deep lagrangian neural networks (Lutter et al.",,,0,not_related
"Physics Learning There have been a significant research effort in learning physics models from data such as Hamiltonian neural networks (Greydanus et al. [2019]), Lagrangian neural networks (Cranmer et al.",,,0,not_related
"Physics Learning There have been a significant research effort in learning physics models from data such as Hamiltonian neural networks (Greydanus et al. [2019]), Lagrangian neural networks (Cranmer et al. [2020]), Deep lagrangian neural networks (Lutter et al. [2018]). Neural networks have been used to analyze in-situ experiment data for materials under extreme heat and irradiation (Niu et al. [2020]).",,,0,not_related
"Physics Learning There have been a significant research effort in learning physics models from data such as Hamiltonian neural networks (Greydanus et al. [2019]), Lagrangian neural networks (Cranmer et al. [2020]), Deep lagrangian neural networks (Lutter et al. [2018]). Neural networks have been used to analyze in-situ experiment data for materials under extreme heat and irradiation (Niu et al. [2020]). Similar to our work, Xue et al. [2021] proposes to make physics learning more efficient by combining 2 step PDE trajectory extraction and model learning into a single learning method, and Sima and Xue [2021] proposes to use locality sensitive hashing to avoid redundant computations for similar data points make forward simulation more efficient.",,,0,not_related
"For HNN, we use 4 × 100 × 100 × 1 FNN with Tanh activation and without the last layer bias as proposed by [7].",,,1,related
"The neural networks embedded with the induced biases have remarkable abilities in learning and generalizing the intrinsic kinetics of the underlying systems from the noisy data, such as the Hamiltonian neural networks [7, 8], the Lagrangian neural networks [5], and the physics-informed neural networks [13, 9, 6].",,,0,not_related
"We compare our HNKO with the currently mainstream methods, the HNN [7] and the EDMD [19], in terms of the trajectory prediction and the energy conservation.",,,1,related
"Indeed, this framework does not require an injection of the regularization terms into the loss function as those usually required in the literature [7, 5].",,,0,not_related
"HNN D-HNN NODE SymODEN D-SymODEN SympGPR SSGP [14] [9, 39] [4] [48] [47] [35] (a) Energy conservation law X X X X X X (b) Energy dissipation law X X X (c) Learning with ODE solver X X X X (d) Uncertainty X X",,,0,not_related
The Hamiltonian neural network (HNN) [14] introduced prior knowledge of Hamiltonian mechanics as an inductive bias for training DNNs; the core concept is to parameterize the Hamiltonian (i.,,,0,not_related
"We can easily extend our proposed approach to learn Hamiltonians from high-dimensional data (such as images) by combining an autoencoder with an SSGP, as in [14, 42].",,,1,related
"We compared the SSGP with the existing models (see Table 1): Hamiltonian neural network (HNN) [14], dissipative HNN (D-HNN) [39], neural ordinary differential equation (NODE) [4], symplectic ODE-Net (SymODEN) [48], dissipative SymODEN (D-SymODEN) [47] and symplectic Gaussian process regression (SympGPR) [35].",,,1,related
The Hamiltonian neural network (HNN) [14] and variants (e.,,,0,not_related
"At the same time, the deep learning community has developed powerful tools by adapting physical modeling concepts to deep learning, for instance the Neural ODE approach was proposed for the modeling of continuous transformations ([40]), stable neural architectures were also developed by embedding neural networks with invariant structures ([34, 48, 46]).",,,0,not_related
A better approach would be to express dynamics as a set of differential equations since time is actually continuous [23].,,,0,not_related
"An adjacent research direction is the method/models which consider CT dynamics and directly use the state derivatives and often the noiseless states to formulate structured and interpretable models such as Hamiltonian Neural Networks (HNN) [16], Lagrangian Neural Networks (LNN) [10] and Sparse Identification of Nonlinear Dynamics (SINDy) [7].",,,0,not_related
"[16,10]) on the state-space obtaining a widely applicable method.",,,0,not_related
"Hamiltonian dynamics [31] describes a system’s total energyH(q,p) as a function of its canonical coordinates q and momenta p, e.",,,0,not_related
"This line of work originates from Greydanus et al. (2019); Cranmer et al. (2020) and has been extended to NODEs for Hamiltonian and port-Hamiltonian systems (Zhong et al., 2020; Massaroli et al., 2020a).",,,0,not_related
"It also does not conserve energy, which is not surprising when no structure is imposed, as discussed e.g., in (Greydanus et al., 2019).",,,0,not_related
", 2020), reasoning in visual question answering (Kim & Lee, 2019), and physical system identification (Lutter et al., 2019; Greydanus et al., 2019; Finzi et al., 2020) etc.",,,0,not_related
"…due to high training complexity, applied works have so far been limited to few domains like Reinforcement Learning (Du et al., 2020), reasoning in visual question answering (Kim & Lee, 2019), and physical system identification (Lutter et al., 2019; Greydanus et al., 2019; Finzi et al., 2020) etc.",,,0,not_related
[25]) and Lagrangian Neural Networks (Cranmer et al.,,,0,not_related
"Among these, Lagrangian neural networks (LNNs) and Hamiltonian neural networks (HNNs) are two physics-informed neural networks with strong inductive biases that outperform other learning paradigms of dynamical systems (Greydanus et al., 2019; Zhong et al., 2021; Sanchez-Gonzalez et al., 2019; Yang et al., 2020; Cranmer et al., 2020a; Finzi et al., 2020; Lutter et al., 2019; Duong and Atanasov, 2021).",,,0,not_related
"…networks (HNNs) are two physics-informed neural networks with strong inductive biases that outperform other learning paradigms of dynamical systems (Greydanus et al., 2019; Zhong et al., 2021; Sanchez-Gonzalez et al., 2019; Yang et al., 2020; Cranmer et al., 2020a; Finzi et al., 2020; Lutter et…",,,0,not_related
"Most of the works on LNN has focused on relatively simpler particle-based systems such as springs and pendulums (Cranmer et al., 2020a; Finzi et al., 2020; Lutter et al., 2019; Greydanus et al., 2019; Gruver et al., 2021).",,,0,not_related
The model would be more effective if it were true to reality [14].,,,0,not_related
Figure 3: A Hamiltonian neural network as introduced in [3].,,,0,not_related
"However, the number of specialized neural networks for Hamiltonian systems has been growing significantly during the last years [3, 5].",,,0,not_related
"…by the level of inductive bias incorporated, spanning methods which use black-box architectures composed of multilayer perceptrons (Thuruthel et al. (2018)) to grey-box architectures that aim to preserve physical invariants (Thuruthel et al. (2018); Greydanus et al. (2019); Cranmer et al. (2020)).",,,0,not_related
"The spectrum ΛE , has no low-order resonance relationship with any eigenvalue in the outer spectrum Λout (see Haller and Ponsioen (2016); Cenedese et al.",,,0,not_related
"For example, [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40] exploit Lagrangian or Hamiltonian mechanics to learn an energy-conserving system based on position, momentum, and the derivatives thereof along trajectories.",,,0,not_related
"This problem setup is distinct from that of HNN [33], HGN [36], or Symplectic ODE-Net [35].",,,0,not_related
The propagator objective is used within algorithms such as dynamic mode decomposition (DMD) [61] and sparse identification of nonlinear dynamics (SINDy) [10] and for training certain neural networks such as Hamiltonian neural networks [29].,,,0,not_related
"Similar considerations hold for works that further extended the Neural ODE model, such as Hamiltonian Neural Networks (Greydanus et al., 2019).",,,0,not_related
"…been growing interest in developing better deep neural network based dynamics models for physical systems through better inductive biases (Lutter et al., 2019a; Greydanus et al., 2019; Lutter et al., 2019b; Zhong et al., 2019, 2020; Cranmer et al., 2020; Finzi et al., 2020; Zhong et al., 2021a).",,,0,not_related
"Recently there has been growing interest in developing better deep neural network based dynamics models for physical systems through better inductive biases (Lutter et al., 2019a; Greydanus et al., 2019; Lutter et al., 2019b; Zhong et al., 2019, 2020; Cranmer et al., 2020; Finzi et al., 2020; Zhong et al., 2021a).",,,0,not_related
The Hamiltonian Neural Network [16] is among the first methods that attempt to incorporate the structure of dynamical systems into a machine learning framework.,,,0,not_related
"Damped Pendulum (DPL) Now a standard benchmark for hybrid models, we consider the motion of a pendulum of length L damped due to viscous friction (Greydanus et al., 2019; Yin et al., 2021).",,,0,not_related
", 2018) or Hamiltonian priors (Greydanus et al., 2019; Lee et al., 2021), increases generalization power w.",,,0,not_related
"The symplectic Gaussian process dynamic model (SGPD) is compared to the explicit Euler method, which corresponds to the standard state-space approach as explained in Section 1 and to the Hamiltonian neural network (Greydanus et al., 2019) with their code.",,,0,not_related
We took two examples that were also considered in Greydanus et al. (2019).,,,1,related
"Neural networks are able to learn a separable Hamiltonian from data (Greydanus et al., 2019).",,,0,not_related
"…model from a continuous pointof-view, treating the vector field f—and not the discretization φ—as the sole object of interest (Heinonen and d’Alché Buc, 2014; Greydanus et al., 2019), it remains an open question how to automatically choose
the most appropriate discretization for the learned model.",,,1,related
"Among those studies, the most basic ones would be neural differential equations (Chen et al., 2018) and Hamiltonian neural networks (Greydanus et al., 2019).",,,0,not_related
", 2018) and Hamiltonian neural networks (Greydanus et al., 2019).",,,0,not_related
"Thirdly, we consider the behavior of one of the most important energy-based models, Hamiltonian neural networks (Greydanus et al., 2019), especially when the loss function does not completely vanish, under the assumption that the learning target is an integrable system in the sense of Liouville.",,,0,not_related
"…of research on predicting the corresponding physical phenomena by learning the energy function H in such equations with a neural network HNN (e.g., Greydanus et al. (2019); Cranmer et al. (2020); Chen et al. (2020); Zhong et al. (2020); Matsubara et al. (2020)); however, to the best of our…",,,0,not_related
"In such a situation, Hamiltonian neural networks (Greydanus et al., 2019)
d
dt q1q2p1 p2  = ( O I−I O ) ∇HNN(q1, q2, p1, p2)
are not appropriate because this model can be used only when p1 and p2 are generalized momenta; in the case considered here, they are supposed to be unknown.",,,0,not_related
"The performance difference between CLNN and CHNN is minor since their
architectures are similar.",,,0,not_related
"Neural ODE is leveraged by Symplectic ODE-Net (Zhong et al., 2020a) and Constrained Lagrangian/Hamiltonian Neural Network (CLNN/CHNN) (Finzi et al., 2020) to learn unknown sys-
tem properties in rigid body dynamics without contacts.",,,0,not_related
"Deep Lagrangian Networks (Lutter et al., 2019), Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Symplectic ODE-Net (SymODEN) (Zhong et al., 2020a), Symplectic Recurrent Neural Networks (SRNN) (Chen et al., 2020) and Lagrangian Neural Networks (LNN) (Cranmer et al., 2020) incorporate physics prior in the form of Lagrangian/Hamiltonian dynamics to learn physical motions with limited data and improved generalization.",,,0,not_related
"These two versions are combined with CLNN and CHNN to set up the following four neural network models: (i) CM-CD-CLNN, (ii) CM-CD-CHNN, (iii) CMr-CD-CLNN, and (iv) CMrCD-CHNN.",,,0,not_related
"The system properties are parametrized as in CLNN and CHNN (Finzi et al., 2020).",,,0,not_related
"Deep Lagrangian Networks (Lutter et al., 2019), Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Symplectic ODE-Net (SymODEN) (Zhong et al., 2020a), Symplectic Recurrent Neural Networks (SRNN) (Chen et al., 2020) and Lagrangian Neural Networks (LNN) (Cranmer et al., 2020) incorporate…",,,0,not_related
", 2019), Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Symplectic ODE-Net (SymODEN) (Zhong et al.",,,0,not_related
"In each task, we jointly learn system and contact properties from trajectory data by extending CLNN/CHNN with the proposed contact model.",,,1,related
Our contact model extends CHNN/CLNN to enable learning of hybrid dynamics in rigid body systems and offer interpretability about system and contact properties.,,,1,related
Hamiltonian neural networks [17] embeds Hamiltonian mechanics in the architecture of the neural network to ensure that the model follows energy conservation laws.,,,0,not_related
"In order to improve the generalization ability, DL techniques aim to learn the exact physical laws instead of approximating inner law from the data set [35], such as Hamiltonian neural network motivated by Hamiltonian mechanics [35], Symplectic recurrent neural network with Symplectic integration [36] and Lagrangian neural network for simulating Lagrangians [37].",,,0,not_related
"There is also an increasing interest in designing hybrid methods, which use a discretization of an ODE (in particular a Hamiltonian system) in order to learn the continuous representation of the data, see for instance Greydanus et al. (2019); Chen et al. (2020).",,,0,not_related
"This has inspired followup work to include other integrator schemes for deterministic, ordinary differential equations (ODE) such as symplectic integrators [6, 21, 53].",,,0,not_related
"Recently, these ideas have been rediscovered and used in deep architectures such as ResNets [22], with symplectic schemes for Hamiltonian [6, 21, 53] and Poisson networks [25].",,,0,not_related
"Recently, neural networks have been proposed [15, 16] that not only learn the dynamics of the system but also",,,0,not_related
"Recent research [15, 16] features artificial neural networks that incorporate Hamiltonian structure to learn",,,0,not_related
"Hamiltonian neural network (HNN) as proposed in [16] uses partial derivatives of the final output instead of the actual output value, to approximate an energy function and build a Hamiltonian system with a neural network.",,,0,not_related
"Recently, a series of research [16, 29, 40, 9, 39, 36] aim at learning partial differential equations from",,,0,not_related
"To this end, we use the data generation process by (Greydanus et al., 2019).",,,1,related
"This type of behavior presents a difficulty for machine learning and is impossible for methods that assume the pendulum to be closed systems, such as Hamiltonian Neural Networks (HNNs) (Greydanus et al., 2019).",,,0,not_related
"Another related field of HamNet is neural physics engines (Sanchez-Gonzalez et al., 2018; 2019; Greydanus et al., 2019), which learn to conduct simulations that conform to physical laws.",,,0,not_related
"Gradient fields modeling Gradient fields modeling is one of the popular tools for modeling manybody systems from predicting motion trajectories of physical systems (Greydanus et al., 2019; Norcliffe et al., 2020; Li et al., 2020) to estimating probabilistic densities of complex systems (Song & Ermon, 2019; Cai et al.",,,0,not_related
"…modeling Gradient fields modeling is one of the popular tools for modeling manybody systems from predicting motion trajectories of physical systems (Greydanus et al., 2019; Norcliffe et al., 2020; Li et al., 2020) to estimating probabilistic densities of complex systems (Song & Ermon, 2019; Cai et…",,,0,not_related
"Feed-forward models, with necessary inductive biases, have been used for sequence modelling both in language (Bai et al., 2018) and also in dynamical systems (Greydanus et al., 2019; Fotiadis et al., 2020).",,,0,not_related
", DNNs) have been proven effective as solutions to a variety of scientific computing problems, including the approximation of solutions to partial differential equations [13, 14, 15, 16, 17].",,,0,not_related
"Recent work of Greydanus et al. [2019] demonstrates such process can be represented using a neural network similar to ResNet. Backward learning is to discover (the parameters of) the physics model automatically from experimental data, which also attracts recent attention in Niu et al. [2020] and Xue et al. [2021]. Backward learning can be achieved by embedding a neural network modeling the forward simulation into the overall architecture and minimizing a loss function which penalizes the difference between the simulated result and the observed data via back-propagation.",,,0,not_related
Recent work of Greydanus et al. [2019] demonstrates such process can be represented using a neural network similar to ResNet.,,,0,not_related
"Greydanus et al. (2019) use NNs to predict Hamiltonian from phase-space coordinates s = (p,q) and their derivatives.",,,0,not_related
"Hamiltonian neural network (HNN) is a neural network that models the HamiltonianH and defines the dynamics following the Hamiltonian mechanics, thereby ensuring the energy conservation law [11].",,,0,not_related
HNN [11] LNN [7] Skew Matrix Learning (Sec.,,,0,not_related
One way to achieve this is to extract features from the images by using an autoencoder and to learn the equation of motion that the features satisfy [11].,,,1,related
"state [12, 13, 14, 15], analytic constraints [16, 17], or evaluation function [18, 19].",,,0,not_related
"(3) HNN approximates the Hamiltonian H from the data using a neural network (Greydanus et al., 2019).",,,0,not_related
"Hamiltonian neural networks (HNN) and Lagrangian neural networks (LNN) learn Hamiltonian and Lagrangian mechanics, ensuring the energy conservation in continuous time (Greydanus et al., 2019; Cranmer et al., 2020).",,,0,not_related
"Artificial neural network models [45], [46] have been used to deal with the Hamiltonian and Lagrangian mechanics, but these approaches consider an ideal situation.",,,0,not_related
"One line of work aims to understand how neural networks can be structured and trained to reproduce known physical system behavior, with the goal of designing general methods applicable in a variety of settings (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2020; Sanchez-Gonzalez et al., 2018; Raissi et al., 2017; 2019; Lu et al., 2019; Haghighat et al., 2020; Tartakovsky et al., 2018).",,,0,not_related
"…structured and trained to reproduce known physical system behavior, with the goal of designing general methods applicable in a variety of settings (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2020; Sanchez-Gonzalez et al., 2018; Raissi et al., 2017; 2019; Lu et al., 2019;…",,,0,not_related
"In parallel, physics and machine learning have been forging strong ties based for example on Hamiltonian theory and the integration of ordinary differential equations in the latent space to describe the evolution of dynamical systems (Chen et al. (2018); Toth et al. (2019); Greydanus et al. (2019)).",,,0,not_related
We consider a simple environment similar to that studied in Higgins et al. (2018) and Caselles-Dupré et al.,,,1,related
"Another requirement we wish to make concerning the representation to be learned is that it is disentangled in the sense of Higgins et al. (2018). Formally, if there exists a subgroup decomposition of G such that G = G1 × G2.",,,1,related
"Another requirement we wish to make concerning the representation to be learned is that it is disentangled in the sense of Higgins et al. (2018). Formally, if there exists a subgroup decomposition of G such that G = G1 × G2... × Gn, we would like to decompose the representation (ρ, V ) in subrepresentations V = V1 ⊕ V2... ⊕ Vn such that the restricted subrepresentations (ρ|Gi , Vi)i are non-trivial and the restricted subrepresentations (ρ|Gi , Vj)j 6=i are trivial (we recall that a trivial representation of G is equal to the identity for every element of the group G). This definition of disentangled representations has several advantages. First, it maps onto an intuitive notion of disentangled representation as one that separates the data generative factors into different subspaces. It also provides a principled resolution to several points of contention concerning what should be considered a data generative factor, which ones can be disentangled and which ones cannot, and what dimensionality the representation of each factor should have. However, despite theoretical analysis by Higgins et al. (2018) and Caselles-Dupré et al.",,,1,related
"In the rest of the sections, we will talk a little about the theory of Hamiltonian Neural Networks and then we will move on to our implementations and results.",,,1,related
"In this work, we reproduce the paper1 Hamiltonian Neural Network [1].",,,1,related
"After training, we use these models to predict (∂q/∂t, ∂p/∂t) given (q, p), and use this to approximate the trajectory of the system using our differential equation given by equation [1].",,,1,related
"This equation is found using the domain-specific knowledge, but in the paper, the equation is learned by the Hamiltonian Neural Network on its own using the data.",,,0,not_related
"The approach presented in the paper takes inspiration from the Hamiltonian mechanics, a branch of physics that deals with conservation laws and invariances, and defines the concept of Hamiltonian Neural Networks (HNNs).",,,0,not_related
", 2016) or due to the ambiguities in unsupervised learning (Greydanus et al., 2019; Cranmer et al., 2020b).",,,0,not_related
"Recent work on physics-based deep learning injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al., 2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly conserving physical quantities, or with ground truth supervision (Asenov et al.",,,0,not_related
"…in the context of differentiable physics simulation Degrave et al. (2016); de Avila Belbute-Peres et al. (2018); Cranmer et al. (2020b); Toth et al. (2020); Greydanus et al. (2019); Sanchez-Gonzalez et al. (2019), we focus on more challenging systems which have not been studied in prior art.",,,0,not_related
"Recent work on physics-based deep learning injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al., 2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly…",,,0,not_related
"These methods require few prior assumptions about the system itself, but lack interpretability due to entangled variational factors (Chen et al., 2016) or due to the ambiguities in unsupervised learning (Greydanus et al., 2019; Cranmer et al., 2020b).",,,0,not_related
"The use of ordinary differential equation (ODE) solvers within deep learning frameworks has allowed end-to-end training of Neural ODEs (Chen et al., 2018) in a variety of settings.",,,0,not_related
"We consider parametrizing event functions with neural networks in the context of solving ODEs, extending Neural ODEs to implicitly defined termination times.",,,1,related
"…2020), generative modeling (Grathwohl et al., 2018; Zhang et al., 2018; Chen & Duvenaud, 2019; Onken et al., 2020), time series modeling (Rubanova et al., 2019; De Brouwer et al., 2019; Jia & Benson, 2019; Kidger et al., 2020), and physics-based models (Zhong et al., 2019; Greydanus et al., 2019).",,,0,not_related
"Though there exist physics-motivated parameterizations of Neural ODEs (e.g. Zhong et al., 2019; Greydanus et al., 2019; Finzi et al., 2020), these cannot model collision effects.",,,0,not_related
", 2020), and physics-based models (Zhong et al., 2019; Greydanus et al., 2019).",,,0,not_related
"By introducing differentiable termination criteria in Neural ODEs, our approach allows the model to efficiently and automatically handle state discontinuities.",,,1,related
"To further expand the applications of Neural ODEs, we investigate the parameterization and learning of a termination criteria, such that the termination time is only implicitly defined and will depend on changes in the continuous-time state.",,,1,related
"On the other hand, [16] and [17] have utilized Hamiltonian mechanics for learning dynamics from data.",,,0,not_related
"On the other hand, [16] and [17] have utilized Hamiltonian mechanics for learning dynamics from data.",,,0,not_related
"While incorporating physical models into deep learning approaches has been studied in various forms [10, 5, 3, 11], differentiable simulations allow a trained model to autonomously explore and experience the physical environment and receive directed feedback regarding its interactions throughout the solver iterations.",,,0,not_related
"In [3] and its variants[11, 1, 12, 8], the Hamiltonian function can be approximated by neural networks, Hθ, called Hamiltonian Neural Networks (HNN).",,,0,not_related
"Machine learning in fluid systems The rapid advent of machine learning techniques is opening up new possibilities to solve the physical system’s identification problems by statistically exploring the underlying structure of a variety of physical systems, encompassing applications in quantum physics (35), thermodynamics (17), material science (36), rigid body control (12), Lagrangian systems (9), and Hamiltonian systems (14; 19; 37).",,,0,not_related
", also tried to combine an autoencoder with an Hamiltonian Neural Networks (HNN) to model the dynamics of pixel observations of a pendulum (14).",,,0,not_related
"Recently, some works leveraged data specific knowledge to shape the prediction function, for example imposing specific fluid dynamic [26] or Hamiltonian constraints [11, 31].",,,0,not_related
"Most closely related to ours, Greydanus et al. (2019) use a neural network to learn the Hamiltonian of the dynamical system in such a way that its partial derivatives match the time derivatives of the position and momentum variables, which are both assumed to be observed.",,,0,not_related
"Moreover, (Greydanus et al., 2019) mentions that HNN does not outperform a baseline method using O-NET in learning the three-body system’s evolution.",,,0,not_related
"A recent paper (Greydanus et al., 2019), Hamiltonian Neural Networks (HNN), proposes to do so representing the Hamiltonian function H(q, p) as a multilayer neural network.",,,0,not_related
"For instance, with our terminology, the HNN model of Greydanus et al. (2019) is a “single-step E-E H-NET” with the additional subtlety that they supervise the training with actual derivatives instead of relying on finite differences between successive steps of the observed trajectories.",,,1,related
"1 Introduction Can machines learn physical laws from data? A recent paper (Greydanus et al., 2019), Hamiltonian Neural Networks (HNN), proposes to do so representing the Hamiltonian function H(q, p) as a multilayer neural network.",,,0,not_related
"Here, we test our SRNN together with other baselines on the noiseless three-body system with the same configurations as (Greydanus et al., 2019).",,,1,related
"For instance, Greydanus et al. (2019) trains H-NET in a fully supervised manner using the observed tuples (p, q, ṗ, q̇).",,,0,not_related
"Among the possible choices are reproducing kernel Hilbert spaces [18; 19; 5], trigonometric functions, and functions parameterized by neural networks [4; 7].",,,0,not_related
"For related work on learning Hamiltonian systems, see [7; 2].",,,0,not_related
"Many works in recent years have proposed a black-box parameterization for the dynamics of a mechanical system [4, 5, 7, 2, 3].",,,0,not_related
"In recent years, the use of physical loss functions has proven beneficial for the training procedure, yielding substantial improvements over purely supervised training approaches (Tompson et al., 2017; Wu & Tegmark, 2019; Greydanus et al., 2019).",,,0,not_related
"Additional works have shown the advantages of physical loss formulations (Greydanus et al., 2019; Cranmer et al., 2020).",,,0,not_related
