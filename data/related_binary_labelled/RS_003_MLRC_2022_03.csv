text,label_score,label,target_predict,target_predict_label
"7 [5] Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein.",,,0,not_related
"To address this issue, various solutions have been proposed, including modifications to attention mechanism [41, 11, 17] and the design of recurrent networks [46, 4], but their application in VL tasks is nontrivial.",,,0,not_related
"…i.e. transformers with shared layers (Dehghani et al., 2018), because recurrent models are used in prior work on length generalization (Bansal et al., 2022; Kaiser and Sutskever, 2015), and universal transformers proved essential on tasks involving modular arithmetic (Wenger et al.,…",,,0,not_related
", 2018), because recurrent models are used in prior work on length generalization (Bansal et al., 2022; Kaiser and Sutskever, 2015), and universal transformers proved essential on tasks involving modular arithmetic (Wenger et al.",,,0,not_related
"Recent studies also show that iterative inference presents stronger generalizability than one-step forward predictions [61, 6], with explanations of their relations to the ""working memory"" of human minds [4, 5] or human visual systems [44, 34].",,,0,not_related
", 2014; 2016); it could exploit spatial invariances in the algorithmic task through a convolutional architecture (Bansal et al., 2022); it could be based on the transformer self-attentional architecture, as in the Universal Transformer (Dehghani et al.",,,0,not_related
"Copyright 2023 by the author(s).
et al., 2022b; Bansal et al., 2022; Beurer-Kellner et al., 2022).",,,1,related
"…Turing Machines (Graves et al., 2014; 2016); it could exploit spatial invariances in the algorithmic task through a convolutional architecture (Bansal et al., 2022); it could be based on the transformer self-attentional architecture, as in the Universal Transformer (Dehghani et al., 2019); or…",,,0,not_related
"This is likely due to the advent of powerful strategies such as recall (Bansal et al., 2022), wherein the input is fed back to the model at every intermediate step, constantly “reminding” the model of the problem that needs to be solved.",,,0,not_related
"Simple recurrent neural networks can solve this task well because the memory in the recurrent neural network can record the states for finite-state automation (Abnar et al., 2021; Schwarzschild et al., 2021; Veličković et al., 2022; Ibarz et al., 2022; Bansal et al., 2022).",,,0,not_related
"Simple recurrent neural networks can solve this task well because the memory in the recurrent neural network can record the states for finite-state automation (Abnar et al., 2021; Schwarzschild et al., 2021; Veličković et al., 2022; Ibarz et al., 2022; Bansal et al., 2022).",,,0,not_related
"Our current work is closely related to previous work by Schwarzschild et al. [2021b] and [Bansal et al., 2022] that propose architectural choices and training mechanisms that enable weight tied networks to generalize on harder problem instances.",,,1,related
"We used the original code released by [Bansal et al., 2022] to replicate the results for weight-tied input-injected networks trained with progressive training.",,,1,related
"Past work has observed that weight tying and input injection are both crucial for upwards generalization [Bansal et al., 2022].",,,0,not_related
"One particularly important type of out-of-distribution (OOD) generalization is upwards generalization, or the ability to generalize to more difficult problem instances than those encountered at training time [Selsam et al., 2018, Bansal et al., 2022, Schwarzschild et al., 2021b, Nye et al., 2021].",,,0,not_related
"For experiments with truncated backpropagation, we follow the exact setting as specified in [Bansal et al., 2022].",,,1,related
"2a we show upward generalization performance using both equilibrium models and progressive nets [Bansal et al., 2022] – and the lack thereof using non-input-injected networks.",,,1,related
"[2021b] and [Bansal et al., 2022] that propose architectural choices and training mechanisms that enable weight tied networks to generalize on harder problem instances.",,,0,not_related
"Banino et al. (2021), Schwarzschild et al. (2021) and Bansal et al. (2022) propose recurrent neural networks that perform multiple recursive processes depending on the complexity of the task.",,,0,not_related
"Bansal et al. (2022), Schwarzschild et al. (2021) focus on length and algorithmic generalization for recurrent models where they train on simple/easy instances of the underlying problem and evaluate on harder/complex instances using the power of recurrence to simulate extra computational steps,…",,,0,not_related
"Barak et al. (2022), Edelman et al. (2022) perform a theoretical and empirical study of the ability of Transformers (and other architectures) to learn sparse parities where the support size k T .",,,0,not_related
"Barak et al. (2022), Edelman et al. (2022) perform a theoretical and empirical study of the ability of Transformers (and other architectures) to learn sparse parities where the support size k T . Bhattamishra et al. (2020), Schwarzschild et al. (2021) study the task of computing prefix sum in the binary basis (which is essentially parity of the prefix sum) for Transformers and recurrent models, repsectively.",,,0,not_related
"3 454 [34] Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Gold455 blum, and Tom Goldstein.",,,0,not_related
"114 Further, note that at each step, the input encoding is fed directly to these embeddings—this recall 115 mechanism significantly improves the model’s robustness over long trajectories [34].",,,0,not_related
[26] use weight-tied neural networks to generalize from easy to hard examples.,,,0,not_related
This confirms the theoretical derivations by Merrill (2019) but contrasts the results presented by Bansal et al. (2022).,,,0,not_related
Bansal et al. (2022) showed that fully convolutional networks achieve near-perfect length generalization on the prefix sum task in the length generalization setting.,,,0,not_related
"However, the convolutional architecture that Bansal et al. (2022) consider relies on an adaptive number of layers depending on the sequence length (similar to adaptive computation time (Graves, 2016)) and thus crucially differs from a classical CNN (such as the one we study).",,,0,not_related
"While initially applied only to path-finding and spanning-tree algorithms, the prescriptions listed above have been applied for heuristically solving bipartite matching (Georgiev and Lió, 2020), mazes (Schwarzschild et al., 2021; Bansal et al., 2022), min-cut (Awasthi et al.",,,0,not_related
"…neural algorithmic reasoning and it has recently been explored with transformers (Nogueira et al., 2021; Kim et al., 2021; Anil et al., 2022; Zhou et al., 2022; Charton, 2021; Zhang et al., 2021) and recurrent neural networks Bansal et al. (2022); Linsley et al. (2018); Schwarzschild et al. (2021).",,,0,not_related
"Integer calculus and floating-point arithmetic in binary (symbolic) representations have previously received more attention (Nogueira et al., 2021; Talmor et al., 2020; Jiang et al., 2019; Thawani et al., 2021; Zhou et al., 2022; Hendrycks et al., 2021; Bansal et al., 2022).",,,0,not_related
This confirms the theoretical derivations by Merrill (2019) but contrasts the results presented by Bansal et al. (2022).,,,0,not_related
Bansal et al. (2022) showed that fully convolutional networks achieve near-perfect length generalization on the prefix sum task in the length generalization setting.,,,0,not_related
"However, the convolutional architecture that Bansal et al. (2022) consider relies on an adaptive number of layers depending on the sequence length (similar to adaptive computation time (Graves, 2016)) and thus crucially differs from a classical CNN (such as the one we study).",,,0,not_related
