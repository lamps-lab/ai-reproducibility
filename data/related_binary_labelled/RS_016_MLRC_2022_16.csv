text,label_score,label,target_predict,target_predict_label
"In Crabbé & van der Schaar (2022), the feature importance for unlabelled data is proposed, using XAI methods at the initial phase of data prepossessing to distinguish features that may play a key role in further ML tasks.",,,0,not_related
[48] Jonathan Crabbé and Mihaela van der Schaar.,,,0,not_related
"(2)We also restrict to supervised models, since only early works exist to interpret unsupervised models [48, 49].",,,1,related
"a-b, Multiple methods (the proposed likelihood ratio ranking, integrated gradients [26; 27], and Hotspot [28]) were applied to rank the genes in our simulated dataset by how strongly they were captured by multiGroupVI’s group-specific latent spaces.",,,0,not_related
"We compared the performance of our likelihood ratio ranking with two other methods designed for interpreting the latent spaces of unsupervised machine learning models: Hotspot [28], a method that ranks genes by spatial autocorrelation when provided a given metric of cell-cell similarity (e.g. the latent space of an autoencoder), and an adaptation of integrated gradients [26] for unsupervised models proposed in Crabbé and van der Schaar [27] applied to multiGroupVI’s group-specific latent spaces.",,,1,related
"the latent space of an autoencoder), and an adaptation of integrated gradients [26] for unsupervised models proposed in Crabbé and van der Schaar [27] applied to multiGroupVI’s group-specific latent spaces.",,,1,related
[65] Jonathan Crabbé and Mihaela van der Schaar.,,,0,not_related
"We quantitatively compare these various feature importance scores by computing their Pearson correlation r as in [64, 65].",,,1,related
"We do not compare with methods that are not post-hoc, that require exposure to outliers, or are not actionable (most existing XAI methods) to match model monitoring settings.",,,1,related
"To determine trust in model predictions, current works typically use (a) explainability methods (XAI) [11]–[13]; which, though explainable, post-hoc and suitable for providing qualitative insights, require humans to oversee the explanations and are not directly actionable for continuous and automated model monitoring; or (b) uncertainty estimation techniques",,,0,not_related
"To determine trust in model predictions, current works typically use (a) explainability methods (XAI) [11]–[13]; which, though explainable, post-hoc and suitable for providing qualitative insights, require humans to oversee the explanations and are not directly actionable for continuous and automated model monitoring; or (b) uncertainty estimation techniques [6], [14]–[18]; that are difficult to train or are computationally expensive.",,,0,not_related
"XAI methods provide complementary, post-hoc explanations to the predictions of black-box models to induce trust.",,,0,not_related
"To determine trust in model predictions, current works typically use (a) explainability methods (XAI) [11]–[13]; which, though explainable, post-hoc and suitable for providing qualitative insights, require humans to oversee the explanations and are not directly actionable for continuous and automated model monitoring; or (b) uncertainty estimation techniques",,,0,not_related
"To determine trust in model predictions, current works typically use (a) explainability methods (XAI) [11]–[13]; which, though explainable, post-hoc and suitable for providing qualitative insights, require humans to oversee the explanations and are not directly actionable for continuous and automated model monitoring; or (b) uncertainty estimation techniques [6], [14]–[18]; that are difficult to train or are computationally expensive.",,,0,not_related
"We don’t compare with methods that are not post-hoc, that require exposure to outliers, or are not actionable (most existing XAI methods) to match model monitoring settings.",,,1,related
"XAI methods provide complementary, post-hoc explanations to the predictions of black-box models to induce trust.",,,0,not_related
