text,label_score,label,target_predict,target_predict_label
"The former approach accounts for changing proportions of subpopulations, relating it to notions of subpopulation fairness [Duchi et al., 2023, Santurkar et al., 2020, Piratla et al., 2021, Martinez et al., 2021].",,,0,not_related
", 2020), common gradient descent (Piratla et al., 2021), and GradNorm (Chen et al.",,,0,not_related
"CGD (Piratla et al., 2021) aims to improve multitask learning by encouraging update towards common directions of different tasks, which is opposite to our method that encourages task specialties.",,,0,not_related
"Given that CGD aims to improve multi-task learning by encouraging update towards common directions of different tasks, we hypothesize that the need for task specialization is diminished here because the tasks are more similar in difficulty (e.g., in KILT, T-REx and zsRE are much easier than HotpotQA).",,,1,related
The best performance is obtained by CGD and it is the only multitask optimization method that yields noticeable improvements over the standard multitask model.,,,0,not_related
"The performance of the recent general multitask algorithms, PCG (Yu et al., 2020), CGD (Piratla et al., 2021), and GradNorm (Chen et al., 2018), are obtained from our own implementation.",,,1,related
", 2020), CGD (Piratla et al., 2021) and GradNorm (Chen et al.",,,0,not_related
"• Group distributionally robust optimization and the follow-up algorithms, including, GroupDRO [12] is an online optimization algorithm that dynamically assigns importance weights during training to improve the worst-case performance across all the subpopulations; V-REx [81] is an extension of distributionally robust optimization by conducting robust optimization over a perturbation set of extrapolated domain; CGD [63] assigns importance weights to each subpopulation so that the model could be trained on the direction leading to the largest decrease in average training loss.",,,0,not_related
"Specifically, similar to previous works [63], [64], [65], instead of directly setting the weight of each sample to be inversely proportional to the sample size of each subpopulation, we empirically set the importance weight wi of each sample as a function of the square root of its training group size √ kgN , where kg and N denote the g-th subpopulation’s proportion and training samples respectively.",,,0,not_related
"To be same as previous works [42], [63] and WILDs leaderboard [75], we report the average accuracy over 10 different random seeds on the Camelyon17 dataset.",,,1,related
"achieve low error on the scarce corrupted data is distributionally robust optimization (DRO) [39,42,45,54,55,65], which commonly optimizes the model parameter θ by optimizing:",,,0,not_related
"GroupDRO [13, 42, 45] deal with the situation when the correlation between class label y and unknown attribute a differs in the training and test set.",,,0,not_related
"We denote this metric as “Wg Acc”, which is a standard metric when evaluating on datasets with shortcut features (Sagawa et al., 2019; Piratla et al., 2021).",,,1,related
There has been significant recent progress on this benchmark using optimization techniques apart from G-DRO — Zhang & Ré (2022); Zhang et al. (2022); Piratla et al. (2021); Kirichenko et al. (2022).,,,0,not_related
"It favors clusters sharing more ‘common needs’ (Piratla et al., 2022) with others to improve the model robustness across all clusters.",,,0,not_related
"[56] Vihari Piratla, Praneeth Netrapalli, and Sunita Sarawagi.",,,0,not_related
"To be consistent with existing works [33, 56, 70], we report the average accuracy of Camelyon17 over 10 different random seeds.",,,1,related
"Common Gradient Descent (CGD), introduced by Piratla et al. (2022), is based on Group-DRO.",,,0,not_related
"[26] Vihari Piratla, Praneeth Netrapalli, and Sunita Sarawagi.",,,0,not_related
"Recent variants of this approach have sought to avoid over-fitting through group-specific regularization [30] or margin-based losses [24, 16], to handle unknown subgroups [34], and to balance between average and worst-case performance [26].",,,0,not_related
"There are also some group-based works (Sagawa et al., 2019; Bao et al., 2021; Liu et al., 2021b; Sanh et al., 2021; Piratla et al., 2021; Zhou et al., 2021) that improve worst group performance and can be applied to domain generalization problem.",,,0,not_related
