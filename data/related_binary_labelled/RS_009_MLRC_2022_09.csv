text,label_score,label,target_predict,target_predict_label
"Transfer learning has emerged as a crucial paradigm within machine learning due to its ability to use knowledge extracted from one domain (source) to enhance learning in a different, typically related domain (target) [30, 43, 49, 38, 6].",,,0,not_related
"We leave it to future work to further study this behavior and the relationship between the FT loss surface and OOD generalization (Shwartz-Ziv et al., 2022; Juneja et al., 2023).",,,1,related
"…the performance of supervised baselines on many downstream tasks [Larsson et al., 2016, Bachman et al., 2019, Gidaris et al., 2018, 2021, Misra and van der Maaten, 2019, Grill et al., 2020, Shwartz-Ziv et al., 2022b, Chen et al., 2020b, He et al., 2020, Zbontar et al., 2021, Chen and He, 2021].",,,0,not_related
"However, understanding the learned representations and their underlying mechanisms remains a persistent challenge due to the complexity of the models and the lack of labeled training data [Shwartz-Ziv et al., 2022a].",,,0,not_related
"More recently, Shwartz-Ziv et al. (2022) proposes to approximate the prior using SGD trajectory as in SWAG (Maddox et al., 2019) for transfer learning.",,,0,not_related
"Notably, Shwartz-Ziv et al. (2022) use transfer learning to specify informative BNN priors, considering SimCLR
pre-training as a special case.",,,0,not_related
"For instance, employing SWAG inference to learn θs (Maddox et al., 2019) would an yield approach similar to Pre-train Your Loss (Shwartz-Ziv et al., 2022).",,,0,not_related
"These pre-trained representations are then used as a feature extractor for downstream supervised tasks such as image classification, object detection, and transfer learning (Caron et al., 2021; Chen et al., 2020; Misra and Maaten, 2020; Shwartz-Ziv et al., 2022).",,,0,not_related
[46] use a low-rank estimate of the curvature around an optimum of a pre-training task to regularise subsequent supervised learning.,,,0,not_related
Shwartz-Ziv et al. (2022) use a low-rank estimate of the curvature around an optimum of a pre-training task to regularise subsequent supervised learning.,,,0,not_related
"in [40], that the model pre-trained with the high-resolution dataset can provide the desirable initialization of the network for a downstream task i.",,,0,not_related
