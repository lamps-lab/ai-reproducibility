text,label_score,label,target_predict,target_predict_label
"ing [147], data debiasing [38, 149], synthetic sampling [268, 290], and using specialized optimization functions for creating fair classifiers (according to traditional metrics) with unbalanced data [143, 174].",,,0,not_related
"While some researchers generate data to make their data discrimination free or more fair from a causal lens [38, 43, 44], we generate synthetic datasets, not with a de-biasing goal, but to represent different dataset compositions from which models can learn and we can study their effects on impact.",,,1,related
"In our paper (including this appendix) we reproduced the following works for comparison:
1XGBoost library: https://xgboost.readthedocs.io/en/stable/python/python_api.html
• TVAE [9], code from the Synthetic Data Vault [23]: https://github.com/sdv-dev/ SDV,
• CTGAN [9], code from the Synthetic Data Vault [23]: https://github.com/sdv-dev/ SDV,
• GReaT [27], code: https://github.com/kathrinse/be_great,
• AIM [6], code: https://github.com/ryan112358/private-pgm,
• MST [30], code: https://github.com/ryan112358/private-pgm,
• GEM [8], code: https://github.com/terranceliu/iterative-dp,
• Prefair [15], code: https://github.com/David-Pujol/Prefair,
• DECAF [11], code from a reproduction study [45] (downloadable from the supplementary materials on OpenReview: https://openreview.net/forum?id=SVx46hzmhRK),
• TabFairGAN [13], code: https://github.com/amirarsalan90/TabFairGAN.",,,1,related
"We compare ProgSyn to two recent non-private (DECAF [11], and TabFairGAN [13]), and one private (Prefair [15]) fair synthetic data generation methods.",,,1,related
"In a different approach, DECAF [11] trains a causally-aware GAN, and removes undesired causal relationships at generation time to reduce bias.",,,0,not_related
"Prior work has already addressed some of the data sharing concerns: differentially private synthetic data [3, 4, 5, 6, 7, 8, 6], generating data with reduced bias [9, 10, 11, 12, 13, 14], and combining these two objectives [15].",,,0,not_related
"Another line of research that repairs training data is through training data pre-proccessing [13, 14, 32, 24], synthetic fair data [53, 31, 67, 57], and data augmentation [54, 21].",,,0,not_related
"prose, many real world applications benefit greatly from synthetic data generation, for tasks including data augmentation in the training of classifiers/regressors[22], privacy protection of sensitive data[3] or removing bias from data sets [20].",,,0,not_related
"Another line of fair generative modeling focuses on label bias, instead of representation bias (Xu et al. 2018, 2019a,b; Sattigeri et al. 2019; Jang, Zheng, and Wang 2021; Kyono et al. 2021).",,,0,not_related
"Simulation [26,28,35,42,44,45,50,59,61,62,70] is a useful tool in situations where training data for learning-based methods is expensive to annotate or even hard to acquire.",,,0,not_related
"5 In [82, 88, 89, 92], authors show that by carefully constraining a generative model—with constraints given by the fairness requirement—it is possible to generate fair synthetic data based on unfair real data.",,,0,not_related
"1), for example providing better fairness [82, 88, 89], augmenting the dataset size [4, 9, 18, 21], and creating or simulating data for different domains [86, 90].",,,0,not_related
"[82] Boris van Breugel, Trent Kyono, Jeroen Berrevoets, and Mihaela van der Schaar.",,,0,not_related
[82] introduce a causal generative model (DECAF),,,0,not_related
"This can be solved [82], but requires knowledge of the downstream model’s deployment setting, which the data publisher does not always have.",,,0,not_related
"There is a body of literature to help address these biases [106, 107], but tools for model creators are still limited.",,,0,not_related
Causal generative models have seen a variety of success with both learning and utilizing causal information and structural models to generate counterfactual images and datasets [5]–[9].,,,0,not_related
"Recent studies have demonstrated the importance of treating fairness as causation-based notions that concern the causal effect of the sensitive feature on the model outcomes [8,11,2].",,,0,not_related
"…Alaa et al., 2021a; Jarrett et al., 2021), evaluation methods for synthetic data generation (Jordon et al., 2018a; Alaa et al., 2021b), fairness van Breugel et al. (2021), and methods that include models of some of the physical processes underlying clinical parameters in the generation process…",,,0,not_related
"Synthetic data has significant promise, with the potential to improve: (1) fairness & bias by generating data from underrepresented groups (van Breugel et al., 2021); (2) robustness by augmenting an original dataset (Perez and Wang, 2017); (3) privacy by not using identifiable data to train a…",,,0,not_related
"Second, CDG-VAE can be applied to chain graphs (i.e., Partial DAGs) unlikely Xu et al. (2019a); van Breugel et al. (2021); Wen et al. (2021) that re-
ar X
iv :2
30 2.",,,1,related
"…of the ground-truth factors g.
Due to undirected edges between covariates (e.g., the edge between Mortgage and Income), the SCM of covariates is not defined well, and the covariatewise topological generation of Xu et al. (2019a); van Breugel et al. (2021); Wen et al. (2021) is not applicable.",,,0,not_related
"Furthermore, the alignment of the latent structure is also adapted by topological generation, which produces causally-aware generative models Xu et al. (2019a); van Breugel et al. (2021); Wen et al. (2021).",,,0,not_related
"Even though van Breugel et al. (2021) proves that their generator converges to the right distribution for any graph belonging to MECs, incorrect edge directions have the potential risk of misunderstandings of causations.",,,0,not_related
"To exploit causations in the synthetic data generation, Xu et al. (2019a); Wen et al. (2021); van Breugel et al. (2021) generate data in the
order of causal topology, and consequently, they require the completely identified DAG, not the Partial DAG.",,,0,not_related
"Otherwise the model can just learn the protected features by using different proxies which are correlated to them (van Breugel et al., 2021).",,,1,related
"CFGAN (Xu et al., 2019) and DECAF (van Breugel et al., 2021) are two methods to generate fair data that are rooted in this approach to fairness.",,,0,not_related
"Likewise, there have been approaches for generating non-private fair synthetic data [53, 56] GAN based approaches such as FairGAN [56] have been used to generate data that satisfies these associational measures.",,,0,not_related
"These methods include re-sampling, which changes the sampling rate of data during training to ensure each protected class is equally represented [1,23,26] and augmentation methods which add synthetic data to the dataset [22,31,3,34] to balance the protected classes.",,,0,not_related
"Finally, causal generative models have been used to address the issue of fair or “de-biased"" data sets such as DECAF, a causally aware GAN architecture applied explicitly to tabular data [10].",,,0,not_related
"[47] Boris van Breugel, Trent Kyono, Jeroen Berrevoets, and Mihaela van der Schaar.",,,0,not_related
"However, such a match is difficult, especially in the presence of privacy requirements [82], and even undesirable in the presence of biases [23].",,,0,not_related
Definition 3 (Fairness Through Unawareness [23]) A predictor f : X → Y is fair if and only if protected attributes A ⊂ X are not explicitly used by f to predict Y ∈ Y.,,,1,related
"; and/or model fairness properties such as demographic parity, fairness through unawareness, or conditional fairness [23].",,,0,not_related
"Fairness Through Unawareness (FTU) [23, 141] requires that the protected attributes, and only the protected attributes, not be used by the predictor.",,,0,not_related
Definition 4 (Demographic Parity [23]) A predictor f : X → Y is fair if and only if protected attributes A ⊂ X are independent of the predictions.,,,1,related
"Rather than attempting to debias each trained model individually, one could generate a de-biased synthetic dataset and use it to train each model [22, 23], creating a unified approach for handling biases across an organisation.",,,0,not_related
"DPa is a significantly stronger notion of fairness than FTU, which requires adjusting the distributions of all variables that are correlated with the protected attributes.",,,0,not_related
"Definition 3 (Fairness Through Unawareness [23]) A predictor f : X → Y is fair if and only if protected attributes A ⊂ X are not explicitly used by f to predict Y ∈ Y.
Demographic Parity (DPa) [142], instead, requires that a predictors output’s not be correlated with the protected attributes.",,,1,related
"Indeed, with FTU, an attribute that is correlated with the protected attributes can be used as input to a predictor and thus the predictor can indirectly be correlated with the protected attributes (despite not having direct access to them).",,,0,not_related
"FTU, however, fails to take into account the effect that protected attributes might have on other unprotected attributes, such as an individual’s race resulting in them not being afforded the same educational opportunities as an individual of a different race (and thus resulting in them appearing to be disparately qualified).",,,0,not_related
"This aligns with the idea that two equally qualified people deserve the same job opportunities, independent of race or gender [23].",,,0,not_related
"In [23], they explore several notions of fairness and, via causal modelling, identify strategies for generating data that satisfy the given notions.",,,0,not_related
"further and explicitly model the problem of data de-biasing as a one of “groundup” generation [22, 23].",,,0,not_related
"fairness through unawareness, demographic parity, conditional fairness [23]).",,,0,not_related
"Causal structure has been leveraged for tasks other than domain adaptation, such as improving machine learning robustness [26], missing data imputation [25], or even fairness [53].",,,0,not_related
"Furthermore, these approaches often lack any theoretical fairness guarantees; or requires casual structures to be known (van Breugel et al., 2021).",,,0,not_related
"Accordingly, recent methods of data imputation such as DECAF[46] could be leveraged to synthesize much more naturalistic datasets with known causal structures.",,,0,not_related
"For example, synthetic data can be used to simulate settings where real data is scarce or unavailable [7, 10], support better supervised learning by increasing quality in datasets [8], improving robustness and predictive performance [69, 60], and promoting fairness [72].",,,0,not_related
"This is true for all GAN based methods that we discussed, including FairGAN [10], DECAF [11], and CFGAN [12].",,,1,related
"Finally, DECAF [11], is a casually-aware method which is proposed to create fair data for a few notions which does not include CF.",,,1,related
"Our definition of fair SDG can be formulated as a constrained optimization, where the goal is to find 79 a distribution P ′(X,A, Y ) such that it satisfies a certain fairness notion (adopting the notation in 80 [11], we denote it by I ((X,Y,A), P ′)-fairness), while minimizing the distance of P ′ from the real 81 data P , where d is any distance of choice: 82",,,1,related
"Our definition here is in line with the definition of fairness for SDG in FairGAN [10] and [12], while it is different from the definition proposed in DECAF [11].",,,1,related
"This is true for all 177 GAN based methods that we discussed, including FairGAN [10], DECAF [11], and CFGAN [12].",,,1,related
"While this is doable for some of the definitions like 76 demographic parity, it is not possible for notions that depend explicitly to downstream task like equal 77 opportunity as it is also pointed out in previous works [11].",,,0,not_related
"Finally, DECAF [11], 45 is a casually-aware method which is proposed to create fair data for a few notions which does not 46 include CF.",,,1,related
Using the technique proposed in [11][Section 5.,,,0,not_related
"The most relevant works to the problem we consider here are [10, 11, 12].",,,1,related
"49 We also review the existing definitions in previous work [10, 11, 12] and compare them with our 50 definition.",,,1,related
"Our definition here is in line with the definition of fairness for SDG in FairGAN [10] and [12], while 103 it is different from the definition proposed in DECAF [11].",,,1,related
"Causally-aware Generator Besides Causal-TGAN, we explicitly leave out CGNN (Goudet et al., 2018), CausalGAN (Kocaoglu et al., 2017) and DECAF (van Breugel et al., 2021), which incorporate similar causally-aware generator.",,,1,related
